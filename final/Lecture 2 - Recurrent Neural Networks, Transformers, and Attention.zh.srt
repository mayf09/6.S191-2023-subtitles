1
00:00:09,550 --> 00:00:10,335
大家好，

2
00:00:10,335 --> 00:00:13,100
我希望你们喜欢 Alexander 的第一节课。

3
00:00:13,690 --> 00:00:14,355
我是 Ava ，

4
00:00:14,355 --> 00:00:16,730
在第二节课中，

5
00:00:17,020 --> 00:00:20,030
我们将关注序列建模的问题，

6
00:00:20,140 --> 00:00:24,380
我们如何构建神经网络来处理和学习序列数据。

7
00:00:25,370 --> 00:00:26,790
在 Alexander 的第一节课中，

8
00:00:27,260 --> 00:00:29,850
他介绍了神经网络的基本原理，

9
00:00:30,260 --> 00:00:33,180
从感知器，建立前馈模型开始，

10
00:00:33,320 --> 00:00:35,280
以及如何训练这些模型，

11
00:00:35,480 --> 00:00:37,620
并开始考虑部署它们。

12
00:00:38,450 --> 00:00:40,795
现在我们将把注意力转向

13
00:00:40,795 --> 00:00:44,790
涉及数据顺序处理的特定类型的问题，

14
00:00:45,410 --> 00:00:47,940
我们意识到为什么这些类型的问题，

15
00:00:48,440 --> 00:00:51,690
需要一种不同的方式来实现和构建神经网络，

16
00:00:52,580 --> 00:00:54,540
比起我们到目前为止所看到的。

17
00:00:55,370 --> 00:00:57,870
我认为，这节课的一些内容

18
00:00:57,950 --> 00:01:00,840
一开始可能会让人感到困惑或畏惧，

19
00:01:01,100 --> 00:01:02,770
但我真的想要做的是，

20
00:01:02,770 --> 00:01:05,640
从基础开始建立这种理解，

21
00:01:06,080 --> 00:01:07,590
一步一步地走，

22
00:01:07,790 --> 00:01:14,610
一直发展直觉到理解这些网络如何运作背后的数学和操作。

23
00:01:15,500 --> 00:01:17,880
好的，我们开始吧，

24
00:01:20,090 --> 00:01:22,705
为了开始，

25
00:01:22,705 --> 00:01:23,995
我首先想要激发，

26
00:01:23,995 --> 00:01:28,860
我们谈论序列数据或序列建模的确切含义，

27
00:01:29,090 --> 00:01:31,950
所以我们将从一个非常简单、直观的例子开始，

28
00:01:32,660 --> 00:01:34,350
假设我们有一张球的图片，

29
00:01:34,700 --> 00:01:39,450
你的任务是预测这个球下一步会移动到哪里，

30
00:01:40,360 --> 00:01:42,800
现在，如果你没有前置信息，

31
00:01:42,820 --> 00:01:45,470
关于球的轨迹，它的运动，它的历史，

32
00:01:45,940 --> 00:01:52,010
任何关于它的下一个位置的猜测或预测都是随机的猜测，

33
00:01:53,210 --> 00:01:56,220
然而，如果除了球的当前位置之外，

34
00:01:56,240 --> 00:01:59,550
我还给了你一些关于球过去移动位置的信息，

35
00:02:00,200 --> 00:02:02,250
现在问题变得容易多了，

36
00:02:02,300 --> 00:02:04,470
我想希望我们都能同意，

37
00:02:04,790 --> 00:02:08,380
我们最有可能的下一个预测是，

38
00:02:08,380 --> 00:02:12,270
这个球将在下一帧向右移动。

39
00:02:13,470 --> 00:02:17,470
所以这是一个非常精简的，简单的，直观的例子，

40
00:02:18,060 --> 00:02:19,810
但事实是，除此之外，

41
00:02:19,950 --> 00:02:23,195
序列数据真的就在我们周围，

42
00:02:23,195 --> 00:02:24,040
就在我说话的时候，

43
00:02:24,510 --> 00:02:28,030
从我嘴里说出来的话形成了一系列声波，

44
00:02:28,170 --> 00:02:29,140
定义了音频，

45
00:02:29,250 --> 00:02:30,340
我们可以分开，

46
00:02:30,480 --> 00:02:32,380
以这种顺序的方式来思考，

47
00:02:33,150 --> 00:02:40,180
类似地，文本语言可以被分成字符序列或单词序列，

48
00:02:41,180 --> 00:02:43,350
还有很多更多的例子，

49
00:02:43,400 --> 00:02:46,890
表明序列处理，序列数据是存在的，

50
00:02:47,210 --> 00:02:49,590
从医学信号，比如心电图，

51
00:02:50,060 --> 00:02:52,650
到金融市场和预测股票价格，

52
00:02:53,240 --> 00:02:55,710
到 DNA 编码的生物序列，

53
00:02:56,360 --> 00:02:57,775
到气候模式，

54
00:02:57,775 --> 00:02:59,940
到运动模式，等等。

55
00:03:00,820 --> 00:03:06,020
所以，希望你已经对这些类型的问题有一定的了解，

56
00:03:06,130 --> 00:03:08,360
以及它们在现实世界中的哪些方面相关。

57
00:03:10,190 --> 00:03:13,440
当我们考虑序列建模在现实世界中的应用时，

58
00:03:13,820 --> 00:03:17,275
我们可以考虑一些不同类型的问题定义，

59
00:03:17,275 --> 00:03:19,500
我们可以在我们的武器库拥有并使用这些定义。

60
00:03:20,120 --> 00:03:20,920
在第一节课中，

61
00:03:21,600 --> 00:03:26,020
Alexander 介绍了分类和回归的概念，

62
00:03:26,800 --> 00:03:29,640
在那里他谈到了，我们学习了前馈模型，

63
00:03:30,110 --> 00:03:33,450
它可以在这种固定和静态的设置下一对一地运行，

64
00:03:33,890 --> 00:03:36,720
给定单一输入，预测单一输出，

65
00:03:37,270 --> 00:03:38,940
那个二元分类的例子，

66
00:03:38,940 --> 00:03:41,660
你会不会成功通过这门课，

67
00:03:42,290 --> 00:03:45,690
在这里，没有顺序的概念，也没有时间的概念，

68
00:03:47,080 --> 00:03:50,150
现在，如果我们引入这种序列分量的概念，

69
00:03:50,560 --> 00:03:53,780
我们可以处理可能在时间上定义的输入，

70
00:03:54,280 --> 00:03:58,280
并潜在地产生顺序或时间输出。

71
00:03:58,840 --> 00:04:00,720
所以作为一个例子，

72
00:04:00,720 --> 00:04:02,630
我们可以考虑文本语言，

73
00:04:03,070 --> 00:04:05,480
也许我们想要生成一个预测，

74
00:04:05,770 --> 00:04:07,040
给出一个文本序列，

75
00:04:07,600 --> 00:04:12,650
分类消息是积极的情绪还是负面的情绪。

76
00:04:13,700 --> 00:04:16,170
相反，我们可以有一个输入，

77
00:04:16,370 --> 00:04:17,190
比方说一幅图像，

78
00:04:17,780 --> 00:04:23,520
现在我们的目标可能是生成该图像的文本或顺序描述，

79
00:04:23,660 --> 00:04:26,490
给出这张棒球运动员投球的图片，

80
00:04:26,870 --> 00:04:30,870
我们能建立一个神经网络来生成语言字幕吗。

81
00:04:32,100 --> 00:04:37,960
最后，我们还可以考虑有序列输入、序列输出的应用程序和问题，

82
00:04:38,490 --> 00:04:41,320
例如，如果我们想要在两种语言之间进行翻译，

83
00:04:41,880 --> 00:04:45,920
这种类型的想法和这种类型的架构是，

84
00:04:45,920 --> 00:04:49,490
推动了你的手机中的机器翻译任务，

85
00:04:49,490 --> 00:04:53,110
在谷歌翻译和许多其他例子中。

86
00:04:54,450 --> 00:04:58,930
希望这能让你了解序列数据是什么样子的，

87
00:04:59,100 --> 00:05:01,540
这些类型的问题定义可能是什么样子的，

88
00:05:01,980 --> 00:05:05,230
从这一点出发，我们将开始并建立我们的理解，

89
00:05:05,490 --> 00:05:09,490
我们可以建立和训练什么样的神经网络来解决这些类型的问题。

90
00:05:11,340 --> 00:05:15,650
首先我们将从递归的概念开始，

91
00:05:15,650 --> 00:05:18,940
并从这个概念开始定义递归神经网络，

92
00:05:19,320 --> 00:05:20,890
在讲座的最后部分，

93
00:05:21,210 --> 00:05:26,060
我们将讨论底层机制，底层转换器架构，

94
00:05:26,060 --> 00:05:29,950
这些在处理序列数据方面非常、非常、非常强大。

95
00:05:30,680 --> 00:05:32,370
但正如我在开始时所说的，

96
00:05:32,390 --> 00:05:36,630
这节课的主题是逐步建立这种理解，

97
00:05:36,710 --> 00:05:38,790
从基础知识和直觉开始。

98
00:05:39,710 --> 00:05:41,370
要做到这一点，我们回到过去，

99
00:05:41,570 --> 00:05:44,370
回顾感知器，并从那里继续前进，

100
00:05:45,600 --> 00:05:47,680
所以就像 Alexander 介绍的，

101
00:05:48,120 --> 00:05:50,980
我们在第一课中研究了感知器，

102
00:05:51,810 --> 00:05:55,510
感知器是由这个单一的神经操作定义的，

103
00:05:56,010 --> 00:05:57,850
我们有一些输入，

104
00:05:58,080 --> 00:05:59,860
比如 x1 到 xm ，

105
00:06:00,420 --> 00:06:04,090
这些数字中的每一个都乘以相应的权重，

106
00:06:04,650 --> 00:06:07,210
通过一个非线性激活函数，

107
00:06:07,650 --> 00:06:10,480
然后产生一个预测的输出 ŷ 。

108
00:06:11,320 --> 00:06:15,230
在这里，我们可以有多个输入来生成我们的输出，

109
00:06:15,670 --> 00:06:21,500
但这些输入仍然不被认为是序列中的点或序列中的时间步长，

110
00:06:22,340 --> 00:06:24,250
即使我们扩展这个感知器，

111
00:06:24,250 --> 00:06:29,460
并开始将多个感知器堆叠在一起来定义这些前馈神经网络，

112
00:06:30,050 --> 00:06:34,680
我们仍然没有这种时间处理或序列信息的概念。

113
00:06:35,380 --> 00:06:39,860
即使我们能够翻译和转换多个输入，

114
00:06:40,150 --> 00:06:42,980
应用这些方式操作，应用这种非线性，

115
00:06:43,000 --> 00:06:46,010
然后定义多个预测输出。

116
00:06:47,660 --> 00:06:49,650
看看这张图，

117
00:06:50,000 --> 00:06:51,910
左边是蓝色，是输入，

118
00:06:51,910 --> 00:06:54,390
右边是紫色，是输出，

119
00:06:54,410 --> 00:06:57,720
绿色定义了单一神经网络层，

120
00:06:57,860 --> 00:07:00,210
将这些输入转换为输出。

121
00:07:01,070 --> 00:07:03,520
下一步，我将简化这个图表，

122
00:07:03,520 --> 00:07:07,350
我将把这些神经元叠加在一起，

123
00:07:07,850 --> 00:07:10,560
然后用这个绿色的方块来描述，

124
00:07:11,120 --> 00:07:13,800
仍然是同样的操作，

125
00:07:14,000 --> 00:07:18,720
我们有一个输入向量被变换来预测这个输出向量。

126
00:07:20,210 --> 00:07:23,520
现在，我在这里介绍的，你们可能注意到了，

127
00:07:23,930 --> 00:07:25,680
是这个新变量 t ，

128
00:07:26,600 --> 00:07:29,040
我用它来表示单个时间步长，

129
00:07:29,830 --> 00:07:32,390
我们正在考虑单个时间步长的输入，

130
00:07:32,500 --> 00:07:37,670
并使用我们的神经网络生成与该输入对应的单个输出，

131
00:07:38,590 --> 00:07:41,415
我们如何开始扩展和建立它，

132
00:07:41,415 --> 00:07:43,700
现在考虑多个时间步长，

133
00:07:43,900 --> 00:07:47,330
以及我们如何处理一系列信息。

134
00:07:48,610 --> 00:07:50,720
那么，如果我们用这张图，

135
00:07:50,800 --> 00:07:53,600
我所做的就是把它旋转了 90 度，

136
00:07:54,680 --> 00:07:56,400
我们仍然有这个输入向量，

137
00:07:56,900 --> 00:07:59,820
并在产生一个输出向量，

138
00:08:00,350 --> 00:08:02,640
如果我们可以复制这个网络，

139
00:08:03,380 --> 00:08:06,270
并多次执行这个操作，

140
00:08:06,590 --> 00:08:11,700
来尝试处理对应于不同时间馈入的输入。

141
00:08:12,260 --> 00:08:15,810
我们有一个单独的时间步长，从 t0 开始，

142
00:08:16,460 --> 00:08:20,130
我们可以做同样的事情，对下一个时间步长做同样的操作，

143
00:08:20,660 --> 00:08:23,670
再次将其视为一个孤立的实例，

144
00:08:24,320 --> 00:08:26,850
并不断地重复这样做，

145
00:08:27,510 --> 00:08:29,000
你会注意到，

146
00:08:29,000 --> 00:08:31,510
所有这些模型都是彼此的简单副本，

147
00:08:31,740 --> 00:08:34,990
只是在每个不同的时间步长有不同的输入，

148
00:08:36,270 --> 00:08:37,900
我们使这一点变得具体起来，

149
00:08:38,010 --> 00:08:41,140
可以根据这种功能转变所做的事情，

150
00:08:41,890 --> 00:08:45,770
在特定时间步长的预测输出 ŷt ，

151
00:08:46,150 --> 00:08:50,750
是那个输入时间步长 xt 的函数，

152
00:08:51,240 --> 00:08:55,520
而这个函数是由我们的神经网络权重学习和定义的。

153
00:08:57,070 --> 00:08:58,605
好的，我已经告诉你们了，

154
00:08:58,605 --> 00:09:02,840
我们的目标是试图理解序列数据，进行序列建模，

155
00:09:03,400 --> 00:09:07,950
但是，这个图表显示的内容和我在这里显示的内容会有什么问题？

156
00:09:09,440 --> 00:09:10,950
好的，是的，继续。

157
00:09:15,350 --> 00:09:16,750
没错，这是完全正确的，

158
00:09:16,750 --> 00:09:18,630
所以学生的回答是，

159
00:09:18,710 --> 00:09:21,330
x1 可能与 x 无关，

160
00:09:21,530 --> 00:09:23,380
你有这种时间依赖性，

161
00:09:23,380 --> 00:09:26,670
但这些孤立的副本根本无法捕捉到这一点，

162
00:09:27,050 --> 00:09:31,590
这就完美地回答了这个问题。

163
00:09:32,890 --> 00:09:35,540
这里，在较后时间步长的预测输出，

164
00:09:35,800 --> 00:09:39,950
可以精确地依赖于在前时间步长的输入，

165
00:09:40,180 --> 00:09:43,850
如果这确实是具有这种时间依赖性的序列问题。

166
00:09:45,100 --> 00:09:47,460
那么，我们如何开始对此进行推理，

167
00:09:47,460 --> 00:09:48,950
我们如何定义一种关系，

168
00:09:49,450 --> 00:09:53,810
将网络在特定时间步长的计算

169
00:09:54,220 --> 00:09:57,920
与先前时间步长的先前历史和记忆联系起来的关系。

170
00:09:58,960 --> 00:10:01,880
好的，如果我们真的这么做了，

171
00:10:01,900 --> 00:10:09,950
如果我们简单地将计算和网络理解的信息链接到其他副本，

172
00:10:10,300 --> 00:10:12,530
通过我们所说的递归关系，

173
00:10:13,660 --> 00:10:14,835
这意味着，

174
00:10:14,835 --> 00:10:17,990
关于网络在特定时间计算的内容的某些信息

175
00:10:18,160 --> 00:10:20,870
会传递给后面的时间步长，

176
00:10:21,520 --> 00:10:24,600
我们根据这个变量 h 来定义它，

177
00:10:25,220 --> 00:10:27,160
我们称之为内部状态，

178
00:10:27,160 --> 00:10:29,190
或者你可以认为它是一个记忆项，

179
00:10:29,660 --> 00:10:31,950
由神经元和网络维持的，

180
00:10:32,540 --> 00:10:36,240
这种状态被一步一步地传递，

181
00:10:36,650 --> 00:10:40,500
当我们读入并处理这些序列信息时，

182
00:10:42,480 --> 00:10:47,020
这意味着网络输出它的预测和计算，

183
00:10:47,820 --> 00:10:51,050
不仅是输入数据 x 的函数，

184
00:10:51,730 --> 00:10:54,470
我们还有另一个变量 h ，

185
00:10:54,940 --> 00:10:59,270
它捕获了状态，捕获了记忆，

186
00:10:59,560 --> 00:11:03,440
由网络计算并随时间传递的。

187
00:11:04,660 --> 00:11:06,440
具体来说，

188
00:11:06,730 --> 00:11:11,600
我们预测的 ŷt 不仅取决于一次的输入，

189
00:11:12,040 --> 00:11:14,660
还取决于这个过去的记忆，这个过去的状态，

190
00:11:15,770 --> 00:11:20,820
正是这种时间依赖性和重复性的联系，

191
00:11:20,900 --> 00:11:23,850
定义了重复性神经单位的概念。

192
00:11:24,840 --> 00:11:25,840
我所展示的是，

193
00:11:25,890 --> 00:11:28,840
这种随着时间的推移而展开的联系，

194
00:11:29,370 --> 00:11:34,150
但我们也可以根据一个循环来描述这种关系，

195
00:11:34,910 --> 00:11:38,430
对 t 的这个内部状态变量 h 的计算

196
00:11:38,630 --> 00:11:40,830
随着时间的推移被迭代更新，

197
00:11:41,180 --> 00:11:44,100
然后返回到神经元中，

198
00:11:44,150 --> 00:11:47,370
这个递归关系中的神经元计算，

199
00:11:49,070 --> 00:11:51,780
这就是我们如何定义这些递归[细胞]，

200
00:11:51,950 --> 00:11:56,010
组成递归神经网络或 RNN ，

201
00:11:56,600 --> 00:12:00,780
这里的关键是我们有这个递归关系，

202
00:12:01,220 --> 00:12:04,680
捕获了循环时间依赖。

203
00:12:06,150 --> 00:12:08,195
事实上，这个想法正是，

204
00:12:08,195 --> 00:12:12,700
递归神经网络或 RNN 背后的基础。

205
00:12:13,360 --> 00:12:16,110
所以，让我们从这里继续建立我们的理解，

206
00:12:16,340 --> 00:12:22,830
并前进到我们可以如何在数学上和在代码中实际定义 RNN 操作。

207
00:12:23,720 --> 00:12:26,940
所以我们要做的就是让这种关系更正式一点，

208
00:12:27,890 --> 00:12:29,770
这里的关键思想是，

209
00:12:29,770 --> 00:12:31,530
RNN 保持状态，

210
00:12:31,670 --> 00:12:34,320
它在每个时间步长更新状态，

211
00:12:34,610 --> 00:12:37,710
随着序列的处理。

212
00:12:38,410 --> 00:12:40,910
我们通过应用这个递归关系来定义它，

213
00:12:41,680 --> 00:12:44,175
递归关系捕捉到的是，

214
00:12:44,175 --> 00:12:47,720
我们是如何更新内部状态 ht 的，

215
00:12:48,560 --> 00:12:56,280
具体地说，状态更新与我们到目前为止介绍的任何其他神经网络操作完全一样，

216
00:12:56,690 --> 00:13:01,080
我们再次学习由一组权重 w 定义的函数，

217
00:13:01,840 --> 00:13:05,600
我们使用该函数来更新 ht 的单元状态，

218
00:13:06,370 --> 00:13:09,930
另外一个因素，这里的新奇之处在于，

219
00:13:09,930 --> 00:13:15,530
函数依赖于输入和先前的时间步长 ht-1

220
00:13:16,890 --> 00:13:18,430
你所知道的是，

221
00:13:18,660 --> 00:13:22,985
这个函数 fw 是由一组权重定义的，

222
00:13:22,985 --> 00:13:25,780
它是相同的一组权重，同样的一组参数，

223
00:13:26,040 --> 00:13:28,510
随着时间的推移，

224
00:13:28,650 --> 00:13:34,330
递归神经网络处理这些时间信息，这些序列数据。

225
00:13:36,000 --> 00:13:40,505
好的，所以这里的关键思想是，

226
00:13:40,505 --> 00:13:44,200
这个 RNN 状态更新操作接受这个状态，

227
00:13:44,370 --> 00:13:47,950
并在每次处理序列时更新它。

228
00:13:49,320 --> 00:13:51,370
我们还可以将其转换为，

229
00:13:51,690 --> 00:13:56,410
如何考虑在 Python 代码中实现 RNN ，

230
00:13:57,000 --> 00:13:58,250
或者更确切地说伪代码，

231
00:13:58,540 --> 00:14:02,720
有望更好地理解和直觉这些网络是如何工作的。

232
00:14:03,370 --> 00:14:04,695
所以我们现在要做的是，

233
00:14:04,695 --> 00:14:07,010
我们从定义 RNN 开始，

234
00:14:07,840 --> 00:14:09,470
现在这是抽象的 RNN ，

235
00:14:10,230 --> 00:14:12,940
我们初始化它的隐藏状态，

236
00:14:13,290 --> 00:14:14,980
我们有一些句子，

237
00:14:15,060 --> 00:14:16,870
假设这是我们感兴趣的输入，

238
00:14:17,130 --> 00:14:21,640
我们感兴趣的是预测这个句子中可能出现的下一个词，

239
00:14:22,580 --> 00:14:26,880
我们可以做的是循环遍历句子中的单词，

240
00:14:26,960 --> 00:14:29,190
定义我们时间输入，

241
00:14:29,660 --> 00:14:31,860
在每一步，当我们循环遍历时，

242
00:14:32,210 --> 00:14:36,510
句子中的每个单词都被送到 RNN 模型中，

243
00:14:37,260 --> 00:14:39,460
以及先前的隐藏状态，

244
00:14:40,200 --> 00:14:43,420
这就是为下一个单词生成预测，

245
00:14:43,620 --> 00:14:46,390
并依次更新 RNN 状态。

246
00:14:47,300 --> 00:14:50,250
最后，我们对句子中最后一个单词的预测，

247
00:14:50,600 --> 00:14:51,630
我们遗漏的单词，

248
00:14:52,010 --> 00:14:53,890
只是 RNN 的输出，

249
00:14:53,890 --> 00:14:57,240
在所有先前的单词通过模型输入后。

250
00:14:59,120 --> 00:15:02,220
所以，这是分解了 RNN 是如何工作的，

251
00:15:02,330 --> 00:15:04,470
它是如何处理序列信息的。

252
00:15:05,330 --> 00:15:07,050
你已经注意到的是，

253
00:15:07,400 --> 00:15:11,250
RNN 计算既包括对隐藏状态的更新，

254
00:15:11,420 --> 00:15:14,190
也包括在结束时生成一些预测输出，

255
00:15:14,360 --> 00:15:16,830
这是我们感兴趣的终极目标。

256
00:15:17,520 --> 00:15:19,330
让我们来了解一下，

257
00:15:19,530 --> 00:15:22,150
我们是如何生成产出预测本身的，

258
00:15:23,340 --> 00:15:26,560
RNN 计算得到一些输入向量，

259
00:15:27,740 --> 00:15:30,300
然后，它对隐藏状态执行这个更新，

260
00:15:31,650 --> 00:15:36,220
这种对隐藏状态的更新只是一个标准的神经网络操作，

261
00:15:36,630 --> 00:15:38,170
就像我们在第一节课中看到的，

262
00:15:38,700 --> 00:15:45,280
它包括取权重矩阵乘以之前的隐藏状态，

263
00:15:45,950 --> 00:15:49,680
取另一个权重矩阵将其乘以时间步长的输入，

264
00:15:49,940 --> 00:15:51,960
并应用非线性，

265
00:15:52,560 --> 00:15:53,470
在这种情况下，

266
00:15:53,880 --> 00:15:56,200
因为我们有这两个输入流，

267
00:15:56,820 --> 00:16:00,280
输入数据 xt 和先前的状态 h ，

268
00:16:00,690 --> 00:16:02,980
我们有两个单独的权重矩阵，

269
00:16:03,210 --> 00:16:05,920
网络在其训练过程中学习，

270
00:16:06,740 --> 00:16:07,920
这些结合在一起，

271
00:16:08,180 --> 00:16:09,840
我们应用非线性，

272
00:16:10,190 --> 00:16:13,915
然后我们可以在给定的时间步长生成输出，

273
00:16:13,915 --> 00:16:16,590
通过修改隐藏状态，

274
00:16:17,710 --> 00:16:20,900
使用单独的权重矩阵更新这个值，

275
00:16:20,950 --> 00:16:23,120
然后生成预测输出。

276
00:16:24,420 --> 00:16:26,480
这就是问题所在，

277
00:16:26,710 --> 00:16:29,510
这就是 RNN 在其单个操作中

278
00:16:30,700 --> 00:16:34,850
更新隐藏状态并生成预测输出的方式。

279
00:16:36,300 --> 00:16:44,080
好的，现在这为你提供了 RNN 计算如何在特定时间步长发生的内部工作，

280
00:16:44,540 --> 00:16:47,260
接下来，让我们考虑一下随着时间的推移这是什么样子，

281
00:16:47,670 --> 00:16:55,780
并将 RNN 的计算图定义为在时间上展开或扩展。

282
00:16:56,590 --> 00:16:59,835
所以，到目前为止，我所展示的 RNN 的主要方式是，

283
00:16:59,835 --> 00:17:03,440
根据这个左边这个循环的图，

284
00:17:03,760 --> 00:17:05,120
自己反馈，

285
00:17:05,970 --> 00:17:08,920
我们可以想象和思考 RNN 的另一种方式是，

286
00:17:09,150 --> 00:17:15,340
随着时间的推移，在各个时间步长上展开这种递归，

287
00:17:16,730 --> 00:17:17,935
这意味着，

288
00:17:17,935 --> 00:17:21,120
我们可以在第一时间步长时获取网络，

289
00:17:21,650 --> 00:17:25,350
并继续迭代地在时间步长上展开它，

290
00:17:26,390 --> 00:17:31,740
向前，直到我们处理输入中的所有时间步长。

291
00:17:32,620 --> 00:17:35,600
现在，我们可以进一步形式话这个图，

292
00:17:35,950 --> 00:17:42,770
通过定义将输入连接到隐藏状态更新的权重矩阵，

293
00:17:43,410 --> 00:17:49,480
以及用于随时间更新内部状态的权重矩阵，

294
00:17:50,010 --> 00:17:55,600
以及最后定义更新到生成预测输出的权重矩阵。

295
00:17:56,840 --> 00:17:58,170
现在回想一下，

296
00:17:58,460 --> 00:18:00,000
在所有这些情况下，

297
00:18:00,200 --> 00:18:03,810
对于所有这三个权重矩阵加上所有这些时间步长，

298
00:18:03,950 --> 00:18:07,470
我们只是重复使用相同的权重矩阵，

299
00:18:07,670 --> 00:18:10,650
所以这是一组参数，一组权重矩阵，

300
00:18:10,730 --> 00:18:13,380
只是按顺序处理这些信息。

301
00:18:14,950 --> 00:18:15,920
现在你可能在想，

302
00:18:16,030 --> 00:18:19,070
好的，那么我们如何开始思考，

303
00:18:19,240 --> 00:18:20,960
如何训练 RNN ，

304
00:18:21,010 --> 00:18:22,130
如何定义损失。

305
00:18:22,960 --> 00:18:26,550
假设我们在这个时间依赖中有这个时间处理，

306
00:18:27,260 --> 00:18:35,490
好的，在单个时间点的预测将简单地等同于在该特定时间点的计算损失，

307
00:18:36,110 --> 00:18:40,980
所以，现在我们可以将这些预测逐一与真实标签进行比较，

308
00:18:41,450 --> 00:18:44,370
并生成这些时间步长的损失值。

309
00:18:45,040 --> 00:18:47,220
最后，我们可以获得全部损失，

310
00:18:47,540 --> 00:18:51,870
通过将所有这些单独的损失项加在一起，

311
00:18:52,500 --> 00:18:56,380
定义对 RNN 的特定输入的总损失。

312
00:18:58,050 --> 00:18:59,840
如果我们可以演示，

313
00:18:59,840 --> 00:19:03,460
如何从头开始在 TensorFlow 中实现这个 RNN ，

314
00:19:04,410 --> 00:19:09,350
RNN 可以定义为层操作和层类，

315
00:19:09,350 --> 00:19:11,440
像 Alexander 在上节课介绍的。

316
00:19:12,240 --> 00:19:19,400
所以我们可以根据权重矩阵的初始化，隐藏状态的初始化来定义它，

317
00:19:19,750 --> 00:19:23,840
通常相当于将这两个初始化为零。

318
00:19:25,380 --> 00:19:30,070
接下来，我们可以定义如何通过 RNN 网络

319
00:19:30,660 --> 00:19:32,710
以处理给定的输入 X ，

320
00:19:33,500 --> 00:19:34,640
你们会注意到，

321
00:19:34,640 --> 00:19:36,460
在这个正向运算中，

322
00:19:36,990 --> 00:19:39,820
计算和我们刚刚看过的一样，

323
00:19:40,340 --> 00:19:42,040
我们首先更新隐藏状态，

324
00:19:42,840 --> 00:19:45,190
根据前面介绍的方程，

325
00:19:46,060 --> 00:19:48,565
然后生成预测输出，

326
00:19:48,565 --> 00:19:51,390
是隐藏状态的变换版本，

327
00:19:52,540 --> 00:19:54,145
最后，在每个时间步长，

328
00:19:54,145 --> 00:19:58,380
我们返回输出和更新的隐藏状态，

329
00:19:58,550 --> 00:20:04,170
因为这是随着时间的推移继续该 RNN 操作所必需的存储，

330
00:20:05,420 --> 00:20:06,900
非常方便的是，

331
00:20:07,280 --> 00:20:11,730
尽管你可以完全从头开始定义 RNN 网络和 RNN 层，

332
00:20:11,990 --> 00:20:15,570
但 TensorFlow 为你抽象了这一操作，

333
00:20:15,800 --> 00:20:18,480
所以你可以简单地定义一个简单的 RNN ，

334
00:20:18,710 --> 00:20:22,260
根据你在这里看到的这个调用，

335
00:20:22,880 --> 00:20:28,110
这使得所有的计算都非常有效，非常容易，

336
00:20:28,850 --> 00:20:34,410
你将在今天的软件实验中练习实现和使用 RNN 。

337
00:20:36,290 --> 00:20:39,840
好的，这给了我们对 RNN 的理解，

338
00:20:39,950 --> 00:20:46,020
回到我所描述的那种问题设置或问题定义，

339
00:20:46,310 --> 00:20:47,580
在这节课开始时（说的）。

340
00:20:48,340 --> 00:20:49,545
我只想提醒你们，

341
00:20:49,545 --> 00:20:54,740
我们可以应用 RNN 解决序列建模问题，

342
00:20:55,390 --> 00:20:57,980
我们可以考虑取一个输入序列，

343
00:20:58,210 --> 00:21:01,280
在序列的末尾产生一个预测输出，

344
00:21:02,100 --> 00:21:04,910
我们可以考虑采用静态的单一输入，

345
00:21:04,910 --> 00:21:10,180
并尝试根据该单一输入生成文本，

346
00:21:11,230 --> 00:21:14,540
最后，我们可以考虑获取一个输入序列，

347
00:21:15,010 --> 00:21:17,660
在这个序列中的每个时间步长产生一个预测，

348
00:21:18,460 --> 00:21:22,730
然后对这个序列进行预测和翻译。

349
00:21:25,730 --> 00:21:27,200
好的，所以，

350
00:21:28,450 --> 00:21:33,860
所以，这将是今天软件实验的基础，

351
00:21:34,360 --> 00:21:40,850
它将专注于多对多处理和多对多序列建模的问题，

352
00:21:40,930 --> 00:21:42,980
取一个序列，到一个序列。

353
00:21:44,640 --> 00:21:49,600
我们可能想要考虑的所有这些类型的问题和任务的共同之处和普遍之处是，

354
00:21:49,650 --> 00:21:53,030
我们考虑 RNN 的是，

355
00:21:53,030 --> 00:21:57,380
我想我们需要什么样的设计标准，

356
00:21:57,380 --> 00:22:02,530
来建立一个健壮而可靠的网络来处理这些序列建模问题。

357
00:22:03,470 --> 00:22:04,660
我的意思是，

358
00:22:04,660 --> 00:22:06,840
它们的特点是什么，

359
00:22:07,070 --> 00:22:11,070
RNN 需要满足哪些设计要求，

360
00:22:11,270 --> 00:22:14,610
为了能够有效地处理序列数据。

361
00:22:16,200 --> 00:22:20,110
第一个是序列可以有不同的长度，

362
00:22:20,610 --> 00:22:22,210
它们可能很短，也可能很长，

363
00:22:22,530 --> 00:22:25,630
我们希望我们的 RNN 模型通用，

364
00:22:25,770 --> 00:22:28,810
能够处理可变长度的序列。

365
00:22:29,880 --> 00:22:32,140
第二，也是非常重要的一点是，

366
00:22:32,490 --> 00:22:33,820
正如我们之前讨论的那样，

367
00:22:34,320 --> 00:22:37,835
思考序列长度的整个关键是，

368
00:22:37,835 --> 00:22:42,910
试图跟踪和了解数据中随着时间的推移的依赖关系，

369
00:22:43,510 --> 00:22:47,600
所以，我们的模型需要能够处理这些不同的依赖关系，

370
00:22:47,800 --> 00:22:51,530
这些依赖关系可能会在彼此相隔很远发生。

371
00:22:53,520 --> 00:22:56,555
下一个序列是关于顺序的，

372
00:22:56,555 --> 00:22:57,335
有一些概念，

373
00:22:57,335 --> 00:23:01,990
关于当前的输入依赖于先前的输入，

374
00:23:01,990 --> 00:23:04,740
我们看到的观察的特定顺序，

375
00:23:05,600 --> 00:23:10,200
对我们最终可能想要生成的预测有很大影响。

376
00:23:11,630 --> 00:23:17,280
最后，为了能够有效地处理这些信息，

377
00:23:17,720 --> 00:23:21,450
我们的网络需要能够进行我们所说的参数共享，

378
00:23:21,740 --> 00:23:24,210
这意味着给定一组权重，

379
00:23:24,350 --> 00:23:28,020
这组权重应该能够应用于序列中的不同时间步长，

380
00:23:28,370 --> 00:23:30,900
并仍然产生有意义的预测。

381
00:23:31,660 --> 00:23:33,770
所以，今天我们将关注，

382
00:23:34,000 --> 00:23:37,250
递归神经网络如何满足这些设计标准，

383
00:23:37,420 --> 00:23:42,650
以及这些设计标准如何激发对更强大的体系结构的需求，

384
00:23:42,700 --> 00:23:45,830
在序列建模方面可以超越 RNN 。

385
00:23:47,040 --> 00:23:50,320
因此，为了非常具体地理解这些标准，

386
00:23:50,520 --> 00:23:53,260
我们将考虑一个序列建模问题，

387
00:23:53,700 --> 00:23:55,600
在给定一系列单词的情况下，

388
00:23:56,130 --> 00:23:59,350
我们的任务只是预测该句子中的下一个单词，

389
00:24:00,280 --> 00:24:02,300
让我们假设我们有这句话，

390
00:24:02,530 --> 00:24:04,610
今天早上，我带着我的猫去散步，

391
00:24:05,580 --> 00:24:09,550
我们的任务是预测句子中的最后一个单词，

392
00:24:09,690 --> 00:24:10,840
给出之前的单词，

393
00:24:11,280 --> 00:24:14,020
今天早上，我带着我的猫，空格，

394
00:24:15,630 --> 00:24:18,605
我们的目标是用我们的 RNN 来定义它，

395
00:24:18,605 --> 00:24:21,820
在这项任务中对它进行测试。

396
00:24:22,860 --> 00:24:24,880
我们做这件事的第一步是什么，

397
00:24:25,990 --> 00:24:27,780
好的，非常重要的第一步，

398
00:24:27,780 --> 00:24:30,350
在我们考虑定义 RNN 之前，

399
00:24:30,700 --> 00:24:34,580
我们如何表示这些信息给网络，

400
00:24:34,870 --> 00:24:37,580
以一种它可以处理和理解的方式，

401
00:24:39,430 --> 00:24:40,400
如果我们有一个模型，

402
00:24:40,420 --> 00:24:44,390
它处理这些数据，处理这些基于文本的数据，

403
00:24:45,230 --> 00:24:47,730
并希望生成文本作为输出，

404
00:24:48,560 --> 00:24:50,640
我们的问题可能出现在，

405
00:24:51,020 --> 00:24:56,040
神经网络本身显然并不具备处理语言的能力，

406
00:24:56,960 --> 00:25:01,350
记住，神经网络仅仅是函数运算符。它们只是数学运算，

407
00:25:02,100 --> 00:25:03,960
所以我们不能期待它，

408
00:25:03,960 --> 00:25:08,780
它不能从头开始理解单词是什么，或者语言是什么意思，

409
00:25:09,580 --> 00:25:13,020
这意味着我们需要一种用数字表示语言的方法，

410
00:25:13,250 --> 00:25:17,040
这样它才能被传递到网络中进行处理。

411
00:25:19,160 --> 00:25:20,860
所以我们要做的是，

412
00:25:20,860 --> 00:25:21,940
我们需要定义一种方法，

413
00:25:21,940 --> 00:25:30,330
来将文本、语言信息转换成数字编码，一个向量，一个数字数组，

414
00:25:30,440 --> 00:25:33,420
可以输入到我们的神经网络中，

415
00:25:33,590 --> 00:25:37,350
并生成一个数字向量作为它的输出。

416
00:25:39,520 --> 00:25:41,430
因此，现在这就提出了一个问题，

417
00:25:41,430 --> 00:25:43,610
我们如何定义这种转变，

418
00:25:44,260 --> 00:25:47,870
我们如何才能将语言转换成这种数字编码，

419
00:25:48,520 --> 00:25:52,530
关键的解决方案，也是许多这样的网络工作的关键方式，

420
00:25:52,820 --> 00:25:55,110
就是这种嵌入的概念和概念，

421
00:25:55,700 --> 00:25:58,440
这意味着这需要一些转变，

422
00:25:59,300 --> 00:26:05,300
使用索引或可表示为索引的东西，

423
00:26:05,740 --> 00:26:08,810
到给定大小的数值向量。

424
00:26:10,070 --> 00:26:13,560
所以，如果我们考虑一下嵌入的想法是如何对语言数据起作用的，

425
00:26:14,380 --> 00:26:18,720
让我们来考虑一个我们的语言中可能存在的词汇，

426
00:26:19,640 --> 00:26:24,600
我们的目标是能够将我们词汇表中的这些单独的单词

427
00:26:25,130 --> 00:26:27,930
映射到固定大小的数字向量，

428
00:26:29,400 --> 00:26:30,695
我们可以做到这一点的一种方法是，

429
00:26:30,695 --> 00:26:34,720
定义这个词汇表中可能出现的所有单词，

430
00:26:35,400 --> 00:26:36,730
然后对它们进行索引，

431
00:26:36,900 --> 00:26:40,420
为每个不同的单词分配一个索引标签，

432
00:26:41,330 --> 00:26:46,440
a 对应于索引 1 ， cat 对应于索引 2 ，以此类推，

433
00:26:47,020 --> 00:26:52,010
这个索引将这些单词映射到数字，唯一的索引，

434
00:26:53,060 --> 00:26:57,480
然后，这些所以可以定义的是我们所说的嵌入向量，

435
00:26:58,340 --> 00:27:00,275
这是一种固定长度的编码，

436
00:27:00,275 --> 00:27:03,340
当我们观察到该单词时，

437
00:27:04,140 --> 00:27:07,810
我们只需在该单词的索引处表示一个值，

438
00:27:08,550 --> 00:27:10,780
这就是所谓的 one-hot 嵌入，

439
00:27:11,010 --> 00:27:15,280
我们有一个固定长度的词汇量向量，

440
00:27:15,900 --> 00:27:20,600
词汇表的每个实例都对应着 one-hot 中的一个，

441
00:27:21,320 --> 00:27:22,960
在相应的索引处。

442
00:27:24,890 --> 00:27:27,120
这是一种非常简单的方法，

443
00:27:27,500 --> 00:27:32,850
它只是基于纯粹的对计数索引进行计数，

444
00:27:33,260 --> 00:27:35,850
这里没有语义信息的概念，

445
00:27:36,230 --> 00:27:39,600
这意味着在这种基于矢量的编码中捕捉到了语义信息。

446
00:27:40,460 --> 00:27:43,090
或者，通常更常见的做法是，

447
00:27:43,090 --> 00:27:47,310
使用神经网络来学习编码，来学习嵌入，

448
00:27:47,690 --> 00:27:49,590
这里的目标是，我们可以学习，

449
00:27:49,820 --> 00:27:55,710
神经网络捕获我们输入数据中的一些内在含义或内在语义，

450
00:27:56,030 --> 00:28:01,570
并映射相关的单词或相关的输入在这个嵌入空间中距离更近，

451
00:28:01,950 --> 00:28:06,640
这意味着它们的数值向量彼此更相似。

452
00:28:07,780 --> 00:28:09,680
这个概念非常重要，

453
00:28:09,730 --> 00:28:16,760
关于这些序列建模网络的工作方式以及神经网络的一般工作方式。

454
00:28:17,860 --> 00:28:19,455
好的，有了这些，

455
00:28:19,455 --> 00:28:21,500
我们可以回到我们的设计标准了，

456
00:28:22,100 --> 00:28:24,070
考虑到我们所需要的功能，

457
00:28:24,570 --> 00:28:28,300
首先我们需要能够处理可变长度的序列。

458
00:28:29,090 --> 00:28:31,680
如果我们再次想要预测序列中的下一个单词，

459
00:28:31,730 --> 00:28:34,320
我们可以有短序列，我们可以有长序列，

460
00:28:34,370 --> 00:28:35,790
我们可以有更长的句子，

461
00:28:36,720 --> 00:28:42,560
我们的关键任务是希望能够跟踪所有这些不同长度的依赖关系。

462
00:28:43,270 --> 00:28:46,275
我们需要的，我们所说的依赖关系是指，

463
00:28:46,275 --> 00:28:49,520
在一个序列中，可能会有非常非常早的信息，

464
00:28:50,350 --> 00:28:56,000
但这些信息可能不相关，或者出现得很晚，直到序列中很晚的时候，

465
00:28:56,470 --> 00:28:59,070
我们需要能够跟踪这些依赖关系，

466
00:28:59,070 --> 00:29:01,400
并在我们的网络中维护这些信息。

467
00:29:03,470 --> 00:29:07,740
依赖与顺序相关和序列是由顺序定义的，

468
00:29:08,210 --> 00:29:13,770
我们知道不同顺序的相同单词有完全不同的含义，

469
00:29:14,240 --> 00:29:20,280
因此，我们的模型需要能够处理这些顺序的差异和长度的差异，

470
00:29:20,480 --> 00:29:24,450
它们可能导致不同预测产出。

471
00:29:25,860 --> 00:29:30,100
好的，希望通过文本的例子

472
00:29:30,990 --> 00:29:36,160
激励我们如何将输入数据转换成数字编码，

473
00:29:36,180 --> 00:29:38,320
可以传递给 RNN ，

474
00:29:38,580 --> 00:29:44,830
以及我们在处理这些类型的问题时希望满足的关键标准是什么。

475
00:29:47,030 --> 00:29:49,855
到目前为止，我们已经描绘了 RNN 的图片，

476
00:29:49,855 --> 00:29:52,680
它是如何工作的，直觉它们的数学运算，

477
00:29:53,390 --> 00:29:56,790
以及它们需要满足的关键标准是什么。

478
00:29:57,370 --> 00:29:58,755
最后一个问题是，

479
00:29:58,755 --> 00:30:02,300
我们如何实际训练和学习 RNN 的权重。

480
00:30:02,880 --> 00:30:05,600
这是通过反向传播算法完成的，

481
00:30:05,650 --> 00:30:09,020
只是处理序列信息，

482
00:30:10,930 --> 00:30:15,470
如果我们回头想想我们是如何训练前馈神经网络模型的，

483
00:30:16,300 --> 00:30:19,190
这些步骤在思考的过程中会被分解，

484
00:30:19,450 --> 00:30:20,600
从一个输入开始，

485
00:30:21,070 --> 00:30:22,845
我们首先从这个输入开始，

486
00:30:22,845 --> 00:30:24,800
然后向前穿过网络，

487
00:30:25,300 --> 00:30:27,230
从输入到输出。

488
00:30:28,060 --> 00:30:31,425
Alexander 介绍的反向传播的关键是

489
00:30:31,425 --> 00:30:36,500
将预测和反向传播梯度通过网络反向传播的想法，

490
00:30:37,330 --> 00:30:44,900
然后使用该操作来定义和更新关于网络中的每个参数的损耗，

491
00:30:45,250 --> 00:30:49,280
以便逐渐调整网络的参数和权重，

492
00:30:49,510 --> 00:30:51,650
以便最小化总体损耗。

493
00:30:53,180 --> 00:30:55,680
现在，随着 RNN ，正如我们前面介绍的那样，

494
00:30:55,940 --> 00:30:57,960
我们有了这个时间展开，

495
00:30:58,160 --> 00:31:02,820
这意味着我们在序列中的各个步骤中有这些单独的损失，

496
00:31:03,200 --> 00:31:06,180
这些损失加在一起构成了总体损失。

497
00:31:07,710 --> 00:31:08,870
这意味着，

498
00:31:08,870 --> 00:31:10,180
当我们进行反向传播时，

499
00:31:11,300 --> 00:31:16,500
我们现在必须，不是通过单一的网络反向传播错误，

500
00:31:17,400 --> 00:31:20,960
而是通过这些单独的时间步长向后传播损失，

501
00:31:21,760 --> 00:31:25,760
在我们通过每个单独的时间步长反向传播损耗之后，

502
00:31:26,260 --> 00:31:29,240
我们然后在所有的时间步长上这样做，

503
00:31:29,680 --> 00:31:34,520
从我们当前的时间 t 回到序列的开始。

504
00:31:35,740 --> 00:31:41,280
这就是为什么这个算法被称为时间反向传播的原因，

505
00:31:41,600 --> 00:31:42,750
因为正如你所看到的，

506
00:31:42,980 --> 00:31:47,910
数据、预测和由此产生的误差反馈到，

507
00:31:48,080 --> 00:31:52,560
从我们目前所处的位置一直反馈到输入数据序列的最开始，

508
00:31:55,290 --> 00:32:02,170
穿越时间的反向传播是一个在实践中很难实现的算法，

509
00:32:02,960 --> 00:32:04,100
这样的原因是，

510
00:32:04,100 --> 00:32:05,230
如果我们仔细观察，

511
00:32:05,610 --> 00:32:08,350
梯度是如何在 RNN 上流动的，

512
00:32:08,910 --> 00:32:16,060
这个算法涉及到的是这些权重矩阵多次重复计算和乘法，

513
00:32:16,320 --> 00:32:17,770
相互重复，

514
00:32:18,580 --> 00:32:23,480
为了计算相对于第一个时间步长的梯度，

515
00:32:23,590 --> 00:32:28,580
我们必须对权重矩阵进行多次乘法重复。

516
00:32:29,740 --> 00:32:31,790
这为什么会有问题，

517
00:32:32,320 --> 00:32:36,840
好的，如果这个权重矩阵 w 非常大，

518
00:32:37,700 --> 00:32:41,650
这可能导致他们所说的爆炸性梯度问题，

519
00:32:42,090 --> 00:32:46,690
我们试图用来优化网络的梯度就是这样做的，

520
00:32:46,860 --> 00:32:48,400
它们爆炸了，

521
00:32:49,120 --> 00:32:50,220
它们变得非常大，

522
00:32:50,330 --> 00:32:55,290
使得稳定地训练网络变得不可行和不可能，

523
00:32:56,290 --> 00:33:01,370
我们所做的是一个非常简单的解决方案，称为梯度裁剪，

524
00:33:01,570 --> 00:33:05,570
它有效地缩小了这些非常大的梯度，约束它们，

525
00:33:06,160 --> 00:33:09,110
以更受限制的方式。

526
00:33:10,630 --> 00:33:15,590
相反，我们可以有这样的例子，其中权重矩阵非常非常小，

527
00:33:16,030 --> 00:33:18,410
如果这些权重矩阵非常非常小，

528
00:33:19,120 --> 00:33:22,040
我们最终得到一个非常非常小的值，

529
00:33:22,240 --> 00:33:28,840
因为这些重复的权重矩阵计算和重复乘法的结果，

530
00:33:29,430 --> 00:33:32,890
这是一个非常真实的问题，特别是在 RNN 中，

531
00:33:33,210 --> 00:33:37,580
我们可以引入一个叫做梯度消失的[漏斗]，

532
00:33:37,580 --> 00:33:40,640
现在你的梯度已经下降到接近于零，

533
00:33:40,640 --> 00:33:43,300
再次，你不能稳定地训练网络。

534
00:33:44,020 --> 00:33:47,330
现在，我们可以使用一些特定的工具来实现，

535
00:33:47,920 --> 00:33:51,140
我们可以实现来尝试缓解梯度消失的问题，

536
00:33:51,430 --> 00:33:54,560
我们将简要介绍这三个解决方案。

537
00:33:55,420 --> 00:34:00,720
首先，我们如何定义我们网络中的激活函数，

538
00:34:01,100 --> 00:34:03,930
以及我们如何改变网络体系结构本身，

539
00:34:04,010 --> 00:34:06,600
以更好地处理这个梯度消失问题。

540
00:34:08,030 --> 00:34:09,000
在我们这么做之前，

541
00:34:10,010 --> 00:34:11,760
我只想退后一步，

542
00:34:12,230 --> 00:34:18,660
让你们更直观地了解为什么递归神经网络的真正问题是梯度消失。

543
00:34:20,330 --> 00:34:22,555
我一直试图重申的一点是，

544
00:34:22,555 --> 00:34:25,410
序列数据中的依赖关系的概念，

545
00:34:25,580 --> 00:34:27,720
以及跟踪这些依赖关系的意义，

546
00:34:28,480 --> 00:34:31,820
如果依赖关系被限制在一个很小的空间内，

547
00:34:31,930 --> 00:34:33,890
而不是通过时间来分离，

548
00:34:34,510 --> 00:34:41,090
那么重复的梯度计算和重复的权重矩阵乘法就不是什么问题了。

549
00:34:41,920 --> 00:34:43,380
如果我们有一个非常短的序列，

550
00:34:43,550 --> 00:34:47,040
其中单词之间的联系非常紧密，

551
00:34:47,780 --> 00:34:51,100
很明显，我们的下一个产出会是什么。

552
00:34:52,360 --> 00:34:56,360
RNN 可以使用立即传递的信息来进行预测，

553
00:34:57,540 --> 00:35:03,800
因此，学习有效权重的要求就不会那么多，

554
00:35:03,970 --> 00:35:09,020
如果相关信息在时间上彼此接近。

555
00:35:09,930 --> 00:35:15,220
相反，现在如果我们有一个句子，有一个更长的依赖，

556
00:35:16,180 --> 00:35:20,040
这意味着，我们需要来自序列中更靠前的信息，

557
00:35:20,630 --> 00:35:22,350
才能在最后做出预测，

558
00:35:22,910 --> 00:35:28,290
相关信息和我们目前所处位置之间的差距变得非常大，

559
00:35:28,580 --> 00:35:32,910
因此，梯度消失的问题日益加剧，

560
00:35:33,290 --> 00:35:35,700
意味着我们需要，

561
00:35:37,050 --> 00:35:42,190
RNN 变得无法连接这些点并建立这种长期依赖，

562
00:35:42,390 --> 00:35:44,380
这一切都是因为这种梯度消失的问题。

563
00:35:45,400 --> 00:35:47,540
因此，我们可以输入的方法，

564
00:35:47,770 --> 00:35:50,270
我们可以对我们的网络进行的修改，

565
00:35:50,530 --> 00:35:53,750
试图从三个方面缓解这个问题。

566
00:35:54,610 --> 00:36:00,770
首先，我们可以简单地更改每个神经网络层中的激活函数，

567
00:36:01,240 --> 00:36:07,890
以便它们可以有效地尝试缓解和保护在以下情况下的梯度，

568
00:36:07,890 --> 00:36:13,340
在数据大于零的情况下收缩梯度，

569
00:36:13,810 --> 00:36:17,840
对于 ReLU 激活函数来说尤其如此，

570
00:36:18,430 --> 00:36:23,270
原因是在 x 大于零的所有情况下，

571
00:36:23,530 --> 00:36:26,630
对于 ReLU 函数，导数是 1 ，

572
00:36:26,980 --> 00:36:30,835
所以这不小于 1 ，

573
00:36:30,835 --> 00:36:35,250
因此它有助于缓解渐变消失问题。

574
00:36:37,180 --> 00:36:41,750
另一个诀窍是我们如何初始化网络本身的参数，

575
00:36:42,280 --> 00:36:44,960
以防止它们太快地缩小到零，

576
00:36:45,980 --> 00:36:49,780
有一些数学方法可以做到这一点，

577
00:36:50,040 --> 00:36:53,710
也就是把我们的权重初始化为单位矩阵，

578
00:36:54,150 --> 00:37:01,090
这在实践中有效地帮助防止权重更新过快地收缩到零。

579
00:37:02,470 --> 00:37:06,555
然而，对消失梯度问题最稳健的解决方案是

580
00:37:06,555 --> 00:37:12,980
通过引入稍微复杂一点的递归神经单元，

581
00:37:13,240 --> 00:37:18,440
来能够更有效地跟踪和处理数据中的长期依赖关系，

582
00:37:19,130 --> 00:37:21,240
这就是[]的想法，

583
00:37:21,770 --> 00:37:23,040
这个想法是，

584
00:37:23,120 --> 00:37:28,740
通过有选择地控制流入神经单元的信息流，

585
00:37:29,120 --> 00:37:35,100
以便能够过滤掉不重要的东西，同时保持重要的东西，

586
00:37:35,910 --> 00:37:41,990
实现这种门控计算的关键和最受欢迎的递归单元类型被称为

587
00:37:41,990 --> 00:37:45,550
LSTM ，或长期短期记忆网络，

588
00:37:47,030 --> 00:37:50,160
今天，我们不打算详细讨论 LSTM ，

589
00:37:50,510 --> 00:37:54,000
它们的数学细节、它们的操作等等，

590
00:37:54,230 --> 00:37:58,345
但我只想传达有效的关键思想和直观思想，

591
00:37:58,345 --> 00:38:03,030
关于为什么这些 LSTM 在跟踪长期依赖项方面是有效的。

592
00:38:04,000 --> 00:38:05,150
核心是，

593
00:38:05,440 --> 00:38:11,390
LSTM 能够控制通过这些门的信息流，

594
00:38:11,710 --> 00:38:15,230
从而能够更有效地过滤掉不重要的东西，

595
00:38:15,460 --> 00:38:17,540
存储重要的东西，

596
00:38:19,520 --> 00:38:20,670
你可以做的是，

597
00:38:21,440 --> 00:38:25,770
在 TensorFlow 中实现 LSTM ，就像 RNN 一样，

598
00:38:26,460 --> 00:38:31,020
但我希望你们在考虑 LSTM 时的核心概念是，

599
00:38:31,020 --> 00:38:34,490
这个受控信息流通过门的想法，

600
00:38:35,230 --> 00:38:39,120
简而言之， LSTM 的操作方式是，

601
00:38:39,120 --> 00:38:42,800
通过控制像标准 RNN 一样的单元状态，

602
00:38:43,120 --> 00:38:46,850
并且单元状态独立于直接输出的内容，

603
00:38:47,450 --> 00:38:53,520
单元状态更新的方式是根据这些控制信息流的门，

604
00:38:54,440 --> 00:39:00,060
忘记和消除不相关的东西，存储相关的信息，

605
00:39:01,330 --> 00:39:02,870
依次更新单元状态，

606
00:39:03,340 --> 00:39:09,020
然后对更新后的单元状态进行滤波以产生预测输出，

607
00:39:09,130 --> 00:39:10,910
就像标准的 RNN 一样。

608
00:39:11,550 --> 00:39:16,950
再次，我们可以使用时间反向传播算法来训练 LSTM ，

609
00:39:17,330 --> 00:39:24,060
但 LSTM 如何定义的数学允许完全不间断的梯度流动，

610
00:39:24,320 --> 00:39:31,890
这在很大程度上消除了我前面介绍的梯度消失问题。

611
00:39:33,840 --> 00:39:35,140
再次，我们不是，

612
00:39:35,310 --> 00:39:40,120
如果你有兴趣学习更多关于 LSTM 的数学知识和细节，

613
00:39:40,320 --> 00:39:43,180
请在讲座结束后与我们讨论，

614
00:39:43,290 --> 00:39:49,330
但再次强调 LSTM 运作背后的核心概念和直觉。

615
00:39:51,320 --> 00:39:55,315
好的，所以，到目前为止，

616
00:39:55,315 --> 00:39:56,670
我们已经覆盖了很多领域，

617
00:39:57,380 --> 00:39:59,940
我们已经了解了 RNN 的基本工作原理，

618
00:40:00,380 --> 00:40:03,930
架构、训练以及它们所应用的问题类型。

619
00:40:04,370 --> 00:40:09,000
我想通过考虑一些具体的例子来结束这一部分，

620
00:40:09,350 --> 00:40:13,050
这些例子说明如何在软件实验中使用 RNN 。

621
00:40:14,280 --> 00:40:17,170
这将是音乐生成的任务，

622
00:40:17,940 --> 00:40:20,590
你将致力于建立一个 RNN ，

623
00:40:20,820 --> 00:40:24,520
它可以预测序列中的下一个音符，

624
00:40:24,930 --> 00:40:29,980
并用它来生成以前从未实现过的全新音乐序列。

625
00:40:31,120 --> 00:40:32,490
所以给你们举个例子，

626
00:40:32,490 --> 00:40:37,790
说明你们可以尝试瞄准的输出的质量和类型，

627
00:40:38,140 --> 00:40:45,740
几年前，有一部作品在 RNN 上接受了古典音乐数据语料库的训练，

628
00:40:46,300 --> 00:40:48,810
著名的作曲家 Schubert ，

629
00:40:48,810 --> 00:40:52,380
写了一首著名的未完成的交响曲，

630
00:40:52,380 --> 00:40:53,900
由两个乐章组成，

631
00:40:54,460 --> 00:40:59,270
但他无法在去世前完成他的交响乐，

632
00:40:59,500 --> 00:41:03,200
他去世了，然后留下了未完成的第三乐章，

633
00:41:03,750 --> 00:41:07,450
几年前，一个小组训练了一个基于 RNN 的模型，

634
00:41:07,800 --> 00:41:13,360
试图生成 Schubert 著名的未完成交响乐的第三乐章，

635
00:41:13,440 --> 00:41:15,040
给定之前的两个乐章，

636
00:41:15,800 --> 00:41:18,700
所以我现在就来播放结果。

637
00:41:37,710 --> 00:41:40,805
我暂停一下，我在那里突然地打断了它，

638
00:41:40,805 --> 00:41:44,380
但如果有任何古典音乐爱好者，

639
00:41:44,520 --> 00:41:51,040
希望你能欣赏到音乐质量方面产生的某种质量，

640
00:41:51,270 --> 00:41:53,495
而这已经是几年前的事了，

641
00:41:53,495 --> 00:41:55,600
正如我们将在接下来的课程中看到的，

642
00:41:55,980 --> 00:41:58,270
并继续这个生成性人工智能的主题，

643
00:41:58,590 --> 00:42:02,395
这些算法的能力已经取得了进步，

644
00:42:02,395 --> 00:42:04,200
自从我们第一次播放这个例子以来，

645
00:42:05,060 --> 00:42:10,510
特别是在，我很兴奋地谈到了一系列领域，

646
00:42:10,510 --> 00:42:11,850
但目前还不是时候。

647
00:42:12,350 --> 00:42:17,340
好的，你将在今天的实验 RNN 音乐生成中正面解决这个问题。

648
00:42:20,590 --> 00:42:27,770
此外，我们还可以考虑将输入序列转换为具有情感分类的单个输出的简单示例，

649
00:42:29,130 --> 00:42:31,990
例如，我们可以考虑像推特这样的文本，

650
00:42:32,220 --> 00:42:36,040
并为这些文本赋予积极或消极的标签，

651
00:42:36,270 --> 00:42:41,590
这些文本例子基于网络学习的内容。

652
00:42:43,450 --> 00:42:48,320
好的，关于 RNN 的部分已经结束了，

653
00:42:48,790 --> 00:42:50,510
我认为这是相当了不起的，

654
00:42:50,560 --> 00:42:55,430
使用我们到目前为止讨论的所有基本概念和操作，

655
00:42:55,810 --> 00:43:01,670
我们能够尝试建立网络来处理这个复杂的序列建模问题。

656
00:43:02,450 --> 00:43:05,380
但就像任何技术一样，

657
00:43:05,380 --> 00:43:07,410
RNN 也不是没有局限性的，

658
00:43:07,790 --> 00:43:10,015
那么，这些限制有哪些，

659
00:43:10,015 --> 00:43:15,690
使用 RNN 甚至 LSTM 可能会出现哪些潜在问题。

660
00:43:17,110 --> 00:43:22,490
第一种是编码和依赖的思想，

661
00:43:23,050 --> 00:43:27,140
就我们试图处理的数据的时间分离而言，

662
00:43:28,050 --> 00:43:34,690
RNN 要求对时序信息进行逐次馈入和时间处理，

663
00:43:35,320 --> 00:43:38,790
这造成了我们所说的编码瓶颈，

664
00:43:39,170 --> 00:43:42,240
我们试图编码大量内容的地方，

665
00:43:42,620 --> 00:43:44,670
例如，将非常大的文本主体，

666
00:43:45,170 --> 00:43:48,000
许多不同的单词到单个输出中，

667
00:43:48,580 --> 00:43:50,970
这可能只是最后一个时间点，

668
00:43:51,110 --> 00:43:54,690
我们如何确保该时间点之前的所有信息

669
00:43:54,770 --> 00:43:58,080
都得到了适当的维护、编码和网络学习，

670
00:43:58,600 --> 00:44:00,590
在实践中，这是非常具有挑战性的，

671
00:44:00,700 --> 00:44:02,870
很多信息可能会丢失。

672
00:44:04,060 --> 00:44:05,510
另一个限制是，

673
00:44:05,800 --> 00:44:07,970
通过逐个时间地进行这种时间步进处理，

674
00:44:08,560 --> 00:44:10,190
RNN 可能相当慢，

675
00:44:10,630 --> 00:44:13,940
没有一种真正简单的方法来并行化这种计算。

676
00:44:15,190 --> 00:44:19,550
最后，编码瓶颈的这些组件加在一起，

677
00:44:19,630 --> 00:44:24,800
要求逐步处理这些数据，造成了最大的问题，

678
00:44:25,000 --> 00:44:27,560
这就是当我们谈论长记忆时，

679
00:44:28,380 --> 00:44:32,560
RNN 和 LSTM 的容量确实没有那么长，

680
00:44:33,180 --> 00:44:37,910
我们真的无法处理数万或数十万的数据，

681
00:44:38,170 --> 00:44:42,230
甚至超出了有效学习的序列信息，

682
00:44:42,520 --> 00:44:44,910
完整信息量和模式，

683
00:44:44,910 --> 00:44:47,810
出现在如此大的数据源。

684
00:44:49,150 --> 00:44:50,630
正因为如此，

685
00:44:50,860 --> 00:44:52,965
最近，人们非常关注，

686
00:44:52,965 --> 00:44:57,830
我们如何超越这种循序渐进的循环处理的概念，

687
00:44:58,360 --> 00:45:02,390
建立更强大的架构来处理顺序数据。

688
00:45:03,720 --> 00:45:06,490
为了理解我们是如何做的，我们如何开始做这件事，

689
00:45:06,600 --> 00:45:08,590
让我们后退一大步，

690
00:45:08,820 --> 00:45:13,420
想想我在一开始介绍的序列建模的高级目标。

691
00:45:14,140 --> 00:45:16,020
给定一些输入，一个数据序列，

692
00:45:16,430 --> 00:45:19,290
我们想要构建一个特征编码，

693
00:45:19,840 --> 00:45:22,045
并使用我们的神经网络来学习，

694
00:45:22,045 --> 00:45:26,610
然后将特征编码转换为预测输出。

695
00:45:27,380 --> 00:45:28,630
我们看到的是，

696
00:45:28,630 --> 00:45:32,970
RNN 使用这种递归的概念来维护顺序信息，

697
00:45:33,500 --> 00:45:36,450
一步一步地处理信息，

698
00:45:37,290 --> 00:45:38,435
但正如我刚才提到的，

699
00:45:38,435 --> 00:45:41,200
我们面临着这三个关键的瓶颈，

700
00:45:42,760 --> 00:45:44,280
我们真正想要实现的是，

701
00:45:44,280 --> 00:45:50,900
超越这些瓶颈，在这些模型的能力方面实现更高的能力，

702
00:45:51,690 --> 00:45:53,690
不是存在编码瓶颈，

703
00:45:53,690 --> 00:45:59,050
理想情况下，我们希望将信息作为连续的信息流持续处理，

704
00:45:59,710 --> 00:46:00,855
不是变慢，

705
00:46:00,855 --> 00:46:04,520
我们希望能够并行化计算以加快处理速度，

706
00:46:05,350 --> 00:46:10,305
最后，当然，我们的主要目标是真正尝试建立长期记忆，

707
00:46:10,305 --> 00:46:14,750
以建立对顺序数据的细微差别和丰富的理解。

708
00:46:16,110 --> 00:46:17,600
RNN 的局限性，

709
00:46:17,600 --> 00:46:24,335
与我们无法实现这些能力的所有这些问题和问题联系在一起的是，

710
00:46:24,335 --> 00:46:27,070
它们需要时间一步一步地处理，

711
00:46:28,650 --> 00:46:30,515
那么，如果我们可以超越这一点，又会怎样，

712
00:46:30,515 --> 00:46:33,340
如果我们可以完全消除这种重复的需要，

713
00:46:33,690 --> 00:46:36,490
并且不必逐个时间地处理数据。

714
00:46:37,800 --> 00:46:40,790
那么，第一个也是天真的方法是，

715
00:46:40,790 --> 00:46:45,550
把所有的数据，所有的步骤放在一起，

716
00:46:45,750 --> 00:46:49,925
创建一个有效连接的向量，

717
00:46:49,925 --> 00:46:53,740
时间步长被消除了，只有一个流，

718
00:46:54,510 --> 00:46:58,810
我们有一个包含所有时间点数据的矢量输入，

719
00:46:59,160 --> 00:47:00,520
然后将其输入到模型中，

720
00:47:01,290 --> 00:47:02,870
它计算一些特征向量，

721
00:47:02,870 --> 00:47:04,900
然后生成一些输出，

722
00:47:05,010 --> 00:47:06,490
希望这是有意义的，

723
00:47:07,280 --> 00:47:09,810
因为我们一起挤压了所有这些时间步长，

724
00:47:10,430 --> 00:47:14,340
我们可以简单地考虑建立一个前馈网络，

725
00:47:14,600 --> 00:47:16,740
可以进行这种计算，

726
00:47:17,450 --> 00:47:20,730
有了这一点，我们就消除了递归的必要性，

727
00:47:21,400 --> 00:47:23,340
但我们仍然有这样的问题，

728
00:47:24,140 --> 00:47:25,270
它是不可扩展的，

729
00:47:25,680 --> 00:47:29,380
因为密集的前馈网络必须非常大，

730
00:47:29,490 --> 00:47:31,480
由许多不同的连接定义，

731
00:47:32,360 --> 00:47:35,965
更重要的是，我们已经完全失去了我们的有序信息，

732
00:47:35,965 --> 00:47:38,430
因为我们只是盲目地将所有东西挤压在一起，

733
00:47:38,720 --> 00:47:40,350
没有时间依赖性，

734
00:47:40,580 --> 00:47:45,660
我们就会被困在试图建立长期记忆的能力中。

735
00:47:48,070 --> 00:47:53,240
那么，如果我们仍然可以考虑将这些时间步长集中在一起，

736
00:47:53,680 --> 00:47:58,700
但在如何尝试从这些输入数据中提取信息时更聪明一点，

737
00:47:59,710 --> 00:48:06,315
关键的想法是能够识别和关注，

738
00:48:06,315 --> 00:48:10,430
潜在顺序信息流中的重要内容，

739
00:48:11,320 --> 00:48:14,130
这就是注意力或自我注意力的概念，

740
00:48:14,920 --> 00:48:19,440
在现代深度学习和人工智能中，这是一个非常、非常强大的概念，

741
00:48:19,520 --> 00:48:22,750
我不能轻描淡写，或者，夸大，

742
00:48:22,750 --> 00:48:26,340
这个概念有多么强大，我怎么强调都不为过，

743
00:48:27,640 --> 00:48:31,520
注意力是转换器体系结构的基本机制，

744
00:48:32,140 --> 00:48:34,250
你们中的许多人可能已经听说过，

745
00:48:34,840 --> 00:48:40,080
转换器的概念通常会非常令人生畏，

746
00:48:40,080 --> 00:48:43,485
因为有时它们是用这些非常复杂的图表呈现的，

747
00:48:43,485 --> 00:48:46,070
或者部署在复杂的应用程序中，

748
00:48:46,570 --> 00:48:47,415
你可能会想，

749
00:48:47,415 --> 00:48:49,580
好的，我怎么才能理解这一点，

750
00:48:50,540 --> 00:48:51,570
不过，在它的核心，

751
00:48:51,650 --> 00:48:54,630
注意力，关键操作是一个非常直观的想法，

752
00:48:54,890 --> 00:48:57,600
在这节课的最后一部分，

753
00:48:57,950 --> 00:48:59,490
我们将一步一步地分析它，

754
00:48:59,540 --> 00:49:01,290
看看为什么它如此强大，

755
00:49:01,640 --> 00:49:05,610
以及我们如何将它用作更大的神经网络的一部分，比如转换器。

756
00:49:07,550 --> 00:49:11,640
具体地说，我们将讨论并关注这种自我注意力的概念，

757
00:49:12,750 --> 00:49:16,690
注意输入示例中最重要的部分，

758
00:49:17,460 --> 00:49:19,870
那么让我们来考虑一张图片，

759
00:49:20,040 --> 00:49:23,230
我认为首先考虑一幅图像是最直观的，

760
00:49:23,760 --> 00:49:25,150
这是一张钢铁侠的照片，

761
00:49:25,530 --> 00:49:29,650
如果我们的目标是试图从这张图像中提取重要的信息，

762
00:49:30,540 --> 00:49:34,900
我们可以做的也许是用我们的眼睛天真地扫描这张图像，

763
00:49:35,190 --> 00:49:37,810
逐个像素，只是浏览图像，

764
00:49:39,360 --> 00:49:44,590
然而，我们的大脑可能在内部进行某种类型的计算，

765
00:49:44,910 --> 00:49:46,990
但你和我，我们可以简单地看着这张图像，

766
00:49:47,280 --> 00:49:49,870
并能够关注重要的部分，

767
00:49:50,590 --> 00:49:53,720
我们可以看到钢铁侠在图像中向你走来，

768
00:49:54,220 --> 00:49:56,490
然后我们可以更进一步地关注，

769
00:49:56,490 --> 00:49:59,180
好的，关于钢铁侠的哪些细节可能是重要的，

770
00:50:00,170 --> 00:50:00,990
什么是关键，

771
00:50:01,340 --> 00:50:02,425
你所做的是，

772
00:50:02,425 --> 00:50:07,510
你的大脑正在识别哪些部分正在关注，

773
00:50:07,510 --> 00:50:11,820
然后提取那些值得最高关注的特征，

774
00:50:13,060 --> 00:50:17,570
这个问题的第一部分确实是最有趣和最具挑战性的，

775
00:50:18,060 --> 00:50:21,290
它与搜索的概念非常相似，

776
00:50:21,760 --> 00:50:23,420
实际上，这就是搜索做的事情，

777
00:50:23,800 --> 00:50:26,640
获取一些更大的信息主体，

778
00:50:26,640 --> 00:50:30,020
并试图提取和识别重要的部分。

779
00:50:30,810 --> 00:50:33,070
所以，我们我们去那里，搜索是如何工作的，

780
00:50:33,670 --> 00:50:35,120
你认为你是这个班的学生，

781
00:50:35,260 --> 00:50:36,950
我如何才能了解更多关于神经网络的知识，

782
00:50:37,330 --> 00:50:38,960
嗯，在这个时代，

783
00:50:38,980 --> 00:50:41,750
除了来到这里加入我们之外，

784
00:50:41,920 --> 00:50:43,250
你可能会做的一件事就是上网，

785
00:50:43,600 --> 00:50:45,180
把所有的视频都放在网上，

786
00:50:45,180 --> 00:50:46,850
试图找到匹配的东西，

787
00:50:47,350 --> 00:50:48,500
进行搜索操作，

788
00:50:49,330 --> 00:50:51,620
所以你有一个巨型数据库，比如 YouTube ，

789
00:50:51,700 --> 00:50:52,820
你想找到一段视频，

790
00:50:53,700 --> 00:50:56,170
你输入你的问题，深度学习，

791
00:50:57,560 --> 00:51:01,440
结果是一些可能的结果，

792
00:51:02,200 --> 00:51:03,650
对于数据库中的每个视频，

793
00:51:04,240 --> 00:51:08,750
都会有一些与该视频相关的关键信息，

794
00:51:09,160 --> 00:51:10,070
比如标题，

795
00:51:11,160 --> 00:51:12,700
现在要进行搜索，

796
00:51:13,710 --> 00:51:20,780
任务是找到你的查询和这些标题之间的重叠部分，

797
00:51:20,780 --> 00:51:22,360
数据库中的键，

798
00:51:23,230 --> 00:51:29,540
我们要计算的是查询和这些键之间的相似性和相关性的度量，

799
00:51:30,520 --> 00:51:33,170
它们与我们所需的查询有多相似，

800
00:51:33,840 --> 00:51:35,500
我们可以一步一步地做到这一点，

801
00:51:36,000 --> 00:51:41,020
让我们来看一段关于优雅的巨型海龟的视频的第一个选项，

802
00:51:41,310 --> 00:51:43,510
与我们对深度学习的查询不太相似，

803
00:51:44,370 --> 00:51:47,420
我们的第二个选项，深度学习入门，

804
00:51:47,740 --> 00:51:49,760
这节课的第一节入门课，

805
00:51:49,960 --> 00:51:51,260
是的，非常相关，

806
00:51:52,480 --> 00:51:56,390
第三个选项，一段关于已故伟大的科比的视频，

807
00:51:56,470 --> 00:51:57,260
并不是很相关，

808
00:51:58,020 --> 00:51:59,610
这里的关键操作是，

809
00:51:59,610 --> 00:52:03,890
进行相似性计算，将查询和关键字结合在一起，

810
00:52:05,000 --> 00:52:06,565
最后一步是，

811
00:52:06,565 --> 00:52:08,730
我们已经确定了哪些关键是相关的，

812
00:52:09,680 --> 00:52:11,425
提取相关信息，

813
00:52:11,425 --> 00:52:14,670
我们想要关注的是什么，那就是视频本身，

814
00:52:15,110 --> 00:52:16,260
我们称这为值，

815
00:52:16,790 --> 00:52:19,290
因为搜索执行得很好，

816
00:52:19,610 --> 00:52:22,890
我们已经成功地确定了深度学习相关的视频，

817
00:52:23,030 --> 00:52:25,320
你将想要关注的。

818
00:52:26,140 --> 00:52:28,460
正是这种，这种想法，这种直觉，

819
00:52:28,540 --> 00:52:31,730
提出一个问题，试图找到相似之处，

820
00:52:31,750 --> 00:52:33,500
试图提取相关的值，

821
00:52:33,940 --> 00:52:36,020
形成自我注意力的基础，

822
00:52:37,040 --> 00:52:39,870
以及它如何在神经网络中工作，比如转换器。

823
00:52:40,520 --> 00:52:43,270
所以具体来说，

824
00:52:43,650 --> 00:52:46,630
让我们回到我们的文本，我们的语言例子，

825
00:52:47,770 --> 00:52:48,500
对于句子，

826
00:52:49,570 --> 00:52:53,600
我们的目标是识别和注意输入中的特征，

827
00:52:53,830 --> 00:52:57,140
与句子语义相关的。

828
00:52:58,670 --> 00:53:02,310
现在，第一步，我们有顺序，

829
00:53:02,600 --> 00:53:04,530
我们已经消除了递归，

830
00:53:04,580 --> 00:53:07,440
我们把所有的时间步长都输入进去，

831
00:53:08,080 --> 00:53:13,980
我们仍然需要一种方法来编码和捕获关于顺序和位置依赖的信息，

832
00:53:14,850 --> 00:53:19,120
如何做到这一点，是位置编码的想法，

833
00:53:19,380 --> 00:53:24,010
它捕获了序列中存在的一些固有顺序信息，

834
00:53:24,570 --> 00:53:26,440
我只想简单地谈一下这一点，

835
00:53:26,910 --> 00:53:30,910
但这个想法与我之前介绍过的嵌入的想法有关，

836
00:53:32,060 --> 00:53:37,170
所做的是使用神经网络层来编码位置信息，

837
00:53:37,520 --> 00:53:43,770
位置信息根据本文中的顺序捕获相对关系，

838
00:53:45,440 --> 00:53:47,280
这是高层次的概念，

839
00:53:48,100 --> 00:53:51,540
我们仍然能够一次处理所有这些时间步长，

840
00:53:51,800 --> 00:53:54,510
没有时间、步骤或其他的概念，数据是单一的，

841
00:53:54,890 --> 00:53:59,910
但我们仍然了解到这种捕获位置顺序信息的编码，

842
00:54:01,100 --> 00:54:03,670
现在，我们的下一步是采用这种编码，

843
00:54:03,670 --> 00:54:05,640
并找出要注意的内容，

844
00:54:05,930 --> 00:54:09,780
就像我在 YouTube 示例中介绍的搜索操作一样，

845
00:54:10,630 --> 00:54:15,260
提取查询，提取关键字，提取值，并将它们彼此关联。

846
00:54:15,960 --> 00:54:19,270
因此，我们使用神经网络层来实现这一点，

847
00:54:19,900 --> 00:54:21,420
对于这种位置编码，

848
00:54:21,980 --> 00:54:25,410
需要注意的是应用一个神经网络层，

849
00:54:26,390 --> 00:54:29,070
对其进行转换，首先生成查询，

850
00:54:30,420 --> 00:54:33,820
我们使用单独的神经网络层再次这样做，

851
00:54:34,230 --> 00:54:37,150
这是一组不同的权重，一组不同的参数，

852
00:54:37,440 --> 00:54:40,750
然后以不同的方式转换位置嵌入，

853
00:54:41,190 --> 00:54:44,680
生成第二个输出，即关键字，

854
00:54:45,340 --> 00:54:49,740
最后，对第三层重复该操作，

855
00:54:49,880 --> 00:54:52,350
第三组权重生成值。

856
00:54:53,260 --> 00:54:57,710
现在，有了这三个，查询、关键字和值，

857
00:54:58,120 --> 00:54:59,870
我们可以将它们相互比较，

858
00:55:00,070 --> 00:55:01,250
试图找出，

859
00:55:01,570 --> 00:55:05,990
网络应该在自我输入中关注什么是重要的，

860
00:55:07,040 --> 00:55:10,500
这就是这个相似性度量背后的关键思想，

861
00:55:10,700 --> 00:55:13,140
或者说你可以认为是注意力得分，

862
00:55:13,760 --> 00:55:14,750
我们所做的是，

863
00:55:14,750 --> 00:55:18,970
我们计算查询和键之间的相似性分数，

864
00:55:19,690 --> 00:55:24,800
记住，这些查询值和键值只是数字的数组，

865
00:55:25,180 --> 00:55:27,470
我们可以将它们定义为数字数组，

866
00:55:27,940 --> 00:55:31,040
你可以将其视为空间中的向量，

867
00:55:31,950 --> 00:55:34,600
查询值是一些向量，

868
00:55:34,950 --> 00:55:38,300
键值是其他一些向量，

869
00:55:38,980 --> 00:55:42,130
在数学上，我们可以比较这两个向量，

870
00:55:42,130 --> 00:55:44,190
以了解它们有多相似，

871
00:55:44,510 --> 00:55:47,910
是通过点积和比例来实现的，

872
00:55:48,050 --> 00:55:50,680
捕捉到这些向量有多相似，

873
00:55:50,680 --> 00:55:54,060
它们是否指向同一个方向，

874
00:55:55,720 --> 00:55:57,615
这是相似性度量，

875
00:55:57,615 --> 00:56:01,080
如果你熟悉一点线性代数，

876
00:56:01,080 --> 00:56:03,710
这也被称为余弦相似性，

877
00:56:04,160 --> 00:56:08,080
这个运算对矩阵的作用与此完全相同，

878
00:56:08,430 --> 00:56:14,350
如果我们将这个点积运算应用于查询和键矩阵，

879
00:56:14,580 --> 00:56:16,630
我们就得到了这个相似性度量。

880
00:56:18,130 --> 00:56:22,650
现在，在定义我们的下一步时，这是非常非常关键的，

881
00:56:23,330 --> 00:56:25,230
计算注意力权重，

882
00:56:25,340 --> 00:56:29,190
根据网络在这一输入中应该实际关注的内容，

883
00:56:30,460 --> 00:56:32,120
这个操作给了我们一个分数，

884
00:56:32,710 --> 00:56:40,570
定义输入数据的组件如何相互关联，

885
00:56:41,650 --> 00:56:43,220
因此，给出一个句子，

886
00:56:43,330 --> 00:56:46,550
当我们计算这个相似性得分度量时，

887
00:56:46,900 --> 00:56:49,815
我们就可以开始考虑权重，

888
00:56:49,815 --> 00:56:56,060
这些权重定义了序列数据的组成部分之间的关系，

889
00:56:56,750 --> 00:57:00,510
例如，在这个带有文本句子的例子中，

890
00:57:01,010 --> 00:57:02,970
他抛出了网球发球，

891
00:57:04,080 --> 00:57:06,010
得分的目标是，

892
00:57:06,120 --> 00:57:10,960
序列中相互关联的单词应该具有较高的关注度，

893
00:57:11,250 --> 00:57:14,110
球与掷球相关，与网球相关。

894
00:57:14,970 --> 00:57:18,310
这一指标本身就是我们正在等待的关注，

895
00:57:19,060 --> 00:57:24,930
我们所做的是通过 softmax 函数传递相似性分数，

896
00:57:25,310 --> 00:57:26,530
它所做的就是，

897
00:57:26,530 --> 00:57:29,730
将这些值限制在 0 到 1 之间，

898
00:57:29,960 --> 00:57:34,470
因此你可以将这些值视为相对关注度的相对分数。

899
00:57:35,880 --> 00:57:38,435
最后，现在我们有了这个指标，

900
00:57:38,435 --> 00:57:44,260
它可以捕捉到这种相似性的概念和这些内在的自我关系，

901
00:57:45,240 --> 00:57:51,500
我们终于可以使用这个度量来提取值得高度关注的特征，

902
00:57:52,570 --> 00:57:56,480
这正是自我注意力机制的最后一步。

903
00:57:57,300 --> 00:58:00,680
在这种情况下，我们采用关注度加权矩阵，

904
00:58:00,940 --> 00:58:02,240
将其乘以该值，

905
00:58:02,590 --> 00:58:08,190
得到的原始数据的变换，

906
00:58:08,480 --> 00:58:09,450
作为我们的输出，

907
00:58:09,590 --> 00:58:13,710
这反过来又反映了与高度关注相对应的特征。

908
00:58:15,960 --> 00:58:17,470
好的，让我们深呼吸，

909
00:58:17,850 --> 00:58:20,440
让我们回顾一下我们到目前为止所讨论的内容。

910
00:58:21,380 --> 00:58:25,260
自我注意力的想法是转换器的支柱，

911
00:58:25,280 --> 00:58:27,300
目标是消除递归，

912
00:58:27,500 --> 00:58:30,480
注意输入数据中最重要的特征。

913
00:58:31,210 --> 00:58:34,880
在架构中，它的实际部署方式是，

914
00:58:35,410 --> 00:58:37,130
首先获取我们的输入数据，

915
00:58:37,570 --> 00:58:40,070
然后计算这些位置编码，

916
00:58:40,880 --> 00:58:44,730
神经网络层被三次应用，

917
00:58:45,110 --> 00:58:51,570
以将位置编码转换为每个键查询和值矩阵，

918
00:58:52,290 --> 00:58:56,200
然后，我们可以计算自我注意力权重分数，

919
00:58:56,670 --> 00:58:59,920
根据之前进行的点积操作，

920
00:59:00,540 --> 00:59:06,220
然后对这些特征，对这些信息进行自我关注，

921
00:59:06,360 --> 00:59:09,550
提取值得高度关注的特征，

922
00:59:11,700 --> 00:59:13,960
这种方法有什么强大的作用，

923
00:59:14,460 --> 00:59:16,150
在获取注意力权重方面，

924
00:59:16,820 --> 00:59:20,410
把它和提取高注意力特征的价值放在一起，

925
00:59:21,000 --> 00:59:24,970
这个操作，我在右边展示的方案，

926
00:59:25,230 --> 00:59:27,520
定义了一个单独的自我注意头部，

927
00:59:28,050 --> 00:59:31,690
多个这些自我注意力头部可以链接在一起，

928
00:59:31,920 --> 00:59:34,420
形成更大的网络架构，

929
00:59:34,680 --> 00:59:36,640
你可以思考这些不同的头部，

930
00:59:36,900 --> 00:59:39,040
试图提取不同的信息，

931
00:59:39,240 --> 00:59:41,080
输入的不同相关部分，

932
00:59:41,190 --> 00:59:47,560
现在把我们正在处理的数据的非常非常丰富的编码和表示放在一起，

933
00:59:48,240 --> 00:59:50,920
直观地回到我们的钢铁侠例子，

934
00:59:51,210 --> 00:59:53,770
这种多个自我注意头部的想法，

935
00:59:54,090 --> 01:00:01,120
可能相当于提取数据中不同的显著特征和显著信息，

936
01:00:01,510 --> 01:00:05,030
首先，也许你认为钢铁侠的注意力一，

937
01:00:05,530 --> 01:00:07,820
你可能有额外的注意力头，

938
01:00:07,960 --> 01:00:10,875
他们正在挑选数据的其他相关部分，

939
01:00:10,875 --> 01:00:12,860
这可能是我们之前没有意识到的，

940
01:00:13,120 --> 01:00:17,060
例如，建筑或背景中正在追逐钢铁侠的宇宙飞船，

941
01:00:17,770 --> 01:00:26,115
因此，这是当今许多功能强大的架构的关键组成部分，

942
01:00:26,115 --> 01:00:31,220
我再一次无法强调这一机制是多么多么强大。

943
01:00:32,330 --> 01:00:36,025
事实上，自我注意的主干概念，

944
01:00:36,025 --> 01:00:38,970
你刚刚建立起来的是，

945
01:00:39,050 --> 01:00:44,730
当今一些最强大的神经网络和深度学习模型的关键操作，

946
01:00:45,260 --> 01:00:49,560
从像 GPT 3 这样非常强大的语言模型，

947
01:00:49,820 --> 01:00:55,680
它能够以非常人性化的方式合成自然语言，

948
01:00:56,000 --> 01:01:01,020
消化大量的文本信息来理解关系和文本，

949
01:01:02,000 --> 01:01:08,640
到在生物学和医学中部署的极具影响力的应用程序的模型，

950
01:01:09,200 --> 01:01:16,290
例如 AlphaFold2 ，它使用自我注意力的概念来查看蛋白质序列的数据，

951
01:01:16,430 --> 01:01:19,890
并能够预测蛋白质的三维结构，

952
01:01:20,270 --> 01:01:22,530
只需给定序列信息，

953
01:01:23,090 --> 01:01:25,680
甚至一直到计算机视觉，

954
01:01:26,030 --> 01:01:28,680
这将是我们明天下一节课的主题，

955
01:01:29,150 --> 01:01:34,740
在那里，最初在顺序数据应用程序中开发的相同的注意力概念

956
01:01:35,090 --> 01:01:37,830
现在已经改变了计算机视觉领域，

957
01:01:38,150 --> 01:01:43,680
并再次使用关注输入中重要特征的关键概念

958
01:01:43,850 --> 01:01:48,420
来构建复杂高维数据的这些非常丰富的表示形式。

959
01:01:50,100 --> 01:01:53,260
好的，今天的课程到此结束，

960
01:01:53,880 --> 01:01:57,700
我知道我们在相当短的时间内覆盖了很多领域，

961
01:01:57,900 --> 01:02:00,820
但这就是这个新兵训练营计划的全部意义所在，

962
01:02:01,350 --> 01:02:04,990
所以希望今天你已经对神经网络的基础有了一个了解，

963
01:02:05,400 --> 01:02:06,700
在 Alexander 的讲座中，

964
01:02:07,260 --> 01:02:08,800
我们讨论了 RNN ，

965
01:02:08,970 --> 01:02:10,990
它们如何很好地适应序列数据，

966
01:02:11,310 --> 01:02:13,300
以及我们如何使用反向传播来训练它们，

967
01:02:14,300 --> 01:02:16,240
我们如何将它们部署到不同的应用程序中，

968
01:02:16,260 --> 01:02:19,150
最后，我们如何超越递归，

969
01:02:19,320 --> 01:02:21,160
以构建自我注意力的概念，

970
01:02:21,480 --> 01:02:23,860
以构建越来越强大的模型，

971
01:02:24,660 --> 01:02:26,530
以便在序列建模中进行深度学习。

972
01:02:27,800 --> 01:02:29,850
好的，希望你喜欢，

973
01:02:29,990 --> 01:02:33,190
我们还有大约 45 分钟，

974
01:02:33,190 --> 01:02:35,940
用于实验部分和开放办公时间，

975
01:02:36,260 --> 01:02:40,555
我们欢迎你向我们和助教提问，

976
01:02:40,555 --> 01:02:42,690
并开始实验，

977
01:02:43,070 --> 01:02:45,450
实验的信息就在上面，

978
01:02:45,830 --> 01:02:47,490
非常感谢你的关注。

