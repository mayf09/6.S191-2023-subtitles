1
00:00:09,040 --> 00:00:10,280
大家好，欢迎回来。

2
00:00:11,080 --> 00:00:14,895
今天，我认为今天的这两节课真的很令人兴奋，

3
00:00:14,895 --> 00:00:16,760
因为它们开始超越，

4
00:00:16,870 --> 00:00:19,790
到目前为止，我们在这门课上讲了很多东西，

5
00:00:20,140 --> 00:00:23,085
主要集中在非常静态的数据集上，

6
00:00:23,085 --> 00:00:25,820
特别是在今天，在这节课中，现在，

7
00:00:26,290 --> 00:00:30,390
我要开始谈论如何学习这个由来已久的领域，

8
00:00:30,390 --> 00:00:33,140
我们如何特别地结合两个话题。

9
00:00:33,310 --> 00:00:35,840
第一个主题是强化学习，

10
00:00:36,480 --> 00:00:38,570
它已经存在了很多，几十年，

11
00:00:38,980 --> 00:00:42,780
还有很多深度学习的最新进展，

12
00:00:42,780 --> 00:00:44,430
作为这门课程的一部分，

13
00:00:44,430 --> 00:00:45,620
你们已经开始学习了。

14
00:00:46,940 --> 00:00:51,600
现在这两个领域的结合对我来说真的很吸引人，

15
00:00:51,680 --> 00:00:53,605
特别是，因为就像我说的，

16
00:00:53,605 --> 00:00:56,250
它背离了整个范例，

17
00:00:56,840 --> 00:01:01,135
或者说是我们到目前为止在课堂上接触到的整个范例，

18
00:01:01,135 --> 00:01:07,180
这种模式就是我们可以使用一些数据集来构建深度学习模型的方式，

19
00:01:07,180 --> 00:01:11,125
但这个数据集通常是固定在我们的世界里的，

20
00:01:11,125 --> 00:01:13,590
我们收集，我们出去收集数据集，

21
00:01:13,760 --> 00:01:16,410
我们将其部署在我们的机器学习或深度学习算法上，

22
00:01:16,520 --> 00:01:18,870
然后我们可以在一个全新的数据集上进行评估，

23
00:01:19,330 --> 00:01:22,830
但这与现实世界中的运作方式有很大不同，

24
00:01:22,830 --> 00:01:23,660
在现实世界中，

25
00:01:23,950 --> 00:01:27,860
你的深度学习模型实际上与数据一起部署在一起，

26
00:01:27,940 --> 00:01:29,270
进入现实，

27
00:01:30,280 --> 00:01:32,690
探索，与其环境互动，

28
00:01:32,830 --> 00:01:36,740
并在该环境中尝试一大堆不同的行动和不同的事情，

29
00:01:36,970 --> 00:01:38,540
以便能够学习，

30
00:01:38,740 --> 00:01:43,310
如何最好地执行任何可能需要完成的特定任务，

31
00:01:43,330 --> 00:01:48,210
通常，我们希望能够在没有明确的人类监督的情况下做到这一点，

32
00:01:48,210 --> 00:01:51,240
这是强化学习的关键动机，

33
00:01:51,240 --> 00:01:53,430
你将尝试通过强化来学习，

34
00:01:53,430 --> 00:01:55,310
在你的世界中犯错误，

35
00:01:55,360 --> 00:01:58,490
然后收集这些错误的数据来学习如何改进。

36
00:01:59,600 --> 00:02:06,700
这显然是机器人和自主领域的一个巨大的领域或一个巨大的主题，

37
00:02:06,700 --> 00:02:09,270
你可以想到自动驾驶汽车和机器人操作，

38
00:02:09,650 --> 00:02:14,455
但最近我们也开始看到深度强化学习令人难以置信的进步，

39
00:02:14,455 --> 00:02:19,860
特别是在游戏和策略制定方面。

40
00:02:20,480 --> 00:02:22,270
所以一件很酷的事情是，

41
00:02:22,270 --> 00:02:24,270
现在你甚至可以想象这个，

42
00:02:26,160 --> 00:02:30,730
这种机器人技术的结合，加上现在的游戏性，

43
00:02:30,840 --> 00:02:33,640
现在训练机器人在现实世界中与我们对抗，

44
00:02:33,870 --> 00:02:37,270
我播放这段很短的星际争霸和 DeepMind 的视频。

45
00:02:41,870 --> 00:02:44,580
完美的信息和实时显示，

46
00:02:45,110 --> 00:02:47,185
它还需要长期规划

47
00:02:47,185 --> 00:02:51,240
以及从千千万万种可能性中选择采取什么行动的能力，

48
00:02:52,430 --> 00:02:54,880
我希望五比零，不会输掉任何一场比赛，

49
00:02:54,880 --> 00:02:57,630
但我认为现实的目标应该是四比一对我有利。

50
00:02:59,900 --> 00:03:01,110
我认为他看起来更自信了，

51
00:03:01,250 --> 00:03:03,330
Taylor 之前也很紧张。

52
00:03:07,970 --> 00:03:09,570
这一次，房间里的气氛要紧张得多，

53
00:03:11,390 --> 00:03:12,870
真的不知道会发生什么，

54
00:03:13,370 --> 00:03:15,960
他从五岁起就一直玩星际争霸。

55
00:03:21,170 --> 00:03:23,640
我没想到人工智能会这么好，

56
00:03:26,030 --> 00:03:27,270
他所做的每一件事都是恰当的，

57
00:03:27,290 --> 00:03:29,490
都是经过计算的，都做得很好，

58
00:03:30,320 --> 00:03:31,740
我想我学到了一些东西。

59
00:03:36,800 --> 00:03:38,910
我认为自己是个好玩家，

60
00:03:39,200 --> 00:03:41,250
但我输掉了每一场比赛。

61
00:03:45,740 --> 00:03:46,920
[]赢了。

62
00:03:48,690 --> 00:03:51,340
所以让我们先开始，退后一步，

63
00:03:51,540 --> 00:03:55,535
首先，想想强化学习是如何适用于

64
00:03:55,535 --> 00:04:00,140
你在这节课上所接触到的所有不同主题的整个范例，

65
00:04:00,140 --> 00:04:01,745
作为一个整体，

66
00:04:01,745 --> 00:04:06,530
我认为到目前为止，我们在这门课程中已经涵盖了两种不同类型的学习，

67
00:04:06,530 --> 00:04:10,160
到目前为止，我们已经开始关注课程的开始部分，

68
00:04:10,160 --> 00:04:12,670
首先是我们所说的监督学习，

69
00:04:13,860 --> 00:04:15,850
监督学习就是在这个领域，

70
00:04:15,990 --> 00:04:18,430
我们给出数据格式，

71
00:04:18,600 --> 00:04:21,670
x 作为输入和 y 作为标签，

72
00:04:22,050 --> 00:04:24,880
我们的目标是学习一个函数或神经网络，

73
00:04:25,110 --> 00:04:27,820
它可以学习预测 y ，给出我们的输入 x 。

74
00:04:28,460 --> 00:04:31,840
因此，例如，如果你考虑这个苹果的例子，

75
00:04:32,340 --> 00:04:35,255
观察一系列我们想要检测的苹果的图像，

76
00:04:35,255 --> 00:04:37,420
在未来，如果我们看到一个新的苹果图像，

77
00:04:37,530 --> 00:04:39,400
来检测这确实是一个苹果。

78
00:04:40,750 --> 00:04:45,320
我们在昨天发现的第二种学习方法，

79
00:04:45,520 --> 00:04:48,210
在昨天的课程中，（发现的）是无监督学习，

80
00:04:48,210 --> 00:04:49,695
在这些算法中，

81
00:04:49,695 --> 00:04:52,400
你只能访问数据，

82
00:04:52,420 --> 00:04:54,390
没有标签的概念，

83
00:04:54,390 --> 00:04:55,820
这就是我们昨天了解到的，

84
00:04:56,860 --> 00:04:58,350
在这些类型的算法中，

85
00:04:58,350 --> 00:04:59,990
你不是在试图预测一个标签，

86
00:05:00,160 --> 00:05:03,380
而是试图揭示一些潜在的结构，

87
00:05:03,460 --> 00:05:05,805
我们所称的这些潜在变量，

88
00:05:05,805 --> 00:05:07,700
这些隐藏在你的数据中的特征，

89
00:05:07,750 --> 00:05:10,130
例如，在这个苹果的例子中，

90
00:05:10,330 --> 00:05:11,780
使用了无监督学习，

91
00:05:12,010 --> 00:05:15,860
类似的例子基本上是建立一个模型，

92
00:05:16,000 --> 00:05:21,555
可以理解这些图像的特定部分并将其聚集在一起，

93
00:05:21,555 --> 00:05:23,925
也许它不需要理解，

94
00:05:23,925 --> 00:05:25,250
这是一个苹果的图像，

95
00:05:25,690 --> 00:05:27,030
但它需要明白，

96
00:05:27,030 --> 00:05:29,640
红色苹果的图像是相似于，

97
00:05:29,640 --> 00:05:32,480
它具有相同的潜在特征和相同的语义，

98
00:05:32,770 --> 00:05:35,840
与这个黑白的苹果轮廓比起来。

99
00:05:37,660 --> 00:05:39,320
现在，在今天的课程中，

100
00:05:39,370 --> 00:05:43,290
我们将讨论另一种类型的学习算法，

101
00:05:43,290 --> 00:05:44,720
在强化学习中，

102
00:05:44,770 --> 00:05:50,655
我们将只给出所谓的状态作用对形式的数据，

103
00:05:50,655 --> 00:05:53,415
现在状态是观察，

104
00:05:53,415 --> 00:05:55,875
这是 agent ，让我们这样称呼它，

105
00:05:55,875 --> 00:05:57,675
神经网络将会观察到，

106
00:05:57,675 --> 00:05:58,580
这就是它所看到的，

107
00:05:59,260 --> 00:06:05,060
这些操作是代理在这些特定状态下采取的行为，

108
00:06:05,440 --> 00:06:07,410
所以，强化学习的目标是，

109
00:06:07,410 --> 00:06:11,540
建立一个能够学习如何最大化奖励的 agent ，

110
00:06:11,800 --> 00:06:15,290
这是第三个特定于强化学习的组件，

111
00:06:15,550 --> 00:06:20,330
你希望在未来的许多时间步骤中最大化所有这些奖励，

112
00:06:20,960 --> 00:06:22,720
所以，在这个苹果的例子中，

113
00:06:23,160 --> 00:06:26,330
我们现在可能会看到 agent 不一定学习，

114
00:06:26,330 --> 00:06:28,540
好的，这是一个苹果，或者它看起来像其他苹果，

115
00:06:28,830 --> 00:06:30,350
现在它必须学习，

116
00:06:30,350 --> 00:06:32,950
比如，吃苹果，采取行动，吃那个苹果，

117
00:06:33,000 --> 00:06:34,205
因为它已经学习到，

118
00:06:34,205 --> 00:06:35,710
吃苹果能让它活得更长，

119
00:06:35,760 --> 00:06:38,050
或者因为它不会挨饿而存活下来。

120
00:06:39,620 --> 00:06:41,500
所以今天，就像我说的，

121
00:06:41,500 --> 00:06:45,840
我们将专门关注第三种类型的学习范式，

122
00:06:45,920 --> 00:06:47,520
也就是强化学习。

123
00:06:47,870 --> 00:06:49,110
在我们继续之前，

124
00:06:49,190 --> 00:06:56,635
我只想先为大家建立一些非常关键的术语和背景知识，

125
00:06:56,635 --> 00:06:58,075
这样我们能够达成一致，

126
00:06:58,075 --> 00:07:01,890
当我们开始讨论今天课程中一些更复杂的部分时。

127
00:07:02,920 --> 00:07:06,990
所以，让我们从建立一些这样的术语开始，

128
00:07:06,990 --> 00:07:10,070
第一个主要术语是 agent ，

129
00:07:10,660 --> 00:07:15,830
agent 是一种可以采取行动的存在，

130
00:07:16,330 --> 00:07:21,075
例如，你可以将 agent 视为一台机器，

131
00:07:21,075 --> 00:07:25,070
也就是，让我们假设一架正在送货的自动无人机，

132
00:07:25,120 --> 00:07:26,430
或者例如，在游戏中，

133
00:07:26,430 --> 00:07:27,615
它可能是超级马里奥，

134
00:07:27,615 --> 00:07:30,500
在你的视频游戏中导航，

135
00:07:31,500 --> 00:07:32,690
算法本身，

136
00:07:33,010 --> 00:07:36,180
重要的是要记住算法就是 agent ，

137
00:07:36,180 --> 00:07:38,670
我们试图构建一个可以完成这些任务的 agent ，

138
00:07:38,670 --> 00:07:40,010
算法就是这个 agent ，

139
00:07:40,420 --> 00:07:41,775
所以在生活中，

140
00:07:41,775 --> 00:07:44,060
你们所有人都是生活中的 agent 。

141
00:07:45,420 --> 00:07:48,520
环境对 agent 来说是另一种相反的方法，

142
00:07:48,600 --> 00:07:51,100
或相反的观点，

143
00:07:51,450 --> 00:07:55,900
环境就是 agent 生活和运作的世界，

144
00:07:56,070 --> 00:07:58,900
就在它存在的地方，它在里面活动，

145
00:07:59,950 --> 00:08:02,595
agnet 可以将命令发送到环境，

146
00:08:02,595 --> 00:08:04,010
以所谓的操作的形式，

147
00:08:04,630 --> 00:08:06,410
你可以在该环境中执行操作，

148
00:08:06,880 --> 00:08:09,740
让我们称为表示法，

149
00:08:09,820 --> 00:08:14,610
假设它可以采取的所有行动的可能集合是，

150
00:08:14,610 --> 00:08:16,850
假设是一组 A ，

151
00:08:17,680 --> 00:08:18,950
现在，应该注意的是，

152
00:08:19,060 --> 00:08:23,330
agent 可以在任何时间点进行选择，

153
00:08:23,350 --> 00:08:25,100
比如可能的操作列表，

154
00:08:25,330 --> 00:08:26,510
当然，在某些情况下，

155
00:08:27,040 --> 00:08:30,740
你的动作空间不一定需要是有限空间，

156
00:08:30,790 --> 00:08:32,985
也许你可以在一个连续的空间里采取行动，

157
00:08:32,985 --> 00:08:34,490
例如，当你驾驶一辆汽车时，

158
00:08:34,810 --> 00:08:38,130
你是在一个连续的角度空间上采取行动，

159
00:08:38,130 --> 00:08:40,095
该空间是你想要驾驶汽车的角度，

160
00:08:40,095 --> 00:08:42,560
它不一定只是向右、向左或直行，

161
00:08:42,820 --> 00:08:45,050
你可以在任何连续的角度上驾驶。

162
00:08:47,080 --> 00:08:51,735
观察本质上是环境对 agent 的反应，

163
00:08:51,735 --> 00:08:53,870
环境可以告诉 agent ，

164
00:08:54,190 --> 00:08:57,890
根据它刚刚采取的操作，它应该看到什么，

165
00:08:58,300 --> 00:09:02,265
并且它以所谓的状态的形式做出响应，

166
00:09:02,265 --> 00:09:05,450
状态是具体和直接的情况，

167
00:09:05,860 --> 00:09:08,930
agent 在那个特定时刻发现自己所处的。

168
00:09:10,570 --> 00:09:12,620
现在重要的是要记住，

169
00:09:13,060 --> 00:09:16,790
不同于我们在本课程中讨论的其他类型的学习，

170
00:09:17,140 --> 00:09:18,680
强化学习是有点独特的，

171
00:09:18,940 --> 00:09:22,970
因为它除了这些其他组件之外，还有一个组件，

172
00:09:23,890 --> 00:09:25,160
它被称为奖励，

173
00:09:25,270 --> 00:09:29,090
奖励是我们衡量的反馈，

174
00:09:29,260 --> 00:09:34,400
或者我们可以尝试衡量某个 agent 在其环境中的成功。

175
00:09:34,630 --> 00:09:36,230
例如，在电子游戏中，

176
00:09:36,580 --> 00:09:38,960
当马里奥拿起一枚硬币时，

177
00:09:39,430 --> 00:09:40,590
他就会赢得分数，

178
00:09:40,590 --> 00:09:41,660
所以，在给定的状态下，

179
00:09:41,830 --> 00:09:47,070
agent 可以发出任何形式的操作来做出某些决定，

180
00:09:47,070 --> 00:09:53,090
这些操作可能会也可能不会导致奖励随着时间的推移而收集和积累。

181
00:09:53,630 --> 00:09:55,795
现在，同样重要的是要记住，

182
00:09:55,795 --> 00:09:58,495
并不是所有的行动都会带来立竿见影的奖励，

183
00:09:58,495 --> 00:10:02,910
你可能会采取一些行动，以一种延迟的方式得到奖励，

184
00:10:02,990 --> 00:10:04,825
也许在未来的几步时间里，

185
00:10:04,825 --> 00:10:06,330
也许在生活中，在几年后，

186
00:10:06,740 --> 00:10:12,090
你今天采取的行动可能会在很长一段时间后产生奖励，

187
00:10:12,530 --> 00:10:16,770
但基本上所有这些都试图有效地评估某种方式

188
00:10:17,300 --> 00:10:22,530
来衡量 agent 采取的特定行动的成功程度。

189
00:10:23,170 --> 00:10:25,940
因此，例如，当我们查看总奖励，

190
00:10:26,050 --> 00:10:28,820
agent 在其生命周期中积累的总奖励时，

191
00:10:28,960 --> 00:10:31,490
我们可以简单地将所有奖励相加，

192
00:10:32,080 --> 00:10:35,550
agetn 在特定时间 t 后获得的所有奖励，

193
00:10:35,550 --> 00:10:42,230
所以这个 Rt 是从那个点到未来无限远的所有奖励的总和，

194
00:10:43,580 --> 00:10:46,540
它可以扩展成像这样的样子，

195
00:10:46,540 --> 00:10:50,310
rt 加上 rt+1 加上 rt+2 ，

196
00:10:50,540 --> 00:10:51,630
以此类推。

197
00:10:52,460 --> 00:10:54,925
通常，对我们所有人来说，

198
00:10:54,925 --> 00:10:58,350
不仅考虑所有这些奖励的总和，

199
00:10:58,430 --> 00:11:00,775
而且考虑所谓的折扣总和，

200
00:11:00,775 --> 00:11:01,680
所以你可以看到，

201
00:11:01,940 --> 00:11:05,250
我在所有奖励的前面添加了这个 γ 因子，

202
00:11:05,660 --> 00:11:10,795
折扣因子是乘以每个未来的奖励，

203
00:11:10,795 --> 00:11:14,490
agent 看到，并被 agent 发现的，

204
00:11:14,510 --> 00:11:16,810
我们想要这么做的原因是，

205
00:11:16,810 --> 00:11:19,470
这个抑制因素的设计，

206
00:11:19,940 --> 00:11:26,280
是使未来的奖励比我们现在可能看到的奖励更有价值，

207
00:11:26,360 --> 00:11:27,690
在这个时刻。

208
00:11:28,010 --> 00:11:36,070
现在，你可以认为这是在执行某种短期算法中的灰色，

209
00:11:36,070 --> 00:11:39,540
例如，如果我今天给你们 5 美元的奖励，

210
00:11:39,710 --> 00:11:42,660
或者 10 年后给你们 5 美元的奖励，

211
00:11:42,890 --> 00:11:45,450
我想你们所有人都会更喜欢今天的 5 美元，

212
00:11:45,890 --> 00:11:50,790
因为我们在这个过程种有同样的折扣因素，

213
00:11:51,170 --> 00:11:52,255
我们有一个因素，

214
00:11:52,255 --> 00:11:54,750
那 5 美元对我们来说不值那么多钱，

215
00:11:54,890 --> 00:11:56,920
如果十年后给我们的话，

216
00:11:56,920 --> 00:11:59,370
这也正是我们在这里捕捉到的，

217
00:11:59,510 --> 00:12:02,980
从数学上讲，这个折扣因子就像乘以，

218
00:12:02,980 --> 00:12:07,400
就像我说的，乘以每一个未来的奖项，将呈指数级增长，

219
00:12:08,260 --> 00:12:10,845
理解这一点也很重要，

220
00:12:10,845 --> 00:12:14,670
通常情况下，折扣系数介于 0 和 1 之间，

221
00:12:14,670 --> 00:12:17,985
在某些特殊情况下，你可能想要一些奇怪的行为，

222
00:12:17,985 --> 00:12:19,950
折现系数大于 1 ，

223
00:12:19,950 --> 00:12:23,270
但总的来说，这不是我们今天要讨论的问题。

224
00:12:24,380 --> 00:12:27,820
最后，这在强化学习中非常重要，

225
00:12:28,170 --> 00:12:31,330
这个特殊的函数被称为 Q 函数，

226
00:12:31,470 --> 00:12:35,830
它与我刚才与你们分享的许多不同的组件联系在一起。

227
00:12:36,570 --> 00:12:39,485
现在让我们来看看这个 Q 函数是什么。

228
00:12:39,485 --> 00:12:42,650
所以我们已经讨论了这个 Rt 函数，

229
00:12:42,650 --> 00:12:45,730
Rt 是奖励折扣和，

230
00:12:45,990 --> 00:12:50,140
从时间 t 一直到未来无限时间，

231
00:12:50,470 --> 00:12:53,715
但请记住，这个 Rt ，

232
00:12:53,715 --> 00:12:56,180
它是折扣 1 和 2 ，

233
00:12:57,160 --> 00:13:00,800
我们将尝试构建 Q 函数，

234
00:13:01,300 --> 00:13:06,590
它捕捉我们可以采取的最大或最好的行动，

235
00:13:06,790 --> 00:13:08,970
以最大化这一奖励。

236
00:13:08,970 --> 00:13:11,120
所以让我用一种不同的方式再说一次，

237
00:13:11,530 --> 00:13:14,720
Q 函数接受两个不同的东西作为输入，

238
00:13:14,860 --> 00:13:16,850
第一个是你当前所处的状态，

239
00:13:16,960 --> 00:13:22,100
第二个是你可以在特定状态下执行的可能操作，

240
00:13:22,990 --> 00:13:25,220
所以这里 st 是时间 t 的状态，

241
00:13:25,330 --> 00:13:28,760
at 是你可能想要在时间 t 采取的行动，

242
00:13:28,990 --> 00:13:33,740
Q 函数的这两个部分将表示或捕捉，

243
00:13:34,480 --> 00:13:37,935
那个 agent 预期总奖励是多少，

244
00:13:37,935 --> 00:13:41,090
如果 agent 在特定状态下采取该行动。

245
00:13:42,780 --> 00:13:48,130
现在，我认为我们现在都应该问问自己，

246
00:13:48,960 --> 00:13:51,140
这似乎是一个非常强大的函数，

247
00:13:51,140 --> 00:13:54,220
如果你可以访问这种类型的函数，这个 Q 函数，

248
00:13:54,450 --> 00:13:57,920
我想你可以立即执行很多任务，

249
00:13:57,920 --> 00:14:04,690
例如，如果你想了解如何在特定的状态下采取什么动作，

250
00:14:05,040 --> 00:14:07,300
让我们假设我给了你这个神奇的 Q 函数，

251
00:14:07,740 --> 00:14:09,080
有人有什么想法，

252
00:14:09,080 --> 00:14:14,260
你如何转换这个 Q 函数来直接推断应该采取什么动作吗？

253
00:14:17,390 --> 00:14:18,270
给定一个状态，

254
00:14:18,320 --> 00:14:20,425
你可以查看可能的操作空间，

255
00:14:20,425 --> 00:14:22,500
并选择给你最高 Q 值的操作空间。

256
00:14:22,910 --> 00:14:25,105
完全正确，所以这是完全正确的。

257
00:14:25,105 --> 00:14:27,030
所以再重复一次，

258
00:14:27,530 --> 00:14:30,570
Q 函数告诉我们任何可能的操作，

259
00:14:31,040 --> 00:14:34,590
采取这一行动的预期奖励是什么，

260
00:14:34,610 --> 00:14:38,730
所以，如果我们想要在特定的状态采取特定的行动，

261
00:14:39,530 --> 00:14:40,950
最终我们需要，

262
00:14:40,970 --> 00:14:43,825
找出哪种行动是最好的行动，

263
00:14:43,825 --> 00:14:46,620
我们通过 Q 函数做到这一点的方法是，

264
00:14:46,700 --> 00:14:50,880
简单地选择将使我们未来的奖励最大化的行动，

265
00:14:51,050 --> 00:14:54,265
我们可以简单地尝试第一个，

266
00:14:54,265 --> 00:14:56,020
如果我们有一个离散的动作空间，

267
00:14:56,020 --> 00:14:58,020
我们可以简单地尝试所有可能的动作，

268
00:14:58,430 --> 00:15:01,560
计算每个可能行动的 Q 值，

269
00:15:01,610 --> 00:15:03,810
根据我们目前所处的状态，

270
00:15:03,950 --> 00:15:08,160
然后我们选择将产生最高 Q 值的动作，

271
00:15:09,690 --> 00:15:11,405
如果我们有一个连续的动作空间，

272
00:15:11,405 --> 00:15:13,150
也许我们会做一些更智能的事情，

273
00:15:13,260 --> 00:15:16,420
也许沿着这条 Q 值曲线的梯度，

274
00:15:16,830 --> 00:15:19,360
并将其最大化作为优化过程的一部分。

275
00:15:20,620 --> 00:15:22,560
但总的来说，在这节课中，

276
00:15:22,790 --> 00:15:24,480
我想要关注的是，

277
00:15:24,740 --> 00:15:27,220
我们如何才能得到这个 Q 函数。

278
00:15:27,220 --> 00:15:31,165
首先，我跳过了上一张幻灯片中的很多步骤，

279
00:15:31,165 --> 00:15:31,735
我刚才说，

280
00:15:31,735 --> 00:15:33,895
假设我给你一个神奇的 Q 函数，

281
00:15:33,895 --> 00:15:35,730
你如何确定要采取什么行动，

282
00:15:35,990 --> 00:15:38,005
但在现实中，我们没有得到 Q 函数，

283
00:15:38,005 --> 00:15:40,410
我们必须使用深度学习来学习 Q 函数，

284
00:15:40,760 --> 00:15:44,140
这就是今天的课程将主要讨论的，

285
00:15:44,140 --> 00:15:47,700
首先，我们如何从数据中构造和学习 Q 函数，

286
00:15:48,340 --> 00:15:51,360
当然，最后一步是使用 Q 函数，

287
00:15:51,560 --> 00:15:53,640
在现实世界中采取一些行动。

288
00:15:53,690 --> 00:15:57,805
总的来说，有两类强化学习算法，

289
00:15:57,805 --> 00:16:00,090
我们将简要介绍一下，作为今天课程的一部分。

290
00:16:00,500 --> 00:16:03,040
第一类就是所谓的价值学习，

291
00:16:03,040 --> 00:16:05,560
这就是我们刚才谈到的这个过程，

292
00:16:05,560 --> 00:16:08,785
价值学习试图估计我们的 Q 函数，

293
00:16:08,785 --> 00:16:10,320
所以为了找到 Q 函数，

294
00:16:10,430 --> 00:16:12,300
Q 给定状态和行为，

295
00:16:12,650 --> 00:16:14,730
然后使用 Q 函数来，

296
00:16:14,900 --> 00:16:20,940
优化给定的特定状态下要采取的最佳行动。

297
00:16:21,700 --> 00:16:23,245
第二类算法，

298
00:16:23,245 --> 00:16:25,470
我们将在今天课程的最后谈到，

299
00:16:25,850 --> 00:16:28,855
是这种方法的一种不同的框架，

300
00:16:28,855 --> 00:16:33,205
但是，不是首先优化 Q 函数并找到 Q 值，

301
00:16:33,205 --> 00:16:36,210
然后使用 Q 函数来优化我们的操作，

302
00:16:36,680 --> 00:16:40,110
如果我们只尝试直接优化我们的策略，

303
00:16:40,370 --> 00:16:45,000
这是根据我们发现自己所处的特定状态采取的操作，

304
00:16:45,590 --> 00:16:48,690
如果我们这样做，如果我们能得到这个函数，

305
00:16:49,120 --> 00:16:51,960
然后，我们可以直接从该策略分布中进行抽样，

306
00:16:52,400 --> 00:16:53,700
以获得最优操作。

307
00:16:54,020 --> 00:16:56,605
我们将在后面的课程中详细讨论这一点，

308
00:16:56,605 --> 00:17:00,840
但首先让我们讨论第一类方法，即 Q 学习方法，

309
00:17:01,520 --> 00:17:06,360
我们将在策略学习的第二部分建立直觉和知识。

310
00:17:07,070 --> 00:17:11,760
所以让我们从更深入地研究 Q 函数开始，

311
00:17:12,260 --> 00:17:15,780
开始理解，我们如何在一开始估计它。

312
00:17:16,440 --> 00:17:18,050
所以首先让我来介绍一下这个游戏，

313
00:17:18,050 --> 00:17:19,490
也许你们中的一些人认识到这一点，

314
00:17:19,490 --> 00:17:21,820
这是一个叫做 Atari Breakout 的游戏，

315
00:17:23,010 --> 00:17:28,085
这里的游戏本质上是一个 agent 能够向左或向右移动，

316
00:17:28,085 --> 00:17:30,070
这个桨在左下角或右下角，

317
00:17:30,240 --> 00:17:36,760
我们的目标是以这样一种方式移动它，

318
00:17:36,960 --> 00:17:40,355
这个向屏幕底部落下的球可以从你的球拍上反弹，反射回来，

319
00:17:40,355 --> 00:17:43,060
基本上你想要突破，

320
00:17:43,290 --> 00:17:46,340
把那个球反射回屏幕顶部的彩虹部分，

321
00:17:46,340 --> 00:17:47,480
然后继续打破，

322
00:17:47,480 --> 00:17:49,685
每当你点击屏幕顶部的一个像素，

323
00:17:49,685 --> 00:17:50,980
你就会打断那个像素，

324
00:17:51,030 --> 00:17:56,300
这个游戏的目标是基本上消除所有的彩虹像素，

325
00:17:56,300 --> 00:17:58,430
所以你想要继续击打屏幕顶部的球，

326
00:17:58,430 --> 00:18:00,160
直到你移除所有的像素。

327
00:18:01,680 --> 00:18:03,760
现在 Q 函数告诉我们，

328
00:18:04,170 --> 00:18:09,430
我们预期的总回报或总奖励，

329
00:18:09,480 --> 00:18:12,070
基于给定的状态和行动，

330
00:18:12,240 --> 00:18:14,170
我们可能会在这场游戏中自己发现。

331
00:18:14,850 --> 00:18:17,105
现在，我想在这里提出的第一点是，

332
00:18:17,105 --> 00:18:22,660
有时候，即使是我们人类，理解 Q 值应该是什么，

333
00:18:22,800 --> 00:18:25,070
有时也是相当不直观的。

334
00:18:25,070 --> 00:18:26,050
这里有一个例子，

335
00:18:26,580 --> 00:18:31,025
假设我们发现这两个状态动作对是 A 和 B ，

336
00:18:31,025 --> 00:18:33,580
两个不同的选项，我们可以在这个游戏中看到，

337
00:18:34,060 --> 00:18:36,820
A ，球径直朝我们飞过来，

338
00:18:36,820 --> 00:18:37,680
那是我们的状态，

339
00:18:37,760 --> 00:18:39,210
我们的行动是什么都不做，

340
00:18:39,260 --> 00:18:42,810
只是把球垂直向上反射回来，

341
00:18:43,190 --> 00:18:45,210
第二种情况，

342
00:18:45,980 --> 00:18:49,050
球的状态是以一个角度微微地飞过来，

343
00:18:49,190 --> 00:18:51,010
我们还没有完全处于它的下面，

344
00:18:51,010 --> 00:18:52,560
我们需要朝着它前进，

345
00:18:52,790 --> 00:18:59,830
并以一种你知道会成功而不是错失的方式来击球，

346
00:18:59,830 --> 00:19:01,510
所以希望那个球不会传到我们下面，

347
00:19:01,510 --> 00:19:02,460
比赛就结束了，

348
00:19:03,350 --> 00:19:04,110
你能想象一下，

349
00:19:04,310 --> 00:19:09,955
这两个选项中哪一个可能对网络具有更高的 Q 值，

350
00:19:09,955 --> 00:19:14,690
哪一个会导致对神经网络或 agent 的回报率，

351
00:19:17,260 --> 00:19:21,410
那么，有多少人相信 A 会带来更高的回报，

352
00:19:23,930 --> 00:19:24,630
B 怎么样，

353
00:19:26,400 --> 00:19:28,175
好的，那选 B 的人，

354
00:19:28,175 --> 00:19:29,740
你能告诉我为什么选 B 吗？

355
00:19:32,160 --> 00:19:34,420
[]，你真的在做一些事情。

356
00:19:35,730 --> 00:19:38,020
好的，是的，还有更多吗？

357
00:19:38,910 --> 00:19:42,970
对于 A ，你可以拿到的最大值就一，

358
00:19:43,020 --> 00:19:44,440
因为当你反射后，

359
00:19:44,910 --> 00:19:49,180
你自动[背景]更多。

360
00:19:50,580 --> 00:19:53,540
正确，这里有一个非常有趣的事情，

361
00:19:53,540 --> 00:19:55,030
所以当我第一次看到这个的时候，

362
00:19:55,110 --> 00:19:58,895
它是，这对我来说是非常不直观的，

363
00:19:58,895 --> 00:20:01,640
为什么 A 比 B 差得多，

364
00:20:01,640 --> 00:20:05,375
但总的来说， B 的这个非常保守的动作，

365
00:20:05,375 --> 00:20:06,850
就像你说的，

366
00:20:07,080 --> 00:20:09,490
两个答案都在暗示，

367
00:20:09,840 --> 00:20:11,450
A 是一个非常保守的动作，

368
00:20:11,450 --> 00:20:12,910
你只是上上下下而已，

369
00:20:13,410 --> 00:20:14,675
它将获得很好的回报，

370
00:20:14,675 --> 00:20:16,270
它会解决这个游戏，

371
00:20:16,350 --> 00:20:18,890
事实上，它像这样解决了这个游戏，

372
00:20:18,890 --> 00:20:19,450
你可以看到，

373
00:20:19,710 --> 00:20:22,430
总的来说，这一行动将是相当保守的，

374
00:20:22,430 --> 00:20:23,470
它只是反弹起来，

375
00:20:23,670 --> 00:20:25,745
从顶端一次击中一分，

376
00:20:25,745 --> 00:20:27,160
然后非常缓慢地脱离，

377
00:20:27,600 --> 00:20:29,080
你可以在这里看到的板，

378
00:20:29,460 --> 00:20:32,795
但总的来说，你可以看到板被拆分的部分

379
00:20:32,795 --> 00:20:35,255
是朝向板的中心的，

380
00:20:35,255 --> 00:20:36,890
不是很多在板的边缘，

381
00:20:37,270 --> 00:20:38,390
如果你查看 B ，

382
00:20:38,560 --> 00:20:41,240
现在，使用 B ，你就有了某种作用力，

383
00:20:41,530 --> 00:20:42,690
就像其中一个答案所说的，

384
00:20:42,690 --> 00:20:44,000
你朝着球走来，

385
00:20:44,140 --> 00:20:45,975
这意味着，

386
00:20:45,975 --> 00:20:49,520
你有时会撞到你的球拍的角落，

387
00:20:49,540 --> 00:20:52,230
在你的球拍上有一个非常极端的角度，

388
00:20:52,230 --> 00:20:54,080
也击中了板的两边，

389
00:20:54,190 --> 00:20:57,740
事实证明，算法， agent 可以了解到，

390
00:20:57,940 --> 00:21:02,060
击中板的两边可能会产生某种意想不到的后果，

391
00:21:02,800 --> 00:21:03,740
就像这样，

392
00:21:03,850 --> 00:21:05,850
所以在这里，你可以看到它试图制定这项策略，

393
00:21:05,850 --> 00:21:07,760
它的目标是板的侧面，

394
00:21:08,170 --> 00:21:10,760
但一旦它到达板一侧的突破口，

395
00:21:10,840 --> 00:21:12,620
它就在解决方案中发现了这个攻击，

396
00:21:12,700 --> 00:21:14,420
现在它正在打破大量的像素，

397
00:21:14,980 --> 00:21:18,260
所以这是这个神经网络学到的一种技巧，

398
00:21:18,790 --> 00:21:23,010
这是一种在球落下的时候它甚至离开球的方式，

399
00:21:23,010 --> 00:21:24,345
这样我们就可以向它移动，

400
00:21:24,345 --> 00:21:25,610
只是在角上击球，

401
00:21:25,840 --> 00:21:28,710
在板的角部执行，

402
00:21:28,710 --> 00:21:31,760
几乎是免费的打破了很多部分。

403
00:21:33,800 --> 00:21:35,190
现在我们可以看到，

404
00:21:35,210 --> 00:21:39,240
有时获得 Q 函数可能有点不直观，

405
00:21:39,380 --> 00:21:41,125
但这里的关键点是，

406
00:21:41,125 --> 00:21:42,120
如果我们有 Q 函数，

407
00:21:42,120 --> 00:21:44,520
我们可以直接使用它来确定，

408
00:21:44,540 --> 00:21:46,080
我们可以采取的最佳行动是什么，

409
00:21:46,340 --> 00:21:49,510
在我们发现自己处于任何给定的状态下。

410
00:21:49,510 --> 00:21:51,210
所以，现在的问题自然是，

411
00:21:51,410 --> 00:21:52,860
我们如何才能训练出一个神经网络，

412
00:21:53,030 --> 00:21:55,620
真正能够学习这个 Q 函数？

413
00:21:57,340 --> 00:22:00,380
所以神经网络的类型，

414
00:22:00,550 --> 00:22:03,315
很自然地，因为我们有一个函数，它接受两个东西作为输入，

415
00:22:03,315 --> 00:22:08,025
让我们想象一下，我们的神经网络也将这两个对象作为输入，

416
00:22:08,025 --> 00:22:10,170
一个目标将是板的状态，

417
00:22:10,170 --> 00:22:14,160
你可以简单地将其视为屏幕上描述该板的像素，

418
00:22:14,160 --> 00:22:16,440
所以这是板在特定时间的图像，

419
00:22:16,440 --> 00:22:18,800
也许你甚至想提供两三张图片，

420
00:22:18,820 --> 00:22:23,235
给它一些时间信息和一些过去的历史的感觉，

421
00:22:23,235 --> 00:22:25,940
但所有这些信息都可以组合在一起，

422
00:22:26,200 --> 00:22:28,400
并以状态的形式提供给网络。

423
00:22:28,950 --> 00:22:33,185
除此之外，你可能还想提供一些操作，

424
00:22:33,185 --> 00:22:33,815
所以，在这种情况下，

425
00:22:33,815 --> 00:22:38,825
神经网络或 agent 在这个游戏中可以采取的行动是

426
00:22:38,825 --> 00:22:41,260
向右移动，向左移动，保持不动，

427
00:22:41,820 --> 00:22:43,240
这可能是三个不同的动作，

428
00:22:43,290 --> 00:22:46,900
可以提供给神经网络的输入并将其参数化，

429
00:22:47,250 --> 00:22:51,340
这里的目标是估计单个数字的产出，

430
00:22:51,810 --> 00:22:56,050
它测量期望值或期望 Q 值是多少，

431
00:22:56,340 --> 00:22:59,620
关于这个神经网络在这个特定状态作用对的，

432
00:23:00,180 --> 00:23:02,675
通常你会看到，

433
00:23:02,675 --> 00:23:04,030
如果你想评估，

434
00:23:04,290 --> 00:23:06,170
假设有一个非常大的动作空间，

435
00:23:06,170 --> 00:23:09,280
尝试左边的方法，效率会非常低，

436
00:23:09,720 --> 00:23:11,770
在一个很大的行动空间中，

437
00:23:11,790 --> 00:23:16,790
因为这意味着你将不得不多次向前运行你的神经网络，

438
00:23:16,790 --> 00:23:19,505
对于你动作空间的每一个元素一次，

439
00:23:19,505 --> 00:23:23,020
所以，如果你只给它提供了你所在状态的一个输入，

440
00:23:23,400 --> 00:23:27,970
作为你给它的输出，假设所有 n 个不同的 Q 值，

441
00:23:28,410 --> 00:23:30,610
每一个可能的动作都有一个 Q 值，

442
00:23:30,630 --> 00:23:34,720
这样，你只需要针对你所处的特定状态运行一次神经网络，

443
00:23:35,290 --> 00:23:39,710
然后神经网络会告诉你所有可能的动作，最大值是多少，

444
00:23:40,390 --> 00:23:42,350
然后，你只需查看该输出，

445
00:23:42,490 --> 00:23:45,200
并选择具有 [] Q 值的动作。

446
00:23:46,940 --> 00:23:49,420
现在，会发生什么，

447
00:23:49,420 --> 00:23:52,140
所以我想在这里提出的问题是，

448
00:23:52,370 --> 00:23:55,285
我们想要训练这两个网络中的一个，

449
00:23:55,285 --> 00:23:57,625
为了简单起见，让我们继续使用右侧的网络，

450
00:23:57,625 --> 00:24:00,480
因为它是左侧网络的一个更高效的版本，

451
00:24:01,010 --> 00:24:02,320
问题是，

452
00:24:02,320 --> 00:24:05,550
我们如何真正在右边训练这个网络，

453
00:24:05,840 --> 00:24:09,780
具体地说，我希望你们所有人都考虑最好的情况，

454
00:24:10,010 --> 00:24:14,910
首先考虑一个 agent 在特定情况下会如何理想地表现，

455
00:24:15,170 --> 00:24:21,120
或者如果一个 agent 在任何给定的状态下采取所有理想的行动会发生什么，

456
00:24:21,350 --> 00:24:24,400
这将意味着，从本质上讲，目标回报，

457
00:24:24,400 --> 00:24:28,920
无论是我们试图预测的值，还是预测的值目标，

458
00:24:29,240 --> 00:24:31,975
总是最大化的，

459
00:24:31,975 --> 00:24:35,130
这可以作为 agent 的基本事实。

460
00:24:36,200 --> 00:24:38,620
现在，例如，要做到这一点，

461
00:24:38,670 --> 00:24:40,690
我们想要建立一个损失函数，

462
00:24:41,280 --> 00:24:44,990
它代表了我们的预期回报，

463
00:24:44,990 --> 00:24:48,140
如果我们能够采取所有最好的行动，

464
00:24:48,140 --> 00:24:50,590
所以，举个例子，如果我们选择一个最初的奖励，

465
00:24:51,000 --> 00:24:53,825
加上在我们的行动空间中选择一些行动，

466
00:24:53,825 --> 00:24:55,840
以最大化我们的预期回报，

467
00:24:56,400 --> 00:24:58,480
那么对于下一个未来状态，

468
00:24:58,740 --> 00:25:03,700
我们需要应用这个贴现因子，并递归地应用相同的等式，

469
00:25:03,750 --> 00:25:05,950
这就变成了我们的目标。

470
00:25:06,420 --> 00:25:11,745
现在，我们可以问，我们的神经网络预测了什么，

471
00:25:11,745 --> 00:25:12,590
这就是我们的目标，

472
00:25:12,940 --> 00:25:15,620
我们从以前的课程中回忆，

473
00:25:15,760 --> 00:25:16,890
如果我们有一个目标值，

474
00:25:16,890 --> 00:25:19,130
在这种情况下，我们的 Q 值是一个连续变量，

475
00:25:19,810 --> 00:25:21,650
我们还有一个预测变量，

476
00:25:22,150 --> 00:25:27,230
它将作为可能采取的每一项潜在行动的输出的一部分，

477
00:25:28,900 --> 00:25:30,770
我们可以定义所谓的 Q 损失，

478
00:25:30,910 --> 00:25:36,080
它本质上就是这两个连续变量之间非常简单的均方误差损失，

479
00:25:36,400 --> 00:25:41,330
我们将它们之间的距离最小化，

480
00:25:41,530 --> 00:25:44,120
在这个环境中[]我们的神经网络，

481
00:25:44,290 --> 00:25:47,420
观察动作，不仅观察动作，

482
00:25:47,500 --> 00:25:49,820
最重要的是在动作之后，

483
00:25:51,150 --> 00:25:52,480
无论是提交还是执行，

484
00:25:52,860 --> 00:25:57,950
我们都能看到实情，预期回报，

485
00:25:57,950 --> 00:26:01,750
我们有事实标签来直接训练和监督这个模型，

486
00:26:02,100 --> 00:26:07,210
作为随机选择的一部分执行的操作。

487
00:26:08,420 --> 00:26:11,120
现在，让我到此为止，

488
00:26:11,120 --> 00:26:13,540
也许再总结一次整个过程，

489
00:26:13,680 --> 00:26:15,665
也许会有一些不同的术语，

490
00:26:15,665 --> 00:26:19,150
只是为了让每个人对这个相同的问题有不同的看法。

491
00:26:19,890 --> 00:26:23,750
所以我们试图训练的深度神经网络是这样的，

492
00:26:23,750 --> 00:26:25,000
它接受 state 作为输入，

493
00:26:25,290 --> 00:26:27,580
尝试输出 n 个不同的数字，

494
00:26:28,020 --> 00:26:32,590
这 n 个不同的数字对应于与 n 个不同动作相关联的 Q 值，

495
00:26:32,880 --> 00:26:34,480
每个动作一个 Q 值，

496
00:26:36,200 --> 00:26:41,100
例如 Atari Breakout 中的行动应该是三个行动，

497
00:26:41,240 --> 00:26:42,930
我们要么向左走，要么向右走，

498
00:26:43,160 --> 00:26:49,120
或者什么都不做，我们可以呆在原地不动，

499
00:26:49,470 --> 00:26:52,295
所以，下一步，我们看到，

500
00:26:52,295 --> 00:26:55,520
如果我们有这个 Q 值输出，

501
00:26:55,520 --> 00:26:57,280
我们可以做的是，

502
00:26:57,690 --> 00:27:01,880
我们用它做一个动作，或者我们甚至可以让我更正式地对待它，

503
00:27:01,880 --> 00:27:03,850
我们可以开发所谓的策略函数，

504
00:27:03,870 --> 00:27:06,820
策略函数是一个给定状态的函数，

505
00:27:07,170 --> 00:27:09,340
它决定什么是最好的行动，

506
00:27:09,420 --> 00:27:11,360
这和 Q 函数是不同的，

507
00:27:11,360 --> 00:27:12,490
Q 函数告诉我们，

508
00:27:12,660 --> 00:27:18,250
给定一个状态，什么是最好的，或者是什么值，以及我们可以采取的每个行动的回报，

509
00:27:18,540 --> 00:27:21,250
策略函数告诉我们更进一步，

510
00:27:21,580 --> 00:27:25,195
给定一个状态，最好的行动是什么，

511
00:27:25,195 --> 00:27:27,960
所以这是一种非常端到端的思考方式，

512
00:27:28,250 --> 00:27:31,860
根据我现在看到的 agent 的决策过程，

513
00:27:32,210 --> 00:27:33,810
我应该采取什么行动，

514
00:27:34,100 --> 00:27:38,770
我们可以直接从 Q 函数本身确定策略函数，

515
00:27:38,770 --> 00:27:42,840
只需最大化和优化所有不同的 Q 值，

516
00:27:43,010 --> 00:27:45,360
对于我们在这里看到的所有不同的操作。

517
00:27:45,950 --> 00:27:47,965
例如，我们在这里可以看到，

518
00:27:47,965 --> 00:27:50,310
在给定这种状态下， Q 函数，

519
00:27:50,810 --> 00:27:52,800
作为这三个不同值的结果，

520
00:27:53,240 --> 00:27:55,920
有一个 Q 值为 20 ，如果它向左移动，

521
00:27:56,090 --> 00:27:58,705
有一个 Q 值为 3 ，如果它停留在同一位置，

522
00:27:58,705 --> 00:28:00,120
它的 Q 值为 0 ，如果它向右移动，

523
00:28:00,380 --> 00:28:02,880
它基本上会在迭代后消亡，

524
00:28:03,050 --> 00:28:04,105
如果它向右移动，

525
00:28:04,105 --> 00:28:06,070
因为你可以看到球正在向它的左侧移动，

526
00:28:06,070 --> 00:28:08,395
如果它向右移动，游戏就结束了，

527
00:28:08,395 --> 00:28:12,630
所以它需要向左移动才能继续比赛，

528
00:28:12,830 --> 00:28:14,430
Q 值反映了这一点，

529
00:28:15,260 --> 00:28:19,890
这里的最优行动就是这三个 Q 值中的最大值，

530
00:28:20,030 --> 00:28:22,120
在这种情况下，它将是 20 ，

531
00:28:22,380 --> 00:28:26,255
然后动作将是来自 20 的相应动作，

532
00:28:26,255 --> 00:28:27,370
也就是向左移动。

533
00:28:29,340 --> 00:28:35,510
现在，我们可以将这个动作发送回环境，

534
00:28:35,740 --> 00:28:39,150
以游戏的形式，以执行下一步，

535
00:28:39,150 --> 00:28:41,480
当 agent 在这个环境中移动时，

536
00:28:41,890 --> 00:28:46,130
它不仅会得到来自游戏的新像素的响应，

537
00:28:46,450 --> 00:28:48,650
更重要的是，一些奖励信号，

538
00:28:48,700 --> 00:28:50,250
现在非常重要的是要记住，

539
00:28:50,250 --> 00:28:56,030
在乒乓球中或在 Atari Breakout 中的奖励信号是非常稀疏的，

540
00:28:56,030 --> 00:29:01,720
你会得到奖励，不一定是基于你在这一时刻所采取的行动，

541
00:29:01,860 --> 00:29:06,460
球通常需要几个时间步骤才能回到屏幕顶部，

542
00:29:06,690 --> 00:29:09,160
所以，通常情况下，你的奖励会非常延迟，

543
00:29:09,450 --> 00:29:11,540
可能至少会延迟几个时间步长，

544
00:29:11,540 --> 00:29:15,070
如果你在屏幕的角落里弹跳，有时甚至会延迟更多。

545
00:29:16,430 --> 00:29:22,150
一种非常流行或非常著名的方法来展示这一点，

546
00:29:22,150 --> 00:29:26,335
是由 Google Deepmind 在几年前提出的，

547
00:29:26,335 --> 00:29:29,070
他们展示了你可以训练 Q 值网络，

548
00:29:29,090 --> 00:29:31,140
你可以看到左侧的输入，

549
00:29:31,340 --> 00:29:33,810
只是来自屏幕的原始像素，

550
00:29:34,190 --> 00:29:38,370
一直到右侧控制器的动作，

551
00:29:38,960 --> 00:29:43,140
你可以训练这个网络来完成各种不同的任务，

552
00:29:43,340 --> 00:29:47,220
对于整个 Atari Breakout 游戏生态系统。

553
00:29:47,910 --> 00:29:49,450
对于这些任务中的每一个，

554
00:29:49,740 --> 00:29:51,965
他们展示的真正令人着迷的是，

555
00:29:51,965 --> 00:29:53,630
对于这个非常简单的算法，

556
00:29:53,630 --> 00:29:57,070
它真的依赖于随机选择动作，

557
00:29:57,150 --> 00:30:00,860
然后，从做得不是很好的动作中学习，

558
00:30:00,860 --> 00:30:05,560
你阻止他们，尝试做那些执行得更好的动作，

559
00:30:06,090 --> 00:30:07,160
非常简单的算法，

560
00:30:07,160 --> 00:30:09,730
但他们发现即使这种类型的算法，

561
00:30:10,160 --> 00:30:14,260
他们在比赛的一半以上时间里都超过了人类的水平，

562
00:30:14,340 --> 00:30:18,310
你可以在这里看到一些游戏的表现仍然低于人类的水平，

563
00:30:18,600 --> 00:30:19,340
但正如我们将看到的，

564
00:30:19,340 --> 00:30:22,390
这真的是一个如此令人兴奋的进步，

565
00:30:22,470 --> 00:30:24,580
因为算法很简单，

566
00:30:24,600 --> 00:30:28,000
你是如何清理训练的公式的，

567
00:30:28,260 --> 00:30:31,180
你只需要将非常少量的先验知识，

568
00:30:31,680 --> 00:30:33,550
强加给这个神经网络，

569
00:30:33,660 --> 00:30:35,735
它就能够学习如何玩这些游戏，

570
00:30:35,735 --> 00:30:37,960
你从来没有教过任何游戏规则，

571
00:30:38,340 --> 00:30:40,515
你只需要让它探索它的环境，

572
00:30:40,515 --> 00:30:42,590
与它自己玩这个游戏很多很多次，

573
00:30:42,730 --> 00:30:44,810
然后直接从这些数据中学习。

574
00:30:47,270 --> 00:30:50,695
Q 学习有几个非常重要的缺点，

575
00:30:50,695 --> 00:30:54,120
希望这些能激励我们今天课程的第二部分，

576
00:30:54,260 --> 00:30:55,320
也就是我们要讨论的。

577
00:30:55,490 --> 00:30:59,340
但我想在这里告诉大家的第一点是，

578
00:30:59,780 --> 00:31:05,170
Q 学习自然适用于离散的动作空间，

579
00:31:05,170 --> 00:31:08,560
因为你可以把我们提供的这个输出空间想象成

580
00:31:08,560 --> 00:31:11,910
每一个可以执行的操作有一个数字，

581
00:31:12,050 --> 00:31:14,010
现在，如果我们有一个连续的行动空间，

582
00:31:14,270 --> 00:31:16,660
我们必须想出聪明的方法来解决这个问题，

583
00:31:16,660 --> 00:31:19,920
事实上，现在有了更多的解决方案，

584
00:31:20,240 --> 00:31:22,740
来实现 Q 学习和连续的动作空间，

585
00:31:22,850 --> 00:31:24,360
但在很大程度上，

586
00:31:24,500 --> 00:31:27,960
Q 学习非常适合离散动作空间，

587
00:31:28,400 --> 00:31:32,040
稍后我们会讨论用其他方法克服这个问题的方法。

588
00:31:32,660 --> 00:31:35,700
这里的第二个组成部分是，

589
00:31:36,830 --> 00:31:39,055
我们正在学习的策略，

590
00:31:39,055 --> 00:31:42,030
Q 函数产生了该策略，

591
00:31:42,080 --> 00:31:45,300
这是我们用来确定要采取什么行动的东西，

592
00:31:45,410 --> 00:31:46,560
给出任何状态，

593
00:31:46,880 --> 00:31:54,540
策略是通过优化 Q 函数来确定的，

594
00:31:54,590 --> 00:31:57,000
我们只需查看 Q 函数的结果，

595
00:31:57,590 --> 00:31:58,830
然后应用我们的，

596
00:31:59,030 --> 00:32:00,985
或者我们查看 Q 函数的结果，

597
00:32:00,985 --> 00:32:04,740
然后选择 Q 值最好或最高的操作，

598
00:32:05,340 --> 00:32:08,170
这在许多情况下是非常危险的，

599
00:32:08,430 --> 00:32:12,665
因为它总是为给定的状态挑选最佳的值，

600
00:32:12,665 --> 00:32:14,890
在管道中没有随机性，

601
00:32:15,000 --> 00:32:18,130
所以你经常会遇到这样的情况，

602
00:32:18,510 --> 00:32:20,110
你一直在重复相同的动作，

603
00:32:20,130 --> 00:32:25,090
而你没有学会探索你可能正在考虑的潜在的不同选择。

604
00:32:25,380 --> 00:32:28,210
因此，为了应对这些非常重要的挑战，

605
00:32:28,890 --> 00:32:32,230
这有望推动今天课程的下一部分，

606
00:32:32,610 --> 00:32:34,870
重点是策略学习，

607
00:32:35,040 --> 00:32:41,140
这是一种不同于 Q 学习算法的强化学习算法。

608
00:32:41,600 --> 00:32:44,330
就像我说的，这些算法被称为策略梯度算法，

609
00:32:44,330 --> 00:32:46,870
策略梯度算法，主要的不同之处在于，

610
00:32:47,370 --> 00:32:51,250
不是试图从 Q 函数中推断出策略，

611
00:32:51,480 --> 00:32:52,970
我们只需要构建一个神经网络，

612
00:32:52,970 --> 00:32:56,740
直接从数据中学习策略函数，

613
00:32:56,970 --> 00:32:58,720
所以它跳过了一步，

614
00:32:58,920 --> 00:33:00,760
我们将看看如何训练这些网络。

615
00:33:03,170 --> 00:33:04,330
所以在我们到达那里之前，

616
00:33:04,330 --> 00:33:06,010
让我再回顾一次，

617
00:33:06,010 --> 00:33:09,055
我们看到的 Q 函数，

618
00:33:09,055 --> 00:33:13,320
Q 函数，我们试图构建一个神经网络输出这些 Q 值，

619
00:33:13,610 --> 00:33:14,730
每个操作一个值，

620
00:33:15,290 --> 00:33:19,590
我们通过查看 Q 值的这种状态来确定策略，

621
00:33:19,970 --> 00:33:21,720
选择具有最高值的值，

622
00:33:22,500 --> 00:33:24,320
看看它对应的动作，

623
00:33:25,120 --> 00:33:26,570
现在通过策略网络，

624
00:33:26,920 --> 00:33:29,265
我们想要保留的想法是，

625
00:33:29,265 --> 00:33:31,760
不是预测 Q 值本身，

626
00:33:32,350 --> 00:33:35,520
而是直接尝试优化这一策略函数，

627
00:33:35,520 --> 00:33:38,370
这里我们叫策略函数 π(s) ，

628
00:33:38,370 --> 00:33:41,000
所以 π 是策略， s 是我们的状态，

629
00:33:41,140 --> 00:33:44,000
所以它是一个只接受状态作为输入的函数，

630
00:33:44,140 --> 00:33:46,370
它将直接输出动作，

631
00:33:47,200 --> 00:33:50,270
所以这里的输出给了我们应该采取的行动，

632
00:33:50,290 --> 00:33:53,780
我们在任何给定的状态下应该采取的，

633
00:33:54,190 --> 00:33:58,545
这不仅代表了我们应该采取的最佳行动，

634
00:33:58,545 --> 00:34:03,470
但是让我们把这个选择行动的概率，

635
00:34:04,000 --> 00:34:06,860
将产生非常理想的结果，

636
00:34:07,280 --> 00:34:08,215
对于我们的网络，

637
00:34:08,215 --> 00:34:12,060
所以不一定是该操作的值，

638
00:34:12,530 --> 00:34:18,210
而是选择该操作将是最高值的概率，

639
00:34:18,290 --> 00:34:19,765
所以，你并不关心，

640
00:34:19,765 --> 00:34:25,050
选择此操作所采取或产生的确切数值是多少，

641
00:34:25,520 --> 00:34:29,215
而是，可能性是什么，

642
00:34:29,215 --> 00:34:33,900
选择此操作将为你提供预期的最佳值是多少，

643
00:34:34,670 --> 00:34:37,075
确切的值本身并不重要，

644
00:34:37,075 --> 00:34:42,030
你只关心选择此操作是否会给您带来最好的结果。

645
00:34:43,570 --> 00:34:47,445
所以我们可以看到，如果这些预测的概率在这里，

646
00:34:47,445 --> 00:34:49,040
在这个 Atari 的例子中，

647
00:34:50,560 --> 00:34:58,190
向左移动的概率是最高值动作， 90% ，

648
00:34:58,720 --> 00:35:03,140
留在中间的概率为 10% ，

649
00:35:03,610 --> 00:35:05,000
向右移动为 0% ，

650
00:35:05,380 --> 00:35:08,025
在理想情况下，我们的神经网络应该做的是，

651
00:35:08,025 --> 00:35:11,420
在这种情况下， 90% 的时间向左，

652
00:35:11,560 --> 00:35:14,790
10% 的时间，它仍然可以保持在原来的位置，

653
00:35:14,790 --> 00:35:16,610
但永远不应该向右，

654
00:35:17,380 --> 00:35:21,015
现在注意，这是一个概率分布，

655
00:35:21,015 --> 00:35:22,620
这与 Q 函数有很大的不同，

656
00:35:22,620 --> 00:35:25,890
Q 函数实际上没有结构，

657
00:35:25,890 --> 00:35:29,150
Q 值本身可以取任何实数，

658
00:35:29,740 --> 00:35:35,030
但在这里，策略网络有非常严格的输出，

659
00:35:35,410 --> 00:35:39,020
输出中的所有数字都必须求和为 1 ，

660
00:35:39,070 --> 00:35:41,810
因为这是一个概率分布，

661
00:35:41,980 --> 00:35:45,620
这给了它一个非常严格的版本，

662
00:35:46,270 --> 00:35:47,630
我们如何训练这个模型，

663
00:35:47,770 --> 00:35:51,380
使它比 Q 函数更容易训练。

664
00:35:53,760 --> 00:36:00,280
输出是概率分布的另一个非常重要的优点是，

665
00:36:00,450 --> 00:36:05,860
将与我们之前看到的 Q 函数和 Q 神经网络的另一个问题联系在一起，

666
00:36:06,150 --> 00:36:11,950
这是 Q 函数自然适合离散动作空间的事实，

667
00:36:12,210 --> 00:36:14,890
现在，当我们看这个策略网络时，

668
00:36:15,300 --> 00:36:17,270
我们考虑的是分布，

669
00:36:17,270 --> 00:36:21,635
这些分布也可以采取连续的形式，

670
00:36:21,635 --> 00:36:24,280
事实上，我们在上两节课中已经看到了这一点，

671
00:36:24,960 --> 00:36:26,345
在生成性课程中，

672
00:36:26,345 --> 00:36:31,750
我们了解了如何使用 VAE 来预测其潜在空间上的高斯分布，

673
00:36:31,800 --> 00:36:35,200
在上一节课中，我们还学习了如何预测不确定性，

674
00:36:35,430 --> 00:36:39,100
这些使用数据的不确定性是连续的概率分布，

675
00:36:40,080 --> 00:36:41,080
就像这样，

676
00:36:41,280 --> 00:36:44,920
我们也可以使用同样的公式，

677
00:36:45,060 --> 00:36:47,615
来超越离散的行动空间，

678
00:36:47,615 --> 00:36:48,700
就像你在这里看到的，

679
00:36:48,780 --> 00:36:51,370
这是一个可能的行动，

680
00:36:51,420 --> 00:36:53,770
一个与一个可能的行动相关的概率，

681
00:36:54,140 --> 00:36:56,080
在一组离散的可能动作中，

682
00:36:56,880 --> 00:36:58,780
现在我们可能有一个空间，

683
00:36:58,800 --> 00:37:01,120
不是我应该采取什么动作，

684
00:37:01,140 --> 00:37:02,770
向左，向右，或者说是居中，

685
00:37:03,000 --> 00:37:05,225
而是我应该移动的速度有多快，

686
00:37:05,225 --> 00:37:06,950
我应该移动的方向，

687
00:37:06,950 --> 00:37:09,940
这是一个连续变量，而不是离散变量，

688
00:37:10,470 --> 00:37:13,690
你可以说现在的答案应该是这样的，

689
00:37:14,490 --> 00:37:17,620
向右移动的速度非常快或非常慢，

690
00:37:17,820 --> 00:37:20,500
抱歉，向左移动的速度非常快或非常慢，

691
00:37:20,970 --> 00:37:24,160
这是我们可能想要建模的连续谱。

692
00:37:25,160 --> 00:37:27,360
现在，当我们画出整个分布，

693
00:37:27,620 --> 00:37:30,060
采取行动，给出状态，

694
00:37:30,260 --> 00:37:34,000
你可以在这里看到一个非常简单的图解，

695
00:37:34,000 --> 00:37:38,910
这个分布的大部分质量都超过了，

696
00:37:39,350 --> 00:37:41,920
抱歉，它的所有质量都在整个实数线上，

697
00:37:41,920 --> 00:37:47,050
首先，它在我们想要采取的最优行动空间中拥有大部分的质量，

698
00:37:47,050 --> 00:37:49,470
所以，如果我们想要确定采取的最佳行动，

699
00:37:49,670 --> 00:37:52,560
我们将简单地采用这种分布模式，

700
00:37:52,790 --> 00:37:57,810
最高点，这将是我们应该前进的速度和我们应该前进的方向，

701
00:37:58,440 --> 00:38:02,830
如果我们也想尝试不同的东西，探索我们的空间，

702
00:38:03,060 --> 00:38:07,210
我们可以从这个分布中取样，仍然获得一些随机性。

703
00:38:09,480 --> 00:38:11,270
现在让我们来看一个例子，

704
00:38:11,270 --> 00:38:14,470
我们如何对这些连续分布进行建模，

705
00:38:14,520 --> 00:38:18,550
我们已经在前面两节课中看到了一些例子，就像我提到的，

706
00:38:18,900 --> 00:38:23,470
但让我们具体看看强化学习和策略梯度学习的背景。

707
00:38:24,060 --> 00:38:28,000
所以不是预测采取行动的概率，

708
00:38:28,050 --> 00:38:29,705
给出所有可能的状态，

709
00:38:29,705 --> 00:38:32,620
在这种情况下，现在有无限多的状态，

710
00:38:32,640 --> 00:38:34,060
因为我们处于连续的域中，

711
00:38:34,380 --> 00:38:38,020
我们不能简单地预测每个可能的行动的单一概率，

712
00:38:38,760 --> 00:38:40,300
因为它们的数量是无限的，

713
00:38:40,560 --> 00:38:46,865
因此，如果我们通过分布来参数化我们的动作空间，

714
00:38:46,865 --> 00:38:48,940
因此，让我们以高斯分布为例，

715
00:38:49,980 --> 00:38:52,060
将高斯分布参数化，

716
00:38:52,320 --> 00:38:54,305
我们只需要两个输出，

717
00:38:54,305 --> 00:38:55,870
我们需要一个均值和一个方差，

718
00:38:55,920 --> 00:38:57,310
给定均值和方差，

719
00:38:57,450 --> 00:38:59,320
我们可以有一个概率质量，

720
00:38:59,430 --> 00:39:02,800
我们可以计算任何可能的行动的概率，

721
00:39:02,940 --> 00:39:05,140
我们可能想要从这两个数字，

722
00:39:05,760 --> 00:39:07,420
例如，在这里的图像中，

723
00:39:07,710 --> 00:39:11,300
我们可能想要输出一个像这样的高斯分布，

724
00:39:11,300 --> 00:39:16,300
它的平均值以 -0.8 为中心，

725
00:39:16,500 --> 00:39:23,440
表示我们应该以每秒 0.8 米的速度向左移动，

726
00:39:23,940 --> 00:39:25,300
我们再一次看到，

727
00:39:25,920 --> 00:39:28,180
因为这是一个概率分布，

728
00:39:28,320 --> 00:39:33,245
因为策略网络的格式，

729
00:39:33,245 --> 00:39:35,440
我们强制要求这是一个概率分布，

730
00:39:35,850 --> 00:39:40,510
这意味着这个输出的积分，

731
00:39:40,620 --> 00:39:44,410
按照它是高斯的定义，也必须积分为 1 。

732
00:39:47,890 --> 00:39:48,405
好的，太好了，

733
00:39:48,405 --> 00:39:54,080
现在让我们来看看如何训练策略梯度网络，

734
00:39:54,460 --> 00:39:56,870
逐步完成这个过程，

735
00:39:57,220 --> 00:39:59,720
我们来看看一个非常具体的例子，

736
00:39:59,770 --> 00:40:04,130
让我们从回顾强化学习循环开始，

737
00:40:04,180 --> 00:40:05,870
这节课就是从这里开始的。

738
00:40:06,520 --> 00:40:11,840
现在，让我们具体考虑训练自动驾驶车辆的例子，

739
00:40:11,860 --> 00:40:15,120
因为我认为这是一个特别非常直观的例子，

740
00:40:15,120 --> 00:40:16,190
我们可以讨论的，

741
00:40:16,450 --> 00:40:19,550
这里的 agent 是车，

742
00:40:20,010 --> 00:40:23,150
状态可以通过很多传感器来获得，

743
00:40:23,150 --> 00:40:25,460
安装在车辆身上，

744
00:40:25,460 --> 00:40:31,750
例如，自动驾驶车辆通常配备摄像头、激光雷达、雷达等传感器，

745
00:40:31,950 --> 00:40:35,980
所有这些都为车辆提供观测输入，

746
00:40:36,850 --> 00:40:39,680
我们可以采取的行动是方向盘角度，

747
00:40:39,910 --> 00:40:42,590
这不是一个离散变量。这是一个连续变量，

748
00:40:42,610 --> 00:40:44,990
这是一个可以取任何实数的角度，

749
00:40:45,820 --> 00:40:49,200
最后奖励，在这个非常简单的例子中，

750
00:40:49,200 --> 00:40:51,440
是我们在撞车前行驶的距离。

751
00:40:53,660 --> 00:40:55,225
好的，现在让我们来看看，

752
00:40:55,225 --> 00:40:58,290
如何训练一个策略梯度神经网络，

753
00:40:58,520 --> 00:41:00,960
来解决这个自动驾驶汽车的任务，

754
00:41:00,980 --> 00:41:02,160
作为一个具体的例子。

755
00:41:03,350 --> 00:41:07,560
我们从初始化 agent 开始，

756
00:41:07,790 --> 00:41:09,475
记住，我们没有训练数据，

757
00:41:09,475 --> 00:41:12,580
所以我们不得不思考，强化学习就像是，

758
00:41:12,580 --> 00:41:16,090
数据获取和学习的管道结合在一起，

759
00:41:16,090 --> 00:41:19,420
所以，数据采集管道的第一部分首先是，

760
00:41:19,420 --> 00:41:22,110
初始化我们的 agent ，以便收集一些数据。

761
00:41:24,510 --> 00:41:27,680
所以我们开始我们的车，我们的 agent ，

762
00:41:28,090 --> 00:41:30,740
当然，在一开始，它对驾驶一无所知，

763
00:41:30,760 --> 00:41:35,600
它以前从来没有接触过任何这些环境规则或观察，

764
00:41:35,650 --> 00:41:37,310
所以它运行它的策略，

765
00:41:37,480 --> 00:41:39,920
现在完全没有训练，

766
00:41:40,660 --> 00:41:44,000
直到它终止，直到它超出了我们定义的一些界限，

767
00:41:44,260 --> 00:41:49,850
我们是用它在终止前行驶的距离来衡量奖励的，

768
00:41:50,670 --> 00:41:53,860
我们记录了所有的状态，所有的行动，

769
00:41:54,390 --> 00:41:59,230
和直到终止的最终的奖励，

770
00:41:59,280 --> 00:42:01,780
这将成为我们的迷你数据集，

771
00:42:01,800 --> 00:42:03,940
我们将在第一轮训练中使用它，

772
00:42:04,620 --> 00:42:06,100
让我们来看看这些数据集，

773
00:42:06,240 --> 00:42:08,650
现在我们来做一个步骤的训练，

774
00:42:08,700 --> 00:42:11,255
我们要做的训练的第一步是，

775
00:42:11,255 --> 00:42:19,520
使用我们的 agent 运行的轨迹的后半部分，

776
00:42:19,990 --> 00:42:24,735
并减少导致低回报的行动的可能性，

777
00:42:24,735 --> 00:42:27,620
现在，因为我们知道车终止了，

778
00:42:28,150 --> 00:42:29,555
我们可以假设，

779
00:42:29,555 --> 00:42:35,720
在这个轨迹的后半部分发生的所有动作都可能不是非常好的动作，

780
00:42:35,860 --> 00:42:37,550
因为它们非常接近终止，

781
00:42:38,110 --> 00:42:41,660
所以，让我们降低所有这些事情在未来再次发生的可能性，

782
00:42:41,950 --> 00:42:46,040
我们将考虑在我们的训练的前半部分发生的所有事情，

783
00:42:46,620 --> 00:42:48,680
我们会增加他们的概率，

784
00:42:48,970 --> 00:42:50,900
现在，再说一次，我们没有理由，

785
00:42:51,010 --> 00:42:57,510
不在这个轨迹的前半部分采取好的行动，

786
00:42:57,510 --> 00:42:58,880
而在后半部分采取糟糕的行动，

787
00:42:59,080 --> 00:43:05,760
但这只是因为后半部分的行动更接近失败，更接近终止，

788
00:43:05,760 --> 00:43:10,430
我们可以假设，这些行动可能是次优行动，

789
00:43:10,870 --> 00:43:14,295
但这些也很可能是嘈杂的奖励，

790
00:43:14,295 --> 00:43:15,680
因为它是如此稀少的信号，

791
00:43:16,000 --> 00:43:18,690
很可能你在最后采取了一些好的行动，

792
00:43:18,690 --> 00:43:20,480
你实际上是在试图恢复你的车，

793
00:43:20,500 --> 00:43:21,890
但你已经太晚了。

794
00:43:24,180 --> 00:43:25,960
现在再次重复这个过程，

795
00:43:26,010 --> 00:43:29,585
再次重新初始化 agent 并运行它，直到完成，

796
00:43:29,585 --> 00:43:31,580
现在 agent 走得更远了一点，

797
00:43:31,580 --> 00:43:33,520
因为你已经降低了末端的概率，

798
00:43:33,570 --> 00:43:35,050
增加了未来的概率，

799
00:43:35,430 --> 00:43:38,200
你不断重复这一过程，

800
00:43:38,460 --> 00:43:43,030
直到你注意到 agent 每次都学习执行得越来越好，

801
00:43:43,110 --> 00:43:44,435
直到它最终收敛，

802
00:43:44,435 --> 00:43:49,570
最后， agent 基本上能够沿着车道行驶，

803
00:43:50,070 --> 00:43:54,675
通常在这样做的过程中会稍微左右转向，而不会撞车，

804
00:43:54,675 --> 00:43:56,900
这真的很有趣，

805
00:43:57,160 --> 00:43:59,340
因为这是一辆自动驾驶汽车，

806
00:43:59,340 --> 00:44:01,485
我们从来没有教过任何东西，

807
00:44:01,485 --> 00:44:02,840
车道标志是什么意思，

808
00:44:02,890 --> 00:44:04,340
或者道路规则是什么，

809
00:44:04,900 --> 00:44:05,930
任何这样的东西，

810
00:44:05,950 --> 00:44:09,285
这是一辆完全通过外出学习的赛车，

811
00:44:09,285 --> 00:44:15,320
经常撞车，试图弄清楚如何做才能在未来不再继续这样做。

812
00:44:16,070 --> 00:44:20,220
剩下的问题是我们如何更新策略，

813
00:44:20,450 --> 00:44:24,330
作为我在左边和右边展示给你们的这个算法的一部分，

814
00:44:24,560 --> 00:44:28,320
例如，我们如何才能制定出相同的算法，

815
00:44:28,520 --> 00:44:32,580
具体地说，就是更新方程式，这里第四步和第五步，

816
00:44:32,720 --> 00:44:34,500
这是两个重要的步骤，

817
00:44:34,820 --> 00:44:37,830
我们如何使用这两个步骤来训练我们的策略，

818
00:44:38,270 --> 00:44:40,180
并降低发生坏事件的可能性，

819
00:44:40,180 --> 00:44:43,050
同时促进所有这些好事件的可能性。

820
00:44:44,870 --> 00:44:48,150
那么让我们假设，让我们来看看损失函数，

821
00:44:48,290 --> 00:44:53,440
首先，策略梯度神经网络的损失函数是这样的，

822
00:44:53,440 --> 00:44:55,380
然后我们将从剖析它开始，

823
00:44:55,610 --> 00:44:58,560
以了解为什么它是这样工作的，

824
00:44:59,180 --> 00:45:01,915
所以在这里我们可以看到，损失由两项组成，

825
00:45:01,915 --> 00:45:04,020
第一项是绿色的项，

826
00:45:04,620 --> 00:45:08,750
被称为选择特定行动的对数概率，

827
00:45:09,340 --> 00:45:12,285
第二项是你们都非常熟悉的东西，

828
00:45:12,285 --> 00:45:15,740
这只是一个特定时间的回报，

829
00:45:16,180 --> 00:45:20,180
所以这是在这个时间点之后你将获得的预期回报。

830
00:45:21,200 --> 00:45:25,750
现在，让我们假设我们从特定操作种获得到大量奖励，

831
00:45:26,130 --> 00:45:30,310
有高的损失概率，

832
00:45:30,930 --> 00:45:34,900
如果我们从概率很高的特定行动中获得了很多奖励，

833
00:45:35,340 --> 00:45:38,420
这意味着我们想要进一步增加这种可能性，

834
00:45:38,420 --> 00:45:41,615
所以我们做的可能性更大，甚至更有可能，

835
00:45:41,615 --> 00:45:43,990
我们在未来再次采样这种行动。

836
00:45:45,060 --> 00:45:47,470
另一方面，如果我们选择，

837
00:45:47,880 --> 00:45:50,560
或比方说，如果我们获得的奖励，

838
00:45:50,940 --> 00:45:54,070
对于一个可能性很高的行动来说是非常低的，

839
00:45:54,210 --> 00:45:55,570
我们想要相反的效果，

840
00:45:56,040 --> 00:45:58,610
我们不想在未来再次尝试这种行为，

841
00:45:58,610 --> 00:46:01,310
因为它导致了低回报，

842
00:46:01,310 --> 00:46:04,300
你们会注意到，这个损失函数，

843
00:46:04,590 --> 00:46:05,860
通过包括这个负值，

844
00:46:06,240 --> 00:46:13,840
我们将最小化在这个轨道上实现任何回报较低的行动的可能性，

845
00:46:14,310 --> 00:46:18,180
现在在我们的简化汽车例子中，

846
00:46:18,590 --> 00:46:20,365
所有奖励较低的东西，

847
00:46:20,365 --> 00:46:26,640
都是那些最接近车辆终止部分的行为，

848
00:46:26,990 --> 00:46:29,580
所有有高回报的东西都是一开始就来的，

849
00:46:29,930 --> 00:46:33,060
这只是我们在定义回报结构时所做的假设。

850
00:46:34,130 --> 00:46:41,440
现在我们可以将其插入到损失梯度下降算法中来训练我们的神经网络，

851
00:46:41,440 --> 00:46:44,575
当我们看到这个策略梯度算法，

852
00:46:44,575 --> 00:46:46,015
你可以在这里突出显示，

853
00:46:46,015 --> 00:46:51,160
这种梯度恰恰是神经网络的策略部分，

854
00:46:51,160 --> 00:46:53,220
这是选择一个动作的概率，

855
00:46:53,630 --> 00:46:55,150
甚至是一个特定的状态，

856
00:46:55,150 --> 00:46:56,950
如果你还记得我们之前定义的，

857
00:46:56,950 --> 00:46:59,725
作为一个策略函数意味着什么，

858
00:46:59,725 --> 00:47:01,420
这就是它的意思，

859
00:47:01,420 --> 00:47:04,260
给定你发现自己所处的特定状态，

860
00:47:04,580 --> 00:47:09,180
选择具有最高可能性的特定操作的概率是多少，

861
00:47:09,650 --> 00:47:12,310
这就是这个方法得名的原因，

862
00:47:12,310 --> 00:47:16,230
你可以在这里看到这个策略梯度部分。

863
00:47:18,040 --> 00:47:23,600
现在，我想在这节课快结束的时候花很短的时间来谈谈，

864
00:47:23,950 --> 00:47:28,155
和今天的第一节课保持一致的一些挑战，

865
00:47:28,155 --> 00:47:34,520
在现实世界中部署这些类型的算法的一些挑战。

866
00:47:35,060 --> 00:47:38,560
当你看到这个训练算法时，你会怎么想，

867
00:47:39,030 --> 00:47:42,190
你认为这种训练算法的缺点是什么，

868
00:47:42,540 --> 00:47:47,920
具体是哪一步，如果我们想要将这种方法部署到现实中？

869
00:47:52,910 --> 00:47:55,500
是的，就是这样，所以是第二步。

870
00:47:55,850 --> 00:47:58,840
如果你想在现实中做到这一点，

871
00:47:58,840 --> 00:48:01,290
那意味着你想出去，

872
00:48:01,310 --> 00:48:03,840
取走你的车，把它撞上很多次，

873
00:48:03,890 --> 00:48:07,375
就是为了学习如何不撞它，

874
00:48:07,375 --> 00:48:11,125
第一，而这是根本不可行的，

875
00:48:11,125 --> 00:48:13,950
第二，这也是非常危险的。

876
00:48:15,440 --> 00:48:18,115
所以有办法绕过这一点，

877
00:48:18,115 --> 00:48:20,380
绕过这一问题的第一种方法是，

878
00:48:20,380 --> 00:48:24,720
人们试图在模拟中训练这些类型的模型，

879
00:48:24,800 --> 00:48:26,340
模拟是非常安全的，

880
00:48:26,510 --> 00:48:29,880
因为我们实际上不会破坏任何真实的东西，

881
00:48:30,260 --> 00:48:31,960
它的效率仍然很低，

882
00:48:31,960 --> 00:48:33,840
因为我们不得不多次运行这些算法，

883
00:48:34,130 --> 00:48:35,605
并多次使它们撞车，

884
00:48:35,605 --> 00:48:36,870
只是学习不要撞车，

885
00:48:37,100 --> 00:48:39,750
但至少现在从安全的角度来看，

886
00:48:40,220 --> 00:48:41,020
这样安全多了。

887
00:48:41,950 --> 00:48:47,220
但是，问题是用于强化学习的现代模拟引擎，

888
00:48:47,220 --> 00:48:52,820
一般来说，特别是视觉的现代模拟器，

889
00:48:53,080 --> 00:48:56,685
根本不能非常准确地捕捉到现实，

890
00:48:56,685 --> 00:49:02,750
事实上，有一个非常著名的概念叫做 sim-to-real 鸿沟，

891
00:49:03,010 --> 00:49:04,550
这是存在的鸿沟，

892
00:49:04,570 --> 00:49:06,830
当你在模拟中训练算法时，

893
00:49:07,180 --> 00:49:09,135
它们不会延伸到，

894
00:49:09,135 --> 00:49:13,010
我们在现实中看到的许多现象和模式。

895
00:49:13,720 --> 00:49:16,870
我想在这里强调的一个非常酷的结果是，

896
00:49:16,870 --> 00:49:18,970
当我们训练强化学习算法时，

897
00:49:18,970 --> 00:49:22,495
我们最终希望它们不是在模拟中运行。

898
00:49:22,495 --> 00:49:23,970
我们希望它们是真实存在的，

899
00:49:24,650 --> 00:49:27,970
作为我们 MIT 实验的一部分，

900
00:49:27,970 --> 00:49:32,940
我们一直在开发这种非常、非常酷的全新照片、逼真的模拟引擎，

901
00:49:33,080 --> 00:49:37,740
它超越了当今模拟器工作的模式，

902
00:49:37,760 --> 00:49:40,440
基本上就是定义一个环境模型，

903
00:49:40,910 --> 00:49:43,920
并试图合成这个模型，

904
00:49:44,270 --> 00:49:47,620
从本质上讲，这些模拟器就像是美化了的游戏引擎，

905
00:49:47,620 --> 00:49:50,280
当你看着他们的时候，他们看起来都很像游戏，

906
00:49:50,660 --> 00:49:54,720
但我们所做的一件事是采用数据驱动的方法，

907
00:49:54,740 --> 00:49:56,760
使用真实世界的真实数据，

908
00:49:57,050 --> 00:50:02,965
我们可以建立一个超逼真的合成环境，看起来就像这样，

909
00:50:02,965 --> 00:50:06,510
这是我们 MIT 创造的一个很酷的结果，

910
00:50:07,130 --> 00:50:09,520
开发照片真实感模拟引擎时，

911
00:50:09,520 --> 00:50:11,100
这实际上是一个自动 agent ，

912
00:50:11,600 --> 00:50:15,460
而不是一辆真正的汽车通过我们的虚拟模拟器，

913
00:50:15,460 --> 00:50:17,910
一堆不同类型的不同场景，

914
00:50:18,230 --> 00:50:19,825
所以这个模拟器被称为 VISTA ，

915
00:50:19,825 --> 00:50:23,730
它允许我们使用我们在真实世界中收集的真实数据，

916
00:50:24,050 --> 00:50:26,940
然后重新模拟那些相同的真实道路，

917
00:50:27,020 --> 00:50:28,735
例如，假设你开着你的车，

918
00:50:28,735 --> 00:50:31,680
开着[质量]的车，收集[质量]数据，

919
00:50:31,940 --> 00:50:36,600
现在你可以把一个虚拟 agent 放到同一个模拟环境中，

920
00:50:37,220 --> 00:50:40,900
观察那个场景可能是什么样子的新视角，

921
00:50:40,900 --> 00:50:43,230
从不同类型的扰动，

922
00:50:43,400 --> 00:50:47,400
或它可能暴露的角度。

923
00:50:47,800 --> 00:50:52,430
这允许我们完全使用强化学习来训练这些 agent ，

924
00:50:52,630 --> 00:50:53,810
没有人类的标签，

925
00:50:54,430 --> 00:50:57,830
但重要的是允许它们转移到现实中，

926
00:50:57,940 --> 00:51:00,620
因为不再有 sim-to-real 鸿沟。

927
00:51:00,790 --> 00:51:03,140
所以事实上，我们就是这么做的，

928
00:51:03,280 --> 00:51:05,895
我们将 agent 放入我们的模拟器中，

929
00:51:05,895 --> 00:51:08,510
我们使用确切的算法，

930
00:51:08,680 --> 00:51:11,990
你们在今天的课程中学到的策略梯度算法，

931
00:51:12,250 --> 00:51:15,440
所有的训练都完全在模拟中完成，

932
00:51:15,910 --> 00:51:17,150
然后我们采取了这些策略，

933
00:51:17,260 --> 00:51:20,630
并将它们部署在我们的全尺寸自动驾驶汽车上，

934
00:51:20,740 --> 00:51:23,330
这现在是在真实世界中，不再是在模拟中，

935
00:51:23,860 --> 00:51:25,970
在左边你可以看到，

936
00:51:26,850 --> 00:51:32,170
基本上这辆车在现实世界中完全自主地通过这个环境，

937
00:51:32,220 --> 00:51:34,420
没有迁移学习在这里做，

938
00:51:34,500 --> 00:51:38,225
不存在从真实世界数据中增加数据的情况，

939
00:51:38,225 --> 00:51:40,715
这完全是使用模拟来训练的，

940
00:51:40,715 --> 00:51:43,030
这是有史以来第一次，

941
00:51:43,410 --> 00:51:47,320
使用强化学习来端到端地训练策略，

942
00:51:47,760 --> 00:51:50,650
我们是一辆可以在现实中部署的自动驾驶汽车，

943
00:51:50,790 --> 00:51:54,700
这是我们在 MIT 创造的很酷的东西。

944
00:51:54,870 --> 00:52:00,790
但现在我们已经讨论了强化学习和策略学习的所有基础，

945
00:52:01,440 --> 00:52:06,160
我想谈谈我们正在看到的其他一些可能非常令人兴奋的应用，

946
00:52:06,510 --> 00:52:08,840
一个受欢迎的应用，

947
00:52:08,840 --> 00:52:12,340
很多人都会告诉你和谈论它的游戏是围棋。

948
00:52:12,940 --> 00:52:15,615
所以在这里，强化学习 agent 可以用来，

949
00:52:15,615 --> 00:52:21,560
对抗围棋大师级别的棋手的测试，

950
00:52:22,180 --> 00:52:26,145
当时取得了令人难以置信的令人印象深刻的结果，

951
00:52:26,145 --> 00:52:28,995
所以，对于那些不熟悉围棋游戏的人来说，

952
00:52:28,995 --> 00:52:32,750
围棋是在 19x19 的棋盘上玩的，

953
00:52:33,130 --> 00:52:35,565
围棋的粗略目标是，

954
00:52:35,565 --> 00:52:39,225
比你的对手拥有更多的棋子，

955
00:52:39,225 --> 00:52:43,245
通过这个网格，

956
00:52:43,245 --> 00:52:45,590
你可以在这里看到这个 19x19 的网格，

957
00:52:46,140 --> 00:52:50,720
虽然游戏本身，逻辑规则，实际上非常简单，

958
00:52:51,010 --> 00:52:56,295
但这块棋盘可以放置的可能行动空间和可能状态的数量

959
00:52:56,295 --> 00:53:00,110
比宇宙中的原子数量更多，

960
00:53:00,280 --> 00:53:05,180
因此，这个游戏，尽管规则在逻辑定义上非常简单，

961
00:53:05,830 --> 00:53:11,180
但对于一个人工算法来说，它是一个非常复杂的游戏。

962
00:53:11,860 --> 00:53:17,190
所以这里的目标是建立一种强化学习算法来掌握围棋游戏，

963
00:53:17,240 --> 00:53:21,295
不仅击败这些黄金标准软件，

964
00:53:21,295 --> 00:53:27,130
而且在当时看起来像是一个惊人的结果是击败了大师级的棋手，

965
00:53:27,130 --> 00:53:32,340
围棋世界排名第一的棋手显然是人类冠军。

966
00:53:33,010 --> 00:53:35,300
所以 Google DeepMind 应对了这个挑战，

967
00:53:35,950 --> 00:53:39,920
他们几年前开发了这个解决方案，

968
00:53:40,240 --> 00:53:46,250
它非常基于你们在今天的课程中学到的完全相同的算法，

969
00:53:46,690 --> 00:53:51,560
将这个网络的价值部分和[残留]层结合在一起，

970
00:53:51,910 --> 00:53:54,410
我们将在明天的下一节课中讨论，

971
00:53:55,390 --> 00:53:57,440
而使用强化学习管道，

972
00:53:58,090 --> 00:54:01,380
他们能够击败人类玩家的大冠军，

973
00:54:01,380 --> 00:54:03,950
而其核心的想法其实很简单，

974
00:54:04,030 --> 00:54:06,855
第一步是你训练一个神经网络，

975
00:54:06,855 --> 00:54:09,830
基本上观看人类水平的专家，

976
00:54:10,420 --> 00:54:13,880
所以这不是使用强化学习，使用监督学习，

977
00:54:14,140 --> 00:54:16,910
使用我们在第一课、第二课和第三课中讨论的技术。

978
00:54:17,460 --> 00:54:19,200
从第一步开始，

979
00:54:19,200 --> 00:54:20,960
我们的目标是建立一个类似于一个策略，

980
00:54:21,130 --> 00:54:25,200
它将模仿人类玩家，

981
00:54:25,200 --> 00:54:26,960
或人类特级大师采取的，

982
00:54:27,190 --> 00:54:30,650
根据给定的棋盘，他们可能采取的行动，

983
00:54:31,120 --> 00:54:33,290
但给出这个预先训练的模型，

984
00:54:33,340 --> 00:54:38,000
你可以用它来引导和强化学习算法，

985
00:54:38,170 --> 00:54:40,400
这将与它自己的竞争，

986
00:54:41,130 --> 00:54:45,875
为了学习如何超越人类的水平来提高，

987
00:54:45,875 --> 00:54:47,825
所以，它需要人类的理解，

988
00:54:47,825 --> 00:54:49,625
首先试图模仿人类，

989
00:54:49,625 --> 00:54:50,770
但通过这种模仿，

990
00:54:51,390 --> 00:54:53,920
它们会将这两个神经网络与他们自己联系起来，

991
00:54:54,180 --> 00:54:55,370
与他们自己玩游戏，

992
00:54:55,370 --> 00:54:57,950
获胜者将获得奖励，

993
00:54:57,950 --> 00:55:04,210
失败者会试图否定他们可能从人类同行那里获得的所有行动，

994
00:55:04,410 --> 00:55:08,050
并试图实际学习新类型的规则和新类型的行动，

995
00:55:08,250 --> 00:55:12,880
可能对实现超人表现非常有益。

996
00:55:12,900 --> 00:55:19,790
使这个想法成为可能的一个非常重要的辅助技巧是，

997
00:55:19,790 --> 00:55:21,400
使用第二个网络，

998
00:55:21,420 --> 00:55:25,570
这个辅助网络将棋盘的状态作为输入，

999
00:55:25,830 --> 00:55:35,090
并试图预测，从这个特定的状态可能出现的所有不同的棋盘状态，

1000
00:55:35,090 --> 00:55:36,875
它们的值是什么，

1001
00:55:36,875 --> 00:55:39,620
他们的潜在回报和结果会是什么，

1002
00:55:39,620 --> 00:55:41,760
所以这个网络是一个辅助网络，

1003
00:55:42,080 --> 00:55:43,950
几乎是幻觉，

1004
00:55:44,330 --> 00:55:47,880
不同的棋盘状态，它可以从这个特定的状态中采取行动，

1005
00:55:48,260 --> 00:55:52,740
并使用这些预测值来指导它的规划，

1006
00:55:52,880 --> 00:55:55,320
它应该在未来采取什么行动。

1007
00:55:55,790 --> 00:55:58,020
最后，就在最近，

1008
00:55:58,400 --> 00:55:59,740
他们扩展了这个算法，

1009
00:55:59,740 --> 00:56:05,230
表明它们甚至不能从一开始就使用人类大师来模仿，

1010
00:56:05,230 --> 00:56:06,415
引导这些算法，

1011
00:56:06,415 --> 00:56:08,590
如果他们只是完全从头开始，

1012
00:56:08,590 --> 00:56:11,220
只有两个从未训练过的神经网络，

1013
00:56:11,600 --> 00:56:13,740
然后他们开始相互依附，

1014
00:56:13,970 --> 00:56:15,175
你实际上可以看到，

1015
00:56:15,175 --> 00:56:17,700
你可以在没有任何人类监督的情况下，

1016
00:56:18,460 --> 00:56:26,330
让神经网络学习不仅要超越超越人类的解决方案，

1017
00:56:26,800 --> 00:56:29,510
而且还要超越人类创造的解决方案，

1018
00:56:29,560 --> 00:56:32,300
后者也是由人类引导的。

1019
00:56:34,460 --> 00:56:38,140
到此为止，我将非常快速地总结我们今天所学的内容，

1020
00:56:38,140 --> 00:56:39,660
并结束这一天。

1021
00:56:40,160 --> 00:56:45,160
所以我们已经讨论了很多关于强化学习的基本算法，

1022
00:56:45,160 --> 00:56:48,330
我们看到了两种不同类型的强化学习方法，

1023
00:56:48,350 --> 00:56:50,070
来优化这些解决方案，

1024
00:56:50,660 --> 00:56:51,810
首先是 Q 学习，

1025
00:56:51,830 --> 00:56:53,845
我们试图实际估计，

1026
00:56:53,845 --> 00:56:55,080
给定一个状态，

1027
00:56:55,130 --> 00:56:58,345
我们可能期望的任何可能的行动的值是什么，

1028
00:56:58,345 --> 00:57:01,260
第二种方式是采取更端到端的方法，

1029
00:57:01,580 --> 00:57:04,480
并说，考虑到我们所处的状态，

1030
00:57:04,480 --> 00:57:06,600
我应该采取任何给定行动，

1031
00:57:07,160 --> 00:57:11,610
来最大化我在这种特定状态下的潜力。

1032
00:57:12,010 --> 00:57:14,805
我希望所有的这些对你们来说都非常令人兴奋，

1033
00:57:14,805 --> 00:57:16,820
我们有一个非常令人兴奋的实验，

1034
00:57:16,900 --> 00:57:18,800
竞赛开始了，

1035
00:57:19,510 --> 00:57:22,040
竞赛的最后期限会是，

1036
00:57:22,780 --> 00:57:25,040
原定是星期四，

1037
00:57:25,510 --> 00:57:27,950
也就是明天晚上 11 点。

1038
00:57:28,090 --> 00:57:28,670
谢谢。

