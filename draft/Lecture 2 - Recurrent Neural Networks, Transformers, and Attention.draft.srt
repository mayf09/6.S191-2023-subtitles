1
00:00:09,550 --> 00:00:11,175
0,400 420,785 785,1145 1145,1430 1430,1625
Hello everyone,| and I hope
大家好，|我希望你们喜欢 Alexander 的第一节课。

2
00:00:11,175 --> 00:00:13,100
0,305 325,725 955,1440 1440,1605 1605,1925
you enjoyed Alexander's first lecture.|
|

3
00:00:13,690 --> 00:00:15,405
0,335 335,665 665,970 1170,1460 1460,1715
I'm Ava,| and in this
我是 Ava ，|在第二节课中，

4
00:00:15,405 --> 00:00:17,340
0,285 285,570 570,930 930,1325 1615,1935
second lecture, lecture two,| we're
|我们将关注序列建模的问题，

5
00:00:17,340 --> 00:00:18,330
0,120 120,270 270,510 510,780 780,990
going to focus on this

6
00:00:18,330 --> 00:00:20,460
0,320 370,735 735,1095 1095,1700 1810,2130
question of sequence modeling,| how
|我们如何构建神经网络来处理和学习序列数据。

7
00:00:20,460 --> 00:00:21,560
0,180 180,330 330,555 555,840 840,1100
we can build neural networks

8
00:00:21,700 --> 00:00:23,295
0,275 275,470 470,790 1020,1325 1325,1595
that can handle and learn

9
00:00:23,295 --> 00:00:25,000
0,285 285,810 810,1085
from sequential data.|
|

10
00:00:25,370 --> 00:00:26,790
0,290 290,530 530,950 950,1115 1115,1420
So in Alexander's first lecture,|
在 Alexander 的第一节课中，|

11
00:00:27,260 --> 00:00:29,365
0,400 480,860 860,1240 1320,1880 1880,2105
he introduced the essentials of
他介绍了神经网络的基本原理，

12
00:00:29,365 --> 00:00:31,645
0,210 210,485 895,1290 1290,1620 1620,2280
neural networks,| starting with perceptons,
|从感知器，建立前馈模型开始，

13
00:00:31,645 --> 00:00:32,815
0,330 330,570 570,765 765,930 930,1170
building up to feed forward

14
00:00:32,815 --> 00:00:34,210
0,365 505,810 810,1005 1005,1155 1155,1395
models| and how you can
|以及如何训练这些模型，

15
00:00:34,210 --> 00:00:35,880
0,300 300,555 555,780 780,1070 1270,1670
actually train these models| and
|并开始考虑部署它们。

16
00:00:35,960 --> 00:00:37,150
0,290 290,425 425,575 575,860 860,1190
start to think about deploying

17
00:00:37,150 --> 00:00:39,310
0,165 165,470 1300,1695 1695,2025 2025,2160
them forward.| {Now\,,we're -} going
|现在我们将把注意力转向

18
00:00:39,310 --> 00:00:40,795
0,195 195,390 390,615 615,950 1090,1485
to turn our attention to|
|

19
00:00:40,795 --> 00:00:42,715
0,395 565,900 900,1140 1140,1445 1555,1920
specific types of problems that
涉及数据顺序处理的特定类型的问题，

20
00:00:42,715 --> 00:00:44,790
0,365 475,1200 1200,1505 1525,1800 1800,2075
involve sequential processing of data,|
|

21
00:00:45,410 --> 00:00:47,290
0,305 305,635 635,1000 1080,1480 1560,1880
and we realize why these
我们意识到为什么这些类型的问题，

22
00:00:47,290 --> 00:00:49,105
0,195 195,360 360,650 1150,1545 1545,1815
types of problems| require a
|需要一种不同的方式来实现和构建神经网络，

23
00:00:49,105 --> 00:00:50,860
0,240 240,585 585,965 1015,1575 1575,1755
different way of implementing and

24
00:00:50,860 --> 00:00:53,500
0,255 255,570 570,830 1720,2120 2320,2640
building neural networks,| from what
|比起我们到目前为止所看到的。

25
00:00:53,500 --> 00:00:55,000
0,360 360,555 555,750 750,1040
we've seen so far.|
|

26
00:00:55,370 --> 00:00:56,290
0,245 245,380 380,605 605,800 800,920
And I think some of
我认为，这节课的一些内容

27
00:00:56,290 --> 00:00:57,870
0,180 180,500 850,1140 1140,1305 1305,1580
the components in this lecture|
|

28
00:00:57,950 --> 00:00:59,260
0,610 690,950 950,1040 1040,1130 1130,1310
traditionally can be a bit
一开始可能会让人感到困惑或畏惧，

29
00:00:59,260 --> 00:01:00,840
0,465 465,720 720,1140 1140,1290 1290,1580
confusing or daunting at first,|
|

30
00:01:01,100 --> 00:01:02,050
0,275 275,395 395,530 530,740 740,950
{but,what -} I really, really
但我真的想要做的是，

31
00:01:02,050 --> 00:01:02,995
0,180 180,330 330,510 510,720 720,945
want to do is| to
|从基础开始建立这种理解，

32
00:01:02,995 --> 00:01:05,020
0,270 270,605 865,1245 1245,1625 1735,2025
build this understanding up from

33
00:01:05,020 --> 00:01:07,030
0,150 150,620 1060,1460 1480,1800 1800,2010
the foundations,| walking through step
|一步一步地走，

34
00:01:07,030 --> 00:01:09,325
0,225 225,560 760,1160 1180,1820 1990,2295
by step,| developing intuition all
|一直发展直觉到理解这些网络如何运作背后的数学和操作。

35
00:01:09,325 --> 00:01:11,035
0,150 150,345 345,695 745,1145 1435,1710
the way to understanding the

36
00:01:11,035 --> 00:01:12,720
0,255 255,495 495,735 735,1115 1285,1685
math and the operations behind

37
00:01:13,010 --> 00:01:15,060
0,320 320,545 545,850 1200,1600
how these networks operate.|
|

38
00:01:15,500 --> 00:01:17,575
0,400 690,1090 1110,1510 1650,1910 1910,2075
Okay, so {let's -} get
好的，我们开始吧，

39
00:01:17,575 --> 00:01:19,040
0,305
started,|
|

40
00:01:20,090 --> 00:01:22,705
0,400 570,875 875,1180 2160,2465 2465,2615
to, to begin, to begin,|
为了开始，|

41
00:01:22,705 --> 00:01:23,995
0,150 150,375 375,615 615,795 795,1290
I first want to motivate,|
我首先想要激发，|

42
00:01:23,995 --> 00:01:25,735
0,330 330,675 675,930 930,1205 1435,1740
what exactly we mean when
我们谈论序列数据或序列建模的确切含义，

43
00:01:25,735 --> 00:01:27,700
0,305 385,720 720,1055 1105,1725 1725,1965
we talk about sequential data

44
00:01:27,700 --> 00:01:29,485
0,270 270,690 690,1160 1390,1650 1650,1785
or sequential modeling,| {so,we're -}
|所以我们将从一个非常简单、直观的例子开始，

45
00:01:29,485 --> 00:01:30,355
0,45 45,195 195,420 420,615 615,870
going to begin with a

46
00:01:30,355 --> 00:01:33,025
0,330 330,660 660,1275 1275,1595 2305,2670
really simple, intuitive example,| let's
|假设我们有一张球的图片，

47
00:01:33,025 --> 00:01:33,805
0,135 135,300 300,450 450,600 600,780
say we have this picture

48
00:01:33,805 --> 00:01:35,410
0,165 165,285 285,545 895,1275 1275,1605
of a ball,| and your
|你的任务是预测这个球下一步会移动到哪里，

49
00:01:35,410 --> 00:01:37,495
0,350 370,770 790,1140 1140,1490 1780,2085
task is to predict where

50
00:01:37,495 --> 00:01:38,650
0,195 195,435 435,690 690,915 915,1155
this ball is going to

51
00:01:38,650 --> 00:01:40,120
0,285 285,525 525,800
travel to next,|
|

52
00:01:40,360 --> 00:01:41,655
0,400 450,740 740,905 905,1145 1145,1295
now, if you don't have
现在，如果你没有前置信息，

53
00:01:41,655 --> 00:01:43,395
0,255 255,605 745,1145 1165,1515 1515,1740
any prior information| about the
|关于球的轨迹，它的运动，它的历史，

54
00:01:43,395 --> 00:01:44,520
0,450 450,555 555,675 675,870 870,1125
trajectory of the ball, its

55
00:01:44,520 --> 00:01:46,670
0,320 370,660 660,950 1420,1785 1785,2150
motion, its history,| any guess
|任何关于它的下一个位置的猜测或预测都是随机的猜测，

56
00:01:46,720 --> 00:01:48,405
0,350 350,820 900,1190 1190,1385 1385,1685
or prediction about its next

57
00:01:48,405 --> 00:01:50,445
0,395 715,1115 1435,1755 1755,1905 1905,2040
position is going to be

58
00:01:50,445 --> 00:01:52,010
0,270 270,635 685,960 960,1200 1200,1565
exactly that a random guess,|
|

59
00:01:53,210 --> 00:01:54,835
0,380 380,760 780,1100 1100,1400 1400,1625
if, however, in addition to
然而，如果除了球的当前位置之外，

60
00:01:54,835 --> 00:01:55,960
0,165 165,485 505,840 840,1020 1020,1125
the current location of the

61
00:01:55,960 --> 00:01:57,235
0,260 280,600 600,825 825,1005 1005,1275
ball,| I gave you some
|我还给了你一些关于球过去移动位置的信息，

62
00:01:57,235 --> 00:01:58,630
0,395 415,750 750,1035 1035,1245 1245,1395
information about where it was

63
00:01:58,630 --> 00:02:00,600
0,270 270,510 510,645 645,920 1570,1970
moving in the past,| now
|现在问题变得容易多了，

64
00:02:00,620 --> 00:02:02,250
0,275 275,550 570,890 890,1210 1230,1630
the problem becomes much easier|
|

65
00:02:02,300 --> 00:02:03,775
0,245 245,395 395,700 840,1235 1235,1475
and I think hopefully we
我想希望我们都能同意，

66
00:02:03,775 --> 00:02:05,515
0,120 120,345 345,695 1015,1395 1395,1740
can all agree,| that most
|我们最有可能的下一个预测是，

67
00:02:05,515 --> 00:02:07,240
0,365 595,900 900,1140 1140,1440 1440,1725
likely, our most likely next

68
00:02:07,240 --> 00:02:08,785
0,410 730,990 990,1140 1140,1320 1320,1545
prediction is that,| this ball
|这个球将在下一帧向右移动。

69
00:02:08,785 --> 00:02:09,780
0,210 210,375 375,510 510,675 675,995
is going to move forward

70
00:02:10,160 --> 00:02:11,710
0,290 290,455 455,730 870,1270 1290,1550
to the right in the

71
00:02:11,710 --> 00:02:12,920
0,210 210,560
next frame.|
|

72
00:02:13,470 --> 00:02:14,650
0,380 380,635 635,770 770,905 905,1180
So this is a really,
所以这是一个非常精简的，简单的，直观的例子，

73
00:02:14,760 --> 00:02:16,175
0,260 260,520 540,860 860,1130 1130,1415
you know, reduced down, bare

74
00:02:16,175 --> 00:02:18,470
0,300 300,960 960,1295 1885,2145 2145,2295
bones, intuitive example,| {but,the -}
|但事实是，除此之外，

75
00:02:18,470 --> 00:02:19,810
0,195 195,405 405,675 675,990 990,1340
truth is that beyond this,|
|

76
00:02:19,950 --> 00:02:21,935
0,650 650,920 920,1300 1320,1655 1655,1985
sequential data is really all
序列数据真的就在我们周围，

77
00:02:21,935 --> 00:02:23,750
0,300 300,605 865,1260 1260,1545 1545,1815
around us, right,| as I'm
|就在我说话的时候，

78
00:02:23,750 --> 00:02:25,400
0,290 760,1020 1020,1230 1230,1485 1485,1650
speaking,| the words coming out
|从我嘴里说出来的话形成了一系列声波，

79
00:02:25,400 --> 00:02:26,710
0,120 120,255 255,530 580,945 945,1310
of my mouth form a

80
00:02:26,820 --> 00:02:28,505
0,365 365,620 620,860 860,1210 1350,1685
sequence of sound waves,| that
|定义了音频，

81
00:02:28,505 --> 00:02:29,885
0,285 285,635 745,1050 1050,1215 1215,1380
define audio,| which we can
|我们可以分开，

82
00:02:29,885 --> 00:02:31,235
0,180 180,455 595,885 885,1095 1095,1350
split up| to think about
|以这种顺序的方式来思考，

83
00:02:31,235 --> 00:02:33,020
0,225 225,405 405,885 885,1145
in this sequential manner,|
|

84
00:02:33,150 --> 00:02:35,945
0,740 740,1120 1350,1750 2310,2585 2585,2795
similarly, text language can be
类似地，文本语言可以被分成字符序列或单词序列，

85
00:02:35,945 --> 00:02:37,300
0,195 195,375 375,695 775,1065 1065,1355
split up into a sequence

86
00:02:37,350 --> 00:02:39,680
0,350 350,700 1410,1810 1830,2105 2105,2330
of characters or a sequence

87
00:02:39,680 --> 00:02:41,040
0,225 225,500
of words,|
|

88
00:02:41,180 --> 00:02:42,460
0,380 380,620 620,770 770,1010 1010,1280
and there are many, many,
还有很多更多的例子，

89
00:02:42,460 --> 00:02:43,870
0,255 255,540 540,890 940,1200 1200,1410
many more examples| in which
|表明序列处理，序列数据是存在的，

90
00:02:43,870 --> 00:02:46,165
0,570 570,860 1180,1755 1755,2010 2010,2295
sequential, processing sequential data is

91
00:02:46,165 --> 00:02:48,580
0,305 325,725 1045,1445 1495,1895 1915,2415
present, right,| from medical signals,
|从医学信号，比如心电图，

92
00:02:48,580 --> 00:02:51,295
0,300 300,1010 1480,1845 1845,2210 2320,2715
like EKGs,| to financial markets
|到金融市场和预测股票价格，

93
00:02:51,295 --> 00:02:53,545
0,345 345,780 780,1020 1020,1355 1945,2250
and projecting stock prices,| to
|到 DNA 编码的生物序列，

94
00:02:53,545 --> 00:02:55,710
0,305 595,1200 1200,1725 1725,1875 1875,2165
biological sequences encoded in DNA,|
|

95
00:02:56,360 --> 00:02:57,775
0,365 365,730 750,1010 1010,1145 1145,1415
to patterns in the climate,|
到气候模式，|

96
00:02:57,775 --> 00:02:59,215
0,270 270,525 525,780 780,1055 1135,1440
to patterns of motion and
到运动模式，等等。

97
00:02:59,215 --> 00:03:00,680
0,305 325,725
many more.|
|

98
00:03:00,820 --> 00:03:02,610
0,290 290,580 810,1145 1145,1460 1460,1790
And so already, hopefully, you're
所以，希望你已经对这些类型的问题有一定的了解，

99
00:03:02,610 --> 00:03:03,855
0,165 165,360 360,645 645,990 990,1245
getting a sense of what

100
00:03:03,855 --> 00:03:04,965
0,210 210,405 405,585 585,840 840,1110
these types of questions and

101
00:03:04,965 --> 00:03:06,450
0,300 300,570 570,750 750,1055 1165,1485
problems may look like| and
|以及它们在现实世界中的哪些方面相关。

102
00:03:06,450 --> 00:03:07,710
0,240 240,420 420,555 555,830 1000,1260
where they are relevant in

103
00:03:07,710 --> 00:03:08,960
0,120 120,315 315,650
the real world.|
|

104
00:03:10,190 --> 00:03:11,905
0,290 290,530 530,880 1050,1445 1445,1715
When we consider applications of
当我们考虑序列建模在现实世界中的应用时，

105
00:03:11,905 --> 00:03:13,120
0,450 450,795 795,915 915,1035 1035,1215
sequential modeling in the real

106
00:03:13,120 --> 00:03:14,710
0,320 700,960 960,1110 1110,1320 1320,1590
world,| we can think about
|我们可以考虑一些不同类型的问题定义，

107
00:03:14,710 --> 00:03:16,150
0,240 240,450 450,660 660,950 1180,1440
a number of different kind

108
00:03:16,150 --> 00:03:17,710
0,180 180,495 495,1125 1125,1410 1410,1560
of problem definitions,| that we
|我们可以在我们的武器库拥有并使用这些定义。

109
00:03:17,710 --> 00:03:18,775
0,135 135,300 300,450 450,585 585,1065
can have in our arsenal

110
00:03:18,775 --> 00:03:20,120
0,195 195,405 405,725
and work with.|
|

111
00:03:20,120 --> 00:03:22,000
0,225 225,330 330,495 495,800 1480,1880
In the first lecture,| Alexander
在第一节课中，|Alexander 介绍了分类和回归的概念，

112
00:03:22,140 --> 00:03:24,220
0,400 510,845 845,1280 1280,1520 1520,2080
introduced the notions of classification

113
00:03:24,570 --> 00:03:26,020
0,275 275,410 410,665 665,995 995,1450
and the notion of regression,|
|

114
00:03:26,800 --> 00:03:28,015
0,225 225,480 480,720 720,1005 1005,1215
where he talked about and
在那里他谈到了，我们学习了前馈模型，

115
00:03:28,015 --> 00:03:29,290
0,150 150,360 360,690 690,1005 1005,1275
we learned about feed forward

116
00:03:29,290 --> 00:03:31,390
0,350 820,1140 1140,1460 1510,1845 1845,2100
models,| that can operate one
|它可以在这种固定和静态的设置下一对一地运行，

117
00:03:31,390 --> 00:03:32,485
0,180 180,375 375,585 585,795 795,1095
to one in this fixed

118
00:03:32,485 --> 00:03:34,465
0,300 300,615 615,965 1405,1710 1710,1980
and static setting,| given the
|给定单一输入，预测单一输出，

119
00:03:34,465 --> 00:03:36,240
0,365 445,840 840,1215 1215,1485 1485,1775
single input, predict a single

120
00:03:36,320 --> 00:03:37,260
0,400
output,|
|

121
00:03:37,270 --> 00:03:39,320
0,260 260,635 635,1240 1290,1670 1670,2050
the binary classification example,| of
那个二元分类的例子，|你会不会成功通过这门课，

122
00:03:39,520 --> 00:03:41,070
0,320 320,640 840,1130 1130,1310 1310,1550
will you succeed or pass

123
00:03:41,070 --> 00:03:42,280
0,270 270,590
this class,|
|

124
00:03:42,290 --> 00:03:43,705
0,290 290,580 870,1100 1100,1220 1220,1415
and here, {there's -} no
在这里，没有顺序的概念，也没有时间的概念，

125
00:03:43,705 --> 00:03:44,785
0,240 240,465 465,750 750,975 975,1080
notion {of -} sequence, there's

126
00:03:44,785 --> 00:03:46,640
0,150 150,360 360,600 600,905
no notion of time,|
|

127
00:03:47,080 --> 00:03:48,705
0,400 420,695 695,970 1020,1325 1325,1625
Now, if we introduce this
现在，如果我们引入这种序列分量的概念，

128
00:03:48,705 --> 00:03:50,150
0,300 300,450 450,570 570,1125 1125,1445
idea of a sequential component,|
|

129
00:03:50,560 --> 00:03:52,110
0,275 275,455 455,760 1020,1385 1385,1550
we can handle inputs that
我们可以处理可能在时间上定义的输入，

130
00:03:52,110 --> 00:03:54,680
0,165 165,405 405,770 790,1670 2170,2570
may be defined temporally| and
|并潜在地产生顺序或时间输出。

131
00:03:54,850 --> 00:03:57,030
0,400 630,980 980,1265 1265,1550 1550,2180
potentially also produce a sequential

132
00:03:57,030 --> 00:03:58,800
0,210 210,740 850,1250
or temporal output.|
|

133
00:03:58,840 --> 00:04:00,720
0,400 690,1070 1070,1340 1340,1565 1565,1880
So, for as one example,|
所以作为一个例子，|

134
00:04:00,720 --> 00:04:02,630
0,255 255,510 510,890 970,1370 1510,1910
we can consider text language,|
我们可以考虑文本语言，|

135
00:04:03,070 --> 00:04:04,275
0,290 290,530 530,755 755,965 965,1205
and maybe we want to
也许我们想要生成一个预测，

136
00:04:04,275 --> 00:04:06,285
0,305 445,765 765,1205 1495,1815 1815,2010
generate one prediction,| given a
|给出一个文本序列，

137
00:04:06,285 --> 00:04:08,990
0,225 225,465 465,755 1315,2075 2305,2705
sequence of text,| classifying whether
|分类消息是积极的情绪还是负面的情绪。

138
00:04:09,070 --> 00:04:10,490
0,350 350,700 720,980 980,1130 1130,1420
a message is a positive

139
00:04:10,690 --> 00:04:12,650
0,605 605,845 845,1025 1025,1295 1295,1960
sentiment or a negative sentiment.|
|

140
00:04:13,700 --> 00:04:15,340
0,785 785,1010 1010,1175 1175,1385 1385,1640
Conversely, we could have a
相反，我们可以有一个输入，

141
00:04:15,340 --> 00:04:16,930
0,320 430,830 1030,1380 1380,1470 1470,1590
single input,| let's say an
|比方说一幅图像，

142
00:04:16,930 --> 00:04:18,700
0,260 850,1125 1125,1275 1275,1470 1470,1770
image,| and our goal may
|现在我们的目标可能是生成该图像的文本或顺序描述，

143
00:04:18,700 --> 00:04:20,370
0,285 285,570 570,840 840,1130 1270,1670
be now to generate text

144
00:04:20,990 --> 00:04:23,050
0,320 320,530 530,1070 1070,1420 1740,2060
or a sequential description of

145
00:04:23,050 --> 00:04:24,790
0,195 195,470 610,1010 1240,1560 1560,1740
this image,| right, given this
|给出这张棒球运动员投球的图片，

146
00:04:24,790 --> 00:04:25,870
0,180 180,345 345,450 450,710 730,1080
image of a baseball player

147
00:04:25,870 --> 00:04:27,340
0,240 240,375 375,620 1000,1305 1305,1470
throwing a ball,| can we
|我们能建立一个神经网络来生成语言字幕吗。

148
00:04:27,340 --> 00:04:28,615
0,135 135,270 270,480 480,740 880,1275
build a neural network that

149
00:04:28,615 --> 00:04:29,970
0,540 540,780 780,945 945,1080 1080,1355
generates that as a language

150
00:04:30,230 --> 00:04:31,400
0,640
caption.|
|

151
00:04:32,100 --> 00:04:34,000
0,400 690,965 965,1220 1220,1550 1550,1900
Finally, we can also consider
最后，我们还可以考虑有序列输入、序列输出的应用程序和问题，

152
00:04:34,530 --> 00:04:36,290
0,380 380,650 650,940 990,1390 1500,1760
applications and problems where we

153
00:04:36,290 --> 00:04:37,960
0,195 195,495 495,860 1000,1335 1335,1670
have sequence in, sequence out,|
|

154
00:04:38,490 --> 00:04:39,530
0,290 290,530 530,740 740,875 875,1040
{for,example -}, if we want
例如，如果我们想要在两种语言之间进行翻译，

155
00:04:39,530 --> 00:04:41,320
0,290 370,735 735,1035 1035,1370 1390,1790
to translate between two languages,|
|

156
00:04:41,880 --> 00:04:43,630
0,350 350,700 930,1250 1250,1460 1460,1750
and indeed this type of
这种类型的想法和这种类型的架构是，

157
00:04:43,800 --> 00:04:44,825
0,350 350,560 560,710 710,875 875,1025
thinking and this type of

158
00:04:44,825 --> 00:04:46,925
0,275 805,1095 1095,1305 1305,1625 1825,2100
architecture is| what powers the
|推动了你的手机中的机器翻译任务，

159
00:04:46,925 --> 00:04:49,070
0,275 565,915 915,1170 1170,1475 1885,2145
task of machine translation in

160
00:04:49,070 --> 00:04:50,560
0,150 150,420 420,690 690,980 1090,1490
your phones,| in Google translate
|在谷歌翻译和许多其他例子中。

161
00:04:50,700 --> 00:04:53,110
0,400 510,800 800,1090 1110,1510 2010,2410
and and many other examples.|
|

162
00:04:54,450 --> 00:04:56,540
0,400 720,1120 1350,1625 1625,1835 1835,2090
So hopefully this has given
希望这能让你了解序列数据是什么样子的，

163
00:04:56,540 --> 00:04:57,695
0,180 180,315 315,585 585,900 900,1155
you a picture of what

164
00:04:57,695 --> 00:04:59,405
0,495 495,690 690,930 930,1235 1405,1710
sequential data looks like,| what
|这些类型的问题定义可能是什么样子的，

165
00:04:59,405 --> 00:05:00,830
0,210 210,420 420,615 615,870 870,1425
these types of problem definitions

166
00:05:00,830 --> 00:05:02,885
0,240 240,420 420,710 1150,1550 1750,2055
may look like,| {and,from -}
|从这一点出发，我们将开始并建立我们的理解，

167
00:05:02,885 --> 00:05:03,920
0,240 240,495 495,615 615,795 795,1035
this, we're going to start

168
00:05:03,920 --> 00:05:05,230
0,240 240,435 435,615 615,890 910,1310
and build up our understanding|
|

169
00:05:05,490 --> 00:05:07,280
0,335 335,670 900,1265 1265,1535 1535,1790
of what neural networks we
我们可以建立和训练什么样的神经网络来解决这些类型的问题。

170
00:05:07,280 --> 00:05:08,600
0,135 135,300 300,495 495,800 1030,1320
can build and train for

171
00:05:08,600 --> 00:05:10,120
0,210 210,420 420,600 600,890
these types of problems.|
|

172
00:05:11,340 --> 00:05:12,995
0,400 720,1070 1070,1310 1310,1445 1445,1655
So first we're going to
首先我们将从递归的概念开始，

173
00:05:12,995 --> 00:05:15,125
0,255 255,605 1345,1620 1620,1875 1875,2130
begin with the notion of

174
00:05:15,125 --> 00:05:16,840
0,525 525,780 780,1035 1035,1350 1350,1715
recurrence| and build up from
|并从这个概念开始定义递归神经网络，

175
00:05:16,980 --> 00:05:18,680
0,380 380,695 695,980 980,1505 1505,1700
that to define recurrent neural

176
00:05:18,680 --> 00:05:20,075
0,260 640,900 900,1020 1020,1155 1155,1395
networks,| {and,in -} the last
|在讲座的最后部分，

177
00:05:20,075 --> 00:05:21,560
0,300 300,480 480,570 570,815 1135,1485
portion of the lecture,| we'll
|我们将讨论底层机制，底层转换器架构，

178
00:05:21,560 --> 00:05:23,560
0,195 195,435 435,600 600,860 1240,2000
talk about the underlying mechanisms,

179
00:05:24,030 --> 00:05:26,195
0,400 540,940 990,1430 1430,2030 2030,2165
underlying the transformer architectures,| that
|这些在处理序列数据方面非常、非常、非常强大。

180
00:05:26,195 --> 00:05:27,460
0,165 165,435 435,690 690,930 930,1265
are very, very, very powerful

181
00:05:27,810 --> 00:05:29,690
0,305 305,530 530,845 845,1400 1400,1880
in terms of handling sequential

182
00:05:29,690 --> 00:05:30,540
0,260
data.|
|

183
00:05:30,680 --> 00:05:31,660
0,320 320,515 515,680 680,845 845,980
But as I said at
但正如我在开始时所说的，

184
00:05:31,660 --> 00:05:33,145
0,150 150,375 375,710 730,1130 1210,1485
the beginning, right,| the theme
|这节课的主题是逐步建立这种理解，

185
00:05:33,145 --> 00:05:34,195
0,135 135,285 285,540 540,795 795,1050
of this lecture is building

186
00:05:34,195 --> 00:05:36,310
0,285 285,605 955,1355 1615,1905 1905,2115
up that understanding step by

187
00:05:36,310 --> 00:05:38,035
0,320 400,765 765,1005 1005,1140 1140,1725
step,| starting with the fundamentals
|从基础知识和直觉开始。

188
00:05:38,035 --> 00:05:39,540
0,180 180,300 300,755
and the intuition.|
|

189
00:05:39,710 --> 00:05:40,720
0,275 275,395 395,500 500,725 725,1010
So to do that, we're
要做到这一点，我们回到过去，

190
00:05:40,720 --> 00:05:42,145
0,105 105,240 240,375 375,650 850,1425
going to go back,| revisit
|回顾感知器，并从那里继续前进，

191
00:05:42,145 --> 00:05:43,840
0,90 90,725 865,1155 1155,1395 1395,1695
the perceptron and move forward

192
00:05:43,840 --> 00:05:45,000
0,240 240,530
from there,|
|

193
00:05:45,600 --> 00:05:47,680
0,395 395,680 680,970 1050,1450 1680,2080
right, {so,as -} Alexander introduced,|
所以就像 Alexander 介绍的，|

194
00:05:48,120 --> 00:05:49,390
0,275 275,455 455,665 665,845 845,1270
where we study the perceptron,
我们在第一课中研究了感知器，

195
00:05:49,560 --> 00:05:52,145
0,650 650,815 815,1055 1055,1420 2250,2585
perceptron in lecture one,| the
|感知器是由这个单一的神经操作定义的，

196
00:05:52,145 --> 00:05:53,885
0,465 465,795 795,1125 1125,1425 1425,1740
perception is defined by this

197
00:05:53,885 --> 00:05:56,480
0,390 390,875 1225,1625 2125,2415 2415,2595
single neural operation,| where we
|我们有一些输入，

198
00:05:56,480 --> 00:05:57,850
0,255 255,525 525,675 675,885 885,1370
have some set of inputs,|
|

199
00:05:58,080 --> 00:05:59,375
0,350 350,470 470,710 710,1040 1040,1295
let's say {x1 -} to
比如 x1 到 xm ，

200
00:05:59,375 --> 00:06:01,205
0,485 1045,1350 1350,1530 1530,1665 1665,1830
xm,| and each of these
|这些数字中的每一个都乘以相应的权重，

201
00:06:01,205 --> 00:06:03,200
0,305 415,765 765,1410 1410,1740 1740,1995
numbers are multiplied by a

202
00:06:03,200 --> 00:06:05,690
0,600 600,890 1450,1850 1870,2220 2220,2490
corresponding weight,| passed through a
|通过一个非线性激活函数，

203
00:06:05,690 --> 00:06:08,165
0,720 720,1155 1155,1520 1960,2250 2250,2475
non-linear activation function,| that then
|然后产生一个预测的输出 ŷ 。

204
00:06:08,165 --> 00:06:10,175
0,480 480,690 690,1235 1285,1685 1705,2010
generates a predicted output y

205
00:06:10,175 --> 00:06:10,960
0,305
hat.|
|

206
00:06:11,320 --> 00:06:12,680
0,400 480,755 755,890 890,1055 1055,1360
Here we can have multiple
在这里，我们可以有多个输入来生成我们的输出，

207
00:06:12,880 --> 00:06:14,460
0,335 335,530 530,830 830,1205 1205,1580
inputs coming in to generate

208
00:06:14,460 --> 00:06:16,875
0,375 375,770 1210,1560 1560,1910 2020,2415
our output,| but still these
|但这些输入仍然不被认为是序列中的点或序列中的时间步长，

209
00:06:16,875 --> 00:06:18,195
0,330 330,525 525,795 795,1050 1050,1320
inputs are not thought of

210
00:06:18,195 --> 00:06:20,060
0,365 895,1260 1260,1470 1470,1590 1590,1865
as points in a sequence

211
00:06:20,080 --> 00:06:21,225
0,320 320,590 590,845 845,1010 1010,1145
or time steps in a

212
00:06:21,225 --> 00:06:22,140
0,275
sequence,|
|

213
00:06:22,340 --> 00:06:23,665
0,365 365,605 605,815 815,1085 1085,1325
even if we scale this
即使我们扩展这个感知器，

214
00:06:23,665 --> 00:06:25,380
0,585 585,840 840,1110 1110,1380 1380,1715
perceptron| and start to stack
|并开始将多个感知器堆叠在一起来定义这些前馈神经网络，

215
00:06:25,580 --> 00:06:28,105
0,400 630,1340 1340,1720 1980,2300 2300,2525
multiple perceptions together to define

216
00:06:28,105 --> 00:06:29,460
0,210 210,435 435,750 750,1080 1080,1355
these feed forward neural networks,|
|

217
00:06:30,050 --> 00:06:31,330
0,350 350,590 590,860 860,1055 1055,1280
we still don't have this
我们仍然没有这种时间处理或序列信息的概念。

218
00:06:31,330 --> 00:06:33,685
0,225 225,450 450,950 1480,1880 2050,2355
notion of temporal processing or

219
00:06:33,685 --> 00:06:35,240
0,600 600,995
sequential information.|
|

220
00:06:35,380 --> 00:06:36,555
0,350 350,635 635,815 815,920 920,1175
Even though we are able
即使我们能够翻译和转换多个输入，

221
00:06:36,555 --> 00:06:39,050
0,395 565,965 1105,1470 1470,1835 2095,2495
to translate and convert multiple

222
00:06:39,340 --> 00:06:41,390
0,520 810,1130 1130,1355 1355,1655 1655,2050
inputs,| apply these way operations,
|应用这些方式操作，应用这种非线性，

223
00:06:41,530 --> 00:06:43,260
0,305 305,500 500,725 725,1450 1470,1730
apply this {non-linearity -}| to
|然后定义多个预测输出。

224
00:06:43,260 --> 00:06:46,010
0,260 280,680 970,1370 1570,2250 2250,2750
then define multiple predicted outputs.|
|

225
00:06:47,660 --> 00:06:48,730
0,350 350,635 635,830 830,950 950,1070
So taking a look at
看看这张图，

226
00:06:48,730 --> 00:06:50,410
0,165 165,615 615,920 1270,1545 1545,1680
this diagram, right,| on the
|左边是蓝色，是输入，

227
00:06:50,410 --> 00:06:51,535
0,180 180,420 420,660 660,855 855,1125
left in blue, you have

228
00:06:51,535 --> 00:06:52,555
0,375 375,540 540,675 675,825 825,1020
inputs,| on the right, in
|右边是紫色，是输出，

229
00:06:52,555 --> 00:06:54,390
0,305 535,825 825,1020 1020,1325 1345,1835
purple, you have these outputs,|
|

230
00:06:54,410 --> 00:06:55,980
0,290 290,470 470,695 695,1175 1175,1570
and the green defines the
绿色定义了单一神经网络层，

231
00:06:56,300 --> 00:06:58,380
0,380 380,725 725,1000 1020,1420 1560,2080
single neural network layer,| that's
|将这些输入转换为输出。

232
00:06:58,430 --> 00:06:59,770
0,425 425,680 680,1025 1025,1145 1145,1340
transforming these inputs to the

233
00:06:59,770 --> 00:07:00,760
0,440
outputs.|
|

234
00:07:01,070 --> 00:07:02,185
0,335 335,620 620,890 890,995 995,1115
Next step, I'm going to
下一步，我将简化这个图表，

235
00:07:02,185 --> 00:07:03,790
0,165 165,660 660,825 825,1335 1335,1605
just simplify this diagram,| I'm
|我将把这些神经元叠加在一起，

236
00:07:03,790 --> 00:07:05,760
0,105 105,380 790,1290 1290,1620 1620,1970
going to collapse down those

237
00:07:05,960 --> 00:07:08,890
0,365 365,1025 1025,1390 1890,2290 2490,2930
stack perceptons together| and depict
|然后用这个绿色的方块来描述，

238
00:07:08,890 --> 00:07:10,560
0,255 255,555 555,885 885,1275 1275,1670
this with this green block,|
|

239
00:07:11,120 --> 00:07:12,985
0,335 335,590 590,740 740,1030 1500,1865
{still\,,it's -} the same operation
仍然是同样的操作，

240
00:07:12,985 --> 00:07:14,410
0,270 270,495 495,815 1015,1290 1290,1425
going on, right,| we have
|我们有一个输入向量被变换来预测这个输出向量。

241
00:07:14,410 --> 00:07:17,215
0,210 210,435 435,740 1150,1550 2440,2805
an input vector being transformed

242
00:07:17,215 --> 00:07:18,720
0,240 240,515 535,930 930,1200 1200,1505
to predict this output vector.|
|

243
00:07:20,210 --> 00:07:21,400
0,400
Now,
现在，我在这里介绍的，你们可能注意到了，

244
00:07:21,470 --> 00:07:22,825
0,290 290,650 650,950 950,1175 1175,1355
what I've introduced here, which

245
00:07:22,825 --> 00:07:24,400
0,165 165,375 375,695 1105,1380 1380,1575
you may notice,| is this
|是这个新变量 t ，

246
00:07:24,400 --> 00:07:27,130
0,225 225,530 880,1280 2200,2490 2490,2730
new variable t,| which I'm
|我用它来表示单个时间步长，

247
00:07:27,130 --> 00:07:28,375
0,180 180,390 390,675 675,915 915,1245
using to denote a single

248
00:07:28,375 --> 00:07:29,640
0,330 330,665
time step,|
|

249
00:07:29,830 --> 00:07:31,230
0,260 260,485 485,770 770,1085 1085,1400
we are considering an input
我们正在考虑单个时间步长的输入，

250
00:07:31,230 --> 00:07:32,390
0,180 180,315 315,540 540,825 825,1160
at a single time step|
|

251
00:07:32,500 --> 00:07:34,580
0,380 380,755 755,1150 1470,1820 1820,2080
and using our neural network
并使用我们的神经网络生成与该输入对应的单个输出，

252
00:07:34,630 --> 00:07:36,260
0,290 290,560 560,830 830,1120 1230,1630
to generate a single output

253
00:07:36,280 --> 00:07:38,060
0,695 695,815 815,1025 1025,1390
corresponding to that input,|
|

254
00:07:38,590 --> 00:07:39,810
0,305 305,470 470,695 695,950 950,1220
how could we start to
我们如何开始扩展和建立它，

255
00:07:39,810 --> 00:07:41,415
0,375 375,675 675,945 945,1260 1260,1605
extend and build off this|
|

256
00:07:41,415 --> 00:07:42,920
0,270 270,545 625,945 945,1185 1185,1505
to now, think about multiple
现在考虑多个时间步长，

257
00:07:43,030 --> 00:07:44,520
0,335 335,670 870,1160 1160,1340 1340,1490
time steps| and how we
|以及我们如何处理一系列信息。

258
00:07:44,520 --> 00:07:46,340
0,195 195,530 550,950 1270,1545 1545,1820
could potentially process a sequence

259
00:07:46,510 --> 00:07:49,590
0,400 420,820 2100,2500 2670,2930 2930,3080
of information.| Well, what if
|那么，如果我们用这张图，

260
00:07:49,590 --> 00:07:51,075
0,180 180,360 360,570 570,1130 1210,1485
we took this diagram,| all
|我所做的就是把它旋转了 90 度，

261
00:07:51,075 --> 00:07:52,305
0,210 210,390 390,615 615,810 810,1230
I've done is just rotated

262
00:07:52,305 --> 00:07:54,240
0,305 505,900 900,1295
it 90 degrees,|
|

263
00:07:54,680 --> 00:07:55,810
0,275 275,440 440,635 635,845 845,1130
where we still have this
我们仍然有这个输入向量，

264
00:07:55,810 --> 00:07:57,805
0,270 270,590 1090,1490 1510,1815 1815,1995
input vector| and being fed
|并在产生一个输出向量，

265
00:07:57,805 --> 00:07:59,820
0,275 865,1185 1185,1485 1485,1725 1725,2015
in producing an output vector,|
|

266
00:08:00,350 --> 00:08:01,495
0,400 480,740 740,875 875,1010 1010,1145
{and,what -} if we can
如果我们可以复制这个网络，

267
00:08:01,495 --> 00:08:02,350
0,150 150,315 315,525 525,705 705,855
make a copy of this

268
00:08:02,350 --> 00:08:05,010
0,290 1030,1335 1335,1640 1810,2210 2260,2660
network| and just do this
|并多次执行这个操作，

269
00:08:05,030 --> 00:08:07,210
0,380 380,760 840,1240 1560,1895 1895,2180
operation multiple times| to try
|来尝试处理对应于不同时间馈入的输入。

270
00:08:07,210 --> 00:08:08,545
0,240 240,530 670,1065 1065,1185 1185,1335
to handle inputs that are

271
00:08:08,545 --> 00:08:10,360
0,180 180,455 775,1470 1470,1605 1605,1815
fed in corresponding to different

272
00:08:10,360 --> 00:08:12,120
0,350 940,1340
times, right.|
|

273
00:08:12,260 --> 00:08:13,540
0,275 275,410 410,635 635,980 980,1280
We have an individual time
我们有一个单独的时间步长，从 t0 开始，

274
00:08:13,540 --> 00:08:15,810
0,315 315,705 705,1100 1690,1980 1980,2270
step, starting with {t0 -},|
|

275
00:08:16,460 --> 00:08:17,365
0,305 305,485 485,620 620,755 755,905
and we can do the
我们可以做同样的事情，对下一个时间步长做同样的操作，

276
00:08:17,365 --> 00:08:18,870
0,210 210,465 465,705 705,1025 1105,1505
same thing, the same operation

277
00:08:18,950 --> 00:08:20,130
0,260 260,365 365,545 545,830 830,1180
for the next time step,|
|

278
00:08:20,660 --> 00:08:22,570
0,400 630,1040 1040,1300 1470,1745 1745,1910
again treating that as an
再次将其视为一个孤立的实例，

279
00:08:22,570 --> 00:08:24,220
0,590 700,1100
isolated instance,|
|

280
00:08:24,320 --> 00:08:26,850
0,400 810,1115 1115,1340 1340,1660 1830,2530
and keep doing this repeatedly,|
并不断地重复这样做，|

281
00:08:27,510 --> 00:08:28,685
0,275 275,425 425,620 620,830 830,1175
and what you'll notice, hopefully,
你会注意到，

282
00:08:28,685 --> 00:08:30,200
0,315 315,540 540,765 765,1085 1195,1515
is| all these models are
|所有这些模型都是彼此的简单副本，

283
00:08:30,200 --> 00:08:31,510
0,320 340,795 795,915 915,1035 1035,1310
simply copies of each other,|
|

284
00:08:31,740 --> 00:08:33,635
0,305 305,530 530,850 900,1450 1590,1895
just with different inputs at
只是在每个不同的时间步长有不同的输入，

285
00:08:33,635 --> 00:08:34,640
0,195 195,330 330,480 480,720 720,1005
each of these different time

286
00:08:34,640 --> 00:08:35,780
0,350
steps,|
|

287
00:08:36,270 --> 00:08:37,240
0,305 305,470 470,575 575,695 695,970
and we can make this
我们使这一点变得具体起来，

288
00:08:37,260 --> 00:08:38,735
0,320 320,640 750,1055 1055,1265 1265,1475
concrete right,| in terms of
|可以根据这种功能转变所做的事情，

289
00:08:38,735 --> 00:08:40,835
0,225 225,480 480,1005 1005,1295 1795,2100
what this functional transformation is

290
00:08:40,835 --> 00:08:41,880
0,305
doing,|
|

291
00:08:41,890 --> 00:08:43,335
0,275 275,845 845,1130 1130,1265 1265,1445
the predicted output at a
在特定时间步长的预测输出 ŷt ，

292
00:08:43,335 --> 00:08:45,240
0,335 355,690 690,1025 1345,1680 1680,1905
particular time step {ŷt -

293
00:08:45,240 --> 00:08:47,300
0,210 210,530 910,1305 1305,1680 1680,2060
- -},| is a function
|是那个输入时间步长 xt 的函数，

294
00:08:47,470 --> 00:08:49,290
0,365 365,710 710,1090 1320,1625 1625,1820
of the input {at,that -}

295
00:08:49,290 --> 00:08:50,750
0,210 210,530 640,960 960,1170 1170,1460
time step {xt - -},|
|

296
00:08:51,240 --> 00:08:52,455
0,195 195,375 375,675 675,975 975,1215
and that function is what
而这个函数是由我们的神经网络权重学习和定义的。

297
00:08:52,455 --> 00:08:54,015
0,240 240,525 525,870 870,1230 1230,1560
is learned and defined by

298
00:08:54,015 --> 00:08:56,120
0,255 255,570 570,845 955,1505
our neural network weights.|
|

299
00:08:57,070 --> 00:08:58,455
0,400 660,905 905,1085 1085,1220 1220,1385
Okay, so I've told you
好的，我已经告诉你们了，

300
00:08:58,455 --> 00:08:59,600
0,150 150,330 330,570 570,825 825,1145
that,| our goal here is
|我们的目标是试图理解序列数据，进行序列建模，

301
00:08:59,710 --> 00:09:01,635
0,335 335,670 720,1100 1100,1685 1685,1925
trying to understand sequential data,

302
00:09:01,635 --> 00:09:03,400
0,255 255,705 705,1205
do sequential modeling,|
|

303
00:09:03,400 --> 00:09:04,450
0,350 370,660 660,810 810,930 930,1050
but what could be the
但是，这个图表显示的内容和我在这里显示的内容会有什么问题？

304
00:09:04,450 --> 00:09:06,475
0,240 240,620 730,1130 1360,1650 1650,2025
issue with what this diagram

305
00:09:06,475 --> 00:09:07,345
0,165 165,375 375,570 570,690 690,870
is showing and what I've

306
00:09:07,345 --> 00:09:08,480
0,135 135,315 315,605
shown you here?|
|

307
00:09:09,440 --> 00:09:11,840
0,400 750,1070 1070,1250 1250,1510
Well, yeah, go ahead.|
好的，是的，继续。|

308
00:09:15,350 --> 00:09:17,070
0,400 450,890 890,1145 1145,1400 1400,1720
Exactly, that's exactly right,| {so,the
没错，这是完全正确的，|所以学生的回答是，

309
00:09:17,210 --> 00:09:18,630
0,275 275,560 560,785 785,1100 1100,1420
-} student's answer was that,|
|

310
00:09:18,710 --> 00:09:20,620
0,350 350,700 1230,1475 1475,1640 1640,1910
{x1 -} could be related
x1 可能与 x 无关，

311
00:09:20,620 --> 00:09:21,955
0,195 195,375 375,710 910,1170 1170,1335
to x not,| and you
|你有这种时间依赖性，

312
00:09:21,955 --> 00:09:23,700
0,225 225,435 435,930 930,1425 1425,1745
have this temporal dependence,| but
|但这些孤立的副本根本无法捕捉到这一点，

313
00:09:23,720 --> 00:09:26,065
0,320 320,905 905,1570 1770,2120 2120,2345
these isolated replicas don't capture

314
00:09:26,065 --> 00:09:27,750
0,225 225,345 345,605 985,1260 1260,1685
that {at,all -},| and that's
|这就完美地回答了这个问题。

315
00:09:27,980 --> 00:09:31,140
0,400 1950,2330 2330,2570 2570,2795 2795,3160
exactly answers the question perfectly,

316
00:09:31,190 --> 00:09:32,180
0,400
right.|
|

317
00:09:32,890 --> 00:09:34,485
0,380 380,635 635,1180 1200,1475 1475,1595
Here, a predicted output at
这里，在较后时间步长的预测输出，

318
00:09:34,485 --> 00:09:36,180
0,120 120,390 390,720 720,1055 1315,1695
a later time step| could
|可以精确地依赖于在前时间步长的输入，

319
00:09:36,180 --> 00:09:39,000
0,380 1060,1785 1785,2090 2260,2625 2625,2820
depend precisely on inputs at

320
00:09:39,000 --> 00:09:40,665
0,315 315,630 630,950 1180,1485 1485,1665
previous time step,| if this
|如果这确实是具有这种时间依赖性的序列问题。

321
00:09:40,665 --> 00:09:42,170
0,210 210,510 510,750 750,1230 1230,1505
is truly a sequential problem

322
00:09:42,400 --> 00:09:44,420
0,275 275,440 440,890 890,1450
with this temporal dependence.|
|

323
00:09:45,100 --> 00:09:46,605
0,400 570,890 890,1070 1070,1265 1265,1505
So how could we start
那么，我们如何开始对此进行推理，

324
00:09:46,605 --> 00:09:47,670
0,150 150,360 360,645 645,855 855,1065
to reason about this,| how
|我们如何定义一种关系，

325
00:09:47,670 --> 00:09:48,950
0,165 165,375 375,690 690,960 960,1280
could we define a relation,|
|

326
00:09:49,450 --> 00:09:52,335
0,400 870,1270 1350,1610 1610,2230 2250,2885
that links the network's computations
将网络在特定时间步长的计算

327
00:09:52,335 --> 00:09:53,810
0,210 210,405 405,755 775,1125 1125,1475
at a particular time step|
|

328
00:09:54,220 --> 00:09:55,970
0,395 395,770 770,1150 1170,1460 1460,1750
to prior history and memory
与先前时间步长的先前历史和记忆联系起来的关系。

329
00:09:56,290 --> 00:09:58,860
0,400 600,995 995,1310 1310,1630
from previous time steps.|
|

330
00:09:58,960 --> 00:10:00,780
0,400 690,950 950,1115 1115,1420 1440,1820
Well, what if we did
好的，如果我们真的这么做了，

331
00:10:00,780 --> 00:10:02,265
0,345 345,705 705,1100 1120,1365 1365,1485
exactly that, right,| what if
|如果我们简单地将计算和网络理解的信息链接到其他副本，

332
00:10:02,265 --> 00:10:04,725
0,180 180,485 505,905 1285,1685 1795,2460
we simply linked the computation

333
00:10:04,725 --> 00:10:06,825
0,225 225,485 505,905 1645,1935 1935,2100
and the information understood by

334
00:10:06,825 --> 00:10:09,020
0,180 180,485 955,1355 1525,1860 1860,2195
the network to these other

335
00:10:09,250 --> 00:10:11,385
0,700 1050,1400 1400,1640 1640,1850 1850,2135
replicas,| via what we call
|通过我们所说的递归关系，

336
00:10:11,385 --> 00:10:13,500
0,240 240,780 780,1145
a recurrence relation,|
|

337
00:10:13,660 --> 00:10:14,835
0,290 290,470 470,740 740,980 980,1175
what this means is that,|
这意味着，|

338
00:10:14,835 --> 00:10:16,290
0,335 415,810 810,1095 1095,1245 1245,1455
something about what the network
关于网络在特定时间计算的内容的某些信息

339
00:10:16,290 --> 00:10:17,595
0,330 330,675 675,765 765,960 960,1305
is computing at a particular

340
00:10:17,595 --> 00:10:19,695
0,395 565,900 900,1185 1185,1535 1795,2100
time| is passed on to
|会传递给后面的时间步长，

341
00:10:19,695 --> 00:10:21,520
0,240 240,540 540,840 840,1175
those later time steps,|
|

342
00:10:21,520 --> 00:10:22,980
0,195 195,420 420,705 705,1040 1060,1460
and we define that according
我们根据这个变量 h 来定义它，

343
00:10:23,000 --> 00:10:25,525
0,305 305,530 530,850 1200,1600 2220,2525
to this variable h,| which
|我们称之为内部状态，

344
00:10:25,525 --> 00:10:27,160
0,225 225,450 450,755 865,1260 1260,1635
we call this internal state,|
|

345
00:10:27,160 --> 00:10:28,105
0,330 330,540 540,675 675,825 825,945
or you can think of
或者你可以认为它是一个记忆项，

346
00:10:28,105 --> 00:10:29,190
0,120 120,255 255,390 390,665 685,1085
it as a memory term,|
|

347
00:10:29,660 --> 00:10:31,315
0,410 410,700 840,1130 1130,1280 1280,1655
that's maintained by the neurons
由神经元和网络维持的，

348
00:10:31,315 --> 00:10:33,040
0,240 240,375 375,635 1225,1485 1485,1725
and the network,| {and,it's -}
|这种状态被一步一步地传递，

349
00:10:33,040 --> 00:10:34,470
0,240 240,540 540,870 870,1080 1080,1430
this state that's being passed

350
00:10:34,880 --> 00:10:36,240
0,350 350,605 605,800 800,1025 1025,1360
time step to time step,|
|

351
00:10:36,650 --> 00:10:38,815
0,395 395,790 810,1145 1145,1480 1830,2165
as we read in and
当我们读入并处理这些序列信息时，

352
00:10:38,815 --> 00:10:41,780
0,335 415,705 705,1305 1305,1685
process this sequential information,|
|

353
00:10:42,480 --> 00:10:44,050
0,305 305,530 530,850 960,1265 1265,1570
what this means is that
这意味着网络输出它的预测和计算，

354
00:10:44,250 --> 00:10:46,205
0,275 275,905 905,1190 1190,1400 1400,1955
the network output its predictions,

355
00:10:46,205 --> 00:10:47,820
0,210 210,815
its computations,|
|

356
00:10:47,820 --> 00:10:49,340
0,285 285,570 570,920 940,1230 1230,1520
is not only a function
不仅是输入数据 x 的函数，

357
00:10:49,420 --> 00:10:51,050
0,380 380,740 740,995 995,1250 1250,1630
of the input data x,|
|

358
00:10:51,730 --> 00:10:53,055
0,400 480,725 725,875 875,1100 1100,1325
but also we have this
我们还有另一个变量 h ，

359
00:10:53,055 --> 00:10:55,830
0,270 270,635 1015,1415 1885,2265 2265,2775
other variable h,| which captures
|它捕获了状态，捕获了记忆，

360
00:10:55,830 --> 00:10:57,800
0,195 195,435 435,690 690,1010 1330,1970
this notion of state, captures,

361
00:10:57,880 --> 00:10:59,270
0,485 485,650 650,875 875,1100 1100,1390
captures this notion of memory,|
|

362
00:10:59,560 --> 00:11:01,380
0,395 395,670 690,1210 1320,1640 1640,1820
that's being computed by the
由网络计算并随时间传递的。

363
00:11:01,380 --> 00:11:03,075
0,260 460,780 780,1035 1035,1350 1350,1695
network and passed on over

364
00:11:03,075 --> 00:11:03,880
0,365
time.|
|

365
00:11:04,660 --> 00:11:06,180
0,400 630,935 935,1100 1100,1310 1310,1520
Specifically, right to walk through
具体来说，

366
00:11:06,180 --> 00:11:08,310
0,260 550,855 855,1430 1450,1830 1830,2130
this,| our predicted output {ŷt
|我们预测的 ŷt 不仅取决于一次的输入，

367
00:11:08,310 --> 00:11:10,200
0,210 210,420 420,740 1180,1575 1575,1890
- - -} depends not

368
00:11:10,200 --> 00:11:11,175
0,240 240,435 435,660 660,870 870,975
only on the input at

369
00:11:11,175 --> 00:11:13,020
0,135 135,425 865,1265 1315,1620 1620,1845
a time,| but also this
|还取决于这个过去的记忆，这个过去的状态，

370
00:11:13,020 --> 00:11:14,660
0,285 285,650 670,1005 1005,1290 1290,1640
past memory, this past state,|
|

371
00:11:15,770 --> 00:11:18,205
0,400 570,815 815,965 965,1270 1770,2435
and it is this linkage
正是这种时间依赖性和重复性的联系，

372
00:11:18,205 --> 00:11:20,820
0,395 715,1320 1320,1800 1800,1995 1995,2615
of temporal dependence and recurrence,|
|

373
00:11:20,900 --> 00:11:22,390
0,290 290,695 695,1055 1055,1325 1325,1490
that defines this idea of
定义了重复性神经单位的概念。

374
00:11:22,390 --> 00:11:24,340
0,135 135,710 760,1170 1170,1460
a recurrent neural unit.|
|

375
00:11:24,840 --> 00:11:26,290
0,260 260,485 485,680 680,1000 1050,1450
What I've shown is,| this,
我所展示的是，|这种随着时间的推移而展开的联系，

376
00:11:26,400 --> 00:11:28,235
0,350 350,695 695,1040 1040,1220 1220,1835
this connection that's being unrolled

377
00:11:28,235 --> 00:11:30,040
0,240 240,605 1135,1410 1410,1545 1545,1805
over time,| but we can
|但我们也可以根据一个循环来描述这种关系，

378
00:11:30,060 --> 00:11:33,070
0,400 930,1475 1475,1790 1790,2140 2610,3010
also depict this relationship according

379
00:11:33,270 --> 00:11:34,780
0,365 365,605 605,880
to a loop,|
|

380
00:11:34,910 --> 00:11:36,910
0,320 320,940 1170,1415 1415,1640 1640,2000
this computation to this internal
对 t 的这个内部状态变量 h 的计算

381
00:11:36,910 --> 00:11:38,430
0,300 300,620 700,1005 1005,1215 1215,1520
state variable h of t|
|

382
00:11:38,630 --> 00:11:40,480
0,305 305,530 530,1265 1265,1565 1565,1850
is being iteratively updated over
随着时间的推移被迭代更新，

383
00:11:40,480 --> 00:11:42,295
0,350 700,975 975,1305 1305,1560 1560,1815
time,| and that's fled back
|然后返回到神经元中，

384
00:11:42,295 --> 00:11:44,790
0,365 835,1235 1255,1805 1855,2130 2130,2495
into the neurons,| the neurons
|这个递归关系中的神经元计算，

385
00:11:45,050 --> 00:11:47,370
0,700 960,1250 1250,1430 1430,1955 1955,2320
computation in this recurrence relation,|
|

386
00:11:49,070 --> 00:11:50,400
0,275 275,455 455,680 680,965 965,1330
this is how we define
这就是我们如何定义这些递归[细胞]，

387
00:11:50,480 --> 00:11:52,800
0,365 365,980 980,1300 1470,1790 1790,2320
these recurrent cells,| that comprise
|组成递归神经网络或 RNN ，

388
00:11:53,090 --> 00:11:55,435
0,695 695,1025 1025,1300 1650,2030 2030,2345
recurrent neural networks or {RNN

389
00:11:55,435 --> 00:11:57,340
0,255 255,575 1165,1455 1455,1635 1635,1905
- -},| and the key
|这里的关键是我们有这个递归关系，

390
00:11:57,340 --> 00:11:58,405
0,300 300,495 495,660 660,840 840,1065
here is that we have

391
00:11:58,405 --> 00:12:00,415
0,335 925,1170 1170,1290 1290,1440 1440,2010
this idea of this recurrence

392
00:12:00,415 --> 00:12:03,160
0,365 805,1205 1555,2130 2130,2340 2340,2745
relation| that captures the cyclic
|捕获了循环时间依赖。

393
00:12:03,160 --> 00:12:05,800
0,530 730,1520
temporal dependency.|
|

394
00:12:06,150 --> 00:12:07,625
0,335 335,560 560,800 800,1060 1080,1475
And indeed, it's this idea
事实上，这个想法正是，

395
00:12:07,625 --> 00:12:09,230
0,315 315,570 570,855 855,1050 1050,1605
that is| really the intuitive
|递归神经网络或 RNN 背后的基础。

396
00:12:09,230 --> 00:12:11,770
0,290 970,1370 1510,2085 2085,2280 2280,2540
foundation behind recurrent neural networks

397
00:12:11,880 --> 00:12:13,360
0,290 290,820
or RNNs.|
|

398
00:12:13,360 --> 00:12:14,530
0,225 225,375 375,705 705,975 975,1170
And so let's continue to
所以，让我们从这里继续建立我们的理解，

399
00:12:14,530 --> 00:12:15,820
0,165 165,345 345,620 670,1035 1035,1290
build up our understanding from

400
00:12:15,820 --> 00:12:18,025
0,290 520,920 1090,1485 1485,1860 1860,2205
here| and move forward into
|并前进到我们可以如何在数学上和在代码中实际定义 RNN 操作。

401
00:12:18,025 --> 00:12:19,855
0,270 270,435 435,690 690,1085 1495,1830
how we can actually define

402
00:12:19,855 --> 00:12:22,300
0,180 180,695 715,1115 1195,1805 2155,2445
the RNN operations mathematically and

403
00:12:22,300 --> 00:12:23,340
0,210 210,530
in code.|
|

404
00:12:23,720 --> 00:12:24,535
0,335 335,560 560,665 665,725 725,815
So all {we're -} going
所以我们要做的就是让这种关系更正式一点，

405
00:12:24,535 --> 00:12:25,675
0,105 105,225 225,405 405,885 885,1140
to do is formalize this

406
00:12:25,675 --> 00:12:26,940
0,335 415,660 660,795 795,975 975,1265
relationship a little bit more,|
|

407
00:12:27,890 --> 00:12:29,590
0,290 290,545 545,830 830,1150 1410,1700
the key idea here is
这里的关键思想是，

408
00:12:29,590 --> 00:12:30,295
0,180 180,315 315,450 450,585 585,705
that,| the {RNN - -}
|RNN 保持状态，

409
00:12:30,295 --> 00:12:31,930
0,165 165,455 625,930 930,1235 1375,1635
is maintaining the state,| and
|它在每个时间步长更新状态，

410
00:12:31,930 --> 00:12:33,220
0,315 315,705 705,870 870,1065 1065,1290
it's updating the state at

411
00:12:33,220 --> 00:12:34,320
0,195 195,345 345,540 540,780 780,1100
each of these time steps|
|

412
00:12:34,610 --> 00:12:37,710
0,335 335,670 930,1310 1310,1690 2370,3100
as the sequence is processed.|
随着序列的处理。|

413
00:12:38,410 --> 00:12:39,825
0,335 335,590 590,800 800,1085 1085,1415
We define this by applying
我们通过应用这个递归关系来定义它，

414
00:12:39,825 --> 00:12:42,180
0,225 225,750 750,1085 1855,2160 2160,2355
this recurrence relation,| and what
|递归关系捕捉到的是，

415
00:12:42,180 --> 00:12:44,175
0,150 150,570 570,890 910,1580 1660,1995
the recurrence relation captures is|
|

416
00:12:44,175 --> 00:12:46,125
0,225 225,600 600,995 1015,1575 1575,1950
how we're actually updating that
我们是如何更新内部状态 ht 的，

417
00:12:46,125 --> 00:12:47,720
0,395 475,825 825,1080 1080,1290 1290,1595
internal state {ht - -},|
|

418
00:12:48,560 --> 00:12:51,100
0,400 1080,1445 1445,1810 1890,2225 2225,2540
specifically, that state update is
具体地说，状态更新与我们到目前为止介绍的任何其他神经网络操作完全一样，

419
00:12:51,100 --> 00:12:52,690
0,380 400,705 705,930 930,1245 1245,1590
exactly like any other neural

420
00:12:52,690 --> 00:12:55,810
0,260 400,800 1810,2130 2130,2600 2830,3120
network operation that we've introduced

421
00:12:55,810 --> 00:12:57,565
0,180 180,470 880,1200 1200,1455 1455,1755
so far,| where again, we're
|我们再次学习由一组权重 w 定义的函数，

422
00:12:57,565 --> 00:12:59,980
0,240 240,480 480,755 1855,2205 2205,2415
learning a function defined by

423
00:12:59,980 --> 00:13:01,080
0,165 165,330 330,465 465,780 780,1100
a set of weights w,|
|

424
00:13:01,840 --> 00:13:02,990
0,230 230,335 335,575 575,845 845,1150
{we're -} using that function
我们使用该函数来更新 ht 的单元状态，

425
00:13:03,070 --> 00:13:04,670
0,400 510,815 815,995 995,1235 1235,1600
to update the cell state

426
00:13:04,810 --> 00:13:06,360
0,305 305,500 500,790
{ht - -},|
|

427
00:13:06,370 --> 00:13:08,520
0,400 450,770 770,1090 1590,1925 1925,2150
and the additional component, the
另外一个因素，这里的新奇之处在于，

428
00:13:08,520 --> 00:13:10,140
0,435 435,770 1000,1260 1260,1410 1410,1620
newness here is that,| that
|函数依赖于输入和先前的时间步长 ht-1

429
00:13:10,140 --> 00:13:11,775
0,320 490,870 870,1185 1185,1395 1395,1635
function depends both on the

430
00:13:11,775 --> 00:13:13,575
0,365 715,1050 1050,1260 1260,1500 1500,1800
input and the prior time

431
00:13:13,575 --> 00:13:15,165
0,335 415,720 720,945 945,1245 1245,1590
step {ht-1 - - -

432
00:13:15,165 --> 00:13:16,480
0,365
-},|
|

433
00:13:16,890 --> 00:13:18,140
0,290 290,455 455,665 665,910 960,1250
and what you'll know is
你所知道的是，

434
00:13:18,140 --> 00:13:20,300
0,290 520,840 840,1160 1240,1640 1870,2160
that,| this function {fw -
|这个函数 fw 是由一组权重定义的，

435
00:13:20,300 --> 00:13:22,475
0,290 880,1245 1245,1575 1575,1920 1920,2175
-} is defined by a

436
00:13:22,475 --> 00:13:23,360
0,120 120,240 240,510 510,660 660,885
set of weights| and it's
|它是相同的一组权重，同样的一组参数，

437
00:13:23,360 --> 00:13:24,515
0,210 210,540 540,765 765,885 885,1155
the same set of weights,

438
00:13:24,515 --> 00:13:25,780
0,180 180,420 420,600 600,735 735,1265
the same set of parameters|
|

439
00:13:26,040 --> 00:13:27,725
0,260 260,440 440,760 1080,1430 1430,1685
that are used {time,step -}
随着时间的推移，

440
00:13:27,725 --> 00:13:29,440
0,195 195,435 435,785 925,1320 1320,1715
to time step| as the
|递归神经网络处理这些时间信息，这些序列数据。

441
00:13:29,580 --> 00:13:31,870
0,560 560,740 740,1000 1050,1450 1890,2290
recurrent neural network processes this

442
00:13:31,950 --> 00:13:34,330
0,580 720,1120 1320,1610 1610,2105 2105,2380
temporal information, this sequential data.|
|

443
00:13:36,000 --> 00:13:37,775
0,400 750,1010 1010,1160 1160,1445 1445,1775
Okay, so the key idea
好的，所以这里的关键思想是，

444
00:13:37,775 --> 00:13:39,760
0,335 385,765 765,1145 1315,1650 1650,1985
here, hopefully is coming through

445
00:13:40,080 --> 00:13:41,030
0,275 275,425 425,605 605,800 800,950
is that,| this {RNN -
|这个 RNN 状态更新操作接受这个状态，

446
00:13:41,030 --> 00:13:43,430
0,225 225,590 610,1010 1030,1430 2020,2400
-} state update operation takes

447
00:13:43,430 --> 00:13:45,515
0,375 375,770 940,1340 1540,1830 1830,2085
this state| and updates it
|并在每次处理序列时更新它。

448
00:13:45,515 --> 00:13:47,330
0,300 300,635 1045,1320 1320,1560 1560,1815
each time a sequence is

449
00:13:47,330 --> 00:13:48,340
0,620
processed.|
|

450
00:13:49,320 --> 00:13:50,950
0,260 260,515 515,910 1020,1325 1325,1630
We can also translate this
我们还可以将其转换为，

451
00:13:50,970 --> 00:13:52,610
0,400 720,1040 1040,1250 1250,1430 1430,1640
to| how we can think
|如何考虑在 Python 代码中实现 RNN ，

452
00:13:52,610 --> 00:13:56,075
0,320 730,1320 1320,1910 2080,2480 2950,3465
about implementing RNNs in Python

453
00:13:56,075 --> 00:13:57,000
0,335
code,|
|

454
00:13:57,000 --> 00:13:59,205
0,240 240,525 525,1250 1540,1920 1920,2205
or rather pseudocode,| hopefully getting
或者更确切地说伪代码，|有望更好地理解和直觉这些网络是如何工作的。

455
00:13:59,205 --> 00:14:01,100
0,165 165,425 655,1055 1105,1380 1380,1895
a better understanding and intuition

456
00:14:01,210 --> 00:14:02,720
0,320 320,515 515,740 740,1090 1110,1510
behind how these networks work.|
|

457
00:14:03,370 --> 00:14:04,695
0,260 260,395 395,545 545,820 960,1325
So what we do is,|
所以我们现在要做的是，|

458
00:14:04,695 --> 00:14:06,405
0,270 270,480 480,750 750,1115 1285,1710
we just start by defining
我们从定义 RNN 开始，

459
00:14:06,405 --> 00:14:08,115
0,105 105,240 240,360 360,605 1435,1710
an {RNN - -},| for
|现在这是抽象的 RNN ，

460
00:14:08,115 --> 00:14:09,470
0,180 180,345 345,600 600,1065 1065,1355
now, this is abstracted RNN,|
|

461
00:14:10,230 --> 00:14:12,050
0,400 450,740 740,1025 1025,1340 1340,1820
and we start, we initialize
我们初始化它的隐藏状态，

462
00:14:12,050 --> 00:14:13,730
0,255 255,525 525,890 1240,1530 1530,1680
its hidden state| and we
|我们有一些句子，

463
00:14:13,730 --> 00:14:15,425
0,180 180,405 405,710 850,1250 1330,1695
have some sentence, right,| let's
|假设这是我们感兴趣的输入，

464
00:14:15,425 --> 00:14:16,280
0,135 135,285 285,390 390,615 615,855
say this is our input

465
00:14:16,280 --> 00:14:18,620
0,225 225,590 850,1250 1540,2030 2050,2340
of interest,| where we're interested
|我们感兴趣的是预测这个句子中可能出现的下一个词，

466
00:14:18,620 --> 00:14:19,895
0,195 195,660 660,870 870,1050 1050,1275
in predicting maybe the next

467
00:14:19,895 --> 00:14:21,365
0,365 445,870 870,1170 1170,1320 1320,1470
word that's occurring in this

468
00:14:21,365 --> 00:14:22,340
0,275
sentence,|
|

469
00:14:22,580 --> 00:14:23,760
0,275 275,410 410,560 560,815 815,1180
what we can do is
我们可以做的是循环遍历句子中的单词，

470
00:14:23,990 --> 00:14:26,290
0,400 420,820 960,1360 1620,2000 2000,2300
loop through these individual words

471
00:14:26,290 --> 00:14:27,715
0,180 180,315 315,590 670,1050 1050,1425
in the sentence,| that define
|定义我们时间输入，

472
00:14:27,715 --> 00:14:30,100
0,395 415,995 1075,1475 1945,2205 2205,2385
our temporal input,| and at
|在每一步，当我们循环遍历时，

473
00:14:30,100 --> 00:14:31,540
0,285 285,585 585,870 870,1155 1155,1440
each step, as we're looping

474
00:14:31,540 --> 00:14:33,160
0,320 670,1020 1020,1275 1275,1440 1440,1620
through,| each word in that
|句子中的每个单词都被送到 RNN 模型中，

475
00:14:33,160 --> 00:14:35,490
0,320 790,1125 1125,1460 1570,1950 1950,2330
sentence is fed into the

476
00:14:35,690 --> 00:14:37,220
0,530 530,820
RNN model,|
|

477
00:14:37,260 --> 00:14:39,065
0,365 365,710 710,1025 1025,1360 1410,1805
along with the previous hidden
以及先前的隐藏状态，

478
00:14:39,065 --> 00:14:40,160
0,395
state,|
|

479
00:14:40,200 --> 00:14:41,870
0,305 305,485 485,650 650,940 1110,1670
and this is what generates
这就是为下一个单词生成预测，

480
00:14:41,870 --> 00:14:43,070
0,150 150,530 580,855 855,990 990,1200
a prediction for the next

481
00:14:43,070 --> 00:14:45,575
0,350 550,950 1570,1905 1905,2085 2085,2505
word| and updates the RNN
|并依次更新 RNN 状态。

482
00:14:45,575 --> 00:14:47,020
0,240 240,480 480,815
state in turn.|
|

483
00:14:47,300 --> 00:14:49,105
0,400 750,1055 1055,1445 1445,1670 1670,1805
Finally, our prediction for the
最后，我们对句子中最后一个单词的预测，

484
00:14:49,105 --> 00:14:50,250
0,255 255,555 555,735 735,870 870,1145
final word in the sentence,|
|

485
00:14:50,600 --> 00:14:51,630
0,275 275,440 440,605 605,785 785,1030
the word that we're missing,|
我们遗漏的单词，|

486
00:14:52,010 --> 00:14:53,890
0,290 290,575 575,860 860,1540 1560,1880
is simply the RNN's output,|
只是 RNN 的输出，|

487
00:14:53,890 --> 00:14:55,290
0,300 300,585 585,780 780,1035 1035,1400
after all the prior words
在所有先前的单词通过模型输入后。

488
00:14:55,490 --> 00:14:56,845
0,275 275,470 470,710 710,1030 1080,1355
have been fed in through

489
00:14:56,845 --> 00:14:57,780
0,135 135,395
the model.|
|

490
00:14:59,120 --> 00:15:00,655
0,400 540,830 830,1040 1040,1265 1265,1535
So this is really breaking
所以，这是分解了 RNN 是如何工作的，

491
00:15:00,655 --> 00:15:02,220
0,365 445,750 750,915 915,1275 1275,1565
down how the RNN works,|
|

492
00:15:02,330 --> 00:15:04,090
0,275 275,515 515,790 900,1175 1175,1760
how it's processing the sequential
它是如何处理序列信息的。

493
00:15:04,090 --> 00:15:05,320
0,380
information.|
|

494
00:15:05,330 --> 00:15:06,760
0,290 290,455 455,650 650,910 1140,1430
And what you've noticed is
你已经注意到的是，

495
00:15:06,760 --> 00:15:09,310
0,290 640,915 915,1320 1320,1940 2200,2550
that,| the RNN computation includes
|RNN 计算既包括对隐藏状态的更新，

496
00:15:09,310 --> 00:15:10,675
0,285 285,620 670,1020 1020,1230 1230,1365
both this update to the

497
00:15:10,675 --> 00:15:12,085
0,225 225,575 745,1050 1050,1215 1215,1410
hidden state,| as well as
|也包括在结束时生成一些预测输出，

498
00:15:12,085 --> 00:15:13,810
0,510 510,795 795,1335 1335,1575 1575,1725
generating some predicted output at

499
00:15:13,810 --> 00:15:15,235
0,135 135,380 550,945 945,1200 1200,1425
the end,| {that,is -} our
|这是我们感兴趣的终极目标。

500
00:15:15,235 --> 00:15:16,570
0,300 300,555 555,750 750,1095 1095,1335
ultimate goal that we're interested

501
00:15:16,570 --> 00:15:17,360
0,260
in.|
|

502
00:15:17,520 --> 00:15:19,055
0,305 305,610 840,1100 1100,1310 1310,1535
And so to walk through
让我们来了解一下，

503
00:15:19,055 --> 00:15:20,825
0,275 475,780 780,1110 1110,1350 1350,1770
this,| how we're actually generating
|我们是如何生成产出预测本身的，

504
00:15:20,825 --> 00:15:22,980
0,285 285,495 495,875 925,1325
the output prediction itself,|
|

505
00:15:23,340 --> 00:15:25,120
0,350 350,560 560,950 950,1400 1400,1780
well, the RNN computes is
RNN 计算得到一些输入向量，

506
00:15:25,350 --> 00:15:27,300
0,350 350,665 665,905 905,1210
given some input vector,|
|

507
00:15:27,740 --> 00:15:29,440
0,275 275,550 660,1100 1100,1385 1385,1700
it then performs this update
然后，它对隐藏状态执行这个更新，

508
00:15:29,440 --> 00:15:31,020
0,165 165,270 270,495 495,860
to the hidden state,|
|

509
00:15:31,650 --> 00:15:33,080
0,400 540,920 920,1190 1190,1310 1310,1430
and this update to the
这种对隐藏状态的更新只是一个标准的神经网络操作，

510
00:15:33,080 --> 00:15:34,640
0,255 255,540 540,860 1000,1320 1320,1560
hidden state is just a

511
00:15:34,640 --> 00:15:36,950
0,320 370,705 705,980 1180,1580 1990,2310
standard neural network operation,| just
|就像我们在第一节课中看到的，

512
00:15:36,950 --> 00:15:37,730
0,210 210,375 375,525 525,660 660,780
like we saw in the

513
00:15:37,730 --> 00:15:39,730
0,150 150,440 970,1305 1305,1620 1620,2000
first lecture,| where it consists
|它包括取权重矩阵乘以之前的隐藏状态，

514
00:15:39,750 --> 00:15:42,935
0,400 660,1055 1055,1355 1355,1990 2460,3185
of taking weight matrix multiplying

515
00:15:42,935 --> 00:15:44,930
0,365 745,1125 1125,1410 1410,1680 1680,1995
that by the previous hidden

516
00:15:44,930 --> 00:15:45,940
0,350
state,|
|

517
00:15:45,950 --> 00:15:47,950
0,365 365,665 665,890 890,1475 1475,2000
taking another weight matrix multiplying
取另一个权重矩阵将其乘以时间步长的输入，

518
00:15:47,950 --> 00:15:49,000
0,225 225,420 420,675 675,915 915,1050
that by the input at

519
00:15:49,000 --> 00:15:50,785
0,135 135,345 345,680 940,1340 1420,1785
a time step| and applying
|并应用非线性，

520
00:15:50,785 --> 00:15:52,420
0,240 240,1175
a non-linearity,|
|

521
00:15:52,560 --> 00:15:54,200
0,260 260,395 395,590 590,910 1320,1640
and in this case,| because
在这种情况下，|因为我们有这两个输入流，

522
00:15:54,200 --> 00:15:55,730
0,195 195,360 360,585 585,920 1210,1530
we have these two input

523
00:15:55,730 --> 00:15:56,720
0,470
streams,|
|

524
00:15:56,820 --> 00:15:58,360
0,395 395,755 755,1025 1025,1235 1235,1540
input data {xt - -}
输入数据 xt 和先前的状态 h ，

525
00:15:58,680 --> 00:16:00,280
0,305 305,500 500,790 840,1220 1220,1600
and the previous state h,|
|

526
00:16:00,690 --> 00:16:01,970
0,290 290,470 470,695 695,965 965,1280
we have these two separate
我们有两个单独的权重矩阵，

527
00:16:01,970 --> 00:16:03,880
0,315 315,1010 1240,1515 1515,1650 1650,1910
weight matrices,| that the network
|网络在其训练过程中学习，

528
00:16:03,900 --> 00:16:05,300
0,305 305,610 750,1070 1070,1250 1250,1400
is learning over the course

529
00:16:05,300 --> 00:16:06,560
0,135 135,300 300,620
of its training,|
|

530
00:16:06,740 --> 00:16:08,785
0,350 350,700 780,1180 1440,1790 1790,2045
that comes together,| we apply
这些结合在一起，|我们应用非线性，

531
00:16:08,785 --> 00:16:10,800
0,165 165,330 330,1055 1405,1710 1710,2015
the {non-linearity -}| and then
|然后我们可以在给定的时间步长生成输出，

532
00:16:11,300 --> 00:16:12,820
0,275 275,470 470,790 810,1210 1230,1520
we can generate an output

533
00:16:12,820 --> 00:16:13,915
0,135 135,255 255,480 480,765 765,1095
at a given time step|
|

534
00:16:13,915 --> 00:16:16,210
0,345 345,695 925,1655 1765,2040 2040,2295
by just modifying the hidden
通过修改隐藏状态，

535
00:16:16,210 --> 00:16:17,240
0,380
state,|
|

536
00:16:17,710 --> 00:16:19,455
0,365 365,605 605,860 860,1160 1160,1745
using a separate weight matrix
使用单独的权重矩阵更新这个值，

537
00:16:19,455 --> 00:16:21,225
0,305 655,945 945,1140 1140,1445 1495,1770
to update this value| and
|然后生成预测输出。

538
00:16:21,225 --> 00:16:23,120
0,275 325,720 720,990 990,1515 1515,1895
then generate a predicted output.|
|

539
00:16:24,420 --> 00:16:25,860
0,120 120,590 790,1095 1095,1260 1260,1440
And that's what there is
这就是问题所在，

540
00:16:25,860 --> 00:16:27,195
0,180 180,330 330,620 850,1215 1215,1335
to it, right,| that's how
|这就是 RNN 在其单个操作中

541
00:16:27,195 --> 00:16:29,000
0,135 135,665 775,1035 1035,1295 1405,1805
the RNN in its single

542
00:16:29,110 --> 00:16:31,860
0,400 1590,1925 1925,2255 2255,2525 2525,2750
operation| updates both the hidden
|更新隐藏状态并生成预测输出的方式。

543
00:16:31,860 --> 00:16:33,885
0,350 490,890 970,1275 1275,1740 1740,2025
state and also generates a

544
00:16:33,885 --> 00:16:35,660
0,570 570,965
predicted output.|
|

545
00:16:36,300 --> 00:16:38,240
0,400 810,1085 1085,1295 1295,1625 1625,1940
Okay, so now this gives
好的，现在这为你提供了 RNN 计算如何在特定时间步长发生的内部工作，

546
00:16:38,240 --> 00:16:39,880
0,210 210,465 465,780 780,1130 1240,1640
you the internal working of

547
00:16:40,050 --> 00:16:42,040
0,305 305,470 470,845 845,1450 1590,1990
how the RNN computation occurs

548
00:16:42,270 --> 00:16:44,080
0,380 380,755 755,1145 1145,1475 1475,1810
at a particular time step,|
|

549
00:16:44,540 --> 00:16:45,905
0,390 390,615 615,885 885,1140 1140,1365
let's next think about how
接下来，让我们考虑一下随着时间的推移这是什么样子，

550
00:16:45,905 --> 00:16:47,260
0,195 195,435 435,690 690,990 990,1355
this looks like over time|
|

551
00:16:47,670 --> 00:16:50,530
0,400 480,880 900,1300 1740,2465 2465,2860
and define the computational graph
并将 RNN 的计算图定义为在时间上展开或扩展。

552
00:16:50,730 --> 00:16:51,640
0,290 290,425 425,545 545,665 665,910
of the {RNN - -}

553
00:16:51,780 --> 00:16:53,950
0,320 320,640 690,1445 1445,1750 1770,2170
as being unrolled or expanded

554
00:16:54,090 --> 00:16:56,520
0,400 1020,1355 1355,1690
across across time.|
|

555
00:16:56,590 --> 00:16:58,050
0,275 275,455 455,680 680,965 965,1460
So, so far the dominant
所以，到目前为止，我所展示的 RNN 的主要方式是，

556
00:16:58,050 --> 00:16:59,085
0,210 210,480 480,615 615,855 855,1035
way I've been showing the

557
00:16:59,085 --> 00:17:00,200
0,105 105,210 210,420 420,750 750,1115
{RNN - -} is| according
|根据这个左边这个循环的图，

558
00:17:00,250 --> 00:17:02,505
0,320 320,640 720,1040 1040,1360 1650,2255
to this loop like diagram

559
00:17:02,505 --> 00:17:04,155
0,240 240,375 375,585 585,935 1255,1650
on the left, right,| feeding
|自己反馈，

560
00:17:04,155 --> 00:17:05,760
0,210 210,405 405,615 615,965
back in on itself,|
|

561
00:17:05,970 --> 00:17:07,400
0,320 320,530 530,680 680,905 905,1430
another way we can visualize
我们可以想象和思考 RNN 的另一种方式是，

562
00:17:07,400 --> 00:17:08,525
0,210 210,405 405,600 600,870 870,1125
and think about {RNN -}

563
00:17:08,525 --> 00:17:10,790
0,395 625,1025 1195,1440 1440,1575 1575,2265
is| as kind of unrolling
|随着时间的推移，在各个时间步长上展开这种递归，

564
00:17:10,790 --> 00:17:12,665
0,195 195,750 750,1100 1150,1515 1515,1875
this recurrence over time over

565
00:17:12,665 --> 00:17:14,840
0,395 955,1335 1335,1680 1680,1965 1965,2175
the individual time steps in

566
00:17:14,840 --> 00:17:16,660
0,195 195,500
our sequence,|
|

567
00:17:16,730 --> 00:17:17,935
0,290 290,485 485,770 770,1025 1025,1205
what this means is that,|
这意味着，|

568
00:17:17,935 --> 00:17:19,500
0,180 180,455 565,965 1075,1320 1320,1565
we can take the network
我们可以在第一时间步长时获取网络，

569
00:17:19,550 --> 00:17:21,120
0,395 395,695 695,950 950,1235 1235,1570
at our first time steps|
|

570
00:17:21,650 --> 00:17:23,830
0,400 690,1040 1040,1265 1265,1820 1820,2180
and continue to iteratively unroll
并继续迭代地在时间步长上展开它，

571
00:17:23,830 --> 00:17:25,350
0,285 285,675 675,945 945,1170 1170,1520
it across the time steps,|
|

572
00:17:26,390 --> 00:17:27,985
0,305 305,545 545,880 1140,1445 1445,1595
going on forward, all the
向前，直到我们处理输入中的所有时间步长。

573
00:17:27,985 --> 00:17:29,620
0,245 295,600 600,870 870,1235 1315,1635
way until we process all

574
00:17:29,620 --> 00:17:31,375
0,195 195,405 405,740 1240,1515 1515,1755
the time steps in our

575
00:17:31,375 --> 00:17:32,300
0,365
input.|
|

576
00:17:32,620 --> 00:17:34,350
0,400 630,890 890,1040 1040,1520 1520,1730
Now, we can formalize this
现在，我们可以进一步形式话这个图，

577
00:17:34,350 --> 00:17:35,600
0,480 480,645 645,780 780,960 960,1250
diagram a little bit more|
|

578
00:17:35,950 --> 00:17:38,270
0,400 540,1115 1115,1385 1385,1640 1640,2320
by defining the weight matrices
通过定义将输入连接到隐藏状态更新的权重矩阵，

579
00:17:38,560 --> 00:17:41,235
0,400 450,850 1230,1630 1680,2200 2310,2675
that connect the inputs to

580
00:17:41,235 --> 00:17:43,400
0,270 270,525 525,875 1135,1535
the hidden state update,|
|

581
00:17:43,410 --> 00:17:45,850
0,400 570,970 1230,1520 1520,2105 2105,2440
and the weight matrices that
以及用于随时间更新内部状态的权重矩阵，

582
00:17:46,020 --> 00:17:47,680
0,305 305,575 575,940 1050,1355 1355,1660
are used to update the

583
00:17:47,730 --> 00:17:50,315
0,365 365,730 1020,1385 1385,1750 2280,2585
internal state across time,| and
|以及最后定义更新到生成预测输出的权重矩阵。

584
00:17:50,315 --> 00:17:51,740
0,285 285,525 525,660 660,1170 1170,1425
finally, the weight matrices that

585
00:17:51,740 --> 00:17:54,470
0,350 610,1010 1960,2280 2280,2475 2475,2730
define the update to generate

586
00:17:54,470 --> 00:17:55,960
0,240 240,735 735,1130
a predicted output.|
|

587
00:17:56,840 --> 00:17:59,020
0,400 690,1010 1010,1330 1620,1940 1940,2180
Now recall that,| in all
现在回想一下，|在所有这些情况下，

588
00:17:59,020 --> 00:18:00,670
0,255 255,585 585,980 1180,1455 1455,1650
these cases, right,| for all
|对于所有这三个权重矩阵加上所有这些时间步长，

589
00:18:00,670 --> 00:18:02,430
0,240 240,450 450,630 630,1250 1360,1760
these three weight matrices add

590
00:18:02,630 --> 00:18:04,195
0,320 320,575 575,845 845,1180 1320,1565
all these time steps,| we
|我们只是重复使用相同的权重矩阵，

591
00:18:04,195 --> 00:18:05,850
0,135 135,420 420,1050 1050,1350 1350,1655
are simply reusing the same

592
00:18:05,930 --> 00:18:08,260
0,305 305,940 1140,1540 1740,2030 2030,2330
weight matrices, right,| so it's
|所以这是一组参数，一组权重矩阵，

593
00:18:08,260 --> 00:18:09,625
0,270 270,465 465,600 600,1065 1065,1365
one set of parameters, one

594
00:18:09,625 --> 00:18:11,020
0,165 165,255 255,420 420,1025 1105,1395
set of weight matrices,| that
|只是按顺序处理这些信息。

595
00:18:11,020 --> 00:18:13,380
0,210 210,530 580,980 1060,1460 1570,2360
just process this information sequentially.|
|

596
00:18:14,950 --> 00:18:15,920
0,275 275,410 410,530 530,680 680,970
Now you may be thinking,|
现在你可能在想，|

597
00:18:16,030 --> 00:18:17,505
0,400 630,950 950,1145 1145,1265 1265,1475
okay, so how do we
好的，那么我们如何开始思考，

598
00:18:17,505 --> 00:18:18,560
0,270 270,465 465,615 615,765 765,1055
actually start to be thinking

599
00:18:18,670 --> 00:18:20,490
0,400 570,860 860,1085 1085,1420 1560,1820
about| how to train the
|如何训练 RNN ，

600
00:18:20,490 --> 00:18:21,495
0,120 120,225 225,470 520,810 810,1005
{RNN - -},| how to
|如何定义损失。

601
00:18:21,495 --> 00:18:22,960
0,210 210,375 375,635
define the loss.|
|

602
00:18:22,960 --> 00:18:23,995
0,240 240,450 450,630 630,825 825,1035
Given that we have this
假设我们在这个时间依赖中有这个时间处理，

603
00:18:23,995 --> 00:18:26,020
0,515 715,1115 1195,1455 1455,1605 1605,2025
temporal processing in this temporal

604
00:18:26,020 --> 00:18:27,140
0,530
dependence,|
|

605
00:18:27,260 --> 00:18:29,310
0,400 810,1100 1100,1505 1505,1775 1775,2050
well, a prediction at an
好的，在单个时间点的预测将简单地等同于在该特定时间点的计算损失，

606
00:18:29,330 --> 00:18:31,680
0,395 395,710 710,1030 1350,1750 1950,2350
individual time step will simply

607
00:18:31,910 --> 00:18:33,750
0,400 510,785 785,1055 1055,1520 1520,1840
amount to a computed loss

608
00:18:33,950 --> 00:18:35,490
0,305 305,545 545,875 875,1205 1205,1540
at that particular time step,|
|

609
00:18:36,110 --> 00:18:37,420
0,290 290,545 545,770 770,1010 1010,1310
so now we can compare
所以，现在我们可以将这些预测逐一与真实标签进行比较，

610
00:18:37,420 --> 00:18:38,935
0,255 255,840 840,1140 1140,1335 1335,1515
those predictions time step by

611
00:18:38,935 --> 00:18:40,675
0,255 255,605 955,1290 1290,1530 1530,1740
time step to the true

612
00:18:40,675 --> 00:18:42,780
0,305 775,1110 1110,1445 1555,1830 1830,2105
label| and generate a loss
|并生成这些时间步长的损失值。

613
00:18:42,890 --> 00:18:44,370
0,400 450,725 725,920 920,1160 1160,1480
value for those time steps.|
|

614
00:18:45,040 --> 00:18:46,300
0,300 300,620 670,945 945,1095 1095,1260
And finally, we can get
最后，我们可以获得全部损失，

615
00:18:46,300 --> 00:18:48,160
0,225 225,540 540,920 1240,1575 1575,1860
our total loss| by taking
|通过将所有这些单独的损失项加在一起，

616
00:18:48,160 --> 00:18:50,340
0,225 225,500 880,1230 1230,1580 1780,2180
all these individual loss terms

617
00:18:50,390 --> 00:18:52,400
0,400 570,875 875,1190 1190,1480
together and summing them,|
|

618
00:18:52,500 --> 00:18:54,200
0,610 630,920 920,1160 1160,1490 1490,1700
defining the total loss for
定义对 RNN 的特定输入的总损失。

619
00:18:54,200 --> 00:18:55,880
0,165 165,500 1060,1395 1395,1575 1575,1680
a particular input to the

620
00:18:55,880 --> 00:18:57,320
0,500
RNN.|
|

621
00:18:58,050 --> 00:18:59,240
0,275 275,410 410,545 545,820 840,1190
If we can walk through
如果我们可以演示，

622
00:18:59,240 --> 00:19:00,620
0,285 285,600 600,900 900,1125 1125,1380
an example| of how we
|如何从头开始在 TensorFlow 中实现这个 RNN ，

623
00:19:00,620 --> 00:19:02,615
0,285 285,510 510,1020 1020,1350 1350,1995
implement this RNN in TensorFlow

624
00:19:02,615 --> 00:19:04,220
0,270 270,525 525,845
starting from scratch,|
|

625
00:19:04,410 --> 00:19:05,360
0,260 260,380 380,485 485,695 695,950
the {RNN - -} can
RNN 可以定义为层操作和层类，

626
00:19:05,360 --> 00:19:06,850
0,240 240,570 570,945 945,1215 1215,1490
be defined as a layer

627
00:19:07,770 --> 00:19:09,350
0,400 450,770 770,980 980,1250 1250,1580
operation and a layer class|
|

628
00:19:09,350 --> 00:19:10,970
0,350 490,890 1000,1320 1320,1500 1500,1620
that Alexander introduced in the
像 Alexander 在上节课介绍的。

629
00:19:10,970 --> 00:19:12,240
0,165 165,470
first lecture.|
|

630
00:19:12,240 --> 00:19:13,395
0,120 120,410 430,705 705,915 915,1155
And so we can define
所以我们可以根据权重矩阵的初始化，隐藏状态的初始化来定义它，

631
00:19:13,395 --> 00:19:15,920
0,285 285,665 685,1085 1435,1800 1800,2525
it according to an initialization

632
00:19:15,970 --> 00:19:18,675
0,400 420,725 725,1360 1680,2480 2480,2705
of weight matrices, initialization of

633
00:19:18,675 --> 00:19:20,480
0,120 120,360 360,725 1075,1440 1440,1805
a hidden state,| which commonly
|通常相当于将这两个初始化为零。

634
00:19:21,250 --> 00:19:23,090
0,365 365,620 620,1090 1110,1475 1475,1840
amounts to initializing these two

635
00:19:23,290 --> 00:19:24,440
0,275 275,550
to zero.|
|

636
00:19:25,380 --> 00:19:26,930
0,400 510,785 785,995 995,1295 1295,1550
Next, we can define how
接下来，我们可以定义如何通过 RNN 网络

637
00:19:26,930 --> 00:19:28,750
0,150 150,410 460,860 1060,1440 1440,1820
we can actually pass forward

638
00:19:28,890 --> 00:19:30,995
0,305 305,485 485,890 890,1180 1770,2105
through the RNN network| to
|以处理给定的输入 X ，

639
00:19:30,995 --> 00:19:32,710
0,335 505,795 795,1085 1105,1410 1410,1715
process a given input X,|
|

640
00:19:33,500 --> 00:19:34,640
0,210 210,360 360,570 570,825 825,1140
and what you'll notice is,|
你们会注意到，|

641
00:19:34,640 --> 00:19:37,265
0,240 240,560 760,1160 1420,1820 2350,2625
in this forward operation,| the
在这个正向运算中，|计算和我们刚刚看过的一样，

642
00:19:37,265 --> 00:19:39,065
0,540 540,905 925,1320 1320,1620 1620,1800
computations are exactly like we

643
00:19:39,065 --> 00:19:40,340
0,180 180,420 420,755
just walked through,|
|

644
00:19:40,340 --> 00:19:41,675
0,210 210,500 700,975 975,1110 1110,1335
we first update the hidden
我们首先更新隐藏状态，

645
00:19:41,675 --> 00:19:44,225
0,365 1165,1530 1530,1740 1740,1985 2185,2550
state| according to that equation
|根据前面介绍的方程，

646
00:19:44,225 --> 00:19:46,060
0,330 330,630 630,965
we introduced earlier,|
|

647
00:19:46,060 --> 00:19:48,240
0,240 240,530 1000,1380 1380,1650 1650,2180
and then generate a predicted
然后生成预测输出，

648
00:19:48,260 --> 00:19:49,900
0,305 305,515 515,755 755,1090 1260,1640
output| that is a transformed
|是隐藏状态的变换版本，

649
00:19:49,900 --> 00:19:51,390
0,380 400,690 690,870 870,1125 1125,1490
version of that hidden state,|
|

650
00:19:52,540 --> 00:19:53,845
0,180 180,470 490,795 795,1035 1035,1305
and finally, at each time
最后，在每个时间步长，

651
00:19:53,845 --> 00:19:56,170
0,300 300,645 645,1025 1315,1715 1945,2325
step,| we return both the
|我们返回输出和更新的隐藏状态，

652
00:19:56,170 --> 00:19:58,015
0,380 610,945 945,1215 1215,1515 1515,1845
output and the updated hidden

653
00:19:58,015 --> 00:19:59,410
0,365 535,840 840,1020 1020,1185 1185,1395
state,| as this is what
|因为这是随着时间的推移继续该 RNN 操作所必需的存储，

654
00:19:59,410 --> 00:20:01,020
0,320 610,945 945,1125 1125,1275 1275,1610
is necessary to be stored

655
00:20:01,190 --> 00:20:03,505
0,365 365,680 680,980 980,1630 1920,2315
to continue this RNN operation

656
00:20:03,505 --> 00:20:04,660
0,330 330,665
over time,|
|

657
00:20:05,420 --> 00:20:06,900
0,245 245,380 380,665 665,1060 1080,1480
what is very convenient is,|
非常方便的是，|

658
00:20:07,280 --> 00:20:08,680
0,400 510,800 800,950 950,1145 1145,1400
that although you could define
尽管你可以完全从头开始定义 RNN 网络和 RNN 层，

659
00:20:08,680 --> 00:20:09,940
0,210 210,630 630,900 900,1140 1140,1260
your RNN network and your

660
00:20:09,940 --> 00:20:11,730
0,390 390,680 850,1185 1185,1455 1455,1790
RNN layer completely from scratch,|
|

661
00:20:11,990 --> 00:20:14,320
0,260 260,440 440,1120 1650,2075 2075,2330
is that TensorFlow abstracts this
但 TensorFlow 为你抽象了这一操作，

662
00:20:14,320 --> 00:20:16,150
0,375 375,705 705,945 945,1250 1480,1830
operation away for you,| {so,you
|所以你可以简单地定义一个简单的 RNN ，

663
00:20:16,150 --> 00:20:17,590
0,210 210,375 375,680 730,1095 1095,1440
-} can simply define a

664
00:20:17,590 --> 00:20:19,090
0,315 315,525 525,645 645,890 1120,1500
simple {RNN - -},| according
|根据你在这里看到的这个调用，

665
00:20:19,090 --> 00:20:21,580
0,380 730,1130 1270,1670 1810,2210 2230,2490
to this, this call that

666
00:20:21,580 --> 00:20:24,120
0,180 180,360 360,680 1300,1700 2140,2540
you're seeing here,| {} which
|这使得所有的计算都非常有效，非常容易，

667
00:20:24,380 --> 00:20:26,290
0,350 350,650 650,905 905,1210 1260,1910
yeah, makes all the computations

668
00:20:26,290 --> 00:20:28,110
0,330 330,705 705,1100 1180,1500 1500,1820
very efficient and very easy,|
|

669
00:20:28,850 --> 00:20:30,000
0,245 245,515 515,710 710,860 860,1150
and you'll actually get practice
你将在今天的软件实验中练习实现和使用 RNN 。

670
00:20:30,530 --> 00:20:32,830
0,515 515,695 695,980 980,1360 1680,2300
implementing and working with RNNs

671
00:20:32,830 --> 00:20:34,920
0,315 315,885 885,1215 1215,1580
in today's software lab.|
|

672
00:20:36,290 --> 00:20:38,440
0,400 660,1060 1380,1700 1700,1895 1895,2150
Okay, so that gives us
好的，这给了我们对 RNN 的理解，

673
00:20:38,440 --> 00:20:39,595
0,360 360,675 675,885 885,1035 1035,1155
the understanding of {RNNs -

674
00:20:39,595 --> 00:20:41,635
0,245 355,755 985,1335 1335,1665 1665,2040
-}| and going back to
|回到我所描述的那种问题设置或问题定义，

675
00:20:41,635 --> 00:20:43,405
0,255 255,515 805,1065 1065,1325 1435,1770
what I, what I described

676
00:20:43,405 --> 00:20:44,370
0,240 240,390 390,480 480,645 645,965
as kind of the problem

677
00:20:44,420 --> 00:20:46,020
0,470 470,590 590,725 725,980 980,1600
setups or the problem definitions|
|

678
00:20:46,310 --> 00:20:47,305
0,290 290,485 485,695 695,860 860,995
at the beginning of this
在这节课开始时（说的）。

679
00:20:47,305 --> 00:20:48,300
0,275
lecture.|
|

680
00:20:48,340 --> 00:20:49,350
0,275 275,440 440,605 605,785 785,1010
I just want to remind
我只想提醒你们，

681
00:20:49,350 --> 00:20:50,460
0,195 195,360 360,555 555,795 795,1110
you| of the types of
|我们可以应用 RNN 解决序列建模问题，

682
00:20:50,460 --> 00:20:52,755
0,360 360,870 870,1190 1720,2040 2040,2295
sequence modeling problems on which

683
00:20:52,755 --> 00:20:54,740
0,225 225,515 565,965 1015,1620 1620,1985
we can apply RNNs right,|
|

684
00:20:55,390 --> 00:20:56,745
0,260 260,410 410,605 605,910 1020,1355
we can think about taking
我们可以考虑取一个输入序列，

685
00:20:56,745 --> 00:20:58,610
0,225 225,450 450,750 750,1235 1465,1865
a sequence of inputs,| producing
|在序列的末尾产生一个预测输出，

686
00:20:58,630 --> 00:21:00,495
0,400 570,1235 1235,1550 1550,1745 1745,1865
one predicted output at the

687
00:21:00,495 --> 00:21:02,060
0,165 165,360 360,510 510,785
end of the sequence,|
|

688
00:21:02,100 --> 00:21:03,260
0,260 260,395 395,575 575,860 860,1160
we can think about taking
我们可以考虑采用静态的单一输入，

689
00:21:03,260 --> 00:21:05,105
0,225 225,560 880,1280 1360,1650 1650,1845
a static single input| and
|并尝试根据该单一输入生成文本，

690
00:21:05,105 --> 00:21:07,540
0,270 270,525 525,815 1135,1535 2035,2435
trying to generate text according

691
00:21:07,650 --> 00:21:09,760
0,400 1110,1460 1460,1655 1655,1805 1805,2110
to according to that single

692
00:21:09,780 --> 00:21:10,780
0,400
input,|
|

693
00:21:11,230 --> 00:21:12,450
0,335 335,650 650,875 875,1010 1010,1220
and finally, we can think
最后，我们可以考虑获取一个输入序列，

694
00:21:12,450 --> 00:21:14,025
0,320 460,780 780,990 990,1245 1245,1575
about taking a sequence of

695
00:21:14,025 --> 00:21:16,155
0,515 985,1350 1350,1575 1575,1875 1875,2130
inputs,| producing a prediction at
|在这个序列中的每个时间步长产生一个预测，

696
00:21:16,155 --> 00:21:17,340
0,300 300,630 630,870 870,1020 1020,1185
every time step in that

697
00:21:17,340 --> 00:21:20,420
0,320 1120,1520 1540,1940 2470,2775 2775,3080
sequence,| and then doing this
|然后对这个序列进行预测和翻译。

698
00:21:20,470 --> 00:21:21,825
0,380 380,650 650,935 935,1205 1205,1355
sequence to sequence type of

699
00:21:21,825 --> 00:21:24,220
0,360 360,615 615,905
prediction and translation.|
|

700
00:21:25,730 --> 00:21:26,640
0,400
Okay,
好的，所以，

701
00:21:26,800 --> 00:21:28,320
0,400
so,|
|

702
00:21:28,450 --> 00:21:29,505
0,290 290,500 500,695 695,875 875,1055
yeah, {so\,,so -} this will
所以，这将是今天软件实验的基础，

703
00:21:29,505 --> 00:21:32,910
0,225 225,570 570,965 1195,1595 3115,3405
be the foundation for the

704
00:21:32,910 --> 00:21:34,800
0,270 270,600 600,950 1450,1725 1725,1890
software lab today,| which will
|它将专注于多对多处理和多对多序列建模的问题，

705
00:21:34,800 --> 00:21:36,620
0,285 285,680 730,1080 1080,1425 1425,1820
focus on this problem of,

706
00:21:36,790 --> 00:21:38,690
0,400 450,815 815,1040 1040,1300 1500,1900
of many to many processing

707
00:21:38,830 --> 00:21:40,365
0,335 335,605 605,785 785,995 995,1535
and many to many sequential

708
00:21:40,365 --> 00:21:42,375
0,485 565,900 900,1110 1110,1385 1645,2010
modeling,| taking a sequence, going
|取一个序列，到一个序列。

709
00:21:42,375 --> 00:21:43,660
0,210 210,330 330,605
to a sequence.|
|

710
00:21:44,640 --> 00:21:45,935
0,260 260,455 455,790 870,1160 1160,1295
What is common and what
我们可能想要考虑的所有这些类型的问题和任务的共同之处和普遍之处是，

711
00:21:45,935 --> 00:21:47,630
0,245 265,665 865,1230 1230,1485 1485,1695
is universal across all these

712
00:21:47,630 --> 00:21:49,600
0,285 285,650 790,1190 1300,1635 1635,1970
types of problems and tasks|
|

713
00:21:49,650 --> 00:21:50,860
0,290 290,455 455,620 620,860 860,1210
that we may want to
我们考虑 RNN 的是，

714
00:21:51,120 --> 00:21:52,150
0,320 320,515 515,665 665,785 785,1030
consider with {RNN - -}

715
00:21:52,560 --> 00:21:53,480
0,305 305,470 470,605 605,770 770,920
is what,| I like to
|我想我们需要什么样的设计标准，

716
00:21:53,480 --> 00:21:55,420
0,180 180,500 820,1220 1240,1590 1590,1940
think about, what type of

717
00:21:55,560 --> 00:21:57,605
0,380 380,1120 1260,1565 1565,1820 1820,2045
design criteria we need| to
|来建立一个健壮而可靠的网络来处理这些序列建模问题。

718
00:21:57,605 --> 00:21:59,440
0,210 210,495 495,825 825,1205 1435,1835
build a robust and reliable

719
00:21:59,760 --> 00:22:01,850
0,400 510,860 860,1210 1320,1625 1625,2090
network for processing these sequential

720
00:22:01,850 --> 00:22:03,200
0,375 375,680
modeling problems.|
|

721
00:22:03,470 --> 00:22:04,375
0,260 260,395 395,545 545,695 695,905
What I mean by that
我的意思是，

722
00:22:04,375 --> 00:22:06,840
0,285 285,510 510,675 675,965 1525,2465
is,| what are the characteristics,|
|它们的特点是什么，|

723
00:22:07,070 --> 00:22:08,940
0,260 260,410 410,700 990,1390 1470,1870
what are the design requirements
RNN 需要满足哪些设计要求，

724
00:22:09,350 --> 00:22:10,435
0,275 275,410 410,710 710,860 860,1085
that the {RNN -} needs

725
00:22:10,435 --> 00:22:12,145
0,285 285,635 835,1155 1155,1470 1470,1710
to fulfill| in order to
|为了能够有效地处理序列数据。

726
00:22:12,145 --> 00:22:13,975
0,135 135,390 390,675 675,995 1225,1830
be able to handle sequential

727
00:22:13,975 --> 00:22:15,380
0,255 255,635
data effectively.|
|

728
00:22:16,200 --> 00:22:18,155
0,275 275,545 545,830 830,1120 1350,1955
The first is that sequences
第一个是序列可以有不同的长度，

729
00:22:18,155 --> 00:22:19,600
0,195 195,405 405,660 660,930 930,1445
can be of different lengths,

730
00:22:19,710 --> 00:22:21,425
0,400 900,1175 1175,1310 1310,1460 1460,1715
right,| they may be short,
|它们可能很短，也可能很长，

731
00:22:21,425 --> 00:22:22,820
0,240 240,375 375,510 510,785 1105,1395
they may be long,| {we,want
|我们希望我们的 RNN 模型通用，

732
00:22:22,820 --> 00:22:24,215
0,210 210,435 435,870 870,1140 1140,1395
-} our RNN model or

733
00:22:24,215 --> 00:22:25,340
0,165 165,405 405,630 630,915 915,1125
our neural network model in

734
00:22:25,340 --> 00:22:26,675
0,290 430,675 675,810 810,1080 1080,1335
general| to be able to
|能够处理可变长度的序列。

735
00:22:26,675 --> 00:22:28,810
0,275 535,1125 1125,1290 1290,1560 1560,2135
handle sequences of variable lengths.|
|

736
00:22:29,880 --> 00:22:32,140
0,590 590,845 845,1190 1190,1570 1860,2260
Secondly, and really importantly is,|
第二，也是非常重要的一点是，|

737
00:22:32,490 --> 00:22:33,820
0,320 320,485 485,665 665,965 965,1330
as we were discussing earlier,|
正如我们之前讨论的那样，|

738
00:22:34,320 --> 00:22:35,420
0,260 260,410 410,635 635,890 890,1100
that the whole point of
思考序列长度的整个关键是，

739
00:22:35,420 --> 00:22:36,800
0,255 255,620 700,1035 1035,1245 1245,1380
thinking about things through the

740
00:22:36,800 --> 00:22:38,045
0,195 195,390 390,680 730,1035 1035,1245
length of sequence is,| to
|试图跟踪和了解数据中随着时间的推移的依赖关系，

741
00:22:38,045 --> 00:22:39,850
0,300 300,695 865,1230 1230,1500 1500,1805
try to track and learn

742
00:22:39,900 --> 00:22:41,630
0,680 680,830 830,965 965,1240 1470,1730
dependencies in the data that

743
00:22:41,630 --> 00:22:43,500
0,240 240,570 570,900 900,1280
are related over time,|
|

744
00:22:43,510 --> 00:22:44,745
0,260 260,410 410,695 695,1010 1010,1235
so our model really needs
所以，我们的模型需要能够处理这些不同的依赖关系，

745
00:22:44,745 --> 00:22:46,215
0,150 150,270 270,545 595,995 1075,1470
to be able to handle

746
00:22:46,215 --> 00:22:48,500
0,315 315,630 630,1385 1585,1935 1935,2285
those different dependencies,| which may
|这些依赖关系可能会在彼此相隔很远发生。

747
00:22:48,790 --> 00:22:49,875
0,275 275,485 485,755 755,935 935,1085
occur at times that are

748
00:22:49,875 --> 00:22:51,240
0,270 270,570 570,905 925,1200 1200,1365
very, very distant from each

749
00:22:51,240 --> 00:22:52,180
0,290
other.|
|

750
00:22:53,520 --> 00:22:55,565
0,400 540,940 1110,1510 1530,1820 1820,2045
Next right sequence is all
下一个序列是关于顺序的，

751
00:22:55,565 --> 00:22:57,020
0,255 255,575 685,990 990,1260 1260,1455
about order, right,| there's some
|有一些概念，

752
00:22:57,020 --> 00:22:59,260
0,315 315,710 1000,1400
notion| of how
|关于当前的输入依赖于先前的输入，

753
00:22:59,260 --> 00:23:01,470
0,345 345,795 795,1110 1110,1460 1810,2210
current inputs depend on prior

754
00:23:01,580 --> 00:23:03,235
0,410 410,635 635,875 875,1180 1260,1655
inputs,| and the specific order
|我们看到的观察的特定顺序，

755
00:23:03,235 --> 00:23:06,000
0,390 390,900 900,1200 1200,1505 2365,2765
of observations we see,| makes
|对我们最终可能想要生成的预测有很大影响。

756
00:23:06,350 --> 00:23:08,170
0,350 350,695 695,1055 1055,1420 1470,1820
a big effect on what

757
00:23:08,170 --> 00:23:09,340
0,405 405,645 645,810 810,1005 1005,1170
prediction we may want to

758
00:23:09,340 --> 00:23:10,720
0,240 240,495 495,615 615,860
generate at the end.|
|

759
00:23:11,630 --> 00:23:13,060
0,305 305,610 630,920 920,1190 1190,1430
And finally, in order to
最后，为了能够有效地处理这些信息，

760
00:23:13,060 --> 00:23:15,660
0,150 150,440 520,920 1420,1820 2200,2600
be able to process this

761
00:23:15,680 --> 00:23:18,685
0,400 1200,1600 2040,2330 2330,2620 2670,3005
information effectively,| our network needs
|我们的网络需要能够进行我们所说的参数共享，

762
00:23:18,685 --> 00:23:20,095
0,180 180,315 315,605 625,1025 1135,1410
to be able to do

763
00:23:20,095 --> 00:23:21,450
0,150 150,300 300,495 495,1020 1020,1355
what we call parameter sharing,|
|

764
00:23:21,740 --> 00:23:23,560
0,335 335,670 870,1250 1250,1580 1580,1820
meaning that given one set
这意味着给定一组权重，

765
00:23:23,560 --> 00:23:24,985
0,180 180,650 790,1110 1110,1290 1290,1425
of weights,| that set of
|这组权重应该能够应用于序列中的不同时间步长，

766
00:23:24,985 --> 00:23:26,140
0,315 315,495 495,630 630,870 870,1155
weights should be able to

767
00:23:26,140 --> 00:23:27,430
0,255 255,465 465,720 720,1035 1035,1290
apply to different time steps

768
00:23:27,430 --> 00:23:29,050
0,180 180,315 315,590 940,1290 1290,1620
in the sequence| and still
|并仍然产生有意义的预测。

769
00:23:29,050 --> 00:23:30,900
0,360 360,740 760,1035 1035,1310 1360,1850
result in a meaningful prediction.|
|

770
00:23:31,660 --> 00:23:32,940
0,290 290,575 575,905 905,1160 1160,1280
And so today, we're going
所以，今天我们将关注，

771
00:23:32,940 --> 00:23:35,100
0,165 165,435 435,830 1060,1460 1600,2160
to focus on,| how recurrent
|递归神经网络如何满足这些设计标准，

772
00:23:35,100 --> 00:23:36,540
0,255 255,530 580,885 885,1155 1155,1440
neural networks meet these design

773
00:23:36,540 --> 00:23:38,540
0,710 880,1200 1200,1410 1410,1650 1650,2000
criteria,| and how these design
|以及这些设计标准如何激发对更强大的体系结构的需求，

774
00:23:38,590 --> 00:23:40,830
0,820 930,1505 1505,1670 1670,1910 1910,2240
criteria motivate the need for

775
00:23:40,830 --> 00:23:42,960
0,270 270,525 525,860 910,1820 1870,2130
even more powerful architectures,| that
|在序列建模方面可以超越 RNN 。

776
00:23:42,960 --> 00:23:45,225
0,150 150,830 940,1590 1590,1935 1935,2265
can outperform RNNs in sequence

777
00:23:45,225 --> 00:23:46,300
0,605
modeling.|
|

778
00:23:47,040 --> 00:23:49,385
0,400 540,940 1020,1340 1340,1660 1710,2345
So to understand these criteria
因此，为了非常具体地理解这些标准，

779
00:23:49,385 --> 00:23:51,200
0,275 355,935 1135,1455 1455,1575 1575,1815
very concretely,| we're going to
|我们将考虑一个序列建模问题，

780
00:23:51,200 --> 00:23:53,260
0,300 300,650 910,1290 1290,1755 1755,2060
consider a sequence modeling problem,|
|

781
00:23:53,700 --> 00:23:55,325
0,400 570,920 920,1175 1175,1415 1415,1625
where given some series of
在给定一系列单词的情况下，

782
00:23:55,325 --> 00:23:57,335
0,275 805,1170 1170,1515 1515,1800 1800,2010
words,| our task is just
|我们的任务只是预测该句子中的下一个单词，

783
00:23:57,335 --> 00:23:58,480
0,180 180,405 405,615 615,810 810,1145
to predict the next word

784
00:23:58,560 --> 00:24:00,060
0,275 275,470 470,790
in that sentence,|
|

785
00:24:00,280 --> 00:24:01,530
0,380 380,770 770,920 920,1085 1085,1250
so let's say we have
让我们假设我们有这句话，

786
00:24:01,530 --> 00:24:03,465
0,290 370,770 1000,1305 1305,1610 1630,1935
the sentence,| {this,morning -} I
|今天早上，我带着我的猫去散步，

787
00:24:03,465 --> 00:24:04,335
0,180 180,360 360,600 600,765 765,870
took my cat for a

788
00:24:04,335 --> 00:24:05,300
0,275
walk,|
|

789
00:24:05,580 --> 00:24:07,210
0,275 275,455 455,740 740,1120 1230,1630
and our task is to
我们的任务是预测句子中的最后一个单词，

790
00:24:07,230 --> 00:24:09,155
0,400 1020,1295 1295,1535 1535,1775 1775,1925
predict the last word in

791
00:24:09,155 --> 00:24:10,490
0,135 135,395 535,885 885,1110 1110,1335
the sentence,| given the prior
|给出之前的单词，

792
00:24:10,490 --> 00:24:12,245
0,350 790,1080 1080,1335 1335,1590 1590,1755
words,| {This,morning -} I took
|今天早上，我带着我的猫，空格，

793
00:24:12,245 --> 00:24:14,020
0,180 180,420 420,660 660,965 1375,1775
my cat for a, blank,|
|

794
00:24:15,630 --> 00:24:17,210
0,320 320,590 590,875 875,1210 1260,1580
our goal is to take
我们的目标是用我们的 RNN 来定义它，

795
00:24:17,210 --> 00:24:18,970
0,210 210,740 820,1125 1125,1395 1395,1760
our RNN define it| and
|在这项任务中对它进行测试。

796
00:24:20,430 --> 00:24:21,350
0,245 245,365 365,515 515,710 710,920
put it to test on

797
00:24:21,350 --> 00:24:22,520
0,180 180,470
this task.|
|

798
00:24:22,860 --> 00:24:24,170
0,305 305,530 530,770 770,1025 1025,1310
What is our first step
我们做这件事的第一步是什么，

799
00:24:24,170 --> 00:24:25,520
0,210 210,390 390,710
to doing this,|
|

800
00:24:25,990 --> 00:24:27,585
0,400 570,860 860,1100 1100,1370 1370,1595
well, the very, very first
好的，非常重要的第一步，

801
00:24:27,585 --> 00:24:28,695
0,195 195,450 450,660 660,855 855,1110
step,| before we even think
|在我们考虑定义 RNN 之前，

802
00:24:28,695 --> 00:24:30,090
0,285 285,870 870,1125 1125,1260 1260,1395
about defining the {RNN -

803
00:24:30,090 --> 00:24:32,150
0,260 610,1010 1360,1650 1650,1800 1800,2060
-}| is how we can
|我们如何表示这些信息给网络，

804
00:24:32,170 --> 00:24:34,185
0,290 290,580 810,1210 1290,1690 1740,2015
actually represent this information to

805
00:24:34,185 --> 00:24:35,445
0,135 135,395 685,930 930,1050 1050,1260
the network| in a way
|以一种它可以处理和理解的方式，

806
00:24:35,445 --> 00:24:37,160
0,270 270,465 465,725 745,1145 1315,1715
that it can process and

807
00:24:37,180 --> 00:24:38,260
0,400
understand,|
|

808
00:24:39,430 --> 00:24:40,400
0,290 290,455 455,590 590,710 710,970
if we have a model,|
如果我们有一个模型，|

809
00:24:40,420 --> 00:24:42,560
0,335 335,670 1080,1480 1560,1850 1850,2140
that is processing this data,
它处理这些数据，处理这些基于文本的数据，

810
00:24:42,640 --> 00:24:44,390
0,400 450,755 755,995 995,1330 1350,1750
processing this text based data,|
|

811
00:24:45,230 --> 00:24:46,930
0,400 450,845 845,1115 1115,1390 1410,1700
and wanting to generate text
并希望生成文本作为输出，

812
00:24:46,930 --> 00:24:48,400
0,290 400,800
as output,|
|

813
00:24:48,560 --> 00:24:50,350
0,380 380,760 780,1100 1100,1420 1500,1790
our problem can arise in
我们的问题可能出现在，

814
00:24:50,350 --> 00:24:52,195
0,290 670,945 945,1170 1170,1430 1510,1845
that,| the neural network itself
|神经网络本身显然并不具备处理语言的能力，

815
00:24:52,195 --> 00:24:54,180
0,240 240,545 925,1355 1375,1680 1680,1985
is not equipped to handle

816
00:24:54,680 --> 00:24:57,700
0,400 660,1360 2280,2585 2585,2780 2780,3020
language explicitly,| {remember,that -} neural
|记住，神经网络仅仅是函数运算符。它们只是数学运算，

817
00:24:57,700 --> 00:24:59,730
0,260 340,660 660,975 975,1545 1545,2030
networks are simply functional operators,

818
00:24:59,750 --> 00:25:02,100
0,320 320,455 455,1235 1235,1600
they're just mathematical operations,|
|

819
00:25:02,100 --> 00:25:03,510
0,150 150,405 405,675 675,1125 1125,1410
and so we can't expect
所以我们不能期待它，

820
00:25:03,510 --> 00:25:04,530
0,180 180,450 450,690 690,915 915,1020
it, right,| it doesn't have
|它不能从头开始理解单词是什么，或者语言是什么意思，

821
00:25:04,530 --> 00:25:06,405
0,260 280,680 1240,1515 1515,1665 1665,1875
an understanding from the start

822
00:25:06,405 --> 00:25:07,620
0,255 255,480 480,675 675,915 915,1215
of what a word is

823
00:25:07,620 --> 00:25:09,580
0,225 225,405 405,710 760,1160
or what language means,|
|

824
00:25:09,580 --> 00:25:10,585
0,165 165,420 420,660 660,840 840,1005
which means that we need
这意味着我们需要一种用数字表示语言的方法，

825
00:25:10,585 --> 00:25:12,000
0,150 150,375 375,600 600,875 1015,1415
a way to represent language

826
00:25:12,290 --> 00:25:14,020
0,730 960,1220 1220,1400 1400,1595 1595,1730
numerically,| so that it can
|这样它才能被传递到网络中进行处理。

827
00:25:14,020 --> 00:25:15,430
0,165 165,465 465,860 1000,1275 1275,1410
be passed in to the

828
00:25:15,430 --> 00:25:18,480
0,260 700,1100 1210,1610
network to process.|
|

829
00:25:19,160 --> 00:25:20,665
0,400 480,770 770,935 935,1205 1205,1505
So what we do is
所以我们要做的是，

830
00:25:20,665 --> 00:25:21,610
0,195 195,360 360,525 525,735 735,945
that,| we need to define
|我们需要定义一种方法，

831
00:25:21,610 --> 00:25:22,930
0,150 150,330 330,650 790,1080 1080,1320
a way| to translate this
|来将文本、语言信息转换成数字编码，一个向量，一个数字数组，

832
00:25:22,930 --> 00:25:26,500
0,285 285,620 1600,2000 2350,2750 3220,3570
text, this language information, into

833
00:25:26,500 --> 00:25:28,950
0,315 315,855 855,1460 1750,2070 2070,2450
a numerical encoding, a vector,

834
00:25:29,120 --> 00:25:30,700
0,335 335,665 665,935 935,1210 1320,1580
an array of numbers,| that
|可以输入到我们的神经网络中，

835
00:25:30,700 --> 00:25:32,280
0,135 135,360 360,710 1000,1290 1290,1580
can then be fed in

836
00:25:32,570 --> 00:25:33,990
0,245 245,365 365,590 590,850 1020,1420
to our neural network| and
|并生成一个数字向量作为它的输出。

837
00:25:34,100 --> 00:25:36,355
0,610 1200,1490 1490,1745 1745,1970 1970,2255
generating a vector of numbers

838
00:25:36,355 --> 00:25:37,760
0,300 300,600 600,995
as its output.|
|

839
00:25:39,520 --> 00:25:41,220
0,260 260,520 900,1190 1190,1550 1550,1700
So now this raises the
因此，现在这就提出了一个问题，

840
00:25:41,220 --> 00:25:42,230
0,210 210,465 465,660 660,780 780,1010
question| of how do we
|我们如何定义这种转变，

841
00:25:42,280 --> 00:25:44,550
0,365 365,730 750,1040 1040,1330 1980,2270
actually define this transformation,| how
|我们如何才能将语言转换成这种数字编码，

842
00:25:44,550 --> 00:25:46,220
0,150 150,410 430,780 780,1130 1270,1670
can we transform language into

843
00:25:46,420 --> 00:25:48,520
0,365 365,845 845,1450
this numerical encoding,|
|

844
00:25:48,520 --> 00:25:49,735
0,240 240,450 450,770 790,1050 1050,1215
the key solution and the
关键的解决方案，也是许多这样的网络工作的关键方式，

845
00:25:49,735 --> 00:25:50,830
0,210 210,405 405,690 690,945 945,1095
key way that a lot

846
00:25:50,830 --> 00:25:53,110
0,150 150,410 880,1280 1300,1700 1990,2280
of these networks work,| is
|就是这种嵌入的概念和概念，

847
00:25:53,110 --> 00:25:54,595
0,195 195,465 465,735 735,1040 1120,1485
this notion and concept of

848
00:25:54,595 --> 00:25:56,890
0,515 1105,1395 1395,1575 1575,1865 1945,2295
embedding,| {What,that -} means is
|这意味着这需要一些转变，

849
00:25:56,890 --> 00:25:59,970
0,675 675,950 1150,1550 2410,2745 2745,3080
it's some transformation,| that takes
|使用索引或可表示为索引的东西，

850
00:26:00,780 --> 00:26:03,120
0,650 940,1340 1630,1980 1980,2205 2205,2340
indices or something that can

851
00:26:03,120 --> 00:26:05,300
0,255 255,650 850,1140 1140,1430 1780,2180
be represented as an index|
|

852
00:26:05,740 --> 00:26:07,725
0,400 420,770 770,1295 1295,1600 1620,1985
into a numerical vector of
到给定大小的数值向量。

853
00:26:07,725 --> 00:26:09,480
0,365 385,735 735,1085
a given size.|
|

854
00:26:10,070 --> 00:26:10,975
0,245 245,350 350,470 470,650 650,905
So if we think about
所以，如果我们考虑一下嵌入的想法是如何对语言数据起作用的，

855
00:26:10,975 --> 00:26:12,370
0,240 240,525 525,780 780,1005 1005,1395
how this idea of embedding

856
00:26:12,370 --> 00:26:14,380
0,285 285,525 525,795 795,1190
works for language data,|
|

857
00:26:14,380 --> 00:26:16,420
0,165 165,375 375,690 690,1040 1150,2040
{let's -} consider a vocabulary
让我们来考虑一个我们的语言中可能存在的词汇，

858
00:26:16,420 --> 00:26:17,485
0,240 240,495 495,750 750,900 900,1065
of words that we can

859
00:26:17,485 --> 00:26:18,720
0,305 355,660 660,825 825,960 960,1235
possibly have in our language,|
|

860
00:26:19,640 --> 00:26:20,905
0,275 275,470 470,710 710,950 950,1265
and our goal is to
我们的目标是能够将我们词汇表中的这些单独的单词

861
00:26:20,905 --> 00:26:22,530
0,395 445,810 810,1050 1050,1275 1275,1625
be able to map these

862
00:26:22,580 --> 00:26:24,600
0,365 365,730 810,1085 1085,1250 1250,2020
individual words in our vocabulary|
|

863
00:26:25,130 --> 00:26:27,270
0,400 540,905 905,1460 1460,1760 1760,2140
to a numerical vector of
映射到固定大小的数字向量，

864
00:26:27,290 --> 00:26:29,360
0,320 320,640
fixed size,|
|

865
00:26:29,400 --> 00:26:30,275
0,305 305,485 485,620 620,740 740,875
one way we could do
我们可以做到这一点的一种方法是，

866
00:26:30,275 --> 00:26:32,105
0,210 210,420 420,695 1135,1620 1620,1830
this is| by defining all
|定义这个词汇表中可能出现的所有单词，

867
00:26:32,105 --> 00:26:33,500
0,180 180,455 535,900 900,1140 1140,1395
the possible words that could

868
00:26:33,500 --> 00:26:35,675
0,240 240,375 375,525 525,1220 1900,2175
occur in this vocabulary| and
|然后对它们进行索引，

869
00:26:35,675 --> 00:26:37,960
0,275 295,780 780,1055 1225,1805 1885,2285
then indexing them,| assigning an
|为每个不同的单词分配一个索引标签，

870
00:26:38,070 --> 00:26:39,530
0,395 395,790 870,1145 1145,1310 1310,1460
index label to each of

871
00:26:39,530 --> 00:26:41,320
0,255 255,570 570,890
these distinct words,|
|

872
00:26:41,330 --> 00:26:43,110
0,395 395,1025 1025,1220 1220,1460 1460,1780
a corresponds to index one,
a 对应于索引 1 ， cat 对应于索引 2 ，以此类推，

873
00:26:43,730 --> 00:26:45,700
0,410 410,670 810,1190 1190,1570 1710,1970
{cat,responds} to index two, so

874
00:26:45,700 --> 00:26:46,980
0,120 120,255 255,435 435,740
on and so forth,|
|

875
00:26:47,020 --> 00:26:48,830
0,290 290,575 575,1070 1070,1390 1410,1810
and this indexing maps these
这个索引将这些单词映射到数字，唯一的索引，

876
00:26:48,910 --> 00:26:51,170
0,350 350,700 780,1180 1320,1720 1860,2260
individual words to numbers, unique

877
00:26:51,310 --> 00:26:52,800
0,700
indices,|
|

878
00:26:53,060 --> 00:26:54,535
0,305 305,515 515,1040 1040,1235 1235,1475
what these indices can then
然后，这些所以可以定义的是我们所说的嵌入向量，

879
00:26:54,535 --> 00:26:55,990
0,365 475,780 780,960 960,1155 1155,1455
define is what we call

880
00:26:55,990 --> 00:26:58,340
0,380 730,1155 1155,1490
an embedding vector,|
|

881
00:26:58,340 --> 00:26:59,525
0,180 180,300 300,480 480,795 795,1185
which is a fixed length
这是一种固定长度的编码，

882
00:26:59,525 --> 00:27:01,690
0,750 750,1005 1005,1260 1260,1535 1765,2165
encoding,| where we've simply indicated
|当我们观察到该单词时，

883
00:27:01,950 --> 00:27:04,730
0,395 395,790 990,1390 2190,2510 2510,2780
a one value| at the
|我们只需在该单词的索引处表示一个值，

884
00:27:04,730 --> 00:27:06,755
0,350 940,1230 1230,1425 1425,1725 1725,2025
index for that word when

885
00:27:06,755 --> 00:27:08,280
0,270 270,540 540,750 750,1055
we observe that word,|
|

886
00:27:08,550 --> 00:27:09,635
0,275 275,425 425,605 605,845 845,1085
and this is called a
这就是所谓的 one-hot 嵌入，

887
00:27:09,635 --> 00:27:11,900
0,270 270,615 615,1145 1375,1775 1945,2265
{one-hot -} embedding,| where we
|我们有一个固定长度的词汇量向量，

888
00:27:11,900 --> 00:27:13,210
0,225 225,450 450,675 675,930 930,1310
have this fixed length vector

889
00:27:13,560 --> 00:27:14,555
0,305 305,500 500,695 695,860 860,995
of the size of our

890
00:27:14,555 --> 00:27:15,900
0,725
vocabulary,|
|

891
00:27:15,900 --> 00:27:17,415
0,210 210,560 940,1230 1230,1380 1380,1515
and each instance of the
词汇表的每个实例都对应着 one-hot 中的一个，

892
00:27:17,415 --> 00:27:19,665
0,690 690,1410 1410,1715 1735,2025 2025,2250
vocabulary corresponds to a {one-hot

893
00:27:19,665 --> 00:27:21,320
0,335 535,935
-} one,|
|

894
00:27:21,320 --> 00:27:23,600
0,195 195,465 465,1245 1245,1640
at the corresponding index.|
在相应的索引处。|

895
00:27:24,890 --> 00:27:26,320
0,365 365,605 605,755 755,980 980,1430
This is a very sparse
这是一种非常简单的方法，

896
00:27:26,320 --> 00:27:27,900
0,210 210,390 390,525 525,800 1180,1580
way to do this,| and
|它只是基于纯粹的对计数索引进行计数，

897
00:27:28,220 --> 00:27:31,110
0,380 380,650 650,1025 1025,1420 2310,2890
it's simply based on purely,

898
00:27:31,160 --> 00:27:32,850
0,455 455,710 710,950 950,1240 1290,1690
purely count the count index,|
|

899
00:27:33,260 --> 00:27:35,400
0,350 350,515 515,820 840,1240 1380,2140
there's no notion of semantic
这里没有语义信息的概念，

900
00:27:35,450 --> 00:27:37,930
0,400 780,1180 1410,1850 1850,2140 2190,2480
information,| meaning that's captured in
|这意味着在这种基于矢量的编码中捕捉到了语义信息。

901
00:27:37,930 --> 00:27:40,020
0,290 310,705 705,1005 1005,1670
this vector based encoding.|
|

902
00:27:40,460 --> 00:27:42,180
0,725 725,935 935,1115 1115,1370 1370,1720
Alternatively, what is very commonly
或者，通常更常见的做法是，

903
00:27:42,200 --> 00:27:43,825
0,400 600,890 890,1145 1145,1400 1400,1625
done is| to actually use
|使用神经网络来学习编码，来学习嵌入，

904
00:27:43,825 --> 00:27:45,475
0,195 195,420 420,695 1015,1335 1335,1650
a neural network to learn

905
00:27:45,475 --> 00:27:46,825
0,240 240,735 735,915 915,1110 1110,1350
an encoding, to learn an

906
00:27:46,825 --> 00:27:48,640
0,485 865,1140 1140,1290 1290,1530 1530,1815
embedding,| and the goal here
|这里的目标是，我们可以学习，

907
00:27:48,640 --> 00:27:49,590
0,195 195,345 345,480 480,645 645,950
is that, we can learn,|
|

908
00:27:49,820 --> 00:27:51,130
0,275 275,500 500,760 780,1070 1070,1310
a neural network that then
神经网络捕获我们输入数据中的一些内在含义或内在语义，

909
00:27:51,130 --> 00:27:53,215
0,510 510,830 880,1485 1485,1830 1830,2085
captures some inherent meaning or

910
00:27:53,215 --> 00:27:55,435
0,435 435,1205 1435,1710 1710,1965 1965,2220
inherent semantics in our input

911
00:27:55,435 --> 00:27:57,760
0,275 595,900 900,1205 1615,1980 1980,2325
data| and maps related words
|并映射相关的单词或相关的输入在这个嵌入空间中距离更近，

912
00:27:57,760 --> 00:27:59,540
0,330 330,680 850,1370
or related inputs

913
00:27:59,540 --> 00:28:01,250
0,380 490,855 855,1095 1095,1320 1320,1710
closer together in this embedding

914
00:28:01,250 --> 00:28:03,050
0,320 700,1035 1035,1275 1275,1560 1560,1800
space,| meaning that they'll have
|这意味着它们的数值向量彼此更相似。

915
00:28:03,050 --> 00:28:04,990
0,540 540,915 915,1095 1095,1370 1540,1940
numerical vectors that are more

916
00:28:05,490 --> 00:28:07,260
0,400 480,740 740,875 875,1150
similar to each other.|
|

917
00:28:07,780 --> 00:28:09,150
0,320 320,620 620,905 905,1130 1130,1370
This concept is really, really
这个概念非常重要，

918
00:28:09,150 --> 00:28:12,660
0,530 580,870 870,1095 1095,1430 3130,3510
foundational| to how these sequence
|关于这些序列建模网络的工作方式以及神经网络的一般工作方式。

919
00:28:12,660 --> 00:28:14,880
0,590 1060,1455 1455,1770 1770,1995 1995,2220
modeling networks work and how

920
00:28:14,880 --> 00:28:16,760
0,255 255,500 790,1190 1270,1575 1575,1880
neural networks work in general.|
|

921
00:28:17,860 --> 00:28:19,230
0,400 510,770 770,965 965,1190 1190,1370
{Okay\,,so,with - -} that in
好的，有了这些，

922
00:28:19,230 --> 00:28:20,145
0,225 225,420 420,555 555,705 705,915
hand,| we can go back
|我们可以回到我们的设计标准了，

923
00:28:20,145 --> 00:28:22,100
0,150 150,285 285,605 625,1355
to our design criteria,|
|

924
00:28:22,100 --> 00:28:23,555
0,255 255,525 525,705 705,980 1180,1455
thinking about the capabilities that
考虑到我们所需要的功能，

925
00:28:23,555 --> 00:28:25,355
0,195 195,515 1015,1410 1410,1665 1665,1800
we desire,| first we need
|首先我们需要能够处理可变长度的序列。

926
00:28:25,355 --> 00:28:26,290
0,120 120,225 225,420 420,645 645,935
to be able to handle

927
00:28:26,520 --> 00:28:28,860
0,400 420,820 1080,1780
variable length sequences.|
|

928
00:28:29,090 --> 00:28:30,250
0,275 275,500 500,785 785,1010 1010,1160
If we again want to
如果我们再次想要预测序列中的下一个单词，

929
00:28:30,250 --> 00:28:31,300
0,260 280,540 540,720 720,915 915,1050
predict the next word in

930
00:28:31,300 --> 00:28:32,275
0,120 120,380 430,690 690,810 810,975
the sequence,| we can have
|我们可以有短序列，我们可以有长序列，

931
00:28:32,275 --> 00:28:33,430
0,225 225,735 735,885 885,1005 1005,1155
short sequences, we can have

932
00:28:33,430 --> 00:28:34,885
0,225 225,890 940,1200 1200,1320 1320,1455
long sequences,| we can have
|我们可以有更长的句子，

933
00:28:34,885 --> 00:28:36,720
0,195 195,510 510,905
even longer sentences,|
|

934
00:28:36,720 --> 00:28:37,965
0,120 120,330 330,660 660,1005 1005,1245
and our key task is
我们的关键任务是希望能够跟踪所有这些不同长度的依赖关系。

935
00:28:37,965 --> 00:28:38,850
0,150 150,300 300,525 525,735 735,885
that we want to be

936
00:28:38,850 --> 00:28:41,235
0,285 285,570 570,860 880,1670 2020,2385
able to track dependencies across

937
00:28:41,235 --> 00:28:43,060
0,300 300,525 525,780 780,1325
all these different lengths.|
|

938
00:28:43,270 --> 00:28:44,580
0,305 305,485 485,635 635,910 1020,1310
And what we need, what
我们需要的，我们所说的依赖关系是指，

939
00:28:44,580 --> 00:28:46,125
0,180 180,390 390,660 660,1305 1305,1545
we mean by dependencies is

940
00:28:46,125 --> 00:28:47,390
0,150 150,345 345,540 540,815 865,1265
that,| there could be information
|在一个序列中，可能会有非常非常早的信息，

941
00:28:47,710 --> 00:28:49,080
0,350 350,620 620,920 920,1205 1205,1370
very, very early on in

942
00:28:49,080 --> 00:28:52,335
0,150 150,440 1270,1670 2740,3045 3045,3255
a sequence,| but that may
|但这些信息可能不相关，或者出现得很晚，直到序列中很晚的时候，

943
00:28:52,335 --> 00:28:53,700
0,210 210,465 465,815 895,1200 1200,1365
not be relevant or come

944
00:28:53,700 --> 00:28:55,020
0,180 180,500 550,855 855,1065 1065,1320
up late until very much

945
00:28:55,020 --> 00:28:56,760
0,315 315,540 540,690 690,980 1450,1740
later in the sequence,| {and,we
|我们需要能够跟踪这些依赖关系，

946
00:28:56,760 --> 00:28:57,600
0,180 180,360 360,495 495,615 615,840
-} need to be able

947
00:28:57,600 --> 00:28:59,340
0,255 255,480 480,765 765,1470 1470,1740
to track these dependencies| and
|并在我们的网络中维护这些信息。

948
00:28:59,340 --> 00:29:01,125
0,290 370,770 820,1220 1390,1650 1650,1785
maintain this information in our

949
00:29:01,125 --> 00:29:01,920
0,275
network.|
|

950
00:29:03,470 --> 00:29:05,760
0,790 840,1130 1130,1280 1280,1540 1890,2290
Dependencies relate to order and
依赖与顺序相关和序列是由顺序定义的，

951
00:29:05,990 --> 00:29:07,465
0,620 620,830 830,1115 1115,1325 1325,1475
sequences are defined by their

952
00:29:07,465 --> 00:29:09,210
0,275 745,1020 1020,1170 1170,1395 1395,1745
order,| and we know that
|我们知道不同顺序的相同单词有完全不同的含义，

953
00:29:09,680 --> 00:29:11,305
0,335 335,670 870,1115 1115,1310 1310,1625
same words in a completely

954
00:29:11,305 --> 00:29:13,000
0,285 285,605 625,1025 1045,1395 1395,1695
different order have completely different

955
00:29:13,000 --> 00:29:15,280
0,420 420,770 1240,1640 1690,1995 1995,2280
meanings, right,| so our model
|因此，我们的模型需要能够处理这些顺序的差异和长度的差异，

956
00:29:15,280 --> 00:29:16,510
0,315 315,495 495,615 615,885 885,1230
needs to be able to

957
00:29:16,510 --> 00:29:18,480
0,330 330,710 940,1340 1390,1680 1680,1970
handle these differences in order

958
00:29:18,620 --> 00:29:20,280
0,400 510,770 770,1030 1080,1370 1370,1660
and the differences in length|
|

959
00:29:20,480 --> 00:29:23,410
0,290 290,515 515,850 1470,1870 2580,2930
that could result in different
它们可能导致不同预测产出。

960
00:29:23,410 --> 00:29:24,920
0,585 585,1040
predicted outputs.|
|

961
00:29:25,860 --> 00:29:28,030
0,400 510,910 1050,1430 1430,1790 1790,2170
Okay, so hopefully that example
好的，希望通过文本的例子

962
00:29:28,410 --> 00:29:29,780
0,365 365,605 605,785 785,1070 1070,1370
going through the example in

963
00:29:29,780 --> 00:29:32,240
0,320 1210,1785 1785,2085 2085,2295 2295,2460
text| motivates how we can
|激励我们如何将输入数据转换成数字编码，

964
00:29:32,240 --> 00:29:34,300
0,210 210,530 970,1515 1515,1770 1770,2060
think about transforming input data

965
00:29:34,500 --> 00:29:36,440
0,335 335,605 605,1085 1085,1660 1680,1940
into a numerical encoding,| that
|可以传递给 RNN ，

966
00:29:36,440 --> 00:29:37,690
0,105 105,300 300,585 585,885 885,1250
can be passed into the

967
00:29:37,710 --> 00:29:39,650
0,305 305,610 870,1270 1320,1700 1700,1940
{RNN -},| and also what
|以及我们在处理这些类型的问题时希望满足的关键标准是什么。

968
00:29:39,650 --> 00:29:41,240
0,135 135,300 300,590 820,1455 1455,1590
are the key criteria that

969
00:29:41,240 --> 00:29:42,880
0,150 150,440 580,855 855,1130 1240,1640
we want to meet in

970
00:29:42,900 --> 00:29:44,830
0,620 620,940 1200,1490 1490,1655 1655,1930
handling these types of problems.|
|

971
00:29:47,030 --> 00:29:48,325
0,320 320,545 545,785 785,1085 1085,1295
So, so far we've painted
到目前为止，我们已经描绘了 RNN 的图片，

972
00:29:48,325 --> 00:29:49,855
0,195 195,435 435,780 1005,1125 1125,1530
the picture of {RNNs -},|
|

973
00:29:49,855 --> 00:29:51,550
0,270 270,435 435,725 805,1455 1455,1695
how they work, intuition their
它是如何工作的，直觉它们的数学运算，

974
00:29:51,550 --> 00:29:54,610
0,735 735,1130 1840,2240 2560,2880 2880,3060
mathematical operations| and what are
|以及它们需要满足的关键标准是什么。

975
00:29:54,610 --> 00:29:56,200
0,165 165,435 435,1080 1080,1335 1335,1590
the key criteria that they

976
00:29:56,200 --> 00:29:57,300
0,180 180,330 330,590
need to meet.|
|

977
00:29:57,370 --> 00:29:58,500
0,260 260,485 485,770 770,935 935,1130
The final piece to this
最后一个问题是，

978
00:29:58,500 --> 00:29:59,990
0,255 255,450 450,740 760,1125 1125,1490
is,| how we actually train
|我们如何实际训练和学习 RNN 的权重。

979
00:30:00,220 --> 00:30:01,710
0,335 335,670 840,1100 1100,1355 1355,1490
and learn the weights in

980
00:30:01,710 --> 00:30:02,880
0,105 105,590
the RNN.|
|

981
00:30:02,880 --> 00:30:04,275
0,120 120,405 405,630 630,980 1060,1395
And that's done through back
这是通过反向传播算法完成的，

982
00:30:04,275 --> 00:30:06,210
0,635 805,1325 1375,1680 1680,1830 1830,1935
propagation algorithm| with a bit
|只是处理序列信息，

983
00:30:06,210 --> 00:30:07,350
0,105 105,255 255,590 730,990 990,1140
of a twist to just

984
00:30:07,350 --> 00:30:09,920
0,290 550,1290 1290,1670
handle sequential information,|
|

985
00:30:10,930 --> 00:30:12,150
0,290 290,455 455,605 605,880 900,1220
if we go back and
如果我们回头想想我们是如何训练前馈神经网络模型的，

986
00:30:12,150 --> 00:30:13,550
0,240 240,525 525,795 795,1050 1050,1400
think about how we train

987
00:30:13,780 --> 00:30:15,470
0,320 320,620 620,920 920,1180 1290,1690
feed forward neural network models,|
|

988
00:30:16,300 --> 00:30:17,990
0,305 305,560 560,830 830,1150 1290,1690
the steps break down in
这些步骤在思考的过程中会被分解，

989
00:30:18,400 --> 00:30:20,235
0,395 395,790 1050,1400 1400,1610 1610,1835
thinking through,| starting with an
|从一个输入开始，

990
00:30:20,235 --> 00:30:22,260
0,365 835,1110 1110,1305 1305,1625 1735,2025
input,| where we first take
|我们首先从这个输入开始，

991
00:30:22,260 --> 00:30:23,445
0,285 285,585 585,780 780,975 975,1185
this input| and make a
|然后向前穿过网络，

992
00:30:23,445 --> 00:30:24,800
0,305 325,720 720,990 990,1110 1110,1355
forward pass through the network,|
|

993
00:30:25,300 --> 00:30:27,230
0,365 365,730 750,1150 1200,1565 1565,1930
going from input to output.|
从输入到输出。|

994
00:30:28,060 --> 00:30:29,490
0,305 305,545 545,740 740,920 920,1430
The key to back propagation
Alexander 介绍的反向传播的关键是

995
00:30:29,490 --> 00:30:31,695
0,330 330,710 880,1280 1660,1935 1935,2205
that Alexander introduced was| this
|将预测和反向传播梯度通过网络反向传播的想法，

996
00:30:31,695 --> 00:30:33,110
0,330 330,600 600,870 870,1065 1065,1415
idea of taking the prediction

997
00:30:33,160 --> 00:30:35,580
0,350 350,650 650,1205 1205,1780 2070,2420
and back propagating gradients back

998
00:30:35,580 --> 00:30:37,320
0,225 225,500 520,920
through the network,|
|

999
00:30:37,330 --> 00:30:39,225
0,400 480,860 860,1235 1235,1625 1625,1895
and using this operation to
然后使用该操作来定义和更新关于网络中的每个参数的损耗，

1000
00:30:39,225 --> 00:30:41,565
0,275 1015,1415 1435,1835 1885,2175 2175,2340
then define and update the

1001
00:30:41,565 --> 00:30:43,635
0,275 685,1080 1080,1440 1440,1785 1785,2070
loss with respect to each

1002
00:30:43,635 --> 00:30:44,640
0,165 165,285 285,705 705,885 885,1005
of the parameters in the

1003
00:30:44,640 --> 00:30:46,910
0,260 610,915 915,1220 1270,1670 1870,2270
network| in order to gradually
|以便逐渐调整网络的参数和权重，

1004
00:30:47,170 --> 00:30:48,780
0,350 350,575 575,1055 1055,1295 1295,1610
adjust the parameters, the weights

1005
00:30:48,780 --> 00:30:50,100
0,120 120,240 240,500 730,1035 1035,1320
of the network,| in order
|以便最小化总体损耗。

1006
00:30:50,100 --> 00:30:51,650
0,240 240,690 690,885 885,1130 1150,1550
to minimize the overall loss.|
|

1007
00:30:53,180 --> 00:30:54,790
0,290 290,470 470,770 770,1030 1290,1610
Now, with {RNNs -}, as
现在，随着 RNN ，正如我们前面介绍的那样，

1008
00:30:54,790 --> 00:30:56,245
0,195 195,390 390,600 600,890 1150,1455
we walked through earlier,| we
|我们有了这个时间展开，

1009
00:30:56,245 --> 00:30:58,465
0,210 210,465 465,975 975,1715 1915,2220
have this temporal unrolling,| which
|这意味着我们在序列中的各个步骤中有这些单独的损失，

1010
00:30:58,465 --> 00:30:59,580
0,240 240,450 450,615 615,810 810,1115
means that we have these

1011
00:30:59,600 --> 00:31:01,930
0,365 365,730 1290,1625 1625,1955 1955,2330
individual losses across the individual

1012
00:31:01,930 --> 00:31:03,535
0,285 285,435 435,585 585,890 1270,1605
steps in our sequence,| that
|这些损失加在一起构成了总体损失。

1013
00:31:03,535 --> 00:31:05,470
0,360 360,665 775,1065 1065,1505 1675,1935
summed together to comprise the

1014
00:31:05,470 --> 00:31:06,780
0,260 310,710
overall loss.|
|

1015
00:31:07,710 --> 00:31:08,870
0,290 290,485 485,755 755,995 995,1160
What this means is that,|
这意味着，|

1016
00:31:08,870 --> 00:31:10,180
0,210 210,405 405,555 555,750 750,1310
when we do back propagation,|
当我们进行反向传播时，|

1017
00:31:11,300 --> 00:31:13,660
0,305 305,590 590,970 1260,1660 2040,2360
we have to now, instead
我们现在必须，不是通过单一的网络反向传播错误，

1018
00:31:13,660 --> 00:31:15,565
0,210 210,435 435,975 975,1520 1630,1905
of back propagating errors through

1019
00:31:15,565 --> 00:31:17,400
0,180 180,485 535,935
a single network,|
|

1020
00:31:17,400 --> 00:31:19,125
0,315 315,795 795,960 960,1220 1420,1725
back propagate the loss through
而是通过这些单独的时间步长向后传播损失，

1021
00:31:19,125 --> 00:31:20,610
0,210 210,390 390,665 775,1155 1155,1485
each of these individual time

1022
00:31:20,610 --> 00:31:21,660
0,350
steps,|
|

1023
00:31:21,760 --> 00:31:23,625
0,335 335,650 650,1030 1050,1385 1385,1865
and after we back propagate
在我们通过每个单独的时间步长反向传播损耗之后，

1024
00:31:23,625 --> 00:31:24,735
0,285 285,555 555,735 735,885 885,1110
loss through each of the

1025
00:31:24,735 --> 00:31:26,840
0,345 345,675 675,1025 1525,1815 1815,2105
individual time steps,| we then
|我们然后在所有的时间步长上这样做，

1026
00:31:27,280 --> 00:31:28,890
0,260 260,520 600,980 980,1310 1310,1610
do that across all time

1027
00:31:28,890 --> 00:31:30,570
0,350 790,1095 1095,1245 1245,1395 1395,1680
steps,| all the way from
|从我们当前的时间 t 回到序列的开始。

1028
00:31:30,570 --> 00:31:32,480
0,330 330,630 630,980 1180,1545 1545,1910
our current time time t

1029
00:31:32,950 --> 00:31:34,125
0,380 380,605 605,755 755,980 980,1175
back to the beginning of

1030
00:31:34,125 --> 00:31:35,740
0,135 135,395
the sequence.|
|

1031
00:31:35,740 --> 00:31:37,150
0,210 210,375 375,585 585,920 1150,1410
And this is why this
这就是为什么这个算法被称为时间反向传播的原因，

1032
00:31:37,150 --> 00:31:38,800
0,150 150,440 460,860 1090,1470 1470,1650
is why this algorithm is

1033
00:31:38,800 --> 00:31:40,590
0,285 285,585 585,1125 1125,1455 1455,1790
called back propagation through time,

1034
00:31:40,880 --> 00:31:42,445
0,400 720,1025 1025,1235 1235,1400 1400,1565
right,| because as you can
|因为正如你所看到的，

1035
00:31:42,445 --> 00:31:44,070
0,305 535,825 825,1110 1110,1365 1365,1625
see,| the data and the
|数据、预测和由此产生的误差反馈到，

1036
00:31:44,420 --> 00:31:46,440
0,635 635,860 860,1085 1085,1415 1415,2020
predictions and the resulting errors

1037
00:31:46,640 --> 00:31:47,910
0,335 335,575 575,785 785,980 980,1270
are fed back in time|
|

1038
00:31:48,080 --> 00:31:49,030
0,305 305,455 455,605 605,785 785,950
all the way from where
从我们目前所处的位置一直反馈到输入数据序列的最开始，

1039
00:31:49,030 --> 00:31:50,470
0,135 135,300 300,620 1060,1320 1320,1440
we are currently to the

1040
00:31:50,470 --> 00:31:51,970
0,255 255,645 645,975 975,1260 1260,1500
very beginning of the input

1041
00:31:51,970 --> 00:31:54,040
0,240 240,590
data sequence,|
|

1042
00:31:55,290 --> 00:31:57,455
0,400 780,1070 1070,1295 1295,1880 1880,2165
so the back propagation through
穿越时间的反向传播是一个在实践中很难实现的算法，

1043
00:31:57,455 --> 00:31:58,895
0,305 325,725 745,1035 1035,1200 1200,1440
time is actually a very

1044
00:31:58,895 --> 00:32:01,880
0,480 480,900 900,1235 1255,1655 2695,2985
tricky algorithm to implement in

1045
00:32:01,880 --> 00:32:02,960
0,290
practice,|
|

1046
00:32:02,960 --> 00:32:03,920
0,225 225,360 360,585 585,810 810,960
and the reason for this
这样的原因是，

1047
00:32:03,920 --> 00:32:04,715
0,180 180,345 345,510 510,660 660,795
is,| if we take a
|如果我们仔细观察，

1048
00:32:04,715 --> 00:32:06,425
0,195 195,515 895,1245 1245,1485 1485,1710
close look,| looking at how
|梯度是如何在 RNN 上流动的，

1049
00:32:06,425 --> 00:32:07,970
0,555 555,935 955,1275 1275,1440 1440,1545
gradients flow across the {RNN

1050
00:32:07,970 --> 00:32:10,100
0,120 120,380 940,1260 1260,1580 1660,2130
- -},| what this algorithm
|这个算法涉及到的是这些权重矩阵多次重复计算和乘法，

1051
00:32:10,100 --> 00:32:12,130
0,350 370,750 750,1110 1110,1490 1630,2030
involves is many, many repeated

1052
00:32:12,420 --> 00:32:15,170
0,700 720,995 995,1720 2190,2510 2510,2750
computations and multiplications of these

1053
00:32:15,170 --> 00:32:17,495
0,225 225,890 1150,1800 1800,2100 2100,2325
weight matrices| repeatedly against each
|相互重复，

1054
00:32:17,495 --> 00:32:18,320
0,275
other,|
|

1055
00:32:18,580 --> 00:32:19,875
0,305 305,575 575,785 785,1130 1130,1295
in order to compute the
为了计算相对于第一个时间步长的梯度，

1056
00:32:19,875 --> 00:32:21,480
0,420 420,690 690,990 990,1325 1345,1605
gradient with respect to the

1057
00:32:21,480 --> 00:32:23,880
0,225 225,590 1300,1650 1650,2000 2110,2400
very first time step,| we
|我们必须对权重矩阵进行多次乘法重复。

1058
00:32:23,880 --> 00:32:24,900
0,195 195,360 360,555 555,825 825,1020
have to make many of

1059
00:32:24,900 --> 00:32:27,645
0,165 165,980 1360,1850 2050,2400 2400,2745
these multiplicative repeats of the

1060
00:32:27,645 --> 00:32:29,040
0,300 300,935
weight matrix.|
|

1061
00:32:29,740 --> 00:32:31,790
0,395 395,695 695,905 905,1210 1230,2050
Why might this be problematic,|
这为什么会有问题，|

1062
00:32:32,320 --> 00:32:34,465
0,380 910,1245 1245,1485 1485,1695 1695,2145
well, if this weight matrix
好的，如果这个权重矩阵 w 非常大，

1063
00:32:34,465 --> 00:32:36,840
0,275 685,1085 1435,1785 1785,2055 2055,2375
w is very, very big,|
|

1064
00:32:37,700 --> 00:32:38,825
0,270 270,450 450,675 675,930 930,1125
what this can result in
这可能导致他们所说的爆炸性梯度问题，

1065
00:32:38,825 --> 00:32:39,725
0,165 165,345 345,525 525,720 720,900
is what they call, what

1066
00:32:39,725 --> 00:32:41,345
0,180 180,390 390,690 690,1200 1200,1620
we call the exploding gradient

1067
00:32:41,345 --> 00:32:43,625
0,305 745,1145 1285,1620 1620,2085 2085,2280
problem,| where our gradients that
|我们试图用来优化网络的梯度就是这样做的，

1068
00:32:43,625 --> 00:32:44,750
0,255 255,480 480,675 675,855 855,1125
we're trying to use to

1069
00:32:44,750 --> 00:32:46,340
0,330 330,510 510,800 940,1290 1290,1590
optimize our network do exactly

1070
00:32:46,340 --> 00:32:47,990
0,350 520,840 840,1035 1035,1290 1290,1650
that,| {they,blow -} up, they
|它们爆炸了，

1071
00:32:47,990 --> 00:32:49,120
0,410
explode,|
|

1072
00:32:49,120 --> 00:32:50,220
0,225 225,360 360,525 525,765 765,1100
and they get really big|
它们变得非常大，|

1073
00:32:50,330 --> 00:32:52,590
0,400 810,1130 1130,1325 1325,1970 1970,2260
and makes it infeasible and
使得稳定地训练网络变得不可行和不可能，

1074
00:32:52,880 --> 00:32:54,280
0,320 320,640 750,1040 1040,1235 1235,1400
not possible to train the

1075
00:32:54,280 --> 00:32:55,960
0,260 340,1010
network stably,|
|

1076
00:32:56,290 --> 00:32:57,555
0,275 275,455 455,680 680,875 875,1265
what we do to mitigate
我们所做的是一个非常简单的解决方案，称为梯度裁剪，

1077
00:32:57,555 --> 00:32:59,715
0,240 240,525 525,870 870,1265 1795,2160
this is a pretty simple

1078
00:32:59,715 --> 00:33:01,905
0,365 385,720 720,1170 1170,1655 1855,2190
solution called gradient clipping,| which
|它有效地缩小了这些非常大的梯度，约束它们，

1079
00:33:01,905 --> 00:33:03,690
0,335 505,930 930,1260 1260,1545 1545,1785
effectively scales back these very

1080
00:33:03,690 --> 00:33:04,875
0,225 225,630 630,840 840,1035 1035,1185
big gradients to try to

1081
00:33:04,875 --> 00:33:07,950
0,390 390,695 1285,1685 2245,2645 2815,3075
constrain them,| more, in a
|以更受限制的方式。

1082
00:33:07,950 --> 00:33:10,140
0,255 255,780 780,1160
more restricted way.|
|

1083
00:33:10,630 --> 00:33:12,885
0,725 725,890 890,1040 1040,1330 1890,2255
Conversely, we can have the
相反，我们可以有这样的例子，其中权重矩阵非常非常小，

1084
00:33:12,885 --> 00:33:14,550
0,285 285,510 510,815 865,1170 1170,1665
instance where the weight matrices

1085
00:33:14,550 --> 00:33:16,335
0,180 180,375 375,660 660,1040 1480,1785
are very, very small,| and
|如果这些权重矩阵非常非常小，

1086
00:33:16,335 --> 00:33:17,580
0,195 195,375 375,555 555,1065 1065,1245
if these weight matrices are

1087
00:33:17,580 --> 00:33:19,635
0,195 195,465 465,830 1540,1830 1830,2055
very, very small,| we end
|我们最终得到一个非常非常小的值，

1088
00:33:19,635 --> 00:33:20,910
0,255 255,540 540,795 795,1005 1005,1275
up with a very, very

1089
00:33:20,910 --> 00:33:22,040
0,240 240,510 510,765 765,885 885,1130
small value at the end|
|

1090
00:33:22,240 --> 00:33:23,450
0,275 275,485 485,740 740,935 935,1210
as a result of these
因为这些重复的权重矩阵计算和重复乘法的结果，

1091
00:33:23,500 --> 00:33:25,815
0,400 570,890 890,1475 1475,2090 2090,2315
repeated weight matrix computations and

1092
00:33:25,815 --> 00:33:27,980
0,275 325,725
these repeated

1093
00:33:28,080 --> 00:33:30,350
0,760 1350,1655 1655,1880 1880,2090 2090,2270
multiplications,| and this is a
|这是一个非常真实的问题，特别是在 RNN 中，

1094
00:33:30,350 --> 00:33:31,955
0,290 400,735 735,1070 1150,1440 1440,1605
very real problem in {RNN

1095
00:33:31,955 --> 00:33:33,610
0,120 120,285 285,570 570,935 1255,1655
- -} in particular,| where
|我们可以引入一个叫做梯度消失的[漏斗]，

1096
00:33:33,630 --> 00:33:35,050
0,275 275,500 500,785 785,1070 1070,1420
we can lead into this

1097
00:33:35,520 --> 00:33:37,580
0,580 660,980 980,1175 1175,1640 1640,2060
[funnel] called a vanishing gradient,|
|

1098
00:33:37,580 --> 00:33:38,870
0,255 255,540 540,810 810,1125 1125,1290
where now your gradient has
现在你的梯度已经下降到接近于零，

1099
00:33:38,870 --> 00:33:40,460
0,225 225,495 495,830 1090,1410 1410,1590
just dropped down close to

1100
00:33:40,460 --> 00:33:41,840
0,180 180,500 550,900 900,1140 1140,1380
zero,| {and,again -}, you can't
|再次，你不能稳定地训练网络。

1101
00:33:41,840 --> 00:33:43,900
0,150 150,300 300,560 820,1460
train the network stably.|
|

1102
00:33:44,020 --> 00:33:45,945
0,290 290,440 440,700 870,1270 1590,1925
Now, there are particular tools
现在，我们可以使用一些特定的工具来实现，

1103
00:33:45,945 --> 00:33:46,965
0,195 195,315 315,465 465,705 705,1020
that we can use to

1104
00:33:46,965 --> 00:33:48,795
0,365 955,1215 1215,1335 1335,1545 1545,1830
implement,| that we can implement
|我们可以实现来尝试缓解梯度消失的问题，

1105
00:33:48,795 --> 00:33:50,055
0,255 255,495 495,675 675,1050 1050,1260
to try to mitigate the

1106
00:33:50,055 --> 00:33:51,930
0,305 325,780 780,1085 1375,1650 1650,1875
vanishing gradient problem,| and we'll
|我们将简要介绍这三个解决方案。

1107
00:33:51,930 --> 00:33:52,935
0,150 150,390 390,630 630,795 795,1005
touch on each of these

1108
00:33:52,935 --> 00:33:55,420
0,335 445,845 1225,1625
three solutions briefly.|
|

1109
00:33:55,420 --> 00:33:57,400
0,270 270,650 1210,1575 1575,1830 1830,1980
First being, {} how we
首先，我们如何定义我们网络中的激活函数，

1110
00:33:57,400 --> 00:33:59,100
0,195 195,530 610,945 945,1335 1335,1700
can define the activation function

1111
00:33:59,960 --> 00:34:01,600
0,305 305,485 485,760 1140,1445 1445,1640
in our network| and how
|以及我们如何改变网络体系结构本身，

1112
00:34:01,600 --> 00:34:02,875
0,150 150,410 490,825 825,1035 1035,1275
we can change the network

1113
00:34:02,875 --> 00:34:04,675
0,365 655,1055 1135,1425 1425,1620 1620,1800
architecture itself| to try to
|以更好地处理这个梯度消失问题。

1114
00:34:04,675 --> 00:34:06,295
0,225 225,570 570,855 855,1290 1290,1620
better handle this vanishing gradient

1115
00:34:06,295 --> 00:34:06,980
0,305
problem.|
|

1116
00:34:08,030 --> 00:34:09,820
0,365 365,605 605,725 725,970
Before we do that,|
在我们这么做之前，|

1117
00:34:10,010 --> 00:34:10,945
0,275 275,425 425,560 560,710 710,935
I want to take just
我只想退后一步，

1118
00:34:10,945 --> 00:34:12,955
0,300 300,540 540,815 1285,1685 1735,2010
one step back| to give
|让你们更直观地了解为什么递归神经网络的真正问题是梯度消失。

1119
00:34:12,955 --> 00:34:14,250
0,135 135,300 300,525 525,750 750,1295
you a little more intuition

1120
00:34:14,270 --> 00:34:16,390
0,395 395,755 755,1385 1385,1910 1910,2120
about why vanishing gradients can

1121
00:34:16,390 --> 00:34:17,860
0,180 180,375 375,680 760,1160 1210,1470
be a real issue for

1122
00:34:17,860 --> 00:34:19,380
0,345 345,540 540,800
recurrent neural networks.|
|

1123
00:34:20,330 --> 00:34:21,780
0,305 305,545 545,785 785,1115 1115,1450
Point I've kept trying to
我一直试图重申的一点是，

1124
00:34:21,950 --> 00:34:23,670
0,605 605,830 830,1055 1055,1340 1340,1720
reiterate,| is this notion of
|序列数据中的依赖关系的概念，

1125
00:34:23,720 --> 00:34:25,410
0,665 665,815 815,950 950,1400 1400,1690
dependency in the sequential data|
|

1126
00:34:25,580 --> 00:34:26,560
0,290 290,440 440,560 560,755 755,980
and what it means to
以及跟踪这些依赖关系的意义，

1127
00:34:26,560 --> 00:34:28,260
0,210 210,480 480,1160
track those dependencies,|
|

1128
00:34:28,480 --> 00:34:30,165
0,400 510,800 800,1010 1010,1520 1520,1685
well, if the dependencies are
如果依赖关系被限制在一个很小的空间内，

1129
00:34:30,165 --> 00:34:31,470
0,240 240,720 720,885 885,1050 1050,1305
very constrained in a small

1130
00:34:31,470 --> 00:34:33,135
0,350 460,795 795,1130 1150,1470 1470,1665
space,| not separated out that
|而不是通过时间来分离，

1131
00:34:33,135 --> 00:34:35,330
0,180 180,420 420,755 1375,1775 1795,2195
much by time,| this repeated
|那么重复的梯度计算和重复的权重矩阵乘法就不是什么问题了。

1132
00:34:35,860 --> 00:34:37,785
0,500 500,1090 1110,1400 1400,1625 1625,1925
gradient computation and the repeated

1133
00:34:37,785 --> 00:34:40,275
0,285 285,825 825,1535 1915,2235 2235,2490
weight matrix multiplication is not

1134
00:34:40,275 --> 00:34:41,090
0,180 180,300 300,420 420,540 540,815
so much of a problem.|
|

1135
00:34:41,920 --> 00:34:42,775
0,180 180,345 345,480 480,615 615,855
If we have a very
如果我们有一个非常短的序列，

1136
00:34:42,775 --> 00:34:44,905
0,285 285,605 775,1175 1585,1860 1860,2130
short sequence,| where the words
|其中单词之间的联系非常紧密，

1137
00:34:44,905 --> 00:34:46,645
0,395 415,780 780,1145 1195,1545 1545,1740
are very closely related to

1138
00:34:46,645 --> 00:34:47,780
0,120 120,395
each other,|
|

1139
00:34:47,780 --> 00:34:48,815
0,120 120,210 210,345 345,650 700,1035
and {it's -} pretty obvious
很明显，我们的下一个产出会是什么。

1140
00:34:48,815 --> 00:34:50,525
0,225 225,420 420,725 1285,1560 1560,1710
what our next output is

1141
00:34:50,525 --> 00:34:52,320
0,165 165,315 315,575
going to be.|
|

1142
00:34:52,360 --> 00:34:54,045
0,260 260,695 695,950 950,1270 1320,1685
The rnn can use the
RNN 可以使用立即传递的信息来进行预测，

1143
00:34:54,045 --> 00:34:55,845
0,365 475,875 1045,1425 1425,1665 1665,1800
immediately passed information to make

1144
00:34:55,845 --> 00:34:57,540
0,135 135,515
a prediction,|
|

1145
00:34:57,540 --> 00:34:58,605
0,240 240,555 555,795 795,900 900,1065
and so there are not
因此，学习有效权重的要求就不会那么多，

1146
00:34:58,605 --> 00:34:59,600
0,195 195,330 330,450 450,660 660,995
going to be that many,

1147
00:35:00,370 --> 00:35:01,755
0,290 290,440 440,560 560,785 785,1385
that much of a requirement

1148
00:35:01,755 --> 00:35:04,350
0,365 535,935 1105,1485 1485,2045 2215,2595
to learn effective weights,| if
|如果相关信息在时间上彼此接近。

1149
00:35:04,350 --> 00:35:07,070
0,380 850,1250 1480,1830 1830,2180 2320,2720
the related information is close

1150
00:35:07,180 --> 00:35:09,520
0,400 570,845 845,1070 1070,1840
to each other temporally.|
|

1151
00:35:09,930 --> 00:35:11,620
0,680 680,935 935,1190 1190,1385 1385,1690
Conversely, now if we have
相反，现在如果我们有一个句子，有一个更长的依赖，

1152
00:35:11,880 --> 00:35:13,295
0,290 290,580 600,935 935,1175 1175,1415
a sentence where we have

1153
00:35:13,295 --> 00:35:15,220
0,270 270,540 540,840 840,1200 1200,1925
a more long term dependency,|
|

1154
00:35:16,180 --> 00:35:17,305
0,240 240,435 435,705 705,945 945,1125
what this means is that
这意味着，我们需要来自序列中更靠前的信息，

1155
00:35:17,305 --> 00:35:18,835
0,165 165,425 565,945 945,1245 1245,1530
we need information from way

1156
00:35:18,835 --> 00:35:20,040
0,315 315,600 600,795 795,930 930,1205
further back in the sequence|
|

1157
00:35:20,630 --> 00:35:21,955
0,320 320,575 575,815 815,1130 1130,1325
to make our prediction at
才能在最后做出预测，

1158
00:35:21,955 --> 00:35:23,730
0,135 135,395 955,1230 1230,1440 1440,1775
the end,| and that gap
|相关信息和我们目前所处位置之间的差距变得非常大，

1159
00:35:23,870 --> 00:35:25,540
0,305 305,605 605,880 1170,1475 1475,1670
between what's relevant and where

1160
00:35:25,540 --> 00:35:27,390
0,135 135,225 225,405 405,740 1450,1850
we are at currently becomes

1161
00:35:27,470 --> 00:35:29,610
0,545 545,820 1110,1510 1560,1850 1850,2140
exceedingly large,| and therefore the
|因此，梯度消失的问题日益加剧，

1162
00:35:29,630 --> 00:35:31,950
0,575 575,920 920,1210 1440,1840 1920,2320
vanishing gradient problem is increasingly

1163
00:35:32,000 --> 00:35:35,110
0,910 1290,1640 1640,1990 2640,2915 2915,3110
exacerbated,| meaning that we really
|意味着我们需要，

1164
00:35:35,110 --> 00:35:36,680
0,255 255,590
need to,|
|

1165
00:35:37,050 --> 00:35:38,080
0,260 260,380 380,485 485,680 680,1030
the {RNN - -} becomes
RNN 变得无法连接这些点并建立这种长期依赖，

1166
00:35:38,370 --> 00:35:39,875
0,380 380,665 665,875 875,1055 1055,1505
unable to connect the dots

1167
00:35:39,875 --> 00:35:41,510
0,395 415,815 865,1155 1155,1365 1365,1635
and establish this long term

1168
00:35:41,510 --> 00:35:43,370
0,680 880,1275 1275,1545 1545,1695 1695,1860
dependency,| all because of this
|这一切都是因为这种梯度消失的问题。

1169
00:35:43,370 --> 00:35:45,100
0,435 435,735 735,1010
vanishing gradient issue.|
|

1170
00:35:45,400 --> 00:35:46,815
0,400 540,815 815,1025 1025,1250 1250,1415
So the ways that we
因此，我们可以输入的方法，

1171
00:35:46,815 --> 00:35:48,435
0,275 325,725 955,1215 1215,1410 1410,1620
can input,| the ways and
|我们可以对我们的网络进行的修改，

1172
00:35:48,435 --> 00:35:49,740
0,615 615,870 870,990 990,1110 1110,1305
modifications that we can make

1173
00:35:49,740 --> 00:35:51,135
0,150 150,255 255,530 790,1095 1095,1395
to our network| to try
|试图从三个方面缓解这个问题。

1174
00:35:51,135 --> 00:35:53,750
0,395 415,1065 1065,1245 1245,1535 2035,2615
to alleviate this problem threefold.|
|

1175
00:35:54,610 --> 00:35:56,250
0,290 290,575 575,905 905,1240 1380,1640
The first is that we
首先，我们可以简单地更改每个神经网络层中的激活函数，

1176
00:35:56,250 --> 00:35:58,065
0,180 180,500 520,900 900,1280 1330,1815
can simply change the activation

1177
00:35:58,065 --> 00:35:59,340
0,395 505,810 810,1005 1005,1140 1140,1275
functions in each of our

1178
00:35:59,340 --> 00:36:01,830
0,255 255,530 850,1430 1900,2220 2220,2490
neural network layers| to be
|以便它们可以有效地尝试缓解和保护在以下情况下的梯度，

1179
00:36:01,830 --> 00:36:03,950
0,285 285,620 1240,1515 1515,1755 1755,2120
such that they can effectively

1180
00:36:04,180 --> 00:36:05,840
0,305 305,485 485,890 890,1100 1100,1660
try to mitigate and safeguard

1181
00:36:06,100 --> 00:36:08,210
0,335 335,880 900,1190 1190,1790 1790,2110
from gradients in instances,| where
|在数据大于零的情况下收缩梯度，

1182
00:36:09,040 --> 00:36:10,680
0,305 305,695 695,830 830,1270 1350,1640
from shrinking the gradients in

1183
00:36:10,680 --> 00:36:12,320
0,585 585,825 825,1020 1020,1260 1260,1640
instances where the data is

1184
00:36:12,460 --> 00:36:14,235
0,335 335,575 575,880 1350,1625 1625,1775
greater than zero,| {and,this -}
|对于 ReLU 激活函数来说尤其如此，

1185
00:36:14,235 --> 00:36:16,245
0,275 295,645 645,995 1375,1725 1725,2010
is in particular true for

1186
00:36:16,245 --> 00:36:18,705
0,225 225,695 745,1215 1215,1595 2185,2460
the ReLU activation function,| and
|原因是在 x 大于零的所有情况下，

1187
00:36:18,705 --> 00:36:20,730
0,135 135,395 505,900 900,1295 1705,2025
the reason is that in

1188
00:36:20,730 --> 00:36:22,515
0,270 270,960 960,1290 1290,1590 1590,1785
all instances where x is

1189
00:36:22,515 --> 00:36:23,940
0,195 195,435 435,755 1015,1290 1290,1425
greater than zero,| with the
|对于 ReLU 函数，导数是 1 ，

1190
00:36:23,940 --> 00:36:26,150
0,345 345,680 760,1050 1050,1670 1810,2210
ReLU function, the derivative is

1191
00:36:26,230 --> 00:36:28,320
0,400 750,1070 1070,1390
one,| and so
|所以这不小于 1 ，

1192
00:36:28,880 --> 00:36:30,580
0,350 350,700 780,1180 1200,1505 1505,1700
that is not less than

1193
00:36:30,580 --> 00:36:32,490
0,255 255,615 615,960 960,1310 1510,1910
one| and therefore it helps
|因此它有助于缓解渐变消失问题。

1194
00:36:33,050 --> 00:36:34,975
0,400 420,1010 1010,1205 1205,1580 1580,1925
in mitigating the vanishing gradient

1195
00:36:34,975 --> 00:36:36,040
0,275
problem.|
|

1196
00:36:37,180 --> 00:36:39,405
0,365 365,710 710,1090 1650,1970 1970,2225
Another trick is how we
另一个诀窍是我们如何初始化网络本身的参数，

1197
00:36:39,405 --> 00:36:40,950
0,555 555,780 780,1245 1245,1425 1425,1545
initialize the parameters in the

1198
00:36:40,950 --> 00:36:43,215
0,260 400,800 1330,1680 1680,1995 1995,2265
network themselves| to prevent them
|以防止它们太快地缩小到零，

1199
00:36:43,215 --> 00:36:44,685
0,225 225,630 630,765 765,1025 1195,1470
from shrinking to zero too

1200
00:36:44,685 --> 00:36:45,980
0,275
rapidly,|
|

1201
00:36:45,980 --> 00:36:47,800
0,350 430,720 720,1010 1360,1590 1590,1820
and there are, there are
有一些数学方法可以做到这一点，

1202
00:36:48,000 --> 00:36:49,370
0,740 740,920 920,1100 1100,1220 1220,1370
mathematical ways that we can

1203
00:36:49,370 --> 00:36:51,340
0,150 150,410 670,1185 1185,1395 1395,1970
do this,| namely by initializing
|也就是把我们的权重初始化为单位矩阵，

1204
00:36:51,360 --> 00:36:53,710
0,350 350,785 785,1120 1170,1570 1590,2350
our weights to identity matrices,|
|

1205
00:36:54,150 --> 00:36:56,105
0,400 660,1010 1010,1360 1380,1715 1715,1955
{and,this -} effectively helps in
这在实践中有效地帮助防止权重更新过快地收缩到零。

1206
00:36:56,105 --> 00:36:58,985
0,305 355,645 645,935 1045,1445 2485,2880
practice to prevent the weight

1207
00:36:58,985 --> 00:37:00,430
0,360 360,585 585,905 925,1185 1185,1445
updates to shrink too rapidly

1208
00:37:00,540 --> 00:37:01,720
0,275 275,550
to zero.|
|

1209
00:37:02,470 --> 00:37:04,395
0,395 395,665 665,940 1110,1510 1530,1925
However, the most robust solution
然而，对消失梯度问题最稳健的解决方案是

1210
00:37:04,395 --> 00:37:05,750
0,240 240,330 330,735 735,1065 1065,1355
to the vanishing gradient problem

1211
00:37:06,220 --> 00:37:08,055
0,335 335,605 605,1235 1235,1550 1550,1835
is| by introducing a slightly
|通过引入稍微复杂一点的递归神经单元，

1212
00:37:08,055 --> 00:37:11,160
0,330 330,695 2275,2675 2695,2970 2970,3105
more complicated version of the

1213
00:37:11,160 --> 00:37:13,635
0,560 1150,1530 1530,1820 2080,2340 2340,2475
recurrent neural unit| to be
|来能够更有效地跟踪和处理数据中的长期依赖关系，

1214
00:37:13,635 --> 00:37:15,705
0,225 225,465 465,720 720,1085 1705,2070
able to more effectively track

1215
00:37:15,705 --> 00:37:17,895
0,270 270,575 865,1230 1230,1560 1560,2190
and handle long term dependencies

1216
00:37:17,895 --> 00:37:18,960
0,165 165,285 285,545
in the data,|
|

1217
00:37:19,130 --> 00:37:20,335
0,275 275,425 425,575 575,850 870,1205
and this is this idea
这就是[]的想法，

1218
00:37:20,335 --> 00:37:22,435
0,335 505,905 1435,1725 1725,1890 1890,2100
of [dating],| and what the
|这个想法是，

1219
00:37:22,435 --> 00:37:25,300
0,270 270,605 685,1085 1165,1565 2275,2865
idea is,| is by controlling
|通过有选择地控制流入神经单元的信息流，

1220
00:37:25,300 --> 00:37:27,180
0,615 615,825 825,1095 1095,1430 1480,1880
selectively the flow of information

1221
00:37:27,560 --> 00:37:29,365
0,365 365,605 605,875 875,1180 1560,1805
into the neural unit| to
|以便能够过滤掉不重要的东西，同时保持重要的东西，

1222
00:37:29,365 --> 00:37:31,140
0,135 135,425 505,905 1045,1410 1410,1775
be able to filter out

1223
00:37:31,220 --> 00:37:33,660
0,515 515,860 860,1240 1620,2020 2040,2440
what's not important while maintaining

1224
00:37:33,920 --> 00:37:35,740
0,400 420,800 800,1180
what is important,|
|

1225
00:37:35,910 --> 00:37:37,130
0,290 290,500 500,820 840,1100 1100,1220
and the key and the
实现这种门控计算的关键和最受欢迎的递归单元类型被称为

1226
00:37:37,130 --> 00:37:39,035
0,225 225,590 700,990 990,1275 1275,1905
most popular type of recurrent

1227
00:37:39,035 --> 00:37:40,775
0,330 330,665 955,1320 1320,1485 1485,1740
unit that achieves this gated

1228
00:37:40,775 --> 00:37:43,180
0,605 895,1215 1215,1440 1440,1620 1620,2405
computation is| called the LSTM,
|LSTM ，或长期短期记忆网络，

1229
00:37:43,320 --> 00:37:45,100
0,305 305,610 660,1010 1010,1360 1380,1780
or long short term memory

1230
00:37:45,150 --> 00:37:45,960
0,400
network,|
|

1231
00:37:47,030 --> 00:37:48,265
0,395 395,695 695,845 845,1055 1055,1235
today we're not going to
今天，我们不打算详细讨论 LSTM ，

1232
00:37:48,265 --> 00:37:49,930
0,180 180,485 565,885 885,1125 1125,1665
go into detail on {LSTMs

1233
00:37:49,930 --> 00:37:52,930
0,230 580,980 1420,2265 2265,2630 2650,3000
-},| their mathematical details, their
|它们的数学细节、它们的操作等等，

1234
00:37:52,930 --> 00:37:54,535
0,330 330,615 615,795 795,1070 1300,1605
operations and so on,| but
|但我只想传达有效的关键思想和直观思想，

1235
00:37:54,535 --> 00:37:55,590
0,180 180,330 330,480 480,615 615,1055
I just want to convey

1236
00:37:55,910 --> 00:37:58,030
0,400 450,850 930,1295 1295,1535 1535,2120
the key idea and intuitive

1237
00:37:58,030 --> 00:37:59,665
0,315 315,650 700,1050 1050,1305 1305,1635
idea| about why these {LSTMs
|关于为什么这些 LSTM 在跟踪长期依赖项方面是有效的。

1238
00:37:59,665 --> 00:38:01,620
0,240 240,465 465,785 1105,1505 1555,1955
-} are effective at tracking

1239
00:38:01,760 --> 00:38:03,840
0,320 320,590 590,1270
long term dependencies.|
|

1240
00:38:04,000 --> 00:38:05,700
0,305 305,575 575,845 845,1150 1440,1700
The core is that,| the
核心是，|LSTM 能够控制通过这些门的信息流，

1241
00:38:05,700 --> 00:38:09,465
0,645 645,885 885,1190 1270,1670 3460,3765
lstm is able to control

1242
00:38:09,465 --> 00:38:10,845
0,195 195,375 375,665 715,1110 1110,1380
the flow of information through

1243
00:38:10,845 --> 00:38:12,345
0,210 210,545 865,1110 1110,1245 1245,1500
these gates| to be able
|从而能够更有效地过滤掉不重要的东西，

1244
00:38:12,345 --> 00:38:14,130
0,240 240,465 465,815 1135,1470 1470,1785
to more effectively filter out

1245
00:38:14,130 --> 00:38:16,520
0,240 240,810 810,1100 1330,1730 1990,2390
the unimportant things| and store
|存储重要的东西，

1246
00:38:16,540 --> 00:38:18,580
0,320 320,620 620,1000
the important things,|
|

1247
00:38:19,520 --> 00:38:20,670
0,290 290,425 425,560 560,800 800,1150
what you can do is,|
你可以做的是，|

1248
00:38:21,440 --> 00:38:24,145
0,400 1050,1370 1370,2030 2030,2150 2150,2705
implement, implement LSTM in TensorFlow,
在 TensorFlow 中实现 LSTM ，就像 RNN 一样，

1249
00:38:24,145 --> 00:38:25,255
0,300 300,555 555,720 720,915 915,1110
just as you would in

1250
00:38:25,255 --> 00:38:26,460
0,150 150,270 270,515
{RNN - -},|
|

1251
00:38:26,460 --> 00:38:28,155
0,380 640,945 945,1170 1170,1470 1470,1695
but the core concept that
但我希望你们在考虑 LSTM 时的核心概念是，

1252
00:38:28,155 --> 00:38:28,965
0,120 120,300 300,465 465,615 615,810
I want you to take

1253
00:38:28,965 --> 00:38:30,060
0,240 240,480 480,735 735,975 975,1095
away when thinking about the

1254
00:38:30,060 --> 00:38:31,940
0,620 670,960 960,1230 1230,1545 1545,1880
LSTM is| this idea of
|这个受控信息流通过门的想法，

1255
00:38:32,380 --> 00:38:34,490
0,400 600,995 995,1385 1385,1745 1745,2110
controlled information flow through gates,|
|

1256
00:38:35,230 --> 00:38:37,365
0,335 335,670 1320,1655 1655,1910 1910,2135
very briefly, the way that
简而言之， LSTM 的操作方式是，

1257
00:38:37,365 --> 00:38:38,510
0,150 150,315 315,450 450,570 570,1145
{LSTM - - -} operates

1258
00:38:38,830 --> 00:38:40,455
0,290 290,575 575,970 1110,1400 1400,1625
is| by maintaining a cell
|通过控制像标准 RNN 一样的单元状态，

1259
00:38:40,455 --> 00:38:42,180
0,335 835,1140 1140,1320 1320,1470 1470,1725
state just like a standard

1260
00:38:42,180 --> 00:38:44,040
0,620 940,1215 1215,1410 1410,1650 1650,1860
RNN,| and that cell state
|并且单元状态独立于直接输出的内容，

1261
00:38:44,040 --> 00:38:45,740
0,290 400,780 780,1095 1095,1365 1365,1700
is independent from what is

1262
00:38:45,790 --> 00:38:47,160
0,400 540,1060
directly outputted,|
|

1263
00:38:47,450 --> 00:38:48,565
0,290 290,485 485,665 665,875 875,1115
the way the cell state
单元状态更新的方式是根据这些控制信息流的门，

1264
00:38:48,565 --> 00:38:50,740
0,300 300,695 1135,1530 1530,1920 1920,2175
is updated is according to

1265
00:38:50,740 --> 00:38:52,660
0,225 225,590 970,1370 1390,1710 1710,1920
these gates that control the

1266
00:38:52,660 --> 00:38:55,255
0,165 165,440 460,860 1780,2280 2280,2595
flow of information,| forgetting and
|忘记和消除不相关的东西，存储相关的信息，

1267
00:38:55,255 --> 00:38:58,080
0,555 555,825 825,1050 1050,1775 2365,2825
eliminating what is irrelevant, storing

1268
00:38:58,370 --> 00:39:00,060
0,400 570,905 905,1145 1145,1370 1370,1690
the information that is relevant,|
|

1269
00:39:01,330 --> 00:39:02,550
0,425 425,575 575,785 785,1010 1010,1220
updating the cell state in
依次更新单元状态，

1270
00:39:02,550 --> 00:39:04,970
0,320 790,1080 1080,1370 1510,2100 2100,2420
turn,| and then filtering this
|然后对更新后的单元状态进行滤波以产生预测输出，

1271
00:39:06,040 --> 00:39:07,965
0,350 350,620 620,940 1170,1570 1620,1925
updated cell state to produce

1272
00:39:07,965 --> 00:39:09,750
0,165 165,675 675,1055 1165,1485 1485,1785
the predicted output,| just like
|就像标准的 RNN 一样。

1273
00:39:09,750 --> 00:39:10,910
0,270 270,540 540,795 795,915 915,1160
the standard {RNN - -}.|
|

1274
00:39:11,550 --> 00:39:13,000
0,320 320,640
And again,
再次，

1275
00:39:13,010 --> 00:39:13,930
0,260 260,425 425,650 650,815 815,920
we can train the {LSTM
我们可以使用时间反向传播算法来训练 LSTM ，

1276
00:39:13,930 --> 00:39:15,010
0,180 180,345 345,540 540,855 855,1080
- - -} using the

1277
00:39:15,010 --> 00:39:16,950
0,195 195,720 720,1005 1005,1310 1420,1940
back propagation through time algorithm,|
|

1278
00:39:17,330 --> 00:39:18,730
0,290 290,440 440,1055 1055,1235 1235,1400
but the mathematics of how
但 LSTM 如何定义的数学允许完全不间断的梯度流动，

1279
00:39:18,730 --> 00:39:19,300
0,135 135,225 225,375 375,495 495,570
the L S T M

1280
00:39:19,300 --> 00:39:21,325
0,210 210,560 970,1370 1420,1725 1725,2025
is defined allows for a

1281
00:39:21,325 --> 00:39:23,590
0,375 375,1505 1645,1950 1950,2130 2130,2265
completely uninterrupted flow of the

1282
00:39:23,590 --> 00:39:26,580
0,470 730,1130 1180,1580 1720,2330 2590,2990
gradients,| which completely eliminates the
|这在很大程度上消除了我前面介绍的梯度消失问题。

1283
00:39:27,830 --> 00:39:30,100
0,400 510,1055 1055,1370 1370,1910 1910,2270
largely eliminates the vanishing gradient

1284
00:39:30,100 --> 00:39:31,890
0,285 285,600 600,920 1060,1425 1425,1790
problem that I introduced earlier.|
|

1285
00:39:33,840 --> 00:39:35,855
0,400 780,1085 1085,1300 1470,1760 1760,2015
Again, we're not,| {if,you're, -}
再次，我们不是，|如果你有兴趣学习更多关于 LSTM 的数学知识和细节，

1286
00:39:35,855 --> 00:39:37,010
0,135 135,465 465,735 735,915 915,1155
if you're interested in learning

1287
00:39:37,010 --> 00:39:38,390
0,300 300,540 540,690 690,1230 1230,1380
more about the mathematics and

1288
00:39:38,390 --> 00:39:40,640
0,225 225,510 510,830 970,1730 1930,2250
the details of LSTMs,| please
|请在讲座结束后与我们讨论，

1289
00:39:40,640 --> 00:39:42,070
0,320 370,735 735,1005 1005,1170 1170,1430
come and discuss with us

1290
00:39:42,180 --> 00:39:43,900
0,365 365,575 575,1000 1110,1415 1415,1720
after the lectures,| but again,
|但再次强调 LSTM 运作背后的核心概念和直觉。

1291
00:39:44,160 --> 00:39:45,875
0,335 335,920 920,1175 1175,1400 1400,1715
just emphasizing the core concept

1292
00:39:45,875 --> 00:39:48,005
0,270 270,375 375,845 1255,1655 1855,2130
and the intuition behind how

1293
00:39:48,005 --> 00:39:49,900
0,135 135,735 735,1325
the LSTM operates.|
|

1294
00:39:51,320 --> 00:39:53,995
0,400 930,1330 1950,2255 2255,2480 2480,2675
Okay, so, so far where
好的，所以，到目前为止，

1295
00:39:53,995 --> 00:39:55,165
0,180 180,395 505,795 795,1065 1065,1170
we've at, where we've been

1296
00:39:55,165 --> 00:39:56,200
0,150 150,420 420,630 630,855 855,1035
at,| we've covered a lot
|我们已经覆盖了很多领域，

1297
00:39:56,200 --> 00:39:58,165
0,180 180,470 1180,1545 1545,1740 1740,1965
of ground,| we've gone through
|我们已经了解了 RNN 的基本工作原理，

1298
00:39:58,165 --> 00:39:59,695
0,150 150,425 595,1095 1095,1260 1260,1530
the fundamental workings of {RNN

1299
00:39:59,695 --> 00:40:01,795
0,245 685,960 960,1235 1495,1800 1800,2100
-},| the architecture, the training,
|架构、训练以及它们所应用的问题类型。

1300
00:40:01,795 --> 00:40:02,905
0,285 285,465 465,645 645,885 885,1110
the type of problems that

1301
00:40:02,905 --> 00:40:04,770
0,210 210,405 405,690 690,1025 1465,1865
they've been applied to.| {And,I'd
|我想通过考虑一些具体的例子来结束这一部分，

1302
00:40:04,850 --> 00:40:06,610
0,350 350,485 485,790 1200,1520 1520,1760
-} like to close this

1303
00:40:06,610 --> 00:40:08,635
0,320 400,800 820,1185 1185,1550 1660,2025
part by considering some concrete

1304
00:40:08,635 --> 00:40:10,405
0,365 715,1035 1035,1305 1305,1620 1620,1770
examples| of how you're going
|这些例子说明如何在软件实验中使用 RNN 。

1305
00:40:10,405 --> 00:40:12,385
0,180 180,405 405,965 1195,1595 1675,1980
to use RNNs in your

1306
00:40:12,385 --> 00:40:13,520
0,285 285,665
software lab.|
|

1307
00:40:14,280 --> 00:40:15,350
0,290 290,515 515,755 755,950 950,1070
And that is going to
这将是音乐生成的任务，

1308
00:40:15,350 --> 00:40:16,445
0,105 105,255 255,435 435,720 720,1095
be in the task of

1309
00:40:16,445 --> 00:40:18,965
0,360 360,725 1495,1875 1875,2280 2280,2520
music generation,| where you're going
|你将致力于建立一个 RNN ，

1310
00:40:18,965 --> 00:40:20,045
0,225 225,495 495,720 720,900 900,1080
to work to build an

1311
00:40:20,045 --> 00:40:22,505
0,545 775,1125 1125,1440 1440,1805 2185,2460
RNN,| that can predict the
|它可以预测序列中的下一个音符，

1312
00:40:22,505 --> 00:40:24,230
0,270 270,665 715,1115 1345,1590 1590,1725
next musical note in a

1313
00:40:24,230 --> 00:40:26,060
0,290 700,1035 1035,1290 1290,1545 1545,1830
sequence| and use it to
|并用它来生成以前从未实现过的全新音乐序列。

1314
00:40:26,060 --> 00:40:28,360
0,350 610,960 960,1230 1230,1550 1570,2300
generate brand new musical sequences

1315
00:40:28,440 --> 00:40:29,630
0,275 275,440 440,650 650,905 905,1190
that have never been realized

1316
00:40:29,630 --> 00:40:30,560
0,350
before.|
|

1317
00:40:31,120 --> 00:40:32,190
0,400 450,710 710,815 815,905 905,1070
So to give you an
所以给你们举个例子，

1318
00:40:32,190 --> 00:40:33,530
0,300 300,600 600,825 825,1035 1035,1340
example| of just the quality
|说明你们可以尝试瞄准的输出的质量和类型，

1319
00:40:33,640 --> 00:40:35,700
0,400 450,850 1260,1550 1550,1820 1820,2060
and and type of output

1320
00:40:35,700 --> 00:40:37,040
0,135 135,285 285,560 640,990 990,1340
that you can try to

1321
00:40:37,090 --> 00:40:38,850
0,350 350,700 1050,1340 1340,1520 1520,1760
aim towards,| a few years
|几年前，有一部作品在 RNN 上接受了古典音乐数据语料库的训练，

1322
00:40:38,850 --> 00:40:40,215
0,350 610,885 885,1020 1020,1140 1140,1365
ago, there was a work

1323
00:40:40,215 --> 00:40:42,975
0,365 1615,1935 1935,2130 2130,2535 2535,2760
that trained in RNN on

1324
00:40:42,975 --> 00:40:45,345
0,255 255,785 925,1260 1260,1595 1975,2370
a corpus of classical music

1325
00:40:45,345 --> 00:40:47,760
0,395 955,1275 1275,1860 1860,2175 2175,2415
data| and famously there's this
|著名的作曲家 Schubert ，

1326
00:40:47,760 --> 00:40:50,790
0,585 585,1050 1050,1430 2470,2805 2805,3030
composer, Schubert,| who wrote a
|写了一首著名的未完成的交响曲，

1327
00:40:50,790 --> 00:40:53,115
0,285 285,1020 1020,1590 1590,1890 1890,2325
famous unfinished symphony,| that consisted
|由两个乐章组成，

1328
00:40:53,115 --> 00:40:55,080
0,240 240,495 495,785 1345,1725 1725,1965
of two movements,| but he
|但他无法在去世前完成他的交响乐，

1329
00:40:55,080 --> 00:40:56,420
0,225 225,540 540,765 765,990 990,1340
was unable to finish his

1330
00:40:57,370 --> 00:40:59,270
0,275 275,820 1020,1400 1400,1640 1640,1900
his symphony before he died,|
|

1331
00:40:59,500 --> 00:41:00,645
0,275 275,455 455,740 740,980 980,1145
{so,he -} died and then
他去世了，然后留下了未完成的第三乐章，

1332
00:41:00,645 --> 00:41:02,120
0,240 240,495 495,705 705,995 1075,1475
he left the third movement

1333
00:41:02,470 --> 00:41:03,680
0,730
unfinished,|
|

1334
00:41:03,750 --> 00:41:04,685
0,245 245,365 365,530 530,740 740,935
so a few years ago,
几年前，一个小组训练了一个基于 RNN 的模型，

1335
00:41:04,685 --> 00:41:06,500
0,165 165,455 535,930 930,1275 1275,1815
a group trained an RNN

1336
00:41:06,500 --> 00:41:09,065
0,320 550,950 1300,1700 1960,2295 2295,2565
based model| to actually try
|试图生成 Schubert 著名的未完成交响乐的第三乐章，

1337
00:41:09,065 --> 00:41:10,780
0,210 210,485 835,1140 1140,1380 1380,1715
to generate the third movement

1338
00:41:10,860 --> 00:41:13,360
0,290 290,880 1050,1415 1415,1940 1940,2500
to schubert's famous unfinished symphony,|
|

1339
00:41:13,440 --> 00:41:15,040
0,335 335,545 545,800 800,1180 1200,1600
given the prior two movements,|
给定之前的两个乐章，|

1340
00:41:15,800 --> 00:41:16,685
0,315 315,510 510,600 600,735 735,885
so {I'm -} going to
所以我现在就来播放结果。

1341
00:41:16,685 --> 00:41:18,410
0,150 150,345 345,630 630,995 1435,1725
play the result quite right

1342
00:41:18,410 --> 00:41:19,600
0,290
now.|
|

1343
00:41:37,710 --> 00:41:39,820
0,400 600,1130 1130,1475 1475,1865 1865,2110
I paused, {I,interrupted -} it
我暂停一下，我在那里突然地打断了它，

1344
00:41:39,870 --> 00:41:41,525
0,260 260,725 725,935 935,1270 1350,1655
quite abruptly there,| but if
|但如果有任何古典音乐爱好者，

1345
00:41:41,525 --> 00:41:42,910
0,150 150,255 255,515 565,965 985,1385
there are any classical music

1346
00:41:42,930 --> 00:41:45,125
0,890 890,1130 1130,1450 1590,1955 1955,2195
aficionados out there,| hopefully you
|希望你能欣赏到音乐质量方面产生的某种质量，

1347
00:41:45,125 --> 00:41:46,700
0,120 120,365 595,1140 1140,1425 1425,1575
get an appreciation for kind

1348
00:41:46,700 --> 00:41:48,545
0,240 240,510 510,800 1420,1695 1695,1845
of the quality that was

1349
00:41:48,545 --> 00:41:50,450
0,275 985,1380 1380,1665 1665,1800 1800,1905
generated in terms of the

1350
00:41:50,450 --> 00:41:52,030
0,225 225,590 820,1095 1095,1275 1275,1580
{music,quality -},| and this was
|而这已经是几年前的事了，

1351
00:41:52,200 --> 00:41:53,285
0,320 320,485 485,605 605,815 815,1085
already from a {few,years -}

1352
00:41:53,285 --> 00:41:54,370
0,210 210,375 375,570 570,810 810,1085
ago,| and as we'll see
|正如我们将在接下来的课程中看到的，

1353
00:41:54,510 --> 00:41:56,360
0,260 260,395 395,590 590,1090 1470,1850
in the next lectures| and
|并继续这个生成性人工智能的主题，

1354
00:41:56,360 --> 00:41:57,425
0,330 330,540 540,705 705,900 900,1065
continuing with this theme of

1355
00:41:57,425 --> 00:41:59,645
0,495 495,845 1165,1500 1500,1835 1945,2220
generative AI,| the power of
|这些算法的能力已经取得了进步，

1356
00:41:59,645 --> 00:42:01,540
0,240 240,615 615,915 915,1295
these algorithms has advanced

1357
00:42:01,610 --> 00:42:03,610
0,785 785,1120 1200,1505 1505,1760 1760,2000
tremendously,| since we first played
|自从我们第一次播放这个例子以来，

1358
00:42:03,610 --> 00:42:06,700
0,240 240,590 1450,1850 2050,2450
this example,| particularly in,
|特别是在，我很兴奋地谈到了一系列领域，

1359
00:42:06,700 --> 00:42:07,780
0,150 150,405 405,675 675,855 855,1080
you know, a whole range

1360
00:42:07,780 --> 00:42:09,955
0,320 400,870 870,1220 1570,1950 1950,2175
of domains which I'm excited

1361
00:42:09,955 --> 00:42:11,470
0,195 195,360 360,555 555,845 1225,1515
to talk about,| but not
|但目前还不是时候。

1362
00:42:11,470 --> 00:42:13,920
0,135 135,380 880,1280 1390,1790 1900,2450
for now.| {Okay\,,so,you'll - -}
|好的，你将在今天的实验 RNN 音乐生成中正面解决这个问题。

1363
00:42:13,940 --> 00:42:15,235
0,440 440,590 590,845 845,1085 1085,1295
tackle this problem head on

1364
00:42:15,235 --> 00:42:17,005
0,255 255,645 645,905 1015,1530 1530,1770
in today's lab RNN music

1365
00:42:17,005 --> 00:42:18,820
0,335
generation.|
|

1366
00:42:20,590 --> 00:42:22,560
0,400 1080,1340 1340,1475 1475,1670 1670,1970
Also, we can think about
此外，我们还可以考虑将输入序列转换为具有情感分类的单个输出的简单示例，

1367
00:42:22,560 --> 00:42:24,930
0,380 730,1130 1420,1785 1785,2115 2115,2370
the simple example of input

1368
00:42:24,930 --> 00:42:26,355
0,290 340,570 570,675 675,950 1090,1425
sequence to a single output

1369
00:42:26,355 --> 00:42:29,060
0,300 300,840 840,1415
with sentiment classification,|
|

1370
00:42:29,130 --> 00:42:30,220
0,290 290,455 455,605 605,785 785,1090
where we can think about,
例如，我们可以考虑像推特这样的文本，

1371
00:42:30,300 --> 00:42:31,990
0,290 290,560 560,905 905,1205 1205,1690
for example, text like tweets|
|

1372
00:42:32,220 --> 00:42:34,570
0,400 570,1150 1350,1750 1770,2060 2060,2350
and assigning positive or negative
并为这些文本赋予积极或消极的标签，

1373
00:42:34,860 --> 00:42:36,935
0,635 635,905 905,1180 1410,1745 1745,2075
labels to these,| these text
|这些文本例子基于网络学习的内容。

1374
00:42:36,935 --> 00:42:39,730
0,395 685,1080 1080,1475 2245,2520 2520,2795
examples based on the content

1375
00:42:39,840 --> 00:42:41,330
0,400 510,800 800,1085 1085,1355 1355,1490
that is learned by the

1376
00:42:41,330 --> 00:42:42,000
0,260
network.|
|

1377
00:42:43,450 --> 00:42:46,100
0,400 870,1270 1830,2165 2165,2375 2375,2650
Okay, so this kind of
好的，关于 RNN 的部分已经结束了，

1378
00:42:46,210 --> 00:42:48,060
0,485 485,665 665,940 1170,1505 1505,1850
concludes the portion on {RNNs

1379
00:42:48,060 --> 00:42:49,695
0,260 730,1065 1065,1260 1260,1395 1395,1635
-},| and I think it's
|我认为这是相当了不起的，

1380
00:42:49,695 --> 00:42:51,435
0,275 415,815 865,1170 1170,1455 1455,1740
quite remarkable,| that using all
|使用我们到目前为止讨论的所有基本概念和操作，

1381
00:42:51,435 --> 00:42:53,480
0,165 165,605 655,1290 1290,1565 1645,2045
the foundational concepts and operations,

1382
00:42:53,950 --> 00:42:55,110
0,275 275,485 485,650 650,905 905,1160
that we've talked about so

1383
00:42:55,110 --> 00:42:56,930
0,320 700,1050 1050,1170 1170,1440 1440,1820
far,| we've been able to
|我们能够尝试建立网络来处理这个复杂的序列建模问题。

1384
00:42:57,010 --> 00:42:58,790
0,335 335,530 530,725 725,1060 1380,1780
try to build up networks

1385
00:42:58,840 --> 00:43:00,420
0,305 305,575 575,935 935,1265 1265,1580
that handle this complex problem

1386
00:43:00,420 --> 00:43:02,120
0,270 270,735 735,1250
of sequential modeling.|
|

1387
00:43:02,450 --> 00:43:05,380
0,400 1470,1805 1805,2075 2075,2410 2580,2930
But like any technology, right,|
但就像任何技术一样，|

1388
00:43:05,380 --> 00:43:06,250
0,240 240,405 405,525 525,675 675,870
and {RNN - -} is
RNN 也不是没有局限性的，

1389
00:43:06,250 --> 00:43:08,455
0,195 195,500 520,1160 1540,1935 1935,2205
not without limitations,| {so,what -}
|那么，这些限制有哪些，

1390
00:43:08,455 --> 00:43:10,015
0,180 180,345 345,480 480,755 955,1560
are some of those limitations|
|

1391
00:43:10,015 --> 00:43:11,140
0,240 240,375 375,525 525,795 795,1125
and what are some potential
使用 RNN 甚至 LSTM 可能会出现哪些潜在问题。

1392
00:43:11,140 --> 00:43:13,330
0,350 880,1280 1330,1635 1635,1920 1920,2190
issues that can arise with

1393
00:43:13,330 --> 00:43:15,690
0,270 270,840 840,1170 1170,1520 1540,2360
using RNNs or even LSTMs.|
|

1394
00:43:17,110 --> 00:43:18,990
0,275 275,545 545,940 1080,1480 1530,1880
The first is this idea
第一种是编码和依赖的思想，

1395
00:43:18,990 --> 00:43:21,750
0,300 300,960 960,1280 1750,2150 2380,2760
of encoding and depend and

1396
00:43:21,750 --> 00:43:24,290
0,740 1300,1605 1605,1800 1800,2090 2140,2540
dependency,| in terms of the
|就我们试图处理的数据的时间分离而言，

1397
00:43:24,610 --> 00:43:26,325
0,575 575,1085 1085,1295 1295,1505 1505,1715
temporal separation of data that

1398
00:43:26,325 --> 00:43:27,860
0,225 225,375 375,540 540,815
we're trying to process,|
|

1399
00:43:28,050 --> 00:43:29,945
0,320 320,880 960,1360 1380,1670 1670,1895
while RNN require is that
RNN 要求对时序信息进行逐次馈入和时间处理，

1400
00:43:29,945 --> 00:43:31,880
0,300 300,1050 1050,1425 1425,1725 1725,1935
the sequential information is fed

1401
00:43:31,880 --> 00:43:33,860
0,290 340,645 645,1310 1390,1725 1725,1980
in and processed time step

1402
00:43:33,860 --> 00:43:35,320
0,225 225,480 480,830
by time step,|
|

1403
00:43:35,320 --> 00:43:36,685
0,225 225,360 360,830 880,1185 1185,1365
what that imposes is what
这造成了我们所说的编码瓶颈，

1404
00:43:36,685 --> 00:43:38,485
0,165 165,420 420,660 660,1230 1230,1800
we call an encoding bottleneck,

1405
00:43:38,485 --> 00:43:41,005
0,305 685,1020 1020,2190 2190,2370 2370,2520
right,| where we're trying to
|我们试图编码大量内容的地方，

1406
00:43:41,005 --> 00:43:42,240
0,375 375,540 540,750 750,945 945,1235
encode a lot of content,|
|

1407
00:43:42,620 --> 00:43:43,870
0,290 290,515 515,725 725,950 950,1250
for example, a very large
例如，将非常大的文本主体，

1408
00:43:43,870 --> 00:43:45,775
0,255 255,480 480,800 1300,1635 1635,1905
body of text,| many different
|许多不同的单词到单个输出中，

1409
00:43:45,775 --> 00:43:48,000
0,335 685,1020 1020,1275 1275,1595 1825,2225
words into a single output,|
|

1410
00:43:48,580 --> 00:43:49,735
0,210 210,420 420,705 705,975 975,1155
that may be just at
这可能只是最后一个时间点，

1411
00:43:49,735 --> 00:43:50,970
0,135 135,330 330,615 615,900 900,1235
the very last time step,|
|

1412
00:43:51,110 --> 00:43:52,105
0,275 275,395 395,605 605,830 830,995
{how,do -} we ensure that
我们如何确保该时间点之前的所有信息

1413
00:43:52,105 --> 00:43:53,815
0,210 210,515 565,965 1135,1470 1470,1710
all that information leading up

1414
00:43:53,815 --> 00:43:55,105
0,150 150,300 300,540 540,875 955,1290
to that time step| was
|都得到了适当的维护、编码和网络学习，

1415
00:43:55,105 --> 00:43:57,265
0,335 385,785 1165,1440 1440,1965 1965,2160
properly maintained and encoded and

1416
00:43:57,265 --> 00:43:58,540
0,225 225,420 420,555 555,815
learned by the network,|
|

1417
00:43:58,600 --> 00:43:59,970
0,305 305,605 605,860 860,1070 1070,1370
in practice, this is very,
在实践中，这是非常具有挑战性的，

1418
00:43:59,970 --> 00:44:01,245
0,285 285,620 730,975 975,1110 1110,1275
very challenging| and a lot
|很多信息可能会丢失。

1419
00:44:01,245 --> 00:44:02,870
0,275 295,695 895,1200 1200,1365 1365,1625
of information can be lost.|
|

1420
00:44:04,060 --> 00:44:06,120
0,335 335,890 890,1160 1160,1450 1740,2060
Another limitation is that,| by
另一个限制是，|通过逐个时间地进行这种时间步进处理，

1421
00:44:06,120 --> 00:44:07,245
0,240 240,495 495,750 750,945 945,1125
doing this time step by

1422
00:44:07,245 --> 00:44:09,330
0,210 210,420 420,725 1315,1890 1890,2085
time step processing,| RNNs can
|RNN 可能相当慢，

1423
00:44:09,330 --> 00:44:11,025
0,210 210,510 510,860 1300,1545 1545,1695
be quite slow,| {there,is -}
|没有一种真正简单的方法来并行化这种计算。

1424
00:44:11,025 --> 00:44:12,195
0,225 225,450 450,645 645,885 885,1170
not really an easy way

1425
00:44:12,195 --> 00:44:14,740
0,240 240,885 885,1140 1140,1745
to parallelize that computation.|
|

1426
00:44:15,190 --> 00:44:18,030
0,400 960,1360 1800,2150 2150,2495 2495,2840
And finally, together, these components
最后，编码瓶颈的这些组件加在一起，

1427
00:44:18,030 --> 00:44:19,995
0,240 240,390 390,885 885,1520 1600,1965
of the encoding bottleneck,| the
|要求逐步处理这些数据，造成了最大的问题，

1428
00:44:19,995 --> 00:44:21,500
0,480 480,645 645,930 930,1215 1215,1505
requirement to process this data

1429
00:44:21,880 --> 00:44:24,195
0,275 275,485 485,820 1350,1960 2040,2315
step by step, imposes the

1430
00:44:24,195 --> 00:44:26,085
0,240 240,605 805,1080 1080,1355 1615,1890
biggest problem,| which is when
|这就是当我们谈论长记忆时，

1431
00:44:26,085 --> 00:44:27,560
0,165 165,375 375,615 615,935 1075,1475
we talk about long memory,|
|

1432
00:44:28,380 --> 00:44:30,230
0,400 510,910 960,1235 1235,1355 1355,1850
the capacity of the RNN
RNN 和 LSTM 的容量确实没有那么长，

1433
00:44:30,230 --> 00:44:31,820
0,255 255,390 390,1040 1060,1350 1350,1590
and the LSTM is really

1434
00:44:31,820 --> 00:44:33,180
0,240 240,435 435,740
not that long,|
|

1435
00:44:33,180 --> 00:44:35,595
0,225 225,675 675,1020 1020,1370 2050,2415
we can't really handle data
我们真的无法处理数万或数十万的数据，

1436
00:44:35,595 --> 00:44:37,170
0,345 345,645 645,855 855,1145 1285,1575
of tens of thousands or

1437
00:44:37,170 --> 00:44:38,730
0,255 255,480 480,740 1000,1290 1290,1560
hundreds of thousands,| or even
|甚至超出了有效学习的序列信息，

1438
00:44:38,730 --> 00:44:41,540
0,380 610,1335 1335,1700 2080,2445 2445,2810
beyond sequential information that effectively

1439
00:44:41,650 --> 00:44:43,710
0,290 290,580 870,1270 1410,1775 1775,2060
to learn| the complete amount
|完整信息量和模式，

1440
00:44:43,710 --> 00:44:45,135
0,300 300,630 630,900 900,1200 1200,1425
of information and patterns,| that
|出现在如此大的数据源。

1441
00:44:45,135 --> 00:44:46,500
0,245 295,660 660,990 990,1215 1215,1365
are present within such a

1442
00:44:46,500 --> 00:44:48,400
0,290 610,960 960,1310
rich data source.|
|

1443
00:44:49,150 --> 00:44:50,630
0,305 305,610 780,1055 1055,1205 1205,1480
And so because of this,|
正因为如此，|

1444
00:44:50,860 --> 00:44:52,275
0,350 350,700 720,1115 1115,1265 1265,1415
very recently, there's been a
最近，人们非常关注，

1445
00:44:52,275 --> 00:44:53,535
0,150 150,360 360,690 690,1020 1020,1260
lot of attention,| in how
|我们如何超越这种循序渐进的循环处理的概念，

1446
00:44:53,535 --> 00:44:55,340
0,165 165,330 330,615 615,995 1405,1805
we can move beyond this

1447
00:44:55,750 --> 00:44:57,030
0,335 335,605 605,830 830,1040 1040,1280
notion of step by step,

1448
00:44:57,030 --> 00:44:59,160
0,510 510,800 1330,1590 1590,1815 1815,2130
recurrent processing,| to build even
|建立更强大的架构来处理顺序数据。

1449
00:44:59,160 --> 00:45:01,400
0,300 300,650 700,1580 1660,1950 1950,2240
more powerful architectures for processing

1450
00:45:01,540 --> 00:45:02,940
0,590 590,850
sequential data.|
|

1451
00:45:03,720 --> 00:45:05,120
0,400 450,785 785,1010 1010,1175 1175,1400
To understand how we do,
为了理解我们是如何做的，我们如何开始做这件事，

1452
00:45:05,120 --> 00:45:06,110
0,210 210,330 330,540 540,810 810,990
how we can start to

1453
00:45:06,110 --> 00:45:07,415
0,120 120,380 490,870 870,1080 1080,1305
do this,| let's take a
|让我们后退一大步，

1454
00:45:07,415 --> 00:45:09,155
0,195 195,405 405,695 775,1175 1405,1740
big step back, right,| think
|想想我在一开始介绍的序列建模的高级目标。

1455
00:45:09,155 --> 00:45:10,655
0,315 315,660 660,960 960,1230 1230,1500
about the high level goal

1456
00:45:10,655 --> 00:45:12,035
0,240 240,510 510,990 990,1125 1125,1380
of sequence modeling that I

1457
00:45:12,035 --> 00:45:13,420
0,395 475,735 735,840 840,1035 1035,1385
introduced at the very beginning.|
|

1458
00:45:14,140 --> 00:45:15,550
0,240 240,570 570,900 900,1155 1155,1410
Given some input, a sequence
给定一些输入，一个数据序列，

1459
00:45:15,550 --> 00:45:17,320
0,210 210,470 880,1170 1170,1440 1440,1770
of data,| we want to
|我们想要构建一个特征编码，

1460
00:45:17,320 --> 00:45:19,840
0,350 370,770 940,1275 1275,1970
build a feature encoding,|
|

1461
00:45:19,840 --> 00:45:21,180
0,300 300,600 600,840 840,1065 1065,1340
and use our neural network
并使用我们的神经网络来学习，

1462
00:45:21,320 --> 00:45:22,530
0,275 275,470 470,725 725,935 935,1210
to learn that| and then
|然后将特征编码转换为预测输出。

1463
00:45:22,730 --> 00:45:24,690
0,335 335,575 575,800 800,1450 1560,1960
transform that feature encoding into

1464
00:45:24,830 --> 00:45:27,280
0,400 780,1400 1400,1780
a predicted output.|
|

1465
00:45:27,380 --> 00:45:28,630
0,275 275,455 455,760 780,1055 1055,1250
What we saw is that,|
我们看到的是，|

1466
00:45:28,630 --> 00:45:30,310
0,585 585,900 900,1110 1110,1395 1395,1680
RNNs used this notion of
RNN 使用这种递归的概念来维护顺序信息，

1467
00:45:30,310 --> 00:45:32,970
0,620 940,1245 1245,1550 1720,2120 2260,2660
recurrence to maintain order information,|
|

1468
00:45:33,500 --> 00:45:35,875
0,400 1110,1510 1560,1895 1895,2150 2150,2375
processing information time step by
一步一步地处理信息，

1469
00:45:35,875 --> 00:45:36,940
0,240 240,575
time step,|
|

1470
00:45:37,290 --> 00:45:38,435
0,335 335,560 560,710 710,860 860,1145
but as I just mentioned,|
但正如我刚才提到的，|

1471
00:45:38,435 --> 00:45:40,055
0,270 270,435 435,690 690,1055 1285,1620
we had these key three
我们面临着这三个关键的瓶颈，

1472
00:45:40,055 --> 00:45:42,660
0,615 615,750 750,870 870,1145
bottlenecks to RNN,|
|

1473
00:45:42,760 --> 00:45:43,860
0,275 275,425 425,620 620,845 845,1100
what we really want to
我们真正想要实现的是，

1474
00:45:43,860 --> 00:45:45,050
0,240 240,420 420,585 585,825 825,1190
achieve is| to go beyond
|超越这些瓶颈，在这些模型的能力方面实现更高的能力，

1475
00:45:45,250 --> 00:45:47,595
0,400 570,1220 1220,1490 1490,1870 1950,2345
these bottlenecks and achieve even

1476
00:45:47,595 --> 00:45:49,245
0,375 375,755 1015,1305 1305,1485 1485,1650
higher capabilities in terms of

1477
00:45:49,245 --> 00:45:50,900
0,165 165,455 745,1050 1050,1305 1305,1655
the power of these models,|
|

1478
00:45:51,690 --> 00:45:53,165
0,365 365,620 620,830 830,1010 1010,1475
rather than having an encoding
不是存在编码瓶颈，

1479
00:45:53,165 --> 00:45:54,910
0,525 525,1020 1020,1185 1185,1410 1410,1745
bottleneck,| ideally we want to
|理想情况下，我们希望将信息作为连续的信息流持续处理，

1480
00:45:54,960 --> 00:45:57,545
0,400 630,1030 1200,1990 2070,2345 2345,2585
process information continuously as a

1481
00:45:57,545 --> 00:45:59,640
0,365 475,765 765,1055 1105,1505
continuous stream of information,|
|

1482
00:45:59,710 --> 00:46:01,065
0,335 335,560 560,830 830,1145 1145,1355
rather than being slow,| we
不是变慢，|我们希望能够并行化计算以加快处理速度，

1483
00:46:01,065 --> 00:46:01,950
0,195 195,345 345,435 435,645 645,885
want to be able to

1484
00:46:01,950 --> 00:46:04,200
0,710 880,1550 1570,1890 1890,2070 2070,2250
parallelize computations to speed up

1485
00:46:04,200 --> 00:46:05,280
0,320
processing,|
|

1486
00:46:05,350 --> 00:46:07,010
0,320 320,640 810,1115 1115,1340 1340,1660
and finally, of course, our
最后，当然，我们的主要目标是真正尝试建立长期记忆，

1487
00:46:07,150 --> 00:46:08,325
0,335 335,575 575,770 770,935 935,1175
main goal is to really

1488
00:46:08,325 --> 00:46:10,305
0,330 330,645 645,995 1255,1620 1620,1980
try to establish long memory,|
|

1489
00:46:10,305 --> 00:46:12,315
0,255 255,405 405,695 1195,1545 1545,2010
that can build a nuanced
以建立对顺序数据的细微差别和丰富的理解。

1490
00:46:12,315 --> 00:46:14,475
0,270 270,575 775,1175 1255,1605 1605,2160
and rich understanding of sequential

1491
00:46:14,475 --> 00:46:15,640
0,275
data.|
|

1492
00:46:16,110 --> 00:46:17,435
0,275 275,800 800,1040 1040,1205 1205,1325
The limitation of {RNN -
RNN 的局限性，

1493
00:46:17,435 --> 00:46:18,515
0,165 165,495 495,720 720,915 915,1080
-},| that's linked to all
|与我们无法实现这些能力的所有这些问题和问题联系在一起的是，

1494
00:46:18,515 --> 00:46:21,230
0,305 655,1055 1225,1625 1915,2315 2335,2715
these problems and issues in

1495
00:46:21,230 --> 00:46:22,775
0,285 285,825 825,1140 1140,1350 1350,1545
our inability to achieve these

1496
00:46:22,775 --> 00:46:25,010
0,305 1135,1410 1410,1560 1560,1835 1855,2235
capabilities is that,| they require
|它们需要时间一步一步地处理，

1497
00:46:25,010 --> 00:46:26,525
0,380 520,855 855,1080 1080,1290 1290,1515
this time step by time

1498
00:46:26,525 --> 00:46:29,510
0,225 225,545 2125,2525 2575,2835 2835,2985
step processing,| {so,what -} if
|那么，如果我们可以超越这一点，又会怎样，

1499
00:46:29,510 --> 00:46:30,515
0,180 180,360 360,585 585,810 810,1005
we could move beyond that,|
|

1500
00:46:30,515 --> 00:46:31,745
0,165 165,300 300,450 450,690 690,1230
what if we could eliminate
如果我们可以完全消除这种重复的需要，

1501
00:46:31,745 --> 00:46:33,340
0,225 225,465 465,645 645,1175 1195,1595
this need for recurrence entirely|
|

1502
00:46:33,690 --> 00:46:34,790
0,305 305,515 515,710 710,860 860,1100
and not have to process
并且不必逐个时间地处理数据。

1503
00:46:34,790 --> 00:46:36,185
0,240 240,500 670,990 990,1200 1200,1395
the data time step by

1504
00:46:36,185 --> 00:46:36,940
0,305
time.|
|

1505
00:46:37,800 --> 00:46:39,485
0,395 395,680 680,950 950,1235 1235,1685
Well, a first and naive
那么，第一个也是天真的方法是，

1506
00:46:39,485 --> 00:46:41,380
0,365 745,1050 1050,1305 1305,1575 1575,1895
approach would be,| to just
|把所有的数据，所有的步骤放在一起，

1507
00:46:41,730 --> 00:46:43,730
0,430 510,845 845,1040 1040,1300 1710,2000
squash all the data, all

1508
00:46:43,730 --> 00:46:45,200
0,225 225,465 465,770 850,1185 1185,1470
all, all the time steps

1509
00:46:45,200 --> 00:46:47,465
0,350 550,870 870,1190 1480,1845 1845,2265
together| to create a vector
|创建一个有效连接的向量，

1510
00:46:47,465 --> 00:46:50,135
0,435 435,755 895,1745 2125,2460 2460,2670
that's effectively concatenated, right,| the
|时间步长被消除了，只有一个流。

1511
00:46:50,135 --> 00:46:52,040
0,210 210,420 420,615 615,1145 1615,1905
time steps are eliminated, there

1512
00:46:52,040 --> 00:46:54,500
0,180 180,500 910,1305 1305,1700
just one one stream.|
|

1513
00:46:54,510 --> 00:46:55,625
0,275 275,410 410,590 590,845 845,1115
Where we have now one
我们现在有一个包含所有时间点数据的矢量输入，然后将其输入到模型中。

1514
00:46:55,625 --> 00:46:57,515
0,365 445,845 1225,1515 1515,1665 1665,1890
vector input with the data

1515
00:46:57,515 --> 00:46:59,525
0,330 330,675 675,975 975,1295 1645,2010
from all time points, that's

1516
00:46:59,525 --> 00:47:00,520
0,180 180,390 390,570 570,735 735,995
then Fed into the model.|
|

1517
00:47:01,290 --> 00:47:02,870
0,305 305,755 755,965 965,1205 1205,1580
It calculates some feature vector
它计算一些特征向量，然后生成一些输出，希望这是有意义的。

1518
00:47:02,870 --> 00:47:04,900
0,255 255,450 450,950 1180,1580 1630,2030
and then generates some output,

1519
00:47:05,010 --> 00:47:07,160
0,290 290,580 870,1175 1175,1480
which hopefully makes sense.|
|

1520
00:47:07,280 --> 00:47:08,785
0,365 365,635 635,935 935,1250 1250,1505
And because we've squashed all
因为我们一起挤压了所有这些时间步长，我们可以简单地考虑建立一个前馈网络，可以进行这种计算。

1521
00:47:08,785 --> 00:47:10,705
0,180 180,405 405,675 675,1025 1645,1920
these time steps together, we

1522
00:47:10,705 --> 00:47:12,570
0,180 180,485 745,1065 1065,1385 1465,1865
could simply think about maybe

1523
00:47:12,890 --> 00:47:14,340
0,365 365,605 605,800 800,1085 1085,1450
building a feed forward network

1524
00:47:14,600 --> 00:47:16,015
0,290 290,580 840,1115 1115,1265 1265,1415
that could, that could do

1525
00:47:16,015 --> 00:47:17,440
0,165 165,725
this computation.|
|

1526
00:47:17,450 --> 00:47:19,585
0,400 630,935 935,1235 1235,1670 1670,2135
Well, with that, we eliminate
有了这一点，我们就消除了复发的必要性。

1527
00:47:19,585 --> 00:47:21,400
0,165 165,360 360,570 570,1145
the need for recurrence.|
|

1528
00:47:21,400 --> 00:47:22,675
0,260 400,720 720,930 930,1125 1125,1275
But we still have the
但我们仍然有这样的问题。

1529
00:47:22,675 --> 00:47:24,140
0,245 265,665
issues that.|
|

1530
00:47:24,140 --> 00:47:26,180
0,285 285,495 495,1130 1540,1845 1845,2040
It's not scalable because the
它是不可扩展的，因为密集的前馈网络必须非常大，由许多不同的连接定义。

1531
00:47:26,180 --> 00:47:28,130
0,290 370,675 675,945 945,1310 1660,1950
dense feed forward network would

1532
00:47:28,130 --> 00:47:29,380
0,180 180,315 315,495 495,960 960,1250
have to be immensely large,

1533
00:47:29,490 --> 00:47:31,100
0,335 335,670 750,1070 1070,1310 1310,1610
defined by many, many different

1534
00:47:31,100 --> 00:47:32,320
0,380
connections.|
|

1535
00:47:32,360 --> 00:47:34,590
0,400 420,1010 1010,1475 1475,1850 1850,2230
And critically, we've completely lost
更重要的是，我们已经完全失去了我们的有序信息，因为我们只是盲目地将所有东西挤压在一起。没有时间依赖性，我们就会被困在试图建立长期记忆的能力中。

1536
00:47:34,610 --> 00:47:36,250
0,290 290,485 485,790 960,1355 1355,1640
our in order information by

1537
00:47:36,250 --> 00:47:38,430
0,290 340,960 960,1275 1275,1560 1560,2180
just squashing everything together blindly.

1538
00:47:38,720 --> 00:47:40,980
0,395 395,590 590,1070 1070,1630 1860,2260
There's no temporal dependence, and

1539
00:47:41,210 --> 00:47:43,225
0,580 690,1090 1320,1655 1655,1835 1835,2015
we're then stuck in our

1540
00:47:43,225 --> 00:47:44,580
0,330 330,600 600,780 780,1020 1020,1355
ability to try to establish

1541
00:47:44,840 --> 00:47:46,460
0,335 335,545 545,820
long term memory.|
|

1542
00:47:48,070 --> 00:47:50,100
0,400 540,800 800,1040 1040,1420 1740,2030
So what if instead we
那么，如果我们仍然可以考虑将这些时间步长集中在一起，但在如何尝试从这些输入数据中提取信息时更聪明一点，那会怎么样？

1543
00:47:50,100 --> 00:47:52,065
0,290 310,615 615,840 840,1160 1600,1965
could still think about bringing

1544
00:47:52,065 --> 00:47:53,955
0,270 270,510 510,810 810,1175 1615,1890
these time steps together, but

1545
00:47:53,955 --> 00:47:55,010
0,135 135,270 270,435 435,690 690,1055
be a bit more clever

1546
00:47:55,090 --> 00:47:56,400
0,350 350,620 620,845 845,1055 1055,1310
about how we try to

1547
00:47:56,400 --> 00:47:58,425
0,350 490,890 1120,1440 1440,1755 1755,2025
extract information from this input

1548
00:47:58,425 --> 00:47:59,320
0,275
data?|
|

1549
00:47:59,710 --> 00:48:01,610
0,320 320,640 690,1090 1110,1505 1505,1900
The key idea is this
关键的想法是能够识别和关注潜在顺序信息流中的重要内容。

1550
00:48:01,660 --> 00:48:04,070
0,380 380,760 1170,1550 1550,1930 2010,2410
idea of being able to

1551
00:48:04,570 --> 00:48:06,495
0,335 335,670 720,1120 1470,1745 1745,1925
identify and attend to what

1552
00:48:06,495 --> 00:48:08,210
0,270 270,635 655,1050 1050,1380 1380,1715
is important in a potentially

1553
00:48:08,290 --> 00:48:11,320
0,790 960,1340 1340,1720 1740,2140
sequential stream of information.|
|

1554
00:48:11,320 --> 00:48:12,270
0,240 240,405 405,555 555,690 690,950
And this is the notion
这就是注意力或自我关注的概念。

1555
00:48:12,290 --> 00:48:14,130
0,365 365,730 840,1160 1160,1460 1460,1840
of attention or self attention.|
|

1556
00:48:14,920 --> 00:48:16,690
0,225 225,360 360,620 790,1190 1390,1770
Which is an extremely, extremely
在现代深度学习和人工智能中，这是一个非常、非常强大的概念。我不能轻描淡写，或者，我不知道，低调，夸大。这个概念有多么强大，我怎么强调都不为过。

1557
00:48:16,690 --> 00:48:18,580
0,380 490,890 1060,1350 1350,1620 1620,1890
powerful concept in modern deep

1558
00:48:18,580 --> 00:48:20,100
0,225 225,510 510,860 940,1230 1230,1520
learning and AI. {I,cannot -}

1559
00:48:20,180 --> 00:48:21,850
0,710 710,1060 1200,1445 1445,1595 1595,1670
understate or, I don't know,

1560
00:48:21,850 --> 00:48:24,070
0,405 405,900 900,1220 1330,1650 1650,2220
understate, {overstate.,I -} cannot emphasize

1561
00:48:24,070 --> 00:48:25,920
0,330 330,630 630,980 1240,1545 1545,1850
enough how powerful this concept

1562
00:48:25,940 --> 00:48:27,360
0,400
is.|
|

1563
00:48:27,640 --> 00:48:29,570
0,400 450,785 785,980 980,1480 1530,1930
Attention is the foundational mechanism
注意是转换器体系结构的基本机制，你们中的许多人可能已经听说过，而转换器的概念通常会非常令人生畏，因为有时它们是用这些非常复杂的图表呈现的，或者部署在复杂的应用程序中。你可能会想，好吧，我怎么才能理解这一点呢？

1564
00:48:29,740 --> 00:48:32,540
0,400 420,820 840,1400 1400,1780 2400,2800
of the transformer architecture, which

1565
00:48:32,800 --> 00:48:33,765
0,305 305,470 470,620 620,800 800,965
many of you may have

1566
00:48:33,765 --> 00:48:36,950
0,180 180,485 1075,1475 1915,2465 2785,3185
heard about, and it's the

1567
00:48:37,600 --> 00:48:38,835
0,335 335,515 515,755 755,1115 1115,1235
notion of a transformer can

1568
00:48:38,835 --> 00:48:40,335
0,255 255,510 510,705 705,1245 1245,1500
often be very daunting because

1569
00:48:40,335 --> 00:48:41,780
0,270 270,600 600,845 895,1170 1170,1445
sometimes they're presented with these

1570
00:48:41,860 --> 00:48:44,925
0,400 480,800 800,1625 1625,2020 2700,3065
really complex diagrams or deployed

1571
00:48:44,925 --> 00:48:47,040
0,245 265,665 745,1145 1645,1950 1950,2115
in complex applications. {And,you -}

1572
00:48:47,040 --> 00:48:48,060
0,135 135,375 375,690 690,915 915,1020
may think, okay, how do

1573
00:48:48,060 --> 00:48:48,975
0,105 105,315 315,540 540,720 720,915
I even start to make

1574
00:48:48,975 --> 00:48:50,020
0,195 195,345 345,605
sense of this?|
|

1575
00:48:50,540 --> 00:48:52,030
0,290 290,485 485,710 710,1030 1110,1490
At its core, though, attention,
不过，在它的核心，注意，关键操作是一个非常直观的想法，在这节课的最后一部分，我们将一步一步地分析它，看看为什么它如此强大，以及我们如何将它用作更大的神经网络的一部分，比如变压器。

1576
00:48:52,030 --> 00:48:53,320
0,270 270,555 555,915 915,1140 1140,1290
the key operation is a

1577
00:48:53,320 --> 00:48:55,360
0,270 270,945 945,1310 1570,1860 1860,2040
very intuitive idea, and we're

1578
00:48:55,360 --> 00:48:56,710
0,150 150,450 450,705 705,980 1000,1350
going to, in the last

1579
00:48:56,710 --> 00:48:58,240
0,270 270,450 450,600 600,890 1240,1530
portion of this lecture, break

1580
00:48:58,240 --> 00:48:59,490
0,150 150,405 405,690 690,915 915,1250
it down step by step

1581
00:48:59,540 --> 00:49:00,985
0,320 320,640 660,1010 1010,1295 1295,1445
to see why it's so

1582
00:49:00,985 --> 00:49:02,380
0,305 655,945 945,1125 1125,1275 1275,1395
powerful and how we can

1583
00:49:02,380 --> 00:49:03,145
0,150 150,300 300,480 480,660 660,765
use it as part of

1584
00:49:03,145 --> 00:49:04,810
0,120 120,395 655,1005 1005,1265 1375,1665
a larger neural network, like

1585
00:49:04,810 --> 00:49:06,000
0,270 270,800
a transformer.|
|

1586
00:49:07,550 --> 00:49:08,695
0,400 540,770 770,860 860,1010 1010,1145
Specifically, {we're -} going to
具体地说，我们将讨论并关注这种自我关注的概念。

1587
00:49:08,695 --> 00:49:10,180
0,150 150,455 475,780 780,1085 1165,1485
be talking and focusing on

1588
00:49:10,180 --> 00:49:11,640
0,285 285,540 540,765 765,1080 1080,1460
this idea of self attention.|
|

1589
00:49:12,750 --> 00:49:15,095
0,610 1110,1415 1415,1580 1580,1840 1980,2345
Attending to the most important
注意输入示例中最重要的部分。

1590
00:49:15,095 --> 00:49:16,690
0,330 330,645 645,960 960,1260 1260,1595
parts of an input example.|
|

1591
00:49:17,460 --> 00:49:19,870
0,400 750,1220 1220,1570 1830,2120 2120,2410
So let's consider an image.
那么让我们来考虑一张图片。我认为考虑一幅图像是最直观的。首先，这是一张钢铁侠的照片。如果我们的目标是试图从这张图像中提取重要的信息，我们可以做的也许是用我们的眼睛天真地扫描这张图像，逐个像素，对，只是浏览图像。

1592
00:49:20,040 --> 00:49:21,365
0,260 260,395 395,605 605,815 815,1325
{I,think -} it's most intuitive

1593
00:49:21,365 --> 00:49:23,230
0,210 210,435 435,600 600,875 1465,1865
to consider an {image.,First, -}

1594
00:49:23,760 --> 00:49:24,560
0,260 260,365 365,470 470,620 620,800
this is a picture of

1595
00:49:24,560 --> 00:49:26,090
0,240 240,590 970,1230 1230,1350 1350,1530
{iron,man. -} And if our

1596
00:49:26,090 --> 00:49:27,230
0,240 240,450 450,645 645,855 855,1140
goal is to try to

1597
00:49:27,230 --> 00:49:28,775
0,380 580,945 945,1185 1185,1335 1335,1545
extract information from this image

1598
00:49:28,775 --> 00:49:31,025
0,210 210,540 540,875 1765,2070 2070,2250
of what's important, what we

1599
00:49:31,025 --> 00:49:32,330
0,150 150,360 360,645 645,990 990,1305
could do maybe is using

1600
00:49:32,330 --> 00:49:34,355
0,210 210,500 550,1215 1215,1580 1660,2025
our eyes naively scan over

1601
00:49:34,355 --> 00:49:36,185
0,255 255,545 835,1275 1275,1440 1440,1830
this image, pixel by pixel,

1602
00:49:36,185 --> 00:49:37,565
0,255 255,540 540,845 865,1200 1200,1380
right, just going across the

1603
00:49:37,565 --> 00:49:38,360
0,245
image.|
|

1604
00:49:39,360 --> 00:49:41,890
0,380 380,680 680,1000 1740,2135 2135,2530
However, our brains maybe, maybe
然而，我们的大脑可能，也许在内部进行某种类型的计算，就像这样，但你和我，我们可以简单地看着这张图像，并能够关注重要的部分。

1605
00:49:41,970 --> 00:49:43,370
0,470 470,710 710,905 905,1175 1175,1400
internally they're doing some type

1606
00:49:43,370 --> 00:49:45,185
0,195 195,690 690,915 915,1220 1540,1815
of computation like this, but

1607
00:49:45,185 --> 00:49:45,890
0,135 135,240 240,390 390,555 555,705
you and I, we can

1608
00:49:45,890 --> 00:49:46,990
0,270 270,525 525,660 660,810 810,1100
simply look at this image

1609
00:49:47,280 --> 00:49:48,770
0,380 380,650 650,905 905,1205 1205,1490
and be able to attend

1610
00:49:48,770 --> 00:49:50,520
0,195 195,390 390,720 720,1100
to the important parts.|
|

1611
00:49:50,590 --> 00:49:51,660
0,260 260,425 425,635 635,815 815,1070
We can see that it's
我们可以看到钢铁侠在图像中向你走来，然后我们可以更进一步地关注，好的，关于钢铁侠的哪些细节可能是重要的。

1612
00:49:51,660 --> 00:49:52,920
0,225 225,525 525,810 810,1035 1035,1260
iron man coming at you

1613
00:49:52,920 --> 00:49:54,525
0,240 240,450 450,570 570,800 1300,1605
right in the image and

1614
00:49:54,525 --> 00:49:55,500
0,210 210,375 375,525 525,765 765,975
then we can focus in

1615
00:49:55,500 --> 00:49:56,490
0,120 120,315 315,585 585,795 795,990
a little further and say,

1616
00:49:56,490 --> 00:49:57,525
0,225 225,405 405,540 540,750 750,1035
okay, what are the details

1617
00:49:57,525 --> 00:49:58,650
0,225 225,450 450,735 735,960 960,1125
about iron man that may

1618
00:49:58,650 --> 00:49:59,940
0,210 210,530
be important.|
|

1619
00:50:00,170 --> 00:50:01,855
0,260 260,470 470,820 1170,1460 1460,1685
What is key? What you're
什么是关键？你所做的是，你的大脑正在识别哪些部分正在关注，哪些部分需要关注，然后提取那些值得最高关注的特征。

1620
00:50:01,855 --> 00:50:03,325
0,240 240,570 570,825 825,1110 1110,1470
doing is your brain is

1621
00:50:03,325 --> 00:50:05,620
0,660 660,900 900,1095 1095,1385 1855,2295
identifying which parts are attending

1622
00:50:05,620 --> 00:50:07,735
0,290 1000,1350 1350,1620 1620,1890 1890,2115
to, to attend to, and

1623
00:50:07,735 --> 00:50:09,900
0,275 295,825 825,1050 1050,1385 1765,2165
then extracting those features that

1624
00:50:10,280 --> 00:50:12,580
0,400 510,800 800,1090 1140,1540
deserve the highest attention.|
|

1625
00:50:13,060 --> 00:50:14,055
0,290 290,515 515,710 710,830 830,995
The first part of this
这个问题的第一部分确实是最有趣和最具挑战性的。

1626
00:50:14,055 --> 00:50:15,740
0,305 325,645 645,965 1165,1425 1425,1685
problem is really the most

1627
00:50:16,360 --> 00:50:18,060
0,290 290,485 485,790 810,1210
interesting and challenging one.|
|

1628
00:50:18,060 --> 00:50:20,115
0,380 550,945 945,1230 1230,1610 1690,2055
And it's very similar to
它与搜索的概念非常相似。实际上，这就是搜索正在做的事情，获取一些更大的信息主体，并试图提取和识别重要的部分。

1629
00:50:20,115 --> 00:50:22,160
0,255 255,540 540,855 855,1175 1645,2045
the concept of search. {Effectively,,that's

1630
00:50:22,210 --> 00:50:23,420
0,395 395,545 545,740 740,920 920,1210
-} what search is doing,

1631
00:50:23,800 --> 00:50:26,240
0,395 395,755 755,1120 1710,2075 2075,2440
taking some larger body of

1632
00:50:26,260 --> 00:50:27,890
0,380 380,695 695,965 965,1265 1265,1630
information and trying to extract

1633
00:50:28,180 --> 00:50:30,020
0,400 600,920 920,1160 1160,1460 1460,1840
and identify the important parts.|
|

1634
00:50:30,810 --> 00:50:31,595
0,275 275,380 380,485 485,635 635,785
So {let's -} go {there,next.
接下来我们就去那里吧。搜索是如何工作的？

1635
00:50:31,595 --> 00:50:33,070
0,275 445,735 735,915 915,1140 1140,1475
-} How does search work?|
|

1636
00:50:33,670 --> 00:50:34,800
0,350 350,560 560,830 830,935 935,1130
You're thinking you're in this
你认为你是这个班的学生。我如何才能了解更多关于神经网络的知识？嗯，在这个时代，除了来到这里加入我们之外，你可能会做的一件事就是上网，把所有的视频都放在网上，试图找到匹配的东西，进行搜索操作。所以你有一个像YouTube这样的巨型数据库，你想找到一段视频。

1637
00:50:34,800 --> 00:50:36,000
0,320 460,735 735,855 855,990 990,1200
class. {How,can -} I learn

1638
00:50:36,000 --> 00:50:37,730
0,240 240,450 450,690 690,950 1330,1730
more about neural networks? Well,

1639
00:50:38,080 --> 00:50:38,960
0,275 275,425 425,545 545,635 635,880
in this day and age,

1640
00:50:38,980 --> 00:50:40,100
0,305 305,500 500,665 665,830 830,1120
one thing you may do

1641
00:50:40,270 --> 00:50:41,415
0,335 335,590 590,770 770,920 920,1145
besides coming here and joining

1642
00:50:41,415 --> 00:50:42,870
0,335 505,810 810,1065 1065,1245 1245,1455
us is going to the

1643
00:50:42,870 --> 00:50:44,570
0,380 730,1065 1065,1305 1305,1455 1455,1700
Internet, having all the videos

1644
00:50:44,590 --> 00:50:45,840
0,305 305,590 590,875 875,1040 1040,1250
out there, trying to find

1645
00:50:45,840 --> 00:50:47,835
0,350 430,720 720,1010 1510,1815 1815,1995
something that matches, doing a

1646
00:50:47,835 --> 00:50:50,085
0,270 270,665 1495,1860 1860,2100 2100,2250
search {operation.,So -} you have

1647
00:50:50,085 --> 00:50:51,620
0,135 135,330 330,665 895,1215 1215,1535
a giant database like Youtube,

1648
00:50:51,700 --> 00:50:52,575
0,275 275,455 455,605 605,740 740,875
you want to find a

1649
00:50:52,575 --> 00:50:53,520
0,245
video.|
|

1650
00:50:53,700 --> 00:50:55,360
0,290 290,545 545,910 990,1310 1310,1660
You enter in your query,
你输入你的问题，深度学习。

1651
00:50:55,560 --> 00:50:57,020
0,305 305,610
deep learning.|
|

1652
00:50:57,560 --> 00:50:59,170
0,395 395,790 810,1100 1100,1340 1340,1610
And what comes out are
结果是一些可能的结果，对吗？

1653
00:50:59,170 --> 00:51:01,780
0,270 270,620 1060,1580 1870,2270
some possible outputs, right?|
|

1654
00:51:02,200 --> 00:51:03,390
0,275 275,500 500,800 800,1040 1040,1190
For every video in the
对于数据库中的每个视频，都会有一些与该视频相关的关键信息，比如标题。

1655
00:51:03,390 --> 00:51:04,965
0,260 850,1110 1110,1260 1260,1440 1440,1575
database, there is going to

1656
00:51:04,965 --> 00:51:06,600
0,150 150,420 420,785 835,1235 1285,1635
be some key information related

1657
00:51:06,600 --> 00:51:08,750
0,350 1150,1530 1530,1740 1740,1860 1860,2150
to that to that video,

1658
00:51:09,160 --> 00:51:10,780
0,335 335,455 455,620 620,910
let's say the title.|
|

1659
00:51:11,160 --> 00:51:12,700
0,400 540,815 815,980 980,1205 1205,1540
Now to do the search,
现在要进行搜索，任务是找到您的查询和这些标题之间的重叠部分，对吗？数据库中的密钥。

1660
00:51:13,710 --> 00:51:15,760
0,400 660,950 950,1220 1220,1600 1650,2050
what the task is to

1661
00:51:16,140 --> 00:51:18,275
0,350 350,560 560,1150 1590,1895 1895,2135
find the overlaps between your

1662
00:51:18,275 --> 00:51:19,955
0,365 745,1080 1080,1305 1305,1470 1470,1680
query and each of these

1663
00:51:19,955 --> 00:51:21,935
0,480 480,825 825,1125 1125,1445 1675,1980
titles right? The keys in

1664
00:51:21,935 --> 00:51:23,160
0,165 165,425
the database.|
|

1665
00:51:23,230 --> 00:51:24,360
0,290 290,455 455,620 620,770 770,1130
What we want to compute
我们要计算的是查询和这些键之间的相似性和相关性的度量。

1666
00:51:24,360 --> 00:51:26,025
0,150 150,285 285,690 690,960 960,1665
is a metric of similarity

1667
00:51:26,025 --> 00:51:28,160
0,195 195,815 1315,1620 1620,1815 1815,2135
and relevance between the query

1668
00:51:28,570 --> 00:51:30,220
0,335 335,620 620,970
and these keys.|
|

1669
00:51:30,520 --> 00:51:32,220
0,335 335,650 650,950 950,1270 1410,1700
How similar are they to
它们与我们所需的查询有多相似？

1670
00:51:32,220 --> 00:51:33,800
0,195 195,570 570,950
our desired query?|
|

1671
00:51:33,840 --> 00:51:34,730
0,275 275,410 410,545 545,695 695,890
And we can do this
我们可以一步一步地做到这一点，让我们来看一段关于优雅的巨型海龟的视频的第一个选项。与我们对深度学习的质疑不太相似，深度学习是我们的第二个选择。

1672
00:51:34,730 --> 00:51:36,485
0,210 210,435 435,770 1270,1635 1635,1755
step by step, let's say

1673
00:51:36,485 --> 00:51:38,290
0,225 225,575 595,995 1105,1455 1455,1805
this first option of a

1674
00:51:38,370 --> 00:51:40,355
0,400 420,820 960,1220 1220,1685 1685,1985
video about the elegant giant

1675
00:51:40,355 --> 00:51:42,125
0,255 255,665 955,1260 1260,1470 1470,1770
sea turtles. {Not,that -} similar

1676
00:51:42,125 --> 00:51:43,220
0,240 240,390 390,660 660,900 900,1095
to our query about deep

1677
00:51:43,220 --> 00:51:46,140
0,290 1150,1455 1455,1710 1710,2060
learning, our second option.|
|

1678
00:51:46,150 --> 00:51:48,045
0,400 540,800 800,965 965,1270 1590,1895
Introduction to deep learning, the
深度学习入门，这堂课的第一堂入门课？是的，非常相关。第三个选项，一段关于已故伟大的科比的视频。并不是很相关。

1679
00:51:48,045 --> 00:51:49,455
0,255 255,870 870,1050 1050,1230 1230,1410
first introductory lecture on this

1680
00:51:49,455 --> 00:51:52,770
0,305 505,905 1075,1440 1440,1805 3025,3315
class? Yes, highly relevant. {The,third

1681
00:51:52,770 --> 00:51:54,450
0,195 195,500 940,1200 1200,1425 1425,1680
-} option, a video about

1682
00:51:54,450 --> 00:51:55,920
0,150 150,300 300,480 480,770 1090,1470
the late and great Kobe

1683
00:51:55,920 --> 00:51:58,020
0,470 550,855 855,1050 1050,1340
{Bryant.,Not -} that relevant.|
|

1684
00:51:58,020 --> 00:51:59,430
0,240 240,525 525,900 900,1200 1200,1410
The key operation here is
这里的关键操作是进行相似性计算，将查询和关键字结合在一起。

1685
00:51:59,430 --> 00:52:00,780
0,180 180,330 330,480 480,690 690,1350
that there is this similarity

1686
00:52:00,780 --> 00:52:02,985
0,590 1120,1485 1485,1740 1740,1995 1995,2205
computation bringing the query and

1687
00:52:02,985 --> 00:52:04,640
0,150 150,425 505,905
the key together.|
|

1688
00:52:05,000 --> 00:52:06,775
0,290 290,575 575,970 1260,1565 1565,1775
The final step is now
最后一步是我们已经确定了哪些关键是相关的，提取相关信息，我们想要关注的是什么，那就是视频本身。我们称这为价值，因为搜索执行得很好，我们已经成功地确定了您将想要关注的关于深度学习的相关视频。

1689
00:52:06,775 --> 00:52:08,290
0,180 180,545 715,1050 1050,1320 1320,1515
that we've identified what key

1690
00:52:08,290 --> 00:52:10,710
0,150 150,440 1390,1995 1995,2160 2160,2420
is relevant, extracting the relevant

1691
00:52:11,030 --> 00:52:12,205
0,395 395,695 695,860 860,1010 1010,1175
information, what we want to

1692
00:52:12,205 --> 00:52:13,885
0,210 210,540 540,935 1105,1395 1395,1680
pay attention to, and that's

1693
00:52:13,885 --> 00:52:15,640
0,90 90,335 385,785 1225,1530 1530,1755
the video itself. {We,call -}

1694
00:52:15,640 --> 00:52:17,425
0,210 210,360 360,620 1150,1515 1515,1785
this the value, and because

1695
00:52:17,425 --> 00:52:19,290
0,180 180,375 375,695 1045,1445 1465,1865
the search is implemented well,

1696
00:52:19,610 --> 00:52:21,720
0,455 455,790 1380,1685 1685,1850 1850,2110
we've successfully identified the relevant

1697
00:52:21,740 --> 00:52:23,335
0,365 365,650 650,860 860,1150 1290,1595
video on deep learning that

1698
00:52:23,335 --> 00:52:24,055
0,165 165,270 270,405 405,555 555,720
you are going to want

1699
00:52:24,055 --> 00:52:26,100
0,270 270,525 525,815 865,1265
to pay attention to.|
|

1700
00:52:26,140 --> 00:52:27,765
0,260 260,500 500,790 960,1340 1340,1625
And it's this, this idea,
正是这种，这种想法，这种直觉，提出一个问题，试图找到相似之处，试图提取相关的价值观，形成自我关注的基础。

1701
00:52:27,765 --> 00:52:29,850
0,180 180,695 775,1140 1140,1505 1795,2085
this intuition of giving a

1702
00:52:29,850 --> 00:52:31,730
0,320 430,750 750,930 930,1125 1125,1880
query, trying to find similarity,

1703
00:52:31,750 --> 00:52:33,150
0,335 335,620 620,890 890,1130 1130,1400
trying to extract the related

1704
00:52:33,150 --> 00:52:35,000
0,350 790,1125 1125,1395 1395,1590 1590,1850
values that form the basis

1705
00:52:35,050 --> 00:52:36,720
0,305 305,590 590,970
of self attention.|
|

1706
00:52:37,040 --> 00:52:38,260
0,305 305,485 485,635 635,875 875,1220
And how it works in
以及它如何在像变压器这样的神经网络中工作。

1707
00:52:38,260 --> 00:52:40,520
0,315 315,585 585,980 1060,1610
neural networks like transformers.|
|

1708
00:52:40,520 --> 00:52:42,635
0,350 400,660 660,920 1510,1920 1920,2115
So to go concretely into
具体来说，让我们回到我们的课文，我们的语言例子。

1709
00:52:42,635 --> 00:52:44,410
0,285 285,635 1015,1380 1380,1500 1500,1775
this, right, let's go back

1710
00:52:44,430 --> 00:52:45,830
0,365 365,605 605,800 800,1115 1115,1400
now to our text, our

1711
00:52:45,830 --> 00:52:47,300
0,290 400,800
language example.|
|

1712
00:52:47,770 --> 00:52:50,210
0,290 290,455 455,730 1800,2120 2120,2440
With the sentence, our goal
对于句子，我们的目标是识别和注意输入中与句子语义相关的特征。

1713
00:52:50,260 --> 00:52:52,005
0,290 290,580 780,1115 1115,1430 1430,1745
is to identify and attend

1714
00:52:52,005 --> 00:52:53,600
0,240 240,545 595,900 900,1200 1200,1595
to features in this input

1715
00:52:53,830 --> 00:52:55,545
0,260 260,500 500,880 1260,1550 1550,1715
that are relevant to the

1716
00:52:55,545 --> 00:52:57,140
0,495 495,785 835,1125 1125,1305 1305,1595
semantic meaning of the sentence.|
|

1717
00:52:58,670 --> 00:53:00,865
0,400 870,1205 1205,1540 1590,1910 1910,2195
Now, first step, we have
现在，第一步，我们有顺序了。我们有秩序。我们已经消除了复发，对吧？我们一下子把所有的时间步调都输入进去。

1718
00:53:00,865 --> 00:53:03,070
0,365 595,885 885,1110 1110,1445 1735,2205
sequence. {We,have -} order. We've

1719
00:53:03,070 --> 00:53:05,275
0,555 555,1140 1140,1460 1510,1890 1890,2205
eliminated recurrence, right? We're feeding

1720
00:53:05,275 --> 00:53:06,730
0,275 415,720 720,900 900,1125 1125,1455
in all the time steps

1721
00:53:06,730 --> 00:53:08,080
0,255 255,420 420,710
all at once.|
|

1722
00:53:08,080 --> 00:53:09,055
0,210 210,435 435,600 600,735 735,975
We still need a way
我们仍然需要一种方法来编码和捕获关于顺序和位置依赖的信息。

1723
00:53:09,055 --> 00:53:10,980
0,210 210,855 855,1170 1170,1505 1525,1925
to encode and capture this

1724
00:53:11,060 --> 00:53:12,870
0,380 380,680 680,1000 1230,1520 1520,1810
information about order and this

1725
00:53:12,920 --> 00:53:14,600
0,545 545,1060
positional dependence.|
|

1726
00:53:14,850 --> 00:53:16,070
0,290 290,440 440,590 590,880 930,1220
How this is done is
如何做到这一点是位置编码的想法，它捕获了序列中存在的一些固有顺序信息。我只想简单地谈一下这一点，但这个想法与我之前介绍过的嵌入的想法有关。

1727
00:53:16,070 --> 00:53:19,120
0,285 285,600 600,920 1810,2325 2325,3050
this idea of positional encoding,

1728
00:53:19,380 --> 00:53:21,940
0,350 350,845 845,1150 1590,2180 2180,2560
which captures some inherent order

1729
00:53:22,140 --> 00:53:24,010
0,400 720,1120 1140,1430 1430,1595 1595,1870
information present in the sequence.

1730
00:53:24,570 --> 00:53:25,445
0,350 350,455 455,605 605,740 740,875
I'm just going to touch

1731
00:53:25,445 --> 00:53:27,185
0,150 150,375 375,660 660,995 1465,1740
on this very briefly, but

1732
00:53:27,185 --> 00:53:28,640
0,240 240,570 570,915 915,1245 1245,1455
the idea is related to

1733
00:53:28,640 --> 00:53:30,050
0,240 240,495 495,720 720,1185 1185,1410
this idea of embeddings, which

1734
00:53:30,050 --> 00:53:31,960
0,240 240,540 540,860
I introduced earlier.|
|

1735
00:53:32,060 --> 00:53:33,445
0,290 290,470 470,760 900,1205 1205,1385
What is done is a
所做的是使用神经网络层来编码位置信息，该位置信息根据本文中的顺序捕获相对关系。

1736
00:53:33,445 --> 00:53:34,980
0,270 270,540 540,900 900,1200 1200,1535
neural network layer is used

1737
00:53:35,270 --> 00:53:37,855
0,260 260,905 905,1480 1500,1900 2250,2585
to encode positional information that

1738
00:53:37,855 --> 00:53:41,185
0,635 715,1005 1005,1295 1855,2255 3025,3330
captures the relative relationships in

1739
00:53:41,185 --> 00:53:43,195
0,180 180,375 375,695 1105,1505 1675,2010
terms of order within within

1740
00:53:43,195 --> 00:53:44,300
0,255 255,575
this text.|
|

1741
00:53:45,440 --> 00:53:46,590
0,365 365,500 500,635 635,830 830,1150
That the high level concept,
这是高层次的概念，对吗？

1742
00:53:46,880 --> 00:53:48,100
0,400
right?|
|

1743
00:53:48,100 --> 00:53:49,350
0,225 225,390 390,600 600,885 885,1250
We're still being able to
我们仍然能够一次处理所有这些时间步长。没有时间、步骤或其他的概念。数据是单一的，但我们仍然了解到这种捕获位置顺序信息的编码。现在，我们的下一步是采用这种编码，并找出要注意的内容，就像我在YouTube示例中介绍的搜索操作一样。

1744
00:53:49,400 --> 00:53:51,070
0,395 395,710 710,965 965,1300 1380,1670
process these time steps all

1745
00:53:51,070 --> 00:53:52,375
0,180 180,470 730,990 990,1140 1140,1305
at once. {There,is -} no

1746
00:53:52,375 --> 00:53:53,200
0,180 180,360 360,540 540,720 720,825
notion of time, step or

1747
00:53:53,200 --> 00:53:54,510
0,195 195,420 420,630 630,870 870,1310
{other.,The -} data is singular,

1748
00:53:54,890 --> 00:53:56,560
0,335 335,670 750,1130 1130,1445 1445,1670
but still we learn this

1749
00:53:56,560 --> 00:53:58,830
0,525 525,780 780,1395 1395,1725 1725,2270
encoding that captures the positional

1750
00:53:58,880 --> 00:54:02,185
0,400 630,1030 2220,2620 2760,3065 3065,3305
{order,information. -} Now our next

1751
00:54:02,185 --> 00:54:03,175
0,255 255,465 465,645 645,825 825,990
step is to take this

1752
00:54:03,175 --> 00:54:04,855
0,495 495,705 705,945 945,1265 1375,1680
encoding and figure out what

1753
00:54:04,855 --> 00:54:06,670
0,225 225,465 465,785 1075,1475 1495,1815
to attend to exactly like

1754
00:54:06,670 --> 00:54:08,365
0,255 255,590 640,1040 1210,1455 1455,1695
that search operation that I

1755
00:54:08,365 --> 00:54:09,780
0,330 330,540 540,675 675,935 1015,1415
introduced with the Youtube example.|
|

1756
00:54:10,630 --> 00:54:12,390
0,500 500,650 650,970 1110,1610 1610,1760
Extracting a query, extracting a
提取查询、提取关键字、提取值并将它们彼此关联。

1757
00:54:12,390 --> 00:54:14,130
0,290 640,1110 1110,1215 1215,1455 1455,1740
key, extracting a value and

1758
00:54:14,130 --> 00:54:15,260
0,390 390,645 645,780 780,870 870,1130
relating them to each other.|
|

1759
00:54:15,960 --> 00:54:17,645
0,400 690,965 965,1160 1160,1430 1430,1685
So we use neural network
因此，我们使用神经网络层来准确地实现这一点。

1760
00:54:17,645 --> 00:54:19,270
0,450 450,615 615,875 895,1260 1260,1625
layers to do exactly this.|
|

1761
00:54:19,900 --> 00:54:22,345
0,240 240,525 525,930 930,1520 2080,2445
Given this positional encoding, what
对于这种位置编码，需要注意的是应用一个神经网络层，首先对其进行转换，生成查询。

1762
00:54:22,345 --> 00:54:24,535
0,315 315,660 660,1055 1075,1505 1915,2190
attention does is applies a

1763
00:54:24,535 --> 00:54:27,090
0,210 210,480 480,875 1855,2280 2280,2555
neural network layer, transforming that

1764
00:54:27,410 --> 00:54:30,140
0,350 350,910 1080,1355 1355,1660
first, generating the query.|
|

1765
00:54:30,420 --> 00:54:32,150
0,290 290,455 455,730 930,1330 1380,1730
We do this again using
我们使用单独的神经网络层再次这样做，这是一组不同的权重，一组不同的参数，然后以不同的方式转换位置嵌入，生成第二个输出，即密钥。

1766
00:54:32,150 --> 00:54:33,820
0,270 270,570 570,900 900,1160 1270,1670
a separate neural network layer

1767
00:54:34,230 --> 00:54:35,240
0,320 320,515 515,635 635,755 755,1010
and this is a different

1768
00:54:35,240 --> 00:54:36,275
0,240 240,375 375,630 630,780 780,1035
set of weights, a different

1769
00:54:36,275 --> 00:54:37,990
0,210 210,345 345,875 1165,1440 1440,1715
set of parameters that then

1770
00:54:38,160 --> 00:54:40,115
0,335 335,635 635,1115 1115,1570 1710,1955
transform that positional embedding in

1771
00:54:40,115 --> 00:54:42,740
0,105 105,300 300,635 1075,1655 2335,2625
a different way, generating a

1772
00:54:42,740 --> 00:54:45,340
0,290 1120,1425 1425,1635 1635,1940
second output, the key.|
|

1773
00:54:45,340 --> 00:54:47,680
0,270 270,585 585,980 1690,2040 2040,2340
And finally, this operation is
最后，对第三层重复该操作，第三组权重生成值。

1774
00:54:47,680 --> 00:54:49,740
0,350 550,825 825,1020 1020,1340 1660,2060
repeated with a third layer,

1775
00:54:49,880 --> 00:54:51,180
0,305 305,560 560,755 755,875 875,1300
a third set of weights

1776
00:54:51,260 --> 00:54:53,060
0,605 605,845 845,1090
generating the value.|
|

1777
00:54:53,260 --> 00:54:54,645
0,400 510,800 800,1040 1040,1235 1235,1385
Now, with these three in
现在，有了这三个关键字、查询、关键字和值，我们可以将它们相互比较，试图找出网络应该在自我输入中关注什么是重要的。

1778
00:54:54,645 --> 00:54:56,760
0,305 325,645 645,965 1405,1770 1770,2115
hand, the key, the query,

1779
00:54:56,760 --> 00:54:57,710
0,255 255,435 435,570 570,690 690,950
the key and the value,

1780
00:54:58,120 --> 00:54:59,475
0,260 260,520 720,1040 1040,1235 1235,1355
we can compare them to

1781
00:54:59,475 --> 00:55:00,780
0,120 120,395 595,900 900,1125 1125,1305
each other to try to

1782
00:55:00,780 --> 00:55:02,595
0,165 165,470 790,1190 1240,1545 1545,1815
figure out where in that

1783
00:55:02,595 --> 00:55:04,590
0,365 505,905 1075,1335 1335,1595 1645,1995
self input the network should

1784
00:55:04,590 --> 00:55:05,990
0,270 270,555 555,810 810,1050 1050,1400
attend to what is important.|
|

1785
00:55:07,040 --> 00:55:08,460
0,290 290,620 620,755 755,1025 1025,1420
And that's the key idea
这就是这个相似性度量背后的关键思想，或者说你可以认为是注意力得分。

1786
00:55:08,690 --> 00:55:11,005
0,335 335,650 650,1340 1340,1810 2010,2315
behind this similarity metric, or

1787
00:55:11,005 --> 00:55:11,845
0,180 180,315 315,465 465,630 630,840
what you can think of

1788
00:55:11,845 --> 00:55:13,760
0,240 240,495 495,845 895,1295
as an attention score.|
|

1789
00:55:13,760 --> 00:55:15,095
0,255 255,495 495,720 720,990 990,1335
What we're doing is we're
我们所做的是计算查询和键之间的相似性分数。

1790
00:55:15,095 --> 00:55:17,140
0,390 390,600 600,1260 1260,1565 1645,2045
computing a similarity score between

1791
00:55:17,250 --> 00:55:18,970
0,290 290,610 870,1205 1205,1430 1430,1720
a query and the key.|
|

1792
00:55:19,690 --> 00:55:21,300
0,350 350,700 780,1055 1055,1295 1295,1610
And remember that these query
请记住，这些查询值和键值只是数字的数组。我们可以将它们定义为数字数组，您可以将其视为空间中的矢量。

1793
00:55:21,300 --> 00:55:23,910
0,320 820,1220 1510,1910 2080,2355 2355,2610
and key values are just

1794
00:55:23,910 --> 00:55:25,635
0,435 435,600 600,890 1270,1530 1530,1725
arrays of numbers. {We,can -}

1795
00:55:25,635 --> 00:55:27,195
0,240 240,545 715,1095 1095,1425 1425,1560
define them as arrays of

1796
00:55:27,195 --> 00:55:28,680
0,275 745,1035 1035,1185 1185,1320 1320,1485
numbers, which you can think

1797
00:55:28,680 --> 00:55:31,040
0,290 400,800 910,1550 1720,2040 2040,2360
of as vectors in space.|
|

1798
00:55:31,950 --> 00:55:33,770
0,290 290,610 960,1235 1235,1490 1490,1820
The query, the query values
这个查询，查询值是不是一些向量的关键？

1799
00:55:33,770 --> 00:55:35,590
0,285 285,480 480,830 1180,1500 1500,1820
are some vector the key?|
|

1800
00:55:36,600 --> 00:55:37,710
0,240 240,420 420,675 675,930 930,1110
The key values are some
关键字值是其他一些向量。

1801
00:55:37,710 --> 00:55:38,980
0,225 225,590
other vector.|
|

1802
00:55:38,980 --> 00:55:40,630
0,180 180,710 1030,1305 1305,1485 1485,1650
And mathematically, the way that
在数学上，我们可以比较这两个向量，以了解它们有多相似，是通过点积和比例来实现的。它捕捉到这些向量有多相似，它们是否指向同一个方向，对吧。

1803
00:55:40,630 --> 00:55:41,710
0,135 135,390 390,675 675,885 885,1080
we can compare these two

1804
00:55:41,710 --> 00:55:43,675
0,420 420,740 760,1160 1300,1635 1635,1965
vectors to understand how similar

1805
00:55:43,675 --> 00:55:45,445
0,255 255,515 835,1125 1125,1395 1395,1770
they are is by taking

1806
00:55:45,445 --> 00:55:47,635
0,270 270,480 480,815 1465,1800 1800,2190
the dot product and scaling.

1807
00:55:47,635 --> 00:55:50,140
0,275 415,975 975,1230 1230,1565 2215,2505
{It,captures -} how similar these

1808
00:55:50,140 --> 00:55:52,030
0,345 345,540 540,860 1420,1725 1725,1890
vectors are, how whether or

1809
00:55:52,030 --> 00:55:53,095
0,165 165,405 405,675 675,930 930,1065
not they're pointing in the

1810
00:55:53,095 --> 00:55:54,680
0,225 225,570 570,965
same direction, right.|
|

1811
00:55:55,720 --> 00:55:57,615
0,350 350,665 665,935 935,1490 1490,1895
This is the similarity metric,
这是相似性度量，如果你熟悉一点线性代数，这也被称为余弦相似性。

1812
00:55:57,615 --> 00:55:59,160
0,335 715,1005 1005,1140 1140,1275 1275,1545
and if you are familiar

1813
00:55:59,160 --> 00:56:00,110
0,225 225,315 315,450 450,645 645,950
with a little bit of

1814
00:56:00,160 --> 00:56:01,800
0,410 410,920 920,1100 1100,1355 1355,1640
linear algebra, this is also

1815
00:56:01,800 --> 00:56:03,710
0,240 240,540 540,765 765,1170 1170,1910
known as the cosine similarity.|
|

1816
00:56:04,160 --> 00:56:06,395
0,350 370,770 1030,1430 1600,1980 1980,2235
The operation functions exactly the
该运算对矩阵的作用与此完全相同。如果我们将这个点积运算应用于键矩阵、键矩阵中的查询，我们就得到了这个相似性度量。

1817
00:56:06,395 --> 00:56:08,735
0,195 195,495 495,875 925,1685 2035,2340
same way for matrices. {If,we

1818
00:56:08,735 --> 00:56:10,210
0,305 325,645 645,900 900,1155 1155,1475
-} apply this dot product

1819
00:56:10,350 --> 00:56:12,125
0,400 780,1025 1025,1205 1205,1505 1505,1775
operation to our query in

1820
00:56:12,125 --> 00:56:14,885
0,210 210,845 925,1325 1465,2225 2455,2760
key matrices, key matrices, we

1821
00:56:14,885 --> 00:56:16,630
0,195 195,390 390,960 960,1395 1395,1745
get this similarity metric out.|
|

1822
00:56:18,130 --> 00:56:19,240
0,400
Now.|
现在。|

1823
00:56:19,280 --> 00:56:20,670
0,275 275,485 485,755 755,1040 1040,1390
This is very, very key
在定义我们的下一步时，这是非常非常关键的，根据网络在这一输入中应该实际关注的内容来计算注意力权重。

1824
00:56:20,810 --> 00:56:22,650
0,400 570,1070 1070,1250 1250,1490 1490,1840
in defining our next step,

1825
00:56:23,330 --> 00:56:25,645
0,500 500,850 930,1325 1325,1900 2010,2315
computing the attention weighting in

1826
00:56:25,645 --> 00:56:26,860
0,195 195,485 535,825 825,975 975,1215
terms of what the network

1827
00:56:26,860 --> 00:56:28,465
0,375 375,720 720,990 990,1290 1290,1605
should actually attend to within

1828
00:56:28,465 --> 00:56:29,960
0,330 330,725
this input.|
|

1829
00:56:30,460 --> 00:56:31,800
0,395 395,770 770,1025 1025,1160 1160,1340
This operation gives us a
这个操作给了我们一个定义如何操作的分数。

1830
00:56:31,800 --> 00:56:35,440
0,320 910,1230 1230,1730 2260,2660
score which defines how.|
|

1831
00:56:35,670 --> 00:56:38,135
0,400 1170,1570 1650,2030 2030,2270 2270,2465
How the components of the
输入数据的组件如何相互关联。

1832
00:56:38,135 --> 00:56:40,175
0,335 355,755 985,1385 1495,1845 1845,2040
input data are related to

1833
00:56:40,175 --> 00:56:41,160
0,120 120,395
each other.|
|

1834
00:56:41,650 --> 00:56:43,220
0,350 350,620 620,815 815,1090 1170,1570
So, given a sentence, right
因此，给出一个句子，当我们计算这个相似性得分度量时，我们就可以开始考虑权重，这些权重定义了序列数据的组成部分之间的关系。

1835
00:56:43,330 --> 00:56:45,600
0,290 290,455 455,860 860,1180 1560,2270
when we compute this similarity

1836
00:56:45,600 --> 00:56:47,565
0,315 315,950 1300,1560 1560,1710 1710,1965
score metric, we can then

1837
00:56:47,565 --> 00:56:49,815
0,315 315,615 615,900 900,1235 1645,2250
begin to think of weights

1838
00:56:49,815 --> 00:56:52,430
0,375 375,690 690,960 960,1295 2215,2615
that define the relationship between

1839
00:56:52,450 --> 00:56:54,480
0,290 290,910 1110,1460 1460,1790 1790,2030
the sequential, the components of

1840
00:56:54,480 --> 00:56:55,770
0,135 135,585 585,860 880,1140 1140,1290
the sequential data to each

1841
00:56:55,770 --> 00:56:56,620
0,290
other.|
|

1842
00:56:56,750 --> 00:56:58,690
0,290 290,470 470,760 870,1270 1590,1940
So, for example, in this
例如，在这个带有文本句子的例子中，他抛出了网球发球。

1843
00:56:58,690 --> 00:57:00,510
0,315 315,615 615,855 855,1160 1420,1820
example with a text sentence,

1844
00:57:01,010 --> 00:57:02,395
0,320 320,680 680,815 815,1040 1040,1385
he tossed the tennis ball

1845
00:57:02,395 --> 00:57:03,860
0,285 285,575
to serve.|
|

1846
00:57:04,080 --> 00:57:05,510
0,365 365,680 680,920 920,1145 1145,1430
The goal with the score
得分的目标是，序列中相互关联的单词应该具有较高的关注度，与网球相关的掷球相关。

1847
00:57:05,510 --> 00:57:07,205
0,225 225,500 610,1010 1210,1500 1500,1695
is that words in the

1848
00:57:07,205 --> 00:57:08,435
0,300 300,540 540,720 720,1020 1020,1230
sequence that are related to

1849
00:57:08,435 --> 00:57:09,970
0,120 120,395 625,915 915,1170 1170,1535
each other should have high

1850
00:57:10,050 --> 00:57:12,605
0,380 380,910 1200,1600 1920,2300 2300,2555
attention weights ball related to

1851
00:57:12,605 --> 00:57:14,860
0,455 685,1035 1035,1245 1245,1505
toss related to tennis.|
|

1852
00:57:14,970 --> 00:57:17,300
0,335 335,670 750,1340 1340,1720 2010,2330
And this metric itself is
而这一指标本身就是我们正在等待的关注。

1853
00:57:17,300 --> 00:57:19,060
0,285 285,630 630,1010
our attention waiting.|
|

1854
00:57:19,060 --> 00:57:19,960
0,150 150,255 255,375 375,600 600,900
What we have done is
我们所做的是通过软最大函数传递相似性分数，它所做的就是将这些值限制在0到1之间，因此您可以将这些值视为相对关注度的相对分数。

1855
00:57:19,960 --> 00:57:22,690
0,350 610,1010 1150,1920 1920,2240 2410,2730
passed that similarity score through

1856
00:57:22,690 --> 00:57:25,615
0,225 225,480 480,830 1840,2240 2620,2925
a soft Max function, which

1857
00:57:25,615 --> 00:57:26,875
0,195 195,360 360,600 600,915 915,1260
all it does is it

1858
00:57:26,875 --> 00:57:28,495
0,585 585,855 855,1170 1170,1410 1410,1620
constrains those values to be

1859
00:57:28,495 --> 00:57:30,250
0,365 385,720 720,945 945,1235 1465,1755
between zero and one, and

1860
00:57:30,250 --> 00:57:31,045
0,165 165,300 300,450 450,630 630,795
so you can think of

1861
00:57:31,045 --> 00:57:33,040
0,195 195,450 450,785 955,1355 1615,1995
these as relative scores of

1862
00:57:33,040 --> 00:57:34,900
0,380 520,900 900,1430
relative attention weights.|
|

1863
00:57:35,880 --> 00:57:37,790
0,400 1080,1385 1385,1565 1565,1730 1730,1910
Finally, now that we have
最后，现在我们有了这个指标，就可以了。

1864
00:57:37,790 --> 00:57:39,560
0,195 195,645 645,870 870,1160
this metric, that can.|
|

1865
00:57:39,930 --> 00:57:42,245
0,530 530,725 725,995 995,1360 1560,2315
Captures this notion of similarity
捕捉到了这种相似性的概念和这些内在的自我关系。

1866
00:57:42,245 --> 00:57:44,260
0,165 165,455 775,1175 1195,1595 1615,2015
and these internal self relationships.|
|

1867
00:57:45,240 --> 00:57:46,755
0,225 225,390 390,680 910,1260 1260,1515
We can finally use this
我们终于可以使用这个度量来提取值得高度关注的特征。

1868
00:57:46,755 --> 00:57:49,425
0,515 715,1115 1195,1595 1885,2285 2365,2670
metric to extract features that

1869
00:57:49,425 --> 00:57:51,500
0,305 415,1095 1095,1335 1335,1655 1675,2075
are deserving of high attention.|
|

1870
00:57:52,570 --> 00:57:54,080
0,275 275,410 410,670 840,1175 1175,1510
And {that's -} the exact
而这正是自我注意机制的最后一步。

1871
00:57:54,160 --> 00:57:55,785
0,400 420,770 770,995 995,1265 1265,1625
final step in the self

1872
00:57:55,785 --> 00:57:57,300
0,330 330,695
attention mechanism.|
|

1873
00:57:57,300 --> 00:57:59,040
0,150 150,440 460,780 780,1100 1360,1740
In that, we take that
在这种情况下，我们采用关注度加权矩阵，将其乘以该值，得到的变换。

1874
00:57:59,040 --> 00:58:01,710
0,345 345,860 910,1640 1900,2520 2520,2670
attention weighting matrix, multiply it

1875
00:58:01,710 --> 00:58:03,060
0,150 150,285 285,530 880,1170 1170,1350
by the value and get

1876
00:58:03,060 --> 00:58:06,940
0,290 460,860 2050,2450 2800,3200
a transformed transformation of.|
|

1877
00:58:07,130 --> 00:58:08,785
0,290 290,470 470,710 710,1060 1350,1655
Of the initial data as
作为我们的输出，这反过来又反映了与高度关注相对应的特征。

1878
00:58:08,785 --> 00:58:10,410
0,285 285,665 805,1095 1095,1305 1305,1625
our output, which in turn

1879
00:58:10,460 --> 00:58:12,330
0,350 350,560 560,820 1230,1550 1550,1870
reflects the features that correspond

1880
00:58:12,590 --> 00:58:15,420
0,380 380,740 740,1120
to high attention.|
|

1881
00:58:15,960 --> 00:58:17,090
0,245 245,490 630,890 890,1010 1010,1130
All right, {let's -} take
好了，让我们深呼吸。让我们回顾一下我们到目前为止所讨论的内容。

1882
00:58:17,090 --> 00:58:18,995
0,120 120,380 760,1155 1155,1635 1635,1905
a breath. Let's recap what

1883
00:58:18,995 --> 00:58:20,150
0,120 120,240 240,420 420,725 865,1155
we have just covered so

1884
00:58:20,150 --> 00:58:21,100
0,290
far.|
|

1885
00:58:21,380 --> 00:58:22,870
0,275 275,485 485,695 695,970 1230,1490
The goal with this idea
自我注意是变压器的支柱，这种想法的目标是消除重复，注意输入数据中最重要的特征。

1886
00:58:22,870 --> 00:58:24,385
0,150 150,390 390,740 850,1110 1110,1515
of self attention, the backbone

1887
00:58:24,385 --> 00:58:26,680
0,270 270,875 895,1245 1245,1595 1825,2295
of transformers, is to eliminate

1888
00:58:26,680 --> 00:58:28,285
0,620 820,1110 1110,1245 1245,1350 1350,1605
recurrence, attend to the most

1889
00:58:28,285 --> 00:58:30,205
0,360 360,690 690,1055 1225,1625 1645,1920
important features in the input

1890
00:58:30,205 --> 00:58:31,060
0,275
data.|
|

1891
00:58:31,210 --> 00:58:32,760
0,275 275,425 425,700 1050,1370 1370,1550
In an architecture, how this
在架构中，它的实际部署方式是首先获取我们的输入数据，然后计算这些位置编码。

1892
00:58:32,760 --> 00:58:35,810
0,255 255,650 760,1310 1720,2120 2650,3050
is actually deployed is first

1893
00:58:35,950 --> 00:58:37,130
0,305 305,470 470,680 680,905 905,1180
we take our input data,

1894
00:58:37,570 --> 00:58:40,070
0,290 290,710 710,1010 1010,1600 1740,2500
we compute these positional encodings.|
|

1895
00:58:40,880 --> 00:58:42,990
0,380 380,725 725,995 995,1570 1710,2110
The neural network layers are
神经网络层被三次应用，以将位置编码转换为每个关键查询和值矩阵。

1896
00:58:43,490 --> 00:58:46,195
0,400 630,1240 1620,2020 2130,2450 2450,2705
applied threefold to transform the

1897
00:58:46,195 --> 00:58:48,030
0,435 435,990 990,1230 1230,1500 1500,1835
positional encoding into each of

1898
00:58:48,110 --> 00:58:50,370
0,320 320,640 1080,1540 1650,1955 1955,2260
the key query and value

1899
00:58:50,840 --> 00:58:52,280
0,730
matrices.|
|

1900
00:58:52,290 --> 00:58:54,710
0,260 260,410 410,700 1500,2120 2120,2420
We can then compute the
然后，我们可以根据之前进行的点积操作来计算自我注意权重分数，然后对这些特征进行自我关注。

1901
00:58:54,710 --> 00:58:57,050
0,320 340,705 705,1070 1090,1490 1960,2340
self attention weight score according

1902
00:58:57,050 --> 00:58:58,880
0,225 225,470 670,990 990,1310 1480,1830
to the dot product operation

1903
00:58:58,880 --> 00:58:59,920
0,210 210,345 345,510 510,720 720,1040
that we went through prior

1904
00:59:00,540 --> 00:59:02,630
0,305 305,610 690,1090 1140,1540 1800,2090
and then self attend to

1905
00:59:02,630 --> 00:59:05,480
0,210 210,530 670,945 945,1220
these features to these.|
|

1906
00:59:05,820 --> 00:59:08,195
0,400 540,940 990,1390 1620,2015 2015,2375
Information to extract features that
提取值得高度关注的特征的信息这种方法在承担这种关注权重方面有什么强大的作用？

1907
00:59:08,195 --> 00:59:12,170
0,365 595,975 975,1355 3505,3795 3795,3975
deserve high attention what is

1908
00:59:12,170 --> 00:59:13,960
0,210 210,530 880,1200 1200,1455 1455,1790
so powerful about this approach

1909
00:59:14,460 --> 00:59:16,150
0,380 380,725 725,1025 1025,1325 1325,1690
in taking this attention weight?|
|

1910
00:59:16,820 --> 00:59:17,825
0,255 255,465 465,690 690,870 870,1005
Putting it together with the
把它和提取高注意力特征的价值放在一起，这个操作，我在右边展示的方案，定义了一个单独的自我注意头部，多个这些自我注意力头部可以链接在一起，形成更大的网络架构，你可以思考这些不同的头部，试图提取不同的信息，输入的不同相关部分，现在把我们正在处理的数据的非常非常丰富的编码和表示放在一起，直观地回到我们的铁人例子。这种多个自我注意头部的想法可能相当于提取数据中不同的显著特征和显著信息。

1911
00:59:17,825 --> 00:59:20,045
0,275 805,1185 1185,1560 1560,1905 1905,2220
value to extract high attention

1912
00:59:20,045 --> 00:59:22,930
0,365 955,1245 1245,1515 1515,1895 2485,2885
features is that this operation,

1913
00:59:23,130 --> 00:59:24,350
0,350 350,620 620,800 800,1010 1010,1220
the scheme that I'm showing

1914
00:59:24,350 --> 00:59:26,000
0,225 225,360 360,620 880,1380 1380,1650
on the right, defines a

1915
00:59:26,000 --> 00:59:28,385
0,320 340,735 735,1125 1125,1520 2050,2385
single self attention head and

1916
00:59:28,385 --> 00:59:29,960
0,335 505,795 795,990 990,1245 1245,1575
multiple of these self attention

1917
00:59:29,960 --> 00:59:31,690
0,380 610,900 900,1080 1080,1350 1350,1730
heads can be linked together

1918
00:59:31,920 --> 00:59:34,420
0,290 290,580 720,1120 1200,1595 1595,2500
to form larger network architectures

1919
00:59:34,680 --> 00:59:35,735
0,290 290,440 440,590 590,800 800,1055
where you can think about

1920
00:59:35,735 --> 00:59:37,520
0,240 240,525 525,905 1165,1500 1500,1785
these different heads, trying to

1921
00:59:37,520 --> 00:59:39,970
0,350 520,920 1120,1520 1720,2085 2085,2450
extract different information, different relevant

1922
00:59:40,020 --> 00:59:41,450
0,320 320,500 500,710 710,1060 1170,1430
parts of the input, to

1923
00:59:41,450 --> 00:59:42,695
0,225 225,510 510,750 750,975 975,1245
now put together a very,

1924
00:59:42,695 --> 00:59:45,070
0,330 330,695 745,1470 1470,1665 1665,2375
very rich encoding and representation

1925
00:59:45,420 --> 00:59:46,970
0,320 320,500 500,760 1050,1325 1325,1550
of the data that we're

1926
00:59:46,970 --> 00:59:49,670
0,225 225,590 1270,2190 2190,2520 2520,2700
working with intuitively back to

1927
00:59:49,670 --> 00:59:51,485
0,105 105,380 460,855 855,1250 1540,1815
our iron man example. {What,this

1928
00:59:51,485 --> 00:59:52,820
0,240 240,480 480,630 630,905 985,1335
-} idea of multiple self

1929
00:59:52,820 --> 00:59:55,145
0,350 550,950 1270,1665 1665,2025 2025,2325
attention heads can amount to

1930
00:59:55,145 --> 00:59:57,730
0,225 225,515 775,1175 1645,2250 2250,2585
is that different salient features

1931
00:59:58,050 --> 00:59:59,645
0,320 320,850 900,1235 1235,1445 1445,1595
and salient information in the

1932
00:59:59,645 --> 01:00:01,500
0,275 445,840 840,1475
data is extracted.|
|

1933
01:00:01,510 --> 01:00:03,210
0,400 480,800 800,1115 1115,1430 1430,1700
First, maybe you consider iron
首先，也许你认为钢铁侠的注意力是第一，你可能有额外的注意力头脑，他们正在挑选数据的其他相关部分，这可能是我们之前没有意识到的。例如，背景中正在追逐钢铁侠的建筑或宇宙飞船。

1934
01:00:03,210 --> 01:00:05,820
0,350 910,1260 1260,1515 1515,1820 2320,2610
man attention head one, and

1935
01:00:05,820 --> 01:00:07,130
0,150 150,285 285,510 510,860 910,1310
you may have additional attention

1936
01:00:07,420 --> 01:00:08,835
0,400 540,785 785,920 920,1145 1145,1415
heads that are picking out

1937
01:00:08,835 --> 01:00:10,680
0,300 300,665 1225,1545 1545,1725 1725,1845
other relevant parts of the

1938
01:00:10,680 --> 01:00:12,015
0,195 195,435 435,740 970,1230 1230,1335
data, which maybe we did

1939
01:00:12,015 --> 01:00:13,635
0,225 225,525 525,845 1105,1395 1395,1620
not realize before. {For,example, -}

1940
01:00:13,635 --> 01:00:15,225
0,195 195,455 715,1005 1005,1185 1185,1590
the building or the spaceship

1941
01:00:15,225 --> 01:00:16,515
0,135 135,240 240,485 535,945 945,1290
in the background that's chasing

1942
01:00:16,515 --> 01:00:17,460
0,210 210,545
iron man.|
|

1943
01:00:17,770 --> 01:00:20,730
0,305 305,610 1410,1810 2220,2615 2615,2960
And so this is a
因此，这是当今许多功能强大的架构的关键组成部分。今天，我再一次无法强调这一机制是多么足够、多么强大。

1944
01:00:20,730 --> 01:00:22,830
0,255 255,540 540,920 940,1340 1720,2100
key building block of many,

1945
01:00:22,830 --> 01:00:25,215
0,300 300,540 540,860 1210,1610 1660,2385
many, many, many powerful architectures

1946
01:00:25,215 --> 01:00:26,115
0,120 120,225 225,375 375,585 585,900
that are out there today.

1947
01:00:26,115 --> 01:00:28,280
0,330 330,660 660,1055 1195,1545 1545,2165
{Today,,I,again - -} cannot emphasize

1948
01:00:28,600 --> 01:00:30,420
0,400 540,905 905,1190 1190,1510 1530,1820
how enough, how powerful this

1949
01:00:30,420 --> 01:00:32,240
0,290 400,800
mechanism is.|
|

1950
01:00:32,330 --> 01:00:35,230
0,350 350,700 720,1120 1920,2590 2610,2900
And indeed, this backbone idea
事实上，你刚刚建立起来的自我注意的主干概念是当今一些最强大的神经网络和深度学习模型的关键操作，从像G-P-T-Three这样非常强大的语言模型，它能够以非常人性化的方式合成自然语言，消化大量的文本信息来理解关系和文本。

1951
01:00:35,230 --> 01:00:36,730
0,195 195,465 465,795 795,1160 1210,1500
of self attention that you

1952
01:00:36,730 --> 01:00:38,250
0,180 180,360 360,650 730,1125 1125,1520
just built up understanding of

1953
01:00:38,570 --> 01:00:40,420
0,400 480,830 830,1180 1200,1565 1565,1850
is the key operation of

1954
01:00:40,420 --> 01:00:41,520
0,180 180,285 285,450 450,735 735,1100
some of the most powerful

1955
01:00:41,900 --> 01:00:43,615
0,365 365,640 990,1295 1295,1490 1490,1715
neural networks and deep learning

1956
01:00:43,615 --> 01:00:45,835
0,330 330,615 615,810 810,1115 1645,2220
models out there today, ranging

1957
01:00:45,835 --> 01:00:47,820
0,270 270,600 600,930 930,1295 1585,1985
from the very powerful language

1958
01:00:47,870 --> 01:00:49,255
0,400 450,830 830,1070 1070,1205 1205,1385
models like G P T

1959
01:00:49,255 --> 01:00:51,000
0,305 565,840 840,1020 1020,1325 1345,1745
three, which are capable of

1960
01:00:51,320 --> 01:00:54,310
0,700 2100,2375 2375,2645 2645,2885 2885,2990
synthesizing natural language in a

1961
01:00:54,310 --> 01:00:56,740
0,225 225,525 525,860 970,1370 1690,2430
very human like fashion, digesting

1962
01:00:56,740 --> 01:00:58,410
0,300 300,630 630,870 870,1160 1270,1670
large bodies of text information

1963
01:00:58,430 --> 01:01:01,020
0,400 930,1330 1440,1840 2010,2300 2300,2590
to understand relationships and text.|
|

1964
01:01:02,000 --> 01:01:03,690
0,400 600,1000 1020,1265 1265,1400 1400,1690
To models that are being
到在生物学和医学中部署的极具影响力的应用程序的模型，例如Alpha Full Two，它使用自我注意力的概念来查看蛋白质序列的数据，并能够预测蛋白质的三维结构，只需给定序列信息，甚至一直到计算机视觉，这将是我们明天下一堂课的主题，在那里，最初在顺序数据应用程序中开发的相同的注意力概念现在已经改变了计算机视觉领域，并再次使用关注输入中重要特征的关键概念来构建复杂高维数据的这些非常丰富的表示形式。

1965
01:01:03,710 --> 01:01:07,170
0,365 365,640 1410,1810 1890,2590 3060,3460
deployed for extremely impactful applications

1966
01:01:07,310 --> 01:01:09,490
0,290 290,580 780,1055 1055,1330 1890,2180
in biology and medicine such

1967
01:01:09,490 --> 01:01:11,305
0,225 225,600 600,795 795,1130 1450,1815
as Alpha full two, which

1968
01:01:11,305 --> 01:01:12,850
0,360 360,660 660,960 960,1275 1275,1545
uses this notion of self

1969
01:01:12,850 --> 01:01:14,995
0,350 460,860 1030,1335 1335,1640 1780,2145
attention to look at data

1970
01:01:14,995 --> 01:01:16,885
0,285 285,600 600,1295 1435,1725 1725,1890
of protein sequences and be

1971
01:01:16,885 --> 01:01:18,460
0,270 270,540 540,815 1075,1380 1380,1575
able to predict the three

1972
01:01:18,460 --> 01:01:19,890
0,585 585,840 840,1035 1035,1155 1155,1430
dimensional structure of a protein

1973
01:01:20,270 --> 01:01:22,530
0,320 320,640 870,1270 1440,1840 1860,2260
just given sequence information alone

1974
01:01:23,090 --> 01:01:24,235
0,305 305,515 515,680 680,845 845,1145
and all the way even

1975
01:01:24,235 --> 01:01:26,365
0,395 415,810 810,1125 1125,1445 1795,2130
now to computer vision, which

1976
01:01:26,365 --> 01:01:27,565
0,210 210,485 505,780 780,1005 1005,1200
will be the topic of

1977
01:01:27,565 --> 01:01:29,455
0,135 135,345 345,665 715,1115 1585,1890
our next lecture tomorrow where

1978
01:01:29,455 --> 01:01:31,140
0,225 225,545 565,930 930,1290 1290,1685
the same idea of attention

1979
01:01:31,400 --> 01:01:32,920
0,260 260,455 455,790 870,1235 1235,1520
that was initially developed in

1980
01:01:32,920 --> 01:01:35,730
0,740 850,1250 1420,1820 2170,2490 2490,2810
sequential data applications has now

1981
01:01:35,870 --> 01:01:37,525
0,400 720,995 995,1160 1160,1400 1400,1655
transformed the field of computer

1982
01:01:37,525 --> 01:01:39,715
0,305 625,945 945,1265 1525,1905 1905,2190
vision and again using this

1983
01:01:39,715 --> 01:01:41,935
0,210 210,515 895,1295 1405,1920 1920,2220
key concept of attending to

1984
01:01:41,935 --> 01:01:43,300
0,315 315,630 630,945 945,1140 1140,1365
the important features in an

1985
01:01:43,300 --> 01:01:45,220
0,380 550,840 840,1130 1270,1620 1620,1920
input to build these very

1986
01:01:45,220 --> 01:01:47,710
0,255 255,1010 1180,1580 1630,2030 2200,2490
rich representations of complex high

1987
01:01:47,710 --> 01:01:48,820
0,435 435,710
dimensional data.|
|

1988
01:01:50,100 --> 01:01:52,820
0,400 900,1160 1160,1420 1440,2020 2190,2720
Okay, so that concludes lectures
好了，今天的课程到此结束。我知道我们在相当短的时间内覆盖了很多领域，但这就是这个新兵训练营计划的全部意义所在。所以希望今天你已经对神经网络的基础有了一个了解。在与Alexander的讲座中，我们讨论了r和n，它们如何很好地适应序列数据，以及我们如何使用反向传播来训练它们。

1989
01:01:52,820 --> 01:01:54,530
0,165 165,440 1060,1335 1335,1515 1515,1710
for today. {I,know -} we

1990
01:01:54,530 --> 01:01:55,685
0,285 285,585 585,780 780,975 975,1155
have covered a lot of

1991
01:01:55,685 --> 01:01:57,020
0,275 355,600 600,810 810,1095 1095,1335
territory in a pretty short

1992
01:01:57,020 --> 01:01:58,385
0,210 210,390 390,680 880,1170 1170,1365
amount of time, but that

1993
01:01:58,385 --> 01:01:59,525
0,195 195,375 375,615 615,900 900,1140
is what this boot camp

1994
01:01:59,525 --> 01:02:01,655
0,365 475,750 750,960 960,1295 1825,2130
program is all {about.,So -}

1995
01:02:01,655 --> 01:02:02,885
0,305 325,675 675,945 945,1065 1065,1230
hopefully today you've gotten a

1996
01:02:02,885 --> 01:02:04,505
0,255 255,570 570,780 780,1265 1345,1620
sense of the foundations of

1997
01:02:04,505 --> 01:02:05,975
0,225 225,485 895,1170 1170,1290 1290,1470
{neural,networks. -} In the lecture

1998
01:02:05,975 --> 01:02:08,030
0,330 330,725 1285,1590 1590,1830 1830,2055
with Alexander, we talked about

1999
01:02:08,030 --> 01:02:09,590
0,165 165,285 285,770 940,1260 1260,1560
r and n's, how they're

2000
01:02:09,590 --> 01:02:10,990
0,195 195,525 525,675 675,1125 1125,1400
well suited for sequential data,

2001
01:02:11,310 --> 01:02:12,305
0,290 290,455 455,635 635,830 830,995
how we can train them

2002
01:02:12,305 --> 01:02:14,300
0,225 225,465 465,995
using back propagation.|
|

2003
01:02:14,300 --> 01:02:15,320
0,240 240,360 360,570 570,825 825,1020
How we can deploy them
我们如何将它们部署到不同的应用程序中，最后，我们如何超越重复，以构建自我关注的概念，以构建越来越强大的模型，以便在序列建模中进行深度学习。

2004
01:02:15,320 --> 01:02:16,900
0,150 150,410 520,920 940,1260 1260,1580
for different applications, and finally,

2005
01:02:17,310 --> 01:02:18,500
0,320 320,515 515,650 650,875 875,1190
how we can move beyond

2006
01:02:18,500 --> 01:02:20,330
0,650 820,1095 1095,1275 1275,1560 1560,1830
recurrence to build this idea

2007
01:02:20,330 --> 01:02:22,180
0,195 195,465 465,830 1150,1500 1500,1850
of self attention for building

2008
01:02:22,560 --> 01:02:25,190
0,400 480,880 900,1300 2100,2420 2420,2630
increasingly powerful models for deep

2009
01:02:25,190 --> 01:02:27,040
0,255 255,525 525,795 795,1340
learning in sequence modeling.|
|

2010
01:02:27,800 --> 01:02:29,850
0,245 245,490 1080,1430 1430,1715 1715,2050
All right, hopefully you enjoyed.
好的，希望你喜欢。我们还有大约45分钟用于实验部分和开放办公时间，我们欢迎您向我们和助教提问并开始实验室工作。实验室的信息就在上面。非常感谢您的关注。

2011
01:02:29,990 --> 01:02:32,845
0,350 350,700 1800,2120 2120,2570 2570,2855
{We,have -} about 45 minutes

2012
01:02:32,845 --> 01:02:33,970
0,345 345,600 600,810 810,1005 1005,1125
left for the for the

2013
01:02:33,970 --> 01:02:35,650
0,260 280,675 675,1070 1090,1440 1440,1680
lab portion and open office

2014
01:02:35,650 --> 01:02:37,120
0,290 610,885 885,1050 1050,1215 1215,1470
hours in which we welcome

2015
01:02:37,120 --> 01:02:38,730
0,285 285,465 465,720 720,1100 1210,1610
you to ask us questions

2016
01:02:39,440 --> 01:02:40,555
0,275 275,470 470,650 650,815 815,1115
of us and the tas

2017
01:02:40,555 --> 01:02:42,025
0,255 255,405 405,630 630,965 1165,1470
and to start work on

2018
01:02:42,025 --> 01:02:44,005
0,165 165,665 1045,1440 1440,1785 1785,1980
the {labs.,The -} information for

2019
01:02:44,005 --> 01:02:45,450
0,105 105,390 390,635 805,1125 1125,1445
the labs is {up,there. -}

2020
01:02:45,830 --> 01:02:46,885
0,290 290,455 455,590 590,770 770,1055
Thank you so much for

2021
01:02:46,885 --> 01:02:49,968
0,285 285,605
your attention.|
|
