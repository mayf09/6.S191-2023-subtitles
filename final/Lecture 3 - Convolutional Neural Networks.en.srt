1
00:00:09,090 --> 00:00:12,070
Hi, everyone, and welcome back to introduction to deep learning,

2
00:00:12,810 --> 00:00:15,100
we had a really awesome kickoff day yesterday,

3
00:00:15,180 --> 00:00:17,915
so we're looking to keep that same momentum all throughout the week

4
00:00:17,915 --> 00:00:18,970
and starting with today

5
00:00:19,680 --> 00:00:22,720
and today, we're really excited to be talking about

6
00:00:23,010 --> 00:00:25,100
actually one of my favorite topics in this course,

7
00:00:25,100 --> 00:00:30,790
which is how we can build computers that can achieve the sense of sight and vision.

8
00:00:32,320 --> 00:00:34,310
Now, I believe that sight

9
00:00:34,630 --> 00:00:40,310
and specifically, like I said, vision is one of the most important human senses that we all have,

10
00:00:40,900 --> 00:00:44,340
in fact sighted people rely on vision quite a lot

11
00:00:44,340 --> 00:00:45,440
in our day to day lives,

12
00:00:45,490 --> 00:00:49,400
from everything from walking around, navigating the world,

13
00:00:49,810 --> 00:00:53,720
interacting and sensing other emotions in our colleagues and peers.

14
00:00:53,950 --> 00:00:57,860
And today we're going to learn about how we can use deep learning and machine learning

15
00:00:58,600 --> 00:01:00,680
to build powerful vision systems,

16
00:01:01,030 --> 00:01:04,485
that can both see and predict what is where

17
00:01:04,485 --> 00:01:07,550
by only looking at raw visual inputs.

18
00:01:07,840 --> 00:01:09,470
And I like to think of that phrase

19
00:01:09,550 --> 00:01:15,650
as a very concise and sweet definition of what it really means to achieve vision.

20
00:01:16,240 --> 00:01:21,710
But at its core, vision is actually so much more than just understanding what is where,

21
00:01:22,060 --> 00:01:23,540
it also goes much deeper,

22
00:01:23,620 --> 00:01:24,830
takes this scene, for example,

23
00:01:24,850 --> 00:01:27,020
we can build computer vision systems

24
00:01:27,160 --> 00:01:31,130
that can identify, of course, all of the objects in this environment,

25
00:01:31,330 --> 00:01:35,600
starting first with the yellow taxi or the Van parked on the side of the road,

26
00:01:36,220 --> 00:01:40,970
but we also need to understand each of these objects at a much deeper level,

27
00:01:41,020 --> 00:01:42,225
not just where they are,

28
00:01:42,225 --> 00:01:46,200
but actually predicting the future, predicting what may happen in the scene next,

29
00:01:46,200 --> 00:01:52,140
for example, that the yellow taxi is more likely to be moving and dynamic into the future,

30
00:01:52,140 --> 00:01:53,510
because it's in the middle of the lane,

31
00:01:53,830 --> 00:01:56,780
compared to the white Van which is parked on the side of the road,

32
00:01:56,980 --> 00:01:58,910
even though you're just looking at a single image,

33
00:01:59,320 --> 00:02:02,670
your brain can infer all of these very subtle cues

34
00:02:02,670 --> 00:02:04,755
and it goes all the way to the pedestrians on the road

35
00:02:04,755 --> 00:02:10,040
and even these even more subtle cues in the traffic lights and the rest of the scene as well,

36
00:02:10,720 --> 00:02:14,985
now accounting for all of these details in the scene is an extraordinary challenge,

37
00:02:14,985 --> 00:02:18,360
but we as humans do this so seamlessly within a split second,

38
00:02:18,360 --> 00:02:20,670
I probably put that frame up on the slide

39
00:02:20,670 --> 00:02:24,980
and all of you within a split second could reason about many of those subtle details

40
00:02:25,180 --> 00:02:26,870
without me even pointing them out,

41
00:02:27,370 --> 00:02:29,085
but the question of today's class is,

42
00:02:29,085 --> 00:02:32,030
how we can build machine learning and deep learning algorithms,

43
00:02:32,320 --> 00:02:37,200
that can achieve that same type and subtle understanding of our world.

44
00:02:38,210 --> 00:02:43,500
And deep learning in particular is really leading this revolution of computer vision

45
00:02:43,580 --> 00:02:45,570
and achieving site of computers,

46
00:02:46,520 --> 00:02:52,110
for example, allowing robots to pick up on these key visual cues in their environment,

47
00:02:53,030 --> 00:02:56,580
critical for really navigating the world together with us as humans,

48
00:02:56,960 --> 00:03:00,565
these algorithms that you're going to learn about today have become so mainstreamed,

49
00:03:00,565 --> 00:03:03,240
in fact, that they're fitting on all of your smartphones,

50
00:03:03,410 --> 00:03:04,350
in your pockets,

51
00:03:04,580 --> 00:03:06,600
processing every single image that you take,

52
00:03:06,740 --> 00:03:10,380
enhancing those images, detecting faces and so on and so forth.

53
00:03:10,790 --> 00:03:12,940
And we're seeing some exciting advances ranging,

54
00:03:12,940 --> 00:03:14,460
all the way from biology and medicine,

55
00:03:14,630 --> 00:03:16,350
which we'll talk about a bit later today,

56
00:03:16,760 --> 00:03:19,770
to autonomous driving and accessibility as well.

57
00:03:20,630 --> 00:03:21,505
And like I said,

58
00:03:21,505 --> 00:03:27,505
deep learning has taken this field as a whole by storm in over the past decade or so,

59
00:03:27,505 --> 00:03:30,480
because of its ability critically like we were talking about yesterday,

60
00:03:30,800 --> 00:03:37,470
its ability to learn directly from raw data and those raw image inputs in what it sees in its environment

61
00:03:38,120 --> 00:03:40,590
and learn explicitly how to perform,

62
00:03:41,120 --> 00:03:42,240
like we talked about yesterday,

63
00:03:42,650 --> 00:03:46,590
what is called feature extraction of those images in the environment,

64
00:03:46,700 --> 00:03:49,380
and one example of that is through facial detection and recognition,

65
00:03:50,030 --> 00:03:54,270
which all of you are going to get practice with in today's and tomorrow's labs

66
00:03:54,320 --> 00:03:57,240
as part of the grand final competition of this class.

67
00:03:58,100 --> 00:04:01,320
Another really go to example of computer vision is

68
00:04:01,370 --> 00:04:03,600
in autonomous driving and self driving vehicles,

69
00:04:03,710 --> 00:04:05,640
where we can take an image as input

70
00:04:05,660 --> 00:04:08,400
or maybe potentially a video as input, multiple images

71
00:04:09,050 --> 00:04:10,560
and process all of that data,

72
00:04:10,700 --> 00:04:13,495
so that we can train a car to learn

73
00:04:13,495 --> 00:04:18,780
how to steer the wheel or command a throttle or actuate a braking command,

74
00:04:19,100 --> 00:04:20,610
this entire control system,

75
00:04:20,980 --> 00:04:25,200
the steering, the throttle, the braking of a car can be executed end to end,

76
00:04:25,400 --> 00:04:29,580
by taking as input the images and the sensing modalities of the vehicle

77
00:04:29,750 --> 00:04:32,550
and learning how to predict those actuation commands,

78
00:04:32,840 --> 00:04:35,020
now, actually this end to end approach,

79
00:04:35,020 --> 00:04:37,200
having a single neural network do all of this

80
00:04:37,400 --> 00:04:42,175
is actually radically different than the vast majority of autonomous vehicle companies,

81
00:04:42,175 --> 00:04:45,030
like if you look at Waymo for example, that's a radically different approach,

82
00:04:45,260 --> 00:04:48,300
but we'll talk about those approaches in today's class

83
00:04:48,470 --> 00:04:53,760
and in fact this is one of our vehicles that we've been building at MIT

84
00:04:54,050 --> 00:04:56,820
in my lab and cell just a few floors above this room

85
00:04:57,260 --> 00:05:00,450
and we'll again share some of the details on this incredible work.

86
00:05:00,740 --> 00:05:02,970
But of course, it doesn't stop here with autonomous driving,

87
00:05:03,230 --> 00:05:07,230
these algorithms directly the same algorithms that you'll learn about in today's class

88
00:05:07,670 --> 00:05:11,760
can be extended all the way to impact healthcare, medical decision making

89
00:05:12,230 --> 00:05:15,120
and finally even in these accessibility applications,

90
00:05:15,170 --> 00:05:19,350
where we're seeing computer vision algorithms helping the visually impaired,

91
00:05:19,520 --> 00:05:20,910
so for example in this project,

92
00:05:21,020 --> 00:05:25,780
researchers have built deep learning enabled devices, that could detect trails,

93
00:05:26,250 --> 00:05:30,310
so that visually impaired runners could be provided audible feedback,

94
00:05:30,690 --> 00:05:34,330
so that they too could, you know, navigate when they go out for runs.

95
00:05:35,410 --> 00:05:36,480
And like I said,

96
00:05:36,480 --> 00:05:41,030
we often take many of these tasks that we're going to talk about in today's lecture for granted,

97
00:05:41,260 --> 00:05:44,150
because we do them so seamlessly in our day to day lives,

98
00:05:44,770 --> 00:05:48,560
but the question of today's class is going to be,

99
00:05:48,970 --> 00:05:49,760
at its core,

100
00:05:49,930 --> 00:05:53,610
how we can build a computer to do these same types of incredible things,

101
00:05:53,610 --> 00:05:55,940
that all of us take for granted day to day.

102
00:05:56,690 --> 00:05:58,920
And specifically, we'll start with this question of,

103
00:05:59,900 --> 00:06:02,125
how does a computer really see,

104
00:06:02,125 --> 00:06:03,850
and even more detailed than that is,

105
00:06:03,850 --> 00:06:06,000
how does a computer process an image,

106
00:06:06,170 --> 00:06:07,050
if we think of,

107
00:06:07,250 --> 00:06:09,660
you know, site as coming to computers through images,

108
00:06:10,220 --> 00:06:13,170
then how can a computer even start to process those images.

109
00:06:14,060 --> 00:06:17,310
Well, to a computer, images are just numbers, right,

110
00:06:18,020 --> 00:06:21,420
and suppose, for example, we have a picture here of Abraham Lincoln,

111
00:06:22,560 --> 00:06:25,420
okay, this picture is made up of what are called pixels,

112
00:06:25,560 --> 00:06:28,750
every pixel is just a dot in this image,

113
00:06:29,250 --> 00:06:30,875
and since this is a gray scale image,

114
00:06:30,875 --> 00:06:34,120
each of these pixels is just a single number,

115
00:06:34,680 --> 00:06:39,880
now we can represent our image now as this two dimensional matrix of numbers,

116
00:06:40,470 --> 00:06:42,730
and because, like I said, this is a gray scale image,

117
00:06:42,900 --> 00:06:47,290
every pixel is corresponding to just one number at that matrix location,

118
00:06:47,860 --> 00:06:50,580
now assume, for example, we didn't have a gray scale image,

119
00:06:50,580 --> 00:06:53,420
we had a color image that would be an RGB image,

120
00:06:53,920 --> 00:06:57,945
so now every pixel is going to be composed not just of one number, but of three numbers,

121
00:06:57,945 --> 00:07:02,475
so you can think of that as kind of a 3D matrix instead of a 2D matrix,

122
00:07:02,475 --> 00:07:07,640
where you almost have three two dimensional matrix that are stacked on top of each other.

123
00:07:08,410 --> 00:07:13,520
So now with this basis of basically numerical representations of images,

124
00:07:13,540 --> 00:07:15,210
we can start to think about,

125
00:07:15,210 --> 00:07:20,030
how we can or what types of computer vision algorithms we can build,

126
00:07:20,200 --> 00:07:23,600
that can take these systems as input and what they can perform.

127
00:07:23,680 --> 00:07:26,540
So the first thing that I want to talk to you about is,

128
00:07:26,920 --> 00:07:31,340
what kind of tasks do we even want to train these systems to complete with images,

129
00:07:31,900 --> 00:07:35,415
and broadly speaking there are two broad categories of tasks,

130
00:07:35,415 --> 00:07:37,580
we touched on this a little bit in yesterday's lecture,

131
00:07:37,600 --> 00:07:40,310
but just to be a bit more concrete in today's lecture,

132
00:07:40,570 --> 00:07:43,910
those two tasks are either classification or regression,

133
00:07:44,530 --> 00:07:51,150
now in regression, your prediction value is going to take a continuous value, right,

134
00:07:51,150 --> 00:07:53,330
that could be any real number on the number line,

135
00:07:53,560 --> 00:07:59,390
but in classification, your prediction could take one of, let's say, k or n different classes,

136
00:08:00,040 --> 00:08:01,610
these are discrete different classes.

137
00:08:01,630 --> 00:08:05,060
So let's consider first the task of image classification,

138
00:08:06,010 --> 00:08:11,025
in this task we want to predict an individual label for every single image

139
00:08:11,025 --> 00:08:16,820
and this label that we predict is going to be one of n different possible labels that could be considered,

140
00:08:17,110 --> 00:08:20,660
so for example, let's say we have a bunch of images of US preidents

141
00:08:21,130 --> 00:08:23,060
and we want to build a classification pipeline

142
00:08:23,080 --> 00:08:27,710
to tell us which president is in this particular image that you see on the screen,

143
00:08:29,420 --> 00:08:34,960
the goal of our model in this case is going to be basically to output a probability score,

144
00:08:35,100 --> 00:08:39,010
probability of this image containing one of these different preidents,

145
00:08:39,600 --> 00:08:42,140
and the maximum score is going to be ultimately the one

146
00:08:42,140 --> 00:08:45,130
that we infer to be the correct preident in the image.

147
00:08:46,080 --> 00:08:50,980
So in order to correctly perform this task and correctly classify these images,

148
00:08:51,810 --> 00:08:56,495
our pipeline, our computer vision model needs the ability to be able to tell us

149
00:08:56,495 --> 00:09:01,540
what is unique about this particular image of Abraham Lincoln, for example,

150
00:09:01,890 --> 00:09:07,690
versus a different picture of George Washington versus a different picture of Obama for example.

151
00:09:08,310 --> 00:09:13,120
Now another way to think about this whole problem of image classification or image processing

152
00:09:14,070 --> 00:09:16,930
at its high level is in terms of features,

153
00:09:17,340 --> 00:09:22,360
or think of these as almost patterns in your data or characteristics of a particular class,

154
00:09:22,710 --> 00:09:25,240
and classification then is simply done

155
00:09:25,530 --> 00:09:29,050
by detecting all of these different patterns in your data

156
00:09:29,310 --> 00:09:33,730
and identifying when certain patterns occur over other patterns,

157
00:09:34,350 --> 00:09:38,950
so for example, if the features of a particular class are present in an image,

158
00:09:39,300 --> 00:09:42,410
then you might infer that that image is of that class,

159
00:09:42,410 --> 00:09:44,530
so for example, if you want to detect cars,

160
00:09:44,670 --> 00:09:48,995
you might look for patterns in your data, like wheels, license plates or headlights,

161
00:09:48,995 --> 00:09:50,770
and if those things are present in your image,

162
00:09:51,120 --> 00:09:53,290
then you can say with fairly high confidence that,

163
00:09:53,370 --> 00:09:56,590
your image of a car versus one of these other categories.

164
00:09:57,120 --> 00:09:59,230
So if we're building a computer vision pipeline,

165
00:09:59,790 --> 00:10:01,865
we have two main steps really to consider,

166
00:10:01,865 --> 00:10:07,390
The first step is that we need to know what features or what patterns we're looking for in our data,

167
00:10:07,620 --> 00:10:10,870
and the second step is we need to then detect those patterns,

168
00:10:10,950 --> 00:10:11,855
once we detect them,

169
00:10:11,855 --> 00:10:13,870
we can then infer which class we're in.

170
00:10:15,300 --> 00:10:19,640
Now, one way to solve this is to leverage knowledge about our particular field, right,

171
00:10:19,640 --> 00:10:21,620
so if we know something about our field,

172
00:10:21,620 --> 00:10:23,740
for example about human faces,

173
00:10:24,360 --> 00:10:27,160
we can use that knowledge to define our features,

174
00:10:27,720 --> 00:10:28,715
what makes up a face,

175
00:10:28,715 --> 00:10:31,930
we know faces are made up of eyes, noses and ears, for example,

176
00:10:32,340 --> 00:10:35,200
we can define what each of those components look like,

177
00:10:35,460 --> 00:10:36,640
in defining our features,

178
00:10:37,260 --> 00:10:39,410
but there's a big problem with this approach,

179
00:10:39,410 --> 00:10:45,010
and remember that images are just these three dimensional arrays of numbers,

180
00:10:45,810 --> 00:10:50,230
they can have a lot of variation even within the same type of object,

181
00:10:50,670 --> 00:10:53,105
these variations can include really anything,

182
00:10:53,105 --> 00:10:58,750
ranging from occlusions to variations in lighting, rotations, translations into a class variation.

183
00:10:59,780 --> 00:11:01,165
And the problem here is,

184
00:11:01,165 --> 00:11:04,800
that our classification pipeline needs the ability to handle

185
00:11:05,300 --> 00:11:09,150
and be invariant to all of these different types of variations,

186
00:11:09,920 --> 00:11:13,530
while still being sensitive to all of the inter class variations,

187
00:11:13,580 --> 00:11:16,290
the variations that occur between different classes.

188
00:11:17,210 --> 00:11:21,060
Now, even though our pipeline could use features that we as humans,

189
00:11:21,290 --> 00:11:25,080
you know, define manually define based on some of our prior knowledge,

190
00:11:25,730 --> 00:11:27,780
the problem really breaks down,

191
00:11:28,220 --> 00:11:32,370
in that these features, become very non robust,

192
00:11:32,630 --> 00:11:38,010
when considering all of these vast amounts of different variations that images take in the real world.

193
00:11:39,080 --> 00:11:41,635
So in practice, like I said,

194
00:11:41,635 --> 00:11:45,685
your algorithms need to be able to withstand all of those different types of variations,

195
00:11:45,685 --> 00:11:47,380
and then the natural question is that,

196
00:11:47,380 --> 00:11:50,860
how can we build a computer vision algorithm to do that

197
00:11:50,860 --> 00:11:53,730
and still maintain that level of robustness,

198
00:11:53,960 --> 00:11:56,340
and what we want is a way to extract features

199
00:11:56,420 --> 00:12:02,160
that can both detect those features, those patterns in the data,

200
00:12:02,810 --> 00:12:05,730
and do so in a hierarchical fashion,

201
00:12:06,170 --> 00:12:12,360
so going all the way from the ground up, from the pixel level to something with semantic meaning,

202
00:12:12,560 --> 00:12:15,690
like for example, the eyes or the noses in the human face.

203
00:12:16,930 --> 00:12:19,185
Now, we learned in the last class,

204
00:12:19,185 --> 00:12:22,400
that we can use neural networks exactly for this type of problem,

205
00:12:23,050 --> 00:12:27,200
neural networks are capable of learning features directly from data,

206
00:12:28,170 --> 00:12:31,990
and learn, most importantly, a hierarchical set of features,

207
00:12:32,280 --> 00:12:34,900
building on top of previous features that it's learned

208
00:12:35,070 --> 00:12:37,390
to build more and more complex set of features.

209
00:12:39,760 --> 00:12:45,660
Now, we're going to see exactly how neural networks can do this in the image domain,

210
00:12:45,770 --> 00:12:46,980
as part of this lecture,

211
00:12:47,570 --> 00:12:50,040
but specifically neural networks will allow us

212
00:12:50,480 --> 00:12:53,790
to learn these visual features from visual data,

213
00:12:54,050 --> 00:12:55,840
if we construct them cleverly,

214
00:12:55,840 --> 00:12:57,250
and the key point here is that,

215
00:12:57,250 --> 00:13:01,170
actually the models and the architectures that we learned about in yesterday's lecture,

216
00:13:01,610 --> 00:13:02,910
and so far in this course,

217
00:13:03,380 --> 00:13:10,950
we'll see how they're actually not suitable or extensible to today's, you know, problem domain of images

218
00:13:11,120 --> 00:13:15,720
and how we can build and construct neural networks a bit more cleverly to overcome those issues.

219
00:13:15,980 --> 00:13:20,520
So maybe let's start by revisiting what we talked about in lecture one,

220
00:13:20,930 --> 00:13:23,940
which was where we learned about fully connected networks,

221
00:13:24,140 --> 00:13:27,805
now these were networks that, you know, have multiple hidden layers

222
00:13:27,805 --> 00:13:33,030
and each neuron in a given hidden layer is connected to every neuron in its prior layer,

223
00:13:33,500 --> 00:13:39,090
so it receives all of the previous layers inputs as a function of these fully connected layers,

224
00:13:39,580 --> 00:13:44,785
now let's say that we want to directly, without any modifications, use a fully connected network,

225
00:13:44,785 --> 00:13:46,710
like we learned about in lecture one,

226
00:13:47,210 --> 00:13:49,140
with an image processing pipeline,

227
00:13:49,370 --> 00:13:53,280
so directly taking an image and feeding it to a fully connected network,

228
00:13:53,720 --> 00:13:55,050
could we do something like that.

229
00:13:55,550 --> 00:13:56,940
Actually in this case, we could,

230
00:13:56,960 --> 00:13:58,500
the way we would have to do it is,

231
00:13:58,760 --> 00:14:01,860
remember that because our image is a two dimensional array,

232
00:14:02,000 --> 00:14:05,700
the first thing that we would have to do is collapse that to a one dimensional sequence of numbers,

233
00:14:05,990 --> 00:14:10,495
because a fully connected network is not taking in a two dimensional array,

234
00:14:10,495 --> 00:14:12,090
it's taking in a one dimensional sequence.

235
00:14:13,190 --> 00:14:15,180
So the first thing that we have to do is,

236
00:14:15,620 --> 00:14:19,680
flatten that two dimensional array to a vector of pixel values

237
00:14:19,700 --> 00:14:21,000
and feed that to our network,

238
00:14:21,710 --> 00:14:29,710
in this case, every neuron in our first layer is connected to all neurons in that input layer, right,

239
00:14:29,710 --> 00:14:31,830
so in that original image flattened down,

240
00:14:32,330 --> 00:14:34,555
we feed all of those pixels to the first layer,

241
00:14:34,555 --> 00:14:39,210
and here you should already appreciate the very important notion,

242
00:14:39,650 --> 00:14:45,210
that every single piece of spatial information that really defined our image

243
00:14:45,470 --> 00:14:48,940
that makes an image an image is totally lost already,

244
00:14:48,940 --> 00:14:50,560
before we've even started this problem,

245
00:14:50,560 --> 00:14:54,270
because we've flattened that two dimensional image into a one dimensional array,

246
00:14:54,320 --> 00:14:57,390
we've completely destroyed all notion of spatial information.

247
00:14:58,570 --> 00:15:02,720
And in addition, we really have an enormous number of parameters,

248
00:15:03,190 --> 00:15:05,150
because this system is fully connected,

249
00:15:05,200 --> 00:15:07,490
take for example, in a very, very small image,

250
00:15:07,780 --> 00:15:10,200
which is even 100 by 100 pixels,

251
00:15:10,200 --> 00:15:12,290
that's an incredibly small image in today's standards,

252
00:15:12,670 --> 00:15:15,920
but that's going to take 10000 neurons just in the first layer,

253
00:15:16,090 --> 00:15:19,190
which will be connected to, let's say 10000 neurons in the second layer,

254
00:15:19,690 --> 00:15:24,710
the number of parameters that you have just in that one layer alone is going to be 10,000 squared parameters,

255
00:15:24,880 --> 00:15:26,460
it's going to be highly inefficient,

256
00:15:26,460 --> 00:15:28,905
you can imagine if you want to scale this network to

257
00:15:28,905 --> 00:15:32,055
even a reasonably sized image that we have to deal with today,

258
00:15:32,055 --> 00:15:33,530
so not feasible in practice,

259
00:15:34,060 --> 00:15:36,045
but instead we need to ask ourselves,

260
00:15:36,045 --> 00:15:39,920
how we can build and maintain some of that spatial structure,

261
00:15:39,970 --> 00:15:43,920
that's very unique about images here into our input

262
00:15:43,920 --> 00:15:45,860
and here into our model most importantly.

263
00:15:47,830 --> 00:15:53,505
So to do this, let's represent our two 2D as its original form,

264
00:15:53,505 --> 00:15:56,030
as a two dimensional array of numbers,

265
00:15:56,950 --> 00:16:02,325
one way that we can use spatial structure here inherent to our input is

266
00:16:02,325 --> 00:16:08,445
to connect what are called basically these patches of our input to neurons in the hidden layer,

267
00:16:08,445 --> 00:16:13,005
so for example, let's say that each neuron in the hidden layer, that you can see here

268
00:16:13,005 --> 00:16:15,830
only is going to see or respond

269
00:16:16,000 --> 00:16:22,800
to a certain set or a certain patch of neurons in the previous layer, right,

270
00:16:22,800 --> 00:16:25,425
so you could also think of this as almost a receptive field

271
00:16:25,425 --> 00:16:30,075
or what the single neuron in your next layer can attend to in the previous layer,

272
00:16:30,075 --> 00:16:31,310
is not the entire image,

273
00:16:31,690 --> 00:16:34,580
but rather a small receptive field from your previous image,

274
00:16:35,410 --> 00:16:39,080
now notice here how the region of the input layer,

275
00:16:39,730 --> 00:16:41,600
which you can see on the left hand side here,

276
00:16:42,340 --> 00:16:45,345
influences that single neuron on the right hand side

277
00:16:45,345 --> 00:16:47,430
and that's just one neuron in the next layer,

278
00:16:47,430 --> 00:16:53,625
but of course you can imagine basically defining these connections across the whole input, right,

279
00:16:53,625 --> 00:16:56,570
each time you have the single patch on your input,

280
00:16:56,950 --> 00:17:00,530
that corresponds to a single neuron output on the other layer,

281
00:17:01,420 --> 00:17:02,790
and we can apply the same principle

282
00:17:02,790 --> 00:17:08,760
of connecting these patches across the entire image to single neurons in the subsequent layer,

283
00:17:08,760 --> 00:17:11,090
and we do this by essentially sliding that patch,

284
00:17:12,160 --> 00:17:14,510
pixel by pixel across the input image

285
00:17:14,890 --> 00:17:18,920
and we'll be responding with, you know, another image on our output layer.

286
00:17:20,010 --> 00:17:27,590
In this way, we essentially preserve all of that very key and rich spatial information inherent to our input,

287
00:17:27,590 --> 00:17:30,250
but remember that the ultimate task here is,

288
00:17:30,390 --> 00:17:33,110
not only to just preserve that spatial information,

289
00:17:33,110 --> 00:17:35,720
we want to ultimately learn features, learn those patterns,

290
00:17:35,720 --> 00:17:38,200
so that we can detect and classify these images,

291
00:17:38,730 --> 00:17:45,130
and we can do this by waving, waving the connections between the patches of our input,

292
00:17:45,640 --> 00:17:51,000
and, and in order to detect, you know, what those certain features are.

293
00:17:51,800 --> 00:17:53,560
Let me give a practical example here,

294
00:17:53,560 --> 00:17:59,370
and so in practice, this operation that I'm describing, this patching and sliding operation I'm describing,

295
00:17:59,480 --> 00:18:02,700
is actually a mathematical operation formerly known as convolution,

296
00:18:03,470 --> 00:18:05,460
we first think about this as a high level,

297
00:18:05,780 --> 00:18:09,870
supposing that we have what's called a four by four pixel patch,

298
00:18:10,310 --> 00:18:12,180
so you can see this four by four pixel patch,

299
00:18:12,320 --> 00:18:15,720
represented in red as a red box on the left hand side,

300
00:18:16,250 --> 00:18:18,600
and let's suppose, for example,

301
00:18:19,040 --> 00:18:20,550
since we have a four by four patch,

302
00:18:20,630 --> 00:18:25,230
this is going to consist of sixteen different weights in this layer,

303
00:18:26,000 --> 00:18:28,720
we're going to apply this same four by four,

304
00:18:28,720 --> 00:18:30,160
let's call this not a patch anymore,

305
00:18:30,160 --> 00:18:31,620
let's use the terminology filter,

306
00:18:31,760 --> 00:18:35,680
we'll apply this same four by four filter in the input,

307
00:18:35,680 --> 00:18:41,010
and use the result of that operation to define the state of the neuron in the next layer,

308
00:18:41,390 --> 00:18:46,680
and now we're going to shift our filter by, let's say, two pixels to the right,

309
00:18:47,300 --> 00:18:53,130
and that's going to define the next neuron in the adjacent location in the future layer,

310
00:18:53,660 --> 00:18:54,625
and we keep doing this

311
00:18:54,625 --> 00:18:56,760
and you can see that on the right hand side,

312
00:18:57,230 --> 00:18:59,650
you're sliding over not only the input image,

313
00:18:59,650 --> 00:19:03,505
but you're also sliding over the output neurons in the secondary layer,

314
00:19:03,505 --> 00:19:07,800
and this is how we can start to think about convolution at a very, very high level.

315
00:19:09,020 --> 00:19:11,280
But you're probably wondering, right,

316
00:19:11,750 --> 00:19:14,320
not just how the convolution operation works,

317
00:19:14,320 --> 00:19:18,030
but I think the main thing here to really narrow down on is,

318
00:19:18,260 --> 00:19:23,815
how convolution allows us to learn these features, these patterns in the data that we were talking about,

319
00:19:23,815 --> 00:19:25,780
because ultimately that's our final goal,

320
00:19:25,780 --> 00:19:28,350
that's our real goal for this class is to extract those patterns.

321
00:19:28,940 --> 00:19:34,200
So let's make this very concrete by walking through maybe a concrete example, right,

322
00:19:35,230 --> 00:19:40,610
so suppose, for example, we want to build a convolutional algorithm

323
00:19:40,900 --> 00:19:44,590
to detect or classify an X in an image,

324
00:19:44,880 --> 00:19:46,390
this is the letter X in an image,

325
00:19:47,010 --> 00:19:51,725
and here, for simplicity, let's just say we have only black and white images, right,

326
00:19:51,725 --> 00:19:56,080
so every pixel in this image will be represented by either a zero or one,

327
00:19:56,370 --> 00:19:59,050
for simplicity, there's no gray scale in this image

328
00:19:59,460 --> 00:20:04,180
and actually here so we're representing black as -1 and white as 1,

329
00:20:04,710 --> 00:20:11,120
so to classify, we simply cannot, you know, compare the left hand side to the right hand side, right,

330
00:20:11,120 --> 00:20:12,880
because these are both Xs,

331
00:20:13,080 --> 00:20:14,290
but you can see that,

332
00:20:15,030 --> 00:20:19,130
because the one on the right hand side is slightly rotated to some degree,

333
00:20:19,130 --> 00:20:21,620
it's not going to directly align with the X on the left hand side,

334
00:20:21,620 --> 00:20:22,750
even though it is an X,

335
00:20:22,980 --> 00:20:25,450
we want to detect X in both of these images,

336
00:20:25,920 --> 00:20:30,310
so we need to think about how we can detect those features that define an X a bit more cleverly.

337
00:20:30,660 --> 00:20:33,220
So let's see how we can use convolutions to do that,

338
00:20:33,690 --> 00:20:34,990
so in this case for example,

339
00:20:35,280 --> 00:20:41,590
instead we want our model to compare images of this X piece by piece or patch by patch,

340
00:20:42,420 --> 00:20:48,130
and the important patches that we look for are exactly these features that will define our X,

341
00:20:48,660 --> 00:20:54,690
so if our model can find these rough feature patches roughly in the same position in our input,

342
00:20:54,860 --> 00:20:57,000
then we can determine or we can infer,

343
00:20:57,500 --> 00:21:03,265
that these two images are of the same type or the same letter, right,

344
00:21:03,265 --> 00:21:07,710
it can get a lot better than simply measuring the similarity between these two images,

345
00:21:08,000 --> 00:21:09,820
because we're operating at the patch level,

346
00:21:09,820 --> 00:21:13,915
so think of each patch almost like a miniature image, right,

347
00:21:13,915 --> 00:21:16,470
a small two dimensional array of values

348
00:21:16,580 --> 00:21:23,430
and we can use filters to pick up on when these small patches or small images occur,

349
00:21:23,990 --> 00:21:25,555
so in the case of Xs,

350
00:21:25,555 --> 00:21:28,770
these filters may represent semantic things,

351
00:21:28,820 --> 00:21:32,455
for example the diagonal lines or the crossings,

352
00:21:32,455 --> 00:21:35,340
that capture all of the important characteristics of the X,

353
00:21:36,260 --> 00:21:44,400
so we'll probably capture these features in the arms and the center of our letter in any image of an X,

354
00:21:44,480 --> 00:21:48,840
regardless of how that image is, you know, translated or rotated or so on,

355
00:21:49,070 --> 00:21:52,480
and note that even in these smaller matrices, right,

356
00:21:52,480 --> 00:21:55,370
these are filters of weights, right,

357
00:21:55,370 --> 00:22:00,890
these are also just numerical values of each pixel in these many patches

358
00:22:00,890 --> 00:22:02,380
is simply just a numerical value,

359
00:22:02,430 --> 00:22:05,350
there are also images in some effect, right,

360
00:22:05,610 --> 00:22:08,470
and all that's really left in this problem,

361
00:22:09,000 --> 00:22:11,410
and in this idea that we're discussing

362
00:22:11,940 --> 00:22:13,870
is to define that operation,

363
00:22:14,430 --> 00:22:16,570
that can take these miniature patches

364
00:22:16,890 --> 00:22:19,450
and try to pick up, you know, detect,

365
00:22:19,470 --> 00:22:23,080
when those patches occur in your image and when they maybe don't occur.

366
00:22:24,730 --> 00:22:28,370
And that brings us right back to this notion of convolution,

367
00:22:28,480 --> 00:22:31,790
so convolution is exactly that operation that will solve that problem,

368
00:22:32,230 --> 00:22:36,080
convolution preserves all of that spatial information in our input

369
00:22:36,250 --> 00:22:42,135
by learning image features in those smaller squares of regions that preserve our input data.

370
00:22:42,135 --> 00:22:46,730
So just to give another concrete example to perform this operation,

371
00:22:47,290 --> 00:22:50,630
we need to do an element wise multiplication,

372
00:22:50,740 --> 00:22:57,165
between the filter matrix, those miniature patches, as well as the patch of our input image, right,

373
00:22:57,165 --> 00:22:59,100
so you have basically think of two patches,

374
00:22:59,100 --> 00:23:04,610
you have the weight matrix patch, the thing that you want to detect, which you can see on the top left hand here,

375
00:23:04,990 --> 00:23:11,090
and you also have the secondary patch, which is the thing that you are looking to compare it against in your input image,

376
00:23:11,710 --> 00:23:12,860
and the question is how,

377
00:23:13,060 --> 00:23:16,760
how similar are these two patches that you observe between them,

378
00:23:17,410 --> 00:23:22,065
so for example, this results in a three by three matrix,

379
00:23:22,065 --> 00:23:26,240
because you're doing an element wise multiplication between two small three by three matrices,

380
00:23:26,260 --> 00:23:29,360
you're going to be left with another three by three matrix,

381
00:23:29,920 --> 00:23:31,635
in this case, all of the matrix,

382
00:23:31,635 --> 00:23:36,080
all of the elements of this resulting matrix, you can see here are ones,

383
00:23:36,610 --> 00:23:44,520
because in every location in the filter and every location in the image patch we are perfectly matching,

384
00:23:44,520 --> 00:23:47,510
so when we do that element wise multiplication, we get ones everywhere,

385
00:23:48,190 --> 00:23:53,810
the last step is that we need to sum up the result of that matrix, that element wise multiplication,

386
00:23:54,580 --> 00:23:57,440
and the result is let's say 9, in this case,

387
00:23:58,060 --> 00:24:00,890
everything was a one, it's a three by three matrix, so the result is 9.

388
00:24:02,640 --> 00:24:06,225
Now, let's consider one more example, right,

389
00:24:06,225 --> 00:24:08,330
now we have this image in green,

390
00:24:08,410 --> 00:24:11,420
and we want to detect this filter in yellow,

391
00:24:12,640 --> 00:24:18,260
suppose we want to compute the convolution of this 5x5 image with this 3x3 filter,

392
00:24:18,850 --> 00:24:19,440
to do this,

393
00:24:19,440 --> 00:24:23,060
we need to cover basically the entirety of our image

394
00:24:23,290 --> 00:24:25,875
by sliding over this filter piece by piece

395
00:24:25,875 --> 00:24:31,070
and comparing the similarity or the convolution of this filter across the entire image,

396
00:24:32,180 --> 00:24:34,290
and we do that, again through the same mechanism,

397
00:24:34,640 --> 00:24:35,700
at every location,

398
00:24:35,750 --> 00:24:40,020
we compute an element wise multiplication of that patch with that location on the image,

399
00:24:40,220 --> 00:24:42,030
add up all of the resulting entries

400
00:24:42,320 --> 00:24:44,940
and pass that to our next layer.

401
00:24:45,320 --> 00:24:46,360
So let's walk through it,

402
00:24:46,360 --> 00:24:48,990
first, let's start off in the upper left hand corner,

403
00:24:49,550 --> 00:24:53,190
we place our filter over the upper left hand corner of our image,

404
00:24:53,420 --> 00:24:54,700
we element wise multiply,

405
00:24:54,700 --> 00:24:56,460
we add up all the results

406
00:24:56,480 --> 00:24:57,300
and we get 4

407
00:24:57,440 --> 00:25:00,930
and that four is going to be placed into the next layer,

408
00:25:01,250 --> 00:25:03,600
this next layer again is another image,

409
00:25:04,010 --> 00:25:07,170
but it's determined as the result of our convolution operation,

410
00:25:07,790 --> 00:25:10,230
we slide over that filter to the next location,

411
00:25:10,430 --> 00:25:13,750
the next location provides the next value in our image

412
00:25:13,750 --> 00:25:16,950
and we keep repeating this process over and over and over again

413
00:25:17,330 --> 00:25:20,400
until we've covered our filter over the entire image,

414
00:25:20,900 --> 00:25:26,760
and as a result, we've also completely filled out the result of our output feature map,

415
00:25:26,870 --> 00:25:29,370
the output feature map is basically what you can think of is

416
00:25:29,480 --> 00:25:33,960
how closely aligned our filter is to every location in our input image.

417
00:25:35,810 --> 00:25:40,860
So now that we've kind of gone through the mechanism that defines this operation of convolution,

418
00:25:41,480 --> 00:25:43,290
let's see how different filters

419
00:25:43,820 --> 00:25:47,970
could be used to detect different types of patterns in our data,

420
00:25:48,440 --> 00:25:51,120
so for example, let's take this picture of a woman's face

421
00:25:52,130 --> 00:25:58,030
and the output of applying three different types of filters to this picture, right,

422
00:25:58,030 --> 00:25:59,640
so you can see the exact filter,

423
00:26:00,200 --> 00:26:01,615
they're all 3x3 filters,

424
00:26:01,615 --> 00:26:06,060
so the exact filters you can see on the bottom right hand corner of the corresponding face,

425
00:26:06,500 --> 00:26:08,380
and by applying these three different filters,

426
00:26:08,380 --> 00:26:11,035
you can see how we can achieve drastically different results,

427
00:26:11,035 --> 00:26:15,690
and simply by changing the weights that are present in these 3x3 matrices,

428
00:26:15,950 --> 00:26:20,070
you can see the variability of different types of features that we can detect,

429
00:26:20,120 --> 00:26:24,420
so for example, we can design filters that can sharpen an image,

430
00:26:24,650 --> 00:26:27,600
make the edges sharper in the image,

431
00:26:27,710 --> 00:26:30,300
we can design filters that will extract edges,

432
00:26:30,740 --> 00:26:32,980
we can do stronger edge detection

433
00:26:32,980 --> 00:26:35,820
by again modifying the weights in all of those filters.

434
00:26:37,420 --> 00:26:40,730
So I hope now that all of you can kind of appreciate the power of,

435
00:26:40,780 --> 00:26:43,470
you know, number one is these filtering operations

436
00:26:43,470 --> 00:26:46,160
and how we can define them mathematically

437
00:26:46,300 --> 00:26:51,020
in the form of these smaller patch based operations and matrices,

438
00:26:51,160 --> 00:26:52,730
that we can then slide over an image,

439
00:26:53,200 --> 00:26:55,190
and these concepts are so powerful,

440
00:26:55,270 --> 00:26:59,660
because number one, they preserve the spatial information of our original input,

441
00:26:59,920 --> 00:27:02,870
while still performing this feature extraction,

442
00:27:03,520 --> 00:27:04,830
now you can think of,

443
00:27:05,000 --> 00:27:08,380
instead of defining those filters, like we said on the previous slide,

444
00:27:08,380 --> 00:27:09,820
what if we tried to learn them,

445
00:27:09,820 --> 00:27:14,605
and remember again, that those filters are kind of proxies for important patterns in our data,

446
00:27:14,605 --> 00:27:19,770
so our neural network could try to learn those elements of those small patch filters

447
00:27:19,970 --> 00:27:21,790
as weights in the neural network

448
00:27:21,790 --> 00:27:25,380
and learning those would essentially equate to

449
00:27:25,400 --> 00:27:29,310
picking up and learning the patterns that define one class versus another class.

450
00:27:30,900 --> 00:27:35,590
And now that we've gotten this operation and this understanding under our belt,

451
00:27:35,640 --> 00:27:37,180
we can take this one step further,

452
00:27:37,470 --> 00:27:40,150
we can take this singular convolution operation

453
00:27:40,470 --> 00:27:46,210
and start to think about how we can build entire layers, convolutional layers, out of this operation,

454
00:27:46,230 --> 00:27:50,170
so that we can start to even imagine convolutional networks and neural networks.

455
00:27:50,990 --> 00:27:53,610
And first we'll take a look at, you know, what are called,

456
00:27:54,980 --> 00:28:00,085
well, what you ultimately create by creating convolutional layers and convolutional networks is

457
00:28:00,085 --> 00:28:02,310
what's called a CNN, a convolutional neural network.

458
00:28:03,020 --> 00:28:06,210
And that's going to be the core architecture of today's class,

459
00:28:06,620 --> 00:28:08,845
so let's consider a very simple CNN,

460
00:28:08,845 --> 00:28:11,190
that was designed for image classification,

461
00:28:11,750 --> 00:28:16,440
the task here again is to learn the features directly from the raw data

462
00:28:17,120 --> 00:28:23,250
and use these learned features for classification towards some task of object detection that we want to perform.

463
00:28:24,650 --> 00:28:27,450
Now there are three main operations to a CNN

464
00:28:27,530 --> 00:28:29,580
and we'll go through them step by step here,

465
00:28:29,630 --> 00:28:34,495
but then go deeper into each of them in the remainder of this class.

466
00:28:34,495 --> 00:28:36,180
So the first step is convolutions,

467
00:28:36,260 --> 00:28:39,480
which we've already seen a lot of in today's class already,

468
00:28:39,740 --> 00:28:43,110
convolutions are used to generate these feature maps,

469
00:28:43,220 --> 00:28:45,420
so they take us input both the previous image

470
00:28:45,860 --> 00:28:48,940
as well as some filter that they want to detect,

471
00:28:48,940 --> 00:28:55,140
and they output a feature map of how this filter is related to the original image.

472
00:28:55,710 --> 00:28:57,700
The second step is like yesterday,

473
00:28:58,020 --> 00:29:01,030
applying a non-linearity to the result of these feature maps,

474
00:29:01,410 --> 00:29:05,140
that injects some non-linear activations to our neural networks,

475
00:29:05,250 --> 00:29:07,060
allows it to deal with non-linear data.

476
00:29:07,890 --> 00:29:09,460
Third step is pooling,

477
00:29:09,570 --> 00:29:11,770
which is essentially a down sampling operation

478
00:29:12,240 --> 00:29:16,870
to allow our images, allow our networks to deal with larger and larger scale images

479
00:29:17,070 --> 00:29:19,870
by progressively down scaling their size,

480
00:29:19,980 --> 00:29:23,650
so that our filters can progressively grow in receptive field.

481
00:29:25,000 --> 00:29:30,140
And finally, feeding all of these resulting features to some neural network

482
00:29:30,880 --> 00:29:32,750
to infer the class scores,

483
00:29:33,010 --> 00:29:37,220
now, by the time that we get to this fully connected layer,

484
00:29:37,630 --> 00:29:39,650
remember that we've already extracted our features

485
00:29:40,180 --> 00:29:43,670
and essentially you can think of this no longer being a two dimensional image,

486
00:29:44,050 --> 00:29:47,570
we can now use the methods that we learned about in lecture one

487
00:29:47,620 --> 00:29:52,040
to directly take those learned features that the neural network has detected

488
00:29:52,420 --> 00:29:54,750
and infer based on those learned features

489
00:29:54,750 --> 00:29:57,380
and based on if they were detected or if they were not,

490
00:29:57,610 --> 00:29:58,700
what class we're in.

491
00:29:59,890 --> 00:30:05,280
So now let's basically just go through each of these operations one by one in a bit more detail

492
00:30:05,280 --> 00:30:10,070
and see how we could even build up this very basic architecture of a CNN.

493
00:30:10,910 --> 00:30:14,940
So first, let's go back and consider one more time the convolution operation,

494
00:30:15,020 --> 00:30:17,850
that's central, central core to the CNN,

495
00:30:18,500 --> 00:30:21,930
and as before, each neuron in this hidden layer is going to be

496
00:30:22,100 --> 00:30:25,470
computed as a weighted sum of its inputs,

497
00:30:26,120 --> 00:30:29,710
applying a bias and activating with a non-linearity,

498
00:30:29,710 --> 00:30:33,475
should sound very similar to lecture one in yesterday's class,

499
00:30:33,475 --> 00:30:34,320
but except now,

500
00:30:34,700 --> 00:30:36,510
when we're going to do that first step,

501
00:30:37,040 --> 00:30:39,400
instead of just doing a dot product with our weights,

502
00:30:39,400 --> 00:30:41,755
we're going to apply a convolution with our weights,

503
00:30:41,755 --> 00:30:45,290
which is simply that element wise multiplication and addition, right,

504
00:30:45,290 --> 00:30:46,600
and that sliding operation.

505
00:30:47,370 --> 00:30:49,780
Now, what's really special here

506
00:30:49,890 --> 00:30:53,770
and what I really want to stress is the local connectivity,

507
00:30:54,150 --> 00:30:58,450
every single neuron in this hidden layer only sees

508
00:30:58,590 --> 00:31:02,105
a certain patch of inputs in its previous layer,

509
00:31:02,105 --> 00:31:05,320
so if I point at just this one neuron in the output layer,

510
00:31:05,820 --> 00:31:10,265
this neuron only sees the inputs at this red square,

511
00:31:10,265 --> 00:31:13,240
it doesn't see any of the other inputs in the rest of the image,

512
00:31:14,030 --> 00:31:19,050
and that's really important to be able to scale these models to very large scale images,

513
00:31:19,520 --> 00:31:20,590
now, you can imagine that,

514
00:31:20,590 --> 00:31:22,350
as you go deeper and deeper into your network,

515
00:31:22,730 --> 00:31:26,760
eventually, because the next layer you're going to attend to a larger patch,

516
00:31:27,110 --> 00:31:30,540
and that will include data from not only this red square,

517
00:31:30,680 --> 00:31:34,470
but effectively a much larger red square that you could imagine there.

518
00:31:36,780 --> 00:31:40,450
Now let's define this actual computation that's going on,

519
00:31:40,770 --> 00:31:42,500
for a neuron in a hidden layer,

520
00:31:42,500 --> 00:31:47,740
its inputs are those neurons that fell within its patch in the previous layer,

521
00:31:47,880 --> 00:31:52,685
we can apply this matrix of weights here, denoted as a 4x4 filter,

522
00:31:52,685 --> 00:31:54,280
that you can see on the left hand side,

523
00:31:54,630 --> 00:31:57,160
and in this case we do an element wise multiplication,

524
00:31:58,020 --> 00:32:02,140
we add the outputs, we apply a bias and we add that non-linearity,

525
00:32:02,880 --> 00:32:06,845
that's the the core steps that we take in really all of these neural networks,

526
00:32:06,845 --> 00:32:10,900
that you're learning about in today's and this week's class, to be honest.

527
00:32:11,640 --> 00:32:15,820
Now remember that this element wise multiplication and addition operation,

528
00:32:15,960 --> 00:32:18,230
that's sliding operation, that's called convolution

529
00:32:18,230 --> 00:32:20,770
and that's the basis of these layers.

530
00:32:21,180 --> 00:32:25,130
So that defines how neurons in convolutional layers are connected,

531
00:32:25,130 --> 00:32:26,770
how they're mathematically formulated,

532
00:32:27,180 --> 00:32:29,630
but within a single convolutional layer,

533
00:32:29,630 --> 00:32:32,380
it's also really important to understand,

534
00:32:32,820 --> 00:32:37,480
that a single layer could actually try to detect multiple sets of filters,

535
00:32:37,770 --> 00:32:41,740
maybe you want to detect in one image multiple features, not just one feature,

536
00:32:41,760 --> 00:32:44,225
but you know if you are detecting faces,

537
00:32:44,225 --> 00:32:46,150
you don't only want to detect eyes,

538
00:32:46,260 --> 00:32:49,780
you want to detect, you know eyes, noses, mouths, ears,

539
00:32:50,040 --> 00:32:53,650
all of those things are critical patterns that define a face

540
00:32:53,700 --> 00:32:55,360
and can help you classify a face.

541
00:32:56,580 --> 00:32:58,960
So what we need to think of is actually,

542
00:32:59,010 --> 00:33:04,030
convolution operations that can output a volume of different images, right,

543
00:33:04,110 --> 00:33:08,650
every slice of this volume effectively denotes a different filter,

544
00:33:08,910 --> 00:33:12,160
that can be identified in our original input,

545
00:33:12,330 --> 00:33:19,300
and each of those filters is going to basically correspond to a specific pattern or feature in our image as well,

546
00:33:20,050 --> 00:33:25,130
think of the connections in these neurons, in terms of, you know, their receptive field once again,

547
00:33:25,600 --> 00:33:31,550
the locations within the input of that node that they were connected to in the previous layer,

548
00:33:31,930 --> 00:33:33,650
these parameters really define,

549
00:33:34,480 --> 00:33:38,120
what I like to think of as the spatial arrangement of information,

550
00:33:38,530 --> 00:33:42,950
that propagates throughout the network and throughout the convolutional layers in particular.

551
00:33:44,060 --> 00:33:47,260
Now, I think just to summarize what we've seen

552
00:33:47,260 --> 00:33:51,480
and how connections in these types of neural networks are defined,

553
00:33:52,130 --> 00:33:58,800
and let's say how the, how the output of a convolutional network is a volume,

554
00:33:59,750 --> 00:34:05,875
we are well on our way to really understanding, you know, convolutional neural networks and defining them, right,

555
00:34:05,875 --> 00:34:09,810
that's the, what we just covered is really the main component of CNNs,

556
00:34:10,250 --> 00:34:13,540
that's the convolutional operation that defines these convolutional layers,

557
00:34:13,540 --> 00:34:16,195
the remaining steps are very critical as well,

558
00:34:16,195 --> 00:34:18,250
but I want to maybe pause for a second

559
00:34:18,250 --> 00:34:23,970
and make sure that everyone's on the same page with the convolutional operation and the definition of convolutional layers.

560
00:34:28,420 --> 00:34:28,820
Awesome.

561
00:34:28,870 --> 00:34:31,380
Okay, so the next step here is

562
00:34:31,380 --> 00:34:35,540
to take those resulting feature maps that our convolutional layers extract

563
00:34:35,830 --> 00:34:40,320
and apply a non-linearity to the output volume of the convolutional layer,

564
00:34:40,320 --> 00:34:42,020
so as we discussed in the first lecture,

565
00:34:42,220 --> 00:34:44,300
applying these non-linearities is really critical,

566
00:34:44,470 --> 00:34:47,625
because it allows us to deal with non-linear data

567
00:34:47,625 --> 00:34:51,380
and because image data in particular is extremely non-linear,

568
00:34:51,700 --> 00:34:59,030
that's a, you know, a critical component of what makes convolutional neural networks actually operational in practice,

569
00:35:00,280 --> 00:35:02,510
in particular, for convolutional neural networks,

570
00:35:02,650 --> 00:35:09,200
the activation function that is really, really common for these models is the ReLU activation function,

571
00:35:10,300 --> 00:35:13,130
we talked a little bit about this in lecture one and two yesterday,

572
00:35:13,420 --> 00:35:16,370
the ReLU activation function, you can see it on the right hand side,

573
00:35:16,750 --> 00:35:19,310
think of this function as a pixel by pixel operation,

574
00:35:19,330 --> 00:35:22,790
that replaces basically all negative values with zero,

575
00:35:22,840 --> 00:35:24,770
it keeps all positive values the same,

576
00:35:25,090 --> 00:35:27,530
it's the identity function when a value is positive,

577
00:35:27,970 --> 00:35:31,460
but when it's negative, it basically squashes everything back up to zero,

578
00:35:32,430 --> 00:35:34,535
think of this almost as a thresholding function, right,

579
00:35:34,535 --> 00:35:38,440
thresholds is everything at zero, anything less than zero comes back up to zero,

580
00:35:38,880 --> 00:35:45,640
so negative values here indicate basically a negative detection in convolution,

581
00:35:46,230 --> 00:35:49,055
that you may want to just say was no detection, right,

582
00:35:49,055 --> 00:35:52,835
and you can think of that as kind of an intuitive mechanism for understanding

583
00:35:52,835 --> 00:35:57,130
why the ReLU activation functions is so popular in convolutional neural networks,

584
00:35:57,180 --> 00:36:02,920
the other, the other popular belief is that ReLU activation functions,

585
00:36:03,210 --> 00:36:04,070
well, it's not a belief,

586
00:36:04,070 --> 00:36:08,050
they are extremely easy to compute and they're very easy and computationally efficient,

587
00:36:08,310 --> 00:36:10,130
their gradients are very cleanly defined,

588
00:36:10,130 --> 00:36:14,380
they're constants except for a piece wise non-nonlinearity,

589
00:36:15,210 --> 00:36:17,440
so that makes them very popular for these domains.

590
00:36:19,810 --> 00:36:23,870
Now the next key operation in a CNN is that of pooling,

591
00:36:24,130 --> 00:36:27,045
now pooling is an operation that is at its core,

592
00:36:27,045 --> 00:36:28,220
it serves one purpose

593
00:36:28,240 --> 00:36:31,430
and that is to reduce the dimensionality of the image

594
00:36:31,570 --> 00:36:35,660
progressively as you go deeper and deeper through your convolutional layers,

595
00:36:35,980 --> 00:36:38,550
now you can really start to reason about this,

596
00:36:38,550 --> 00:36:42,350
is that when you decrease the dimensionality of your features,

597
00:36:42,760 --> 00:36:47,355
you're effectively increasing the dimensionality of your filters, right,

598
00:36:47,355 --> 00:36:50,090
now because every filter that you slide over a smaller image

599
00:36:50,410 --> 00:36:55,490
is capturing a larger receptive field that occurred previously in that network,

600
00:36:56,200 --> 00:36:58,515
so a very common technique for pooling is

601
00:36:58,515 --> 00:37:01,190
what's called maximum pooling or max pooling for short,

602
00:37:01,570 --> 00:37:04,335
max pooling is exactly, you know what it sounds like,

603
00:37:04,335 --> 00:37:06,950
so it basically operates with these small patches again,

604
00:37:07,120 --> 00:37:08,330
that slide over an image,

605
00:37:08,710 --> 00:37:10,970
but instead of doing this convolution operation,

606
00:37:11,290 --> 00:37:15,170
what these patches will do is simply take the maximum of that patch location,

607
00:37:15,520 --> 00:37:20,420
so think of this as kind of activating the maximum value that comes from that location,

608
00:37:21,520 --> 00:37:23,510
and propagating only the maximums,

609
00:37:24,220 --> 00:37:27,710
I encourage all of you actually to think of maybe brainstorm other ways,

610
00:37:27,790 --> 00:37:32,000
that we could perform even better pooling operations than max pooling,

611
00:37:32,020 --> 00:37:33,350
there are many common ways,

612
00:37:33,370 --> 00:37:35,325
but you could think of some,

613
00:37:35,325 --> 00:37:38,450
for example, or are mean pooling or average pooling,

614
00:37:38,530 --> 00:37:40,250
maybe you don't want to just take the maximum,

615
00:37:40,390 --> 00:37:47,930
you could collapse basically the average of all of these pixels into your into a single value in the result,

616
00:37:48,850 --> 00:37:52,950
but these are the key operations of convolutional neural networks at their core,

617
00:37:52,950 --> 00:37:56,040
and now we're ready to really start to put them together

618
00:37:56,040 --> 00:38:00,570
and form and construct CNN all the way from the ground up,

619
00:38:00,570 --> 00:38:04,340
and with CNNs, we can layer these operations one after the other,

620
00:38:04,750 --> 00:38:07,395
starting first with convolutions non-linearities

621
00:38:07,395 --> 00:38:10,425
and then pooling and repeating these over and over again

622
00:38:10,425 --> 00:38:12,770
to learn these hierarchies of features,

623
00:38:12,850 --> 00:38:15,480
and that's exactly how we obtained pictures like this,

624
00:38:15,480 --> 00:38:17,540
which we started yesterday's lecture with,

625
00:38:17,740 --> 00:38:20,720
and learning these hierarchical decompositions of features

626
00:38:21,010 --> 00:38:25,010
by progressively stacking and stacking these filters on top of each other,

627
00:38:25,030 --> 00:38:29,000
each filter could then use all of the previous filters that it had learned.

628
00:38:30,780 --> 00:38:36,220
So a CNN built for image classification can be really broken down into two parts,

629
00:38:36,240 --> 00:38:38,110
first is the feature learning pipeline,

630
00:38:38,160 --> 00:38:41,260
which we learn the features that we want to detect,

631
00:38:41,430 --> 00:38:45,790
and then the second part is actually detecting those features and doing the classification.

632
00:38:48,010 --> 00:38:53,360
Now the convolutional and pooling layers output from the first part of that model,

633
00:38:53,740 --> 00:38:56,040
the goal of those convolutional and pooling layers is

634
00:38:56,040 --> 00:39:01,215
to output the high level features that are extracted from our input,

635
00:39:01,215 --> 00:39:05,355
but the next step is to actually use those features and detect their presence

636
00:39:05,355 --> 00:39:06,710
in order to classify the image.

637
00:39:07,300 --> 00:39:14,010
So we can feed these output features into the fully connected layers that we learned about in lecture one,

638
00:39:14,010 --> 00:39:17,450
because these are now just a one dimensional array of features

639
00:39:17,980 --> 00:39:21,270
and we can use those to detect, you know, what class we're in,

640
00:39:21,270 --> 00:39:25,910
and we can do this by using a function called a softmax function,

641
00:39:26,640 --> 00:39:30,020
you can think of a softmax function as simply a normalizing function,

642
00:39:30,100 --> 00:39:34,430
whose output represents that of a categorical probability distribution,

643
00:39:34,660 --> 00:39:36,810
so another way to think of this is

644
00:39:36,810 --> 00:39:39,860
basically if you have an array of numbers, you want to collapse

645
00:39:40,000 --> 00:39:42,080
and those numbers could take any real number form,

646
00:39:42,340 --> 00:39:44,835
you want to collapse that into some probability distribution,

647
00:39:44,835 --> 00:39:46,880
probability distribution has several properties,

648
00:39:47,620 --> 00:39:50,120
namely that all of its values have to sum to one,

649
00:39:50,470 --> 00:39:53,000
it always has to be between zero and one as well,

650
00:39:53,470 --> 00:39:57,465
so maintaining those two properties is what a softmax operation does,

651
00:39:57,465 --> 00:39:58,970
you can see its equation right here,

652
00:39:59,140 --> 00:40:01,370
it effectively just makes everything positive

653
00:40:01,510 --> 00:40:04,290
and then it normalizes the result across each other

654
00:40:04,290 --> 00:40:06,740
and that maintains those two properties that I just mentioned.

655
00:40:08,570 --> 00:40:10,470
Great, so let's put all of this together

656
00:40:10,490 --> 00:40:14,640
and actually see how we could program our first convolutional neural network

657
00:40:14,660 --> 00:40:16,960
end to end entirely from scratch.

658
00:40:16,960 --> 00:40:21,420
So let's start by firstly defining our feature extraction head,

659
00:40:22,280 --> 00:40:24,360
which starts with a convolutional layer

660
00:40:24,410 --> 00:40:27,690
and here 32 filters or 32 features,

661
00:40:27,800 --> 00:40:31,770
you can imagine that this first layer, the result of this first layer is to learn,

662
00:40:31,850 --> 00:40:36,270
not one filter, not one pattern in our image, but 32 patterns,

663
00:40:36,710 --> 00:40:41,670
okay, So those 32 results are going to then be passed to a pooling layer

664
00:40:41,960 --> 00:40:46,380
and then passed on to the next set of convolutional operations,

665
00:40:46,940 --> 00:40:50,575
the next set of convolutional operations now will contain 64 features

666
00:40:50,575 --> 00:40:57,000
will keep progressively growing and expanding our set of patterns that we're identifying in this image,

667
00:40:58,330 --> 00:41:02,510
next, we can finally flatten those resulting features that we've identified

668
00:41:02,860 --> 00:41:05,235
and feed all of this through our dense layers,

669
00:41:05,235 --> 00:41:07,670
are fully connected layers that we learned about in lecture one,

670
00:41:07,990 --> 00:41:11,505
these will allow us to predict those final, let's say ten classes,

671
00:41:11,505 --> 00:41:15,110
if we have ten different final possible classes in our image,

672
00:41:15,280 --> 00:41:17,370
this layer will account for that

673
00:41:17,370 --> 00:41:22,940
and allow us to output using softmax the probability distribution across those ten classes.

674
00:41:24,910 --> 00:41:26,240
So, so far we've talked about,

675
00:41:26,860 --> 00:41:31,490
how we can, let's say use CNNs to perform image classification tasks,

676
00:41:31,570 --> 00:41:36,080
but in reality, one thing I really want to stress in today's class, especially towards the end,

677
00:41:36,370 --> 00:41:42,200
is that this same architecture and same building blocks that we've talked about so far are extensible

678
00:41:42,280 --> 00:41:49,280
and they extend to so many different applications and model types, that we can imagine,

679
00:41:49,420 --> 00:41:53,480
so for example, when we considered the CNN for classification,

680
00:41:53,980 --> 00:41:55,910
we saw that it really had two parts,

681
00:41:56,140 --> 00:41:59,540
the first part being feature extraction, learning what features to look for,

682
00:41:59,620 --> 00:42:03,590
and the second part being the classification, the detection of those features.

683
00:42:05,060 --> 00:42:09,900
Now, what makes a convolutional neural network really, really powerful is

684
00:42:10,580 --> 00:42:14,850
exactly the observation that the feature learning part,

685
00:42:14,930 --> 00:42:19,740
this first part of the neural network is extremely flexible,

686
00:42:20,000 --> 00:42:22,350
you can take that first part of the neural network,

687
00:42:22,730 --> 00:42:24,360
chop off what comes after it,

688
00:42:24,440 --> 00:42:27,870
and put a bunch of different heads into the part that comes after,

689
00:42:27,950 --> 00:42:30,780
the goal of the first part is to extract those features,

690
00:42:31,130 --> 00:42:33,880
what you do with the features is entirely up to you,

691
00:42:33,880 --> 00:42:39,840
but you can still leverage the flexibility and the power of the first part to learn all of those core features,

692
00:42:39,860 --> 00:42:45,720
so for example, that portion will look for, you know, all of the different image classification domains,

693
00:42:45,890 --> 00:42:48,270
that future portion after you've extracted the features,

694
00:42:48,350 --> 00:42:51,100
or we can also introduce new architectures,

695
00:42:51,100 --> 00:42:56,065
that take those features and maybe perform tasks like segmentation or image captioning,

696
00:42:56,065 --> 00:42:57,720
like we saw in yesterday's lecture.

697
00:42:59,230 --> 00:43:01,700
So in the case of classification, for example,

698
00:43:01,900 --> 00:43:04,550
just to tie up the, the classification story,

699
00:43:04,810 --> 00:43:09,030
there's a significant impact in domains like healthcare, medical decision making,

700
00:43:09,030 --> 00:43:13,250
where deep learning models are being applied to the analysis of medical scans

701
00:43:13,720 --> 00:43:16,640
across a whole host of different medical imagery.

702
00:43:18,530 --> 00:43:25,530
Now classification tells us basically a discrete prediction of what our image contains,

703
00:43:25,700 --> 00:43:29,190
but we can actually go much deeper into this problem as well,

704
00:43:29,270 --> 00:43:35,310
so for example, imagine that we're not trying to only identify that this image is an image of a taxi,

705
00:43:35,630 --> 00:43:36,690
which you can see here,

706
00:43:37,070 --> 00:43:38,370
but also more importantly,

707
00:43:38,600 --> 00:43:42,990
maybe we want our neural network to tell us not only that this is a taxi,

708
00:43:43,220 --> 00:43:49,290
but identify and draw a specific bounding box over this location of the taxi,

709
00:43:49,400 --> 00:43:51,150
so this is kind of a two phase problem,

710
00:43:51,440 --> 00:43:53,430
number one is that we need to draw a box

711
00:43:53,990 --> 00:43:56,610
and number two is we need to classify what was in that box,

712
00:43:56,990 --> 00:43:58,740
so it's both a regression problem,

713
00:43:59,090 --> 00:44:00,090
where is the box,

714
00:44:00,710 --> 00:44:05,730
that's a continuous problem as well as a classification problem is what is in that box.

715
00:44:06,350 --> 00:44:11,430
Now that's a much, much harder problem than what we've covered so far in the lecture today,

716
00:44:11,540 --> 00:44:14,845
because potentially there are many objects in our scene,

717
00:44:14,845 --> 00:44:15,900
not just one object,

718
00:44:16,130 --> 00:44:17,635
so we have to account for this fact,

719
00:44:17,635 --> 00:44:21,540
that maybe our could constrain arbitrarily many objects.

720
00:44:23,440 --> 00:44:27,950
Now our network needs to be flexible to that degree,

721
00:44:28,150 --> 00:44:32,115
needs to be able to infer a dynamic number of objects in the scene,

722
00:44:32,115 --> 00:44:34,530
and if the scene is only of a taxi,

723
00:44:34,530 --> 00:44:37,250
then it should only output, you know, that one bounding box,

724
00:44:37,480 --> 00:44:43,430
but on the other hand, if the image has many objects, potentially even of different classes,

725
00:44:43,480 --> 00:44:48,260
we need a model that can draw a bounding box for each of these different examples,

726
00:44:48,940 --> 00:44:54,530
as well as associate their predicted classification labels to each one independently.

727
00:44:56,230 --> 00:44:59,240
Now this is actually quite complicated in practice,

728
00:44:59,530 --> 00:45:01,640
because those boxes can be anywhere in the image,

729
00:45:01,930 --> 00:45:04,040
there's no constraints on where the boxes can be

730
00:45:04,390 --> 00:45:06,440
and they can also be of different sizes,

731
00:45:06,580 --> 00:45:08,840
they can be also of different ratios,

732
00:45:08,920 --> 00:45:10,490
some can be tall, some can be wide.

733
00:45:10,990 --> 00:45:14,360
Let's consider a very naive way of doing this first,

734
00:45:14,530 --> 00:45:19,280
let's take our image and start by placing a random box somewhere on that image,

735
00:45:19,390 --> 00:45:22,340
for example, we just pick a random location, a random size,

736
00:45:22,690 --> 00:45:24,110
we'll place a box right there,

737
00:45:24,790 --> 00:45:28,130
this box, like I said, has a random location, random size,

738
00:45:28,150 --> 00:45:29,360
then we can take that box

739
00:45:29,500 --> 00:45:33,225
and only feed that random box through our convolutional neural network

740
00:45:33,225 --> 00:45:36,260
which is trained to do classification, just classification,

741
00:45:37,950 --> 00:45:40,025
and this neural network can detect,

742
00:45:40,025 --> 00:45:44,105
well, number one, is there a class of object in that box or not,

743
00:45:44,105 --> 00:45:45,730
and if so, what class is it,

744
00:45:45,960 --> 00:45:47,030
and then what we could do is,

745
00:45:47,030 --> 00:45:50,375
we could just keep repeating this process over and over again,

746
00:45:50,375 --> 00:45:55,790
for all of these random boxes in our image, you know, many, many instances of random boxes,

747
00:45:55,790 --> 00:45:57,280
we keep sampling a new box,

748
00:45:57,780 --> 00:45:59,510
feed it through our convolutional neural network

749
00:45:59,510 --> 00:46:01,235
and ask this question what was in the box,

750
00:46:01,235 --> 00:46:03,730
if there was something in there, then what is it,

751
00:46:04,080 --> 00:46:09,100
and we keep moving on until we kind of have exhausted all of the the boxes in the image.

752
00:46:09,750 --> 00:46:10,745
But the problem here is that,

753
00:46:10,745 --> 00:46:14,390
there are just way too many potential inputs that we would have to deal with,

754
00:46:14,390 --> 00:46:18,700
this would be totally impractical to run in a real time system,

755
00:46:18,870 --> 00:46:20,530
for example with today's compute,

756
00:46:20,610 --> 00:46:22,390
it results in way too many scales,

757
00:46:22,590 --> 00:46:27,340
especially for the types of resolutions of images that we deal with today.

758
00:46:28,770 --> 00:46:30,370
So instead of picking random boxes,

759
00:46:30,540 --> 00:46:33,370
let's try and use a very simple heuristic, right,

760
00:46:33,810 --> 00:46:38,800
to identify maybe some places with lots of variability in the image,

761
00:46:38,820 --> 00:46:43,870
where there is high likelihood of having an objects might be present, right,

762
00:46:44,040 --> 00:46:47,210
these might have meaningful insights or meaningful objects,

763
00:46:47,210 --> 00:46:49,060
that could be available in our image,

764
00:46:49,350 --> 00:46:50,410
and we can use those

765
00:46:50,580 --> 00:46:56,090
to basically just feed in those high attention locations to our convolutional neural network,

766
00:46:56,090 --> 00:46:59,960
and then we can basically speed up that first part of the pipeline a lot,

767
00:46:59,960 --> 00:47:01,540
because now we're not just picking random boxes,

768
00:47:01,560 --> 00:47:06,250
maybe we use some simple heuristic to identify where interesting parts of the image might be,

769
00:47:06,630 --> 00:47:09,430
but still this is actually very slow, in practice,

770
00:47:09,690 --> 00:47:12,850
we have to feed in each region independently to the model,

771
00:47:13,260 --> 00:47:14,795
and plus it's very brittle,

772
00:47:14,795 --> 00:47:21,580
because ultimately the part of the model that is looking at where potential objects might be

773
00:47:21,630 --> 00:47:25,730
is detached from the part that's doing the detection of those objects,

774
00:47:25,730 --> 00:47:27,130
ideally, we want one model,

775
00:47:27,480 --> 00:47:33,730
that is able to both, you know, figure out where to attend to and do that classification afterwards.

776
00:47:35,570 --> 00:47:39,880
So there have been many variants that have been proposed in this field of object detection,

777
00:47:39,880 --> 00:47:42,000
but I want to, just for the purpose of today's class,

778
00:47:42,410 --> 00:47:45,360
introduce you to one of the most popular ones.

779
00:47:46,570 --> 00:47:51,920
Now, this is a point where this is a model called R-CNN or faster R-CNN,

780
00:47:52,390 --> 00:47:57,380
which actually attempts to learn not only how to classify these boxes,

781
00:47:57,850 --> 00:48:02,760
but learns how to propose those where those boxes might be in the first place,

782
00:48:02,760 --> 00:48:08,450
so that you could learn how to feed or where to feed into the downstream neural network.

783
00:48:09,160 --> 00:48:13,950
Now this means that we can feed in the image to what are called these region proposal networks,

784
00:48:14,300 --> 00:48:19,240
the goal of these networks is to propose certain regions in the image that you should attend to,

785
00:48:19,240 --> 00:48:23,310
and then feed just those regions into the downstream CNNs,

786
00:48:23,930 --> 00:48:25,540
so the goal here is

787
00:48:25,540 --> 00:48:30,330
to directly try to learn or extract all of those key regions

788
00:48:30,890 --> 00:48:33,235
and process them through the later part of the model,

789
00:48:33,235 --> 00:48:37,710
each of these regions are processed with their own independent feature extractors,

790
00:48:38,410 --> 00:48:41,220
and then a classifier can be used to aggregate them all

791
00:48:41,220 --> 00:48:44,990
and perform feature detection as well as object detection.

792
00:48:45,280 --> 00:48:50,510
Now, the beautiful thing about this is that this requires only a single pass through the network,

793
00:48:50,530 --> 00:48:52,395
so it's extraordinarily fast,

794
00:48:52,395 --> 00:48:54,590
it can easily run in real time,

795
00:48:54,640 --> 00:48:58,635
and is very commonly used in many industry applications as well,

796
00:48:58,635 --> 00:49:00,650
even if it can even run on your smartphone.

797
00:49:02,220 --> 00:49:03,310
So in classification,

798
00:49:03,690 --> 00:49:05,230
we just saw how we could predict,

799
00:49:05,880 --> 00:49:10,450
you know, not only a single image, a single object per image,

800
00:49:10,560 --> 00:49:12,100
we saw an object detection

801
00:49:12,480 --> 00:49:17,500
potentially inferring multiple objects with bounding boxes in your image,

802
00:49:18,210 --> 00:49:20,120
there's also one more type of task,

803
00:49:20,120 --> 00:49:21,110
which I want to point out,

804
00:49:21,110 --> 00:49:22,480
which is called segmentation.

805
00:49:22,950 --> 00:49:25,210
Segmentation is the task of classification,

806
00:49:25,680 --> 00:49:27,890
but now done at every single pixel,

807
00:49:27,890 --> 00:49:32,350
this takes the idea of object detection, which bounding boxes to the extreme,

808
00:49:32,790 --> 00:49:34,270
now, instead of drawing boxes,

809
00:49:34,470 --> 00:49:36,070
we're not even going to consider boxes,

810
00:49:36,180 --> 00:49:42,985
we're going to learn how to classify every single pixel in this image in isolation, right,

811
00:49:42,985 --> 00:49:45,960
so it's a huge number of classifications that we're going to do

812
00:49:46,160 --> 00:49:47,130
and we'll do this.

813
00:49:48,290 --> 00:49:49,735
Well, first let me show this example,

814
00:49:49,735 --> 00:49:51,940
so on the left hand side what this looks like,

815
00:49:51,940 --> 00:49:54,090
as you're feeding in an original RGB image,

816
00:49:54,230 --> 00:49:56,845
the goal of the right hand side is to learn,

817
00:49:56,845 --> 00:49:59,130
for every pixel in the left hand side,

818
00:49:59,510 --> 00:50:01,975
what was the class of that pixel, right,

819
00:50:01,975 --> 00:50:06,210
so this is kind of in contrast to just determining, you know, boxes over our image,

820
00:50:06,230 --> 00:50:08,280
now we're looking at every pixel in isolation

821
00:50:08,870 --> 00:50:10,110
and you can see for example,

822
00:50:10,460 --> 00:50:18,685
you know this pixels of the cow are clearly differentiated from the pixels of the sky or the pixels of the grass, right,

823
00:50:18,685 --> 00:50:23,580
and that's a key critical component of semantic segmentation networks,

824
00:50:24,290 --> 00:50:29,160
the output here is created by again using these convolutional operations,

825
00:50:29,840 --> 00:50:31,560
followed by pooling operations,

826
00:50:31,730 --> 00:50:35,460
which learn an encoder which you can think of on the left hand side,

827
00:50:35,750 --> 00:50:38,190
these are learning the features from our RGB image,

828
00:50:39,020 --> 00:50:41,040
learning how to put them into a space,

829
00:50:41,300 --> 00:50:45,450
so that it can reconstruct into a new space of semantic labels,

830
00:50:45,650 --> 00:50:47,875
so you can imagine kind of a down scaling

831
00:50:47,875 --> 00:50:51,190
and then progress of upsscaling into the semantic space,

832
00:50:51,540 --> 00:50:53,405
but when you do that upscaling,

833
00:50:53,405 --> 00:50:56,585
it's important, of course, you can't be pulling down that information,

834
00:50:56,585 --> 00:50:58,690
you need to kind of invert all of those operations,

835
00:50:59,280 --> 00:51:01,810
so instead of doing convolutions with pooling,

836
00:51:02,010 --> 00:51:06,550
you can now do convutions with basically reverse pooling or expansions,

837
00:51:07,050 --> 00:51:10,060
you can grow your feature sets at every labels.

838
00:51:11,010 --> 00:51:16,370
And here's an example on the bottom of just a code piece that actually defines these layers,

839
00:51:16,370 --> 00:51:19,840
you can plug these layers, combine them with convolutional layers

840
00:51:19,920 --> 00:51:24,040
and you can build these fully convolutional networks that can accomplish this type of task.

841
00:51:24,450 --> 00:51:28,040
Now of course, this can be applied in many other applications in health care as well,

842
00:51:28,040 --> 00:51:31,060
especially for segmenting out, let's say cancerous regions

843
00:51:31,590 --> 00:51:35,710
or even identifying parts of the blood which are infected with malaria, for example.

844
00:51:36,330 --> 00:51:39,340
And one final example here of self driving cars,

845
00:51:39,780 --> 00:51:43,120
let's say that we want to build a neural network for autonomous navigation,

846
00:51:43,410 --> 00:51:44,830
specifically building a model,

847
00:51:45,210 --> 00:51:47,050
let's say that can take as input an image

848
00:51:47,520 --> 00:51:52,640
as well as let's say some very coarse maps of where it thinks it is,

849
00:51:52,640 --> 00:51:57,250
think of this as basically a screenshot of, you know, the Google maps essentially

850
00:51:57,330 --> 00:51:58,550
to the neural network, right,

851
00:51:58,550 --> 00:52:01,000
it's the GPS location of the map.

852
00:52:01,890 --> 00:52:07,240
And it wants to directly infer not a classification or a semantic classification of the scene,

853
00:52:07,500 --> 00:52:15,080
but now directly infer the actuation how to drive and steer this car into into the future, right,

854
00:52:15,080 --> 00:52:20,510
now this is a full probability distribution over the entire space of control commands, right,

855
00:52:20,510 --> 00:52:23,020
it's a very large continuous probability space,

856
00:52:23,430 --> 00:52:26,440
and the question is how can we build a neural network to learn this function,

857
00:52:26,880 --> 00:52:28,565
and the key point that I'm stressing,

858
00:52:28,565 --> 00:52:31,340
with all of these different types of architectures here is,

859
00:52:31,340 --> 00:52:34,750
that all of these architectures use the exact same encoder,

860
00:52:34,770 --> 00:52:40,420
we haven't changed anything when going from classification to detection to semantic segmentation, and now to here,

861
00:52:40,710 --> 00:52:46,690
all of them are using the same underlying building blocks of convolutions, nonlinearities and pooling,

862
00:52:46,980 --> 00:52:48,185
the only difference is that,

863
00:52:48,185 --> 00:52:50,740
after we perform those feature extractions,

864
00:52:51,090 --> 00:52:54,185
how do we take those features and learn our ultimate tasks,

865
00:52:54,185 --> 00:52:57,670
so for example, in the case of probabilistic control commands,

866
00:52:57,900 --> 00:52:59,770
we would want to take those learned features

867
00:53:00,540 --> 00:53:06,910
and understand how to predict, you know, the parameters of a full continuous probability distribution,

868
00:53:07,140 --> 00:53:08,890
like you can see on the right hand side

869
00:53:09,300 --> 00:53:12,640
as well as the deterministic control of our desired destination,

870
00:53:13,170 --> 00:53:15,820
and again like we talked about at the very beginning of this class,

871
00:53:16,050 --> 00:53:21,100
this model which goes directly from images all the way to steering wheel angles

872
00:53:21,150 --> 00:53:23,540
essentially of the car is a single model,

873
00:53:23,540 --> 00:53:25,430
it's learned entirely end to end,

874
00:53:25,430 --> 00:53:31,420
we never told the car, for example, what a lane marker is or you know, the rules of the road,

875
00:53:31,800 --> 00:53:34,720
it was able to observe a lot of human driving data,

876
00:53:35,400 --> 00:53:37,090
extract these patterns, these features,

877
00:53:37,320 --> 00:53:40,930
from what makes a good human driver different from a bad human driver

878
00:53:41,460 --> 00:53:45,550
and learn how to imitate those same types of actions that are occurring,

879
00:53:45,990 --> 00:53:50,650
so that without any, you know, human intervention or human rules

880
00:53:50,850 --> 00:53:52,510
that we impose on these systems,

881
00:53:52,860 --> 00:53:55,325
they can simply watch all of this data

882
00:53:55,325 --> 00:53:57,610
and learn how to drive entirely from scratch.

883
00:53:58,120 --> 00:54:00,380
So a human, for example, can actually enter the car,

884
00:54:00,760 --> 00:54:01,880
input a desired destination,

885
00:54:03,190 --> 00:54:07,380
and this end to end CNN will actually actuate the control commands

886
00:54:07,380 --> 00:54:08,960
to bring them to their destination.

887
00:54:10,860 --> 00:54:12,190
I'll conclude today's lecture,

888
00:54:12,750 --> 00:54:16,720
with just saying that the applications of CNNs,

889
00:54:16,860 --> 00:54:18,410
we've touched on a few of them today,

890
00:54:18,410 --> 00:54:20,860
but the applications of CNNs are enormous,

891
00:54:21,390 --> 00:54:23,890
far beyond these examples that I provided today,

892
00:54:24,210 --> 00:54:29,320
they all tie back to this core concept of feature extraction and detection

893
00:54:29,610 --> 00:54:31,460
and after you do that feature extraction,

894
00:54:31,460 --> 00:54:33,970
you can really crop off the rest of your network

895
00:54:34,140 --> 00:54:35,885
and apply it to many different heads

896
00:54:35,885 --> 00:54:39,125
for many different tasks and applications that you might care about,

897
00:54:39,125 --> 00:54:40,220
we've touched on a few today,

898
00:54:40,220 --> 00:54:43,510
but there are really so, so many in different domains.

899
00:54:43,980 --> 00:54:45,275
And with that I'll conclude

900
00:54:45,275 --> 00:54:52,235
and very shortly we'll just be talking about generative modeling,

901
00:54:52,235 --> 00:54:58,810
which is a really central, really central part of today's and this week's lectures series,

902
00:54:59,460 --> 00:55:02,890
and after that, later on we'll have the software lab,

903
00:55:02,910 --> 00:55:06,160
which I'm excited for all of you to, to start participating in.

904
00:55:06,270 --> 00:55:08,650
And yeah, we can take a short five minute break

905
00:55:09,030 --> 00:55:10,840
and continue the lectures from there.

906
00:55:11,200 --> 00:55:11,610
Thank you.

