1
00:00:09,090 --> 00:00:10,745
0,380 380,760 810,1085 1085,1325 1325,1655
Hi, everyone, and welcome back
嗨，大家好，欢迎回到深度学习导论，

2
00:00:10,745 --> 00:00:12,070
0,345 345,675 675,885 885,1035 1035,1325
to introduction to deep learning,|
|

3
00:00:12,810 --> 00:00:14,080
0,245 245,335 335,440 440,700 870,1270
{we,had -} a really awesome
昨天我们有一个非常棒的开讲日，

4
00:00:14,100 --> 00:00:15,560
0,485 485,710 710,1000 1080,1325 1325,1460
kickoff day yesterday,| so we're
|所以我们希望在整个星期保持同样的势头，

5
00:00:15,560 --> 00:00:16,460
0,165 165,360 360,510 510,705 705,900
looking to keep that same

6
00:00:16,460 --> 00:00:17,915
0,290 520,840 840,1095 1095,1275 1275,1455
momentum all throughout the week|
|

7
00:00:17,915 --> 00:00:19,970
0,255 255,510 510,750 750,1055 1765,2055
and starting with today| and
从今天开始，|今天，我们很高兴能够讨论，

8
00:00:19,970 --> 00:00:21,245
0,225 225,480 480,690 690,1050 1050,1275
today, we're really excited to

9
00:00:21,245 --> 00:00:23,375
0,245 475,875 1075,1475 1765,2025 2025,2130
be talking about| actually one
|也是在这门课上我最喜欢的话题之一，

10
00:00:23,375 --> 00:00:24,620
0,105 105,285 285,570 570,935 985,1245
of my favorite topics in

11
00:00:24,620 --> 00:00:25,730
0,180 180,480 480,735 735,915 915,1110
this course,| which is how
|那就是我们如何制造能够实现视觉的计算机。

12
00:00:25,730 --> 00:00:27,910
0,165 165,315 315,590 1030,1430 1780,2180
we can build computers that

13
00:00:28,380 --> 00:00:29,705
0,350 350,605 605,800 800,1025 1025,1325
can achieve the sense of

14
00:00:29,705 --> 00:00:32,160
0,365 475,780 780,1085
sight and vision.|
|

15
00:00:32,320 --> 00:00:34,310
0,365 365,635 635,845 845,1150 1590,1990
Now, I believe that sight|
现在，我相信视觉，|

16
00:00:34,630 --> 00:00:36,285
0,400 450,850 1080,1340 1340,1475 1475,1655
and specifically, like I said,
具体地说，就像我说的，视觉是我们所有人最重要的人类感官之一，

17
00:00:36,285 --> 00:00:37,305
0,270 270,540 540,705 705,855 855,1020
vision is one of the

18
00:00:37,305 --> 00:00:39,585
0,275 325,725 745,1110 1110,1745 1915,2280
most important human senses that

19
00:00:39,585 --> 00:00:41,450
0,225 225,405 405,725 1315,1590 1590,1865
we all have,| {in,fact, -}
|事实上，有视力的人很大程度上依赖于视觉，

20
00:00:41,500 --> 00:00:43,160
0,455 455,730 900,1205 1205,1385 1385,1660
sighted people rely on vision

21
00:00:43,690 --> 00:00:44,685
0,275 275,410 410,650 650,875 875,995
quite a lot| in our
|在我们的日常生活中，

22
00:00:44,685 --> 00:00:45,890
0,180 180,330 330,465 465,755 805,1205
day to day lives,| from
|从四处走动，环游世界，

23
00:00:45,910 --> 00:00:48,975
0,400 450,850 930,1330 1380,1780 2430,3065
everything from walking around, navigating

24
00:00:48,975 --> 00:00:51,030
0,180 180,425 835,1365 1365,1590 1590,2055
the world,| interacting and sensing
|到与同事和同龄人互动和感知其他情感，

25
00:00:51,030 --> 00:00:52,910
0,290 730,1130 1180,1425 1425,1575 1575,1880
other emotions in our colleagues

26
00:00:52,930 --> 00:00:54,750
0,320 320,790 1020,1310 1310,1550 1550,1820
and peers.| And today we're
|今天，我们将学习如何使用深度学习和机器学习

27
00:00:54,750 --> 00:00:55,515
0,120 120,255 255,420 420,615 615,765
going to learn about how

28
00:00:55,515 --> 00:00:56,900
0,150 150,300 300,575 745,1065 1065,1385
we can use deep learning

29
00:00:57,010 --> 00:00:59,180
0,335 335,560 560,850 1590,1880 1880,2170
and machine learning| to build
|来构建强大的视觉系统，

30
00:00:59,260 --> 00:01:01,560
0,400 660,1040 1040,1420 1770,2075 2075,2300
powerful vision systems,| that can
|这些系统可以看到和预测什么是什么，

31
00:01:01,560 --> 00:01:04,005
0,300 300,680 850,1200 1200,1550 2170,2445
both see and predict what

32
00:01:04,005 --> 00:01:05,430
0,180 180,480 480,780 780,1065 1065,1425
is where| by only looking
|只需查看原始的视觉输入，

33
00:01:05,430 --> 00:01:08,100
0,380 460,810 810,1160 1600,2120 2410,2670
at raw {visual,inputs -}.| And
|我喜欢把这句话看作是

34
00:01:08,100 --> 00:01:08,850
0,135 135,300 300,450 450,585 585,750
I like to think of

35
00:01:08,850 --> 00:01:10,280
0,255 255,620 700,990 990,1155 1155,1430
that phrase| as a very
|对实现视觉真正意味着什么的一个非常简洁和甜蜜的定义。

36
00:01:10,450 --> 00:01:12,825
0,640 810,1190 1190,1475 1475,1780 2070,2375
concise and sweet definition of

37
00:01:12,825 --> 00:01:14,240
0,180 180,360 360,585 585,905 1015,1415
what it really means {to,achieve

38
00:01:14,770 --> 00:01:17,190
0,400 480,880 1470,1870 1980,2240 2240,2420
-} vision.| But at its
|但在其核心，视觉实际上远远不止是理解什么是在哪里，

39
00:01:17,190 --> 00:01:19,005
0,240 240,540 540,920 1300,1620 1620,1815
core, vision is actually so

40
00:01:19,005 --> 00:01:20,900
0,240 240,605 745,1050 1050,1355 1495,1895
much more than just {understanding,what

41
00:01:20,920 --> 00:01:22,755
0,275 275,470 470,790 1140,1540 1560,1835
-} is where,| it also
|它也更深入，

42
00:01:22,755 --> 00:01:24,105
0,195 195,450 450,785 865,1155 1155,1350
goes {much,deeper -},| takes this
|比如这个场景，

43
00:01:24,105 --> 00:01:25,275
0,210 210,420 420,725 745,1020 1020,1170
scene, for example,| we can
|我们可以建立计算机视觉系统，

44
00:01:25,275 --> 00:01:27,465
0,275 355,660 660,965 1345,1745 1885,2190
build computer vision systems| that
|当然，它可以识别这个环境中的所有对象，

45
00:01:27,465 --> 00:01:28,995
0,305 475,765 765,945 945,1235 1255,1530
can identify, of course, all

46
00:01:28,995 --> 00:01:30,765
0,135 135,375 375,755 1165,1485 1485,1770
of the objects in this

47
00:01:30,765 --> 00:01:32,450
0,365 565,915 915,1185 1185,1395 1395,1685
environment,| starting first with the
|首先从黄色的出租车或停在路边的货车开始，

48
00:01:32,740 --> 00:01:34,410
0,350 350,700 930,1220 1220,1415 1415,1670
yellow taxi or the Van

49
00:01:34,410 --> 00:01:35,220
0,315 315,420 420,555 555,690 690,810
parked on the side of

50
00:01:35,220 --> 00:01:36,960
0,120 120,380 1000,1305 1305,1545 1545,1740
the road,| {but,we -} also
|但我们也需要在更深的层面上了解这些物体，

51
00:01:36,960 --> 00:01:38,520
0,135 135,410 460,860 1030,1350 1350,1560
need to understand each of

52
00:01:38,520 --> 00:01:40,230
0,290 520,920 1120,1380 1380,1500 1500,1710
these objects at a much

53
00:01:40,230 --> 00:01:41,775
0,345 345,740 790,1095 1095,1305 1305,1545
deeper level,| not just where
|不仅是它们在哪里，

54
00:01:41,775 --> 00:01:43,550
0,195 195,450 450,840 840,1140 1140,1775
they are,| but actually predicting
|而且是预测未来，预测下一步可能发生的事情，

55
00:01:43,570 --> 00:01:45,075
0,275 275,545 545,1160 1160,1340 1340,1505
the future, predicting what may

56
00:01:45,075 --> 00:01:46,200
0,225 225,465 465,645 645,840 840,1125
happen in the scene next,|
|

57
00:01:46,200 --> 00:01:48,050
0,270 270,560 910,1310 1330,1590 1590,1850
for example, that the yellow
例如，黄色出租车未来更有可能移动和动态，

58
00:01:48,550 --> 00:01:49,965
0,380 380,650 650,830 830,1120 1170,1415
taxi is more likely to

59
00:01:49,965 --> 00:01:51,750
0,245 265,665 805,1200 1200,1530 1530,1785
be moving and dynamic into

60
00:01:51,750 --> 00:01:52,725
0,180 180,390 390,600 600,825 825,975
the future,| because it's in
|因为它位于车道中央，

61
00:01:52,725 --> 00:01:53,510
0,120 120,255 255,405 405,525 525,785
the middle of the lane,|
|

62
00:01:53,830 --> 00:01:55,110
0,305 305,470 470,575 575,820 900,1280
compared to the white Van
而不是停在路边的白色面包车，

63
00:01:55,110 --> 00:01:56,205
0,255 255,480 480,855 855,975 975,1095
which is parked on the

64
00:01:56,205 --> 00:01:57,300
0,120 120,210 210,315 315,575 775,1095
side of the road,| even
|即使你只看着一张图片，

65
00:01:57,300 --> 00:01:58,125
0,180 180,375 375,495 495,675 675,825
though you're just looking at

66
00:01:58,125 --> 00:01:59,880
0,135 135,405 405,785 1195,1500 1500,1755
a single image,| your brain
|你的大脑也可以推断出所有这些非常微妙的线索，

67
00:01:59,880 --> 00:02:01,470
0,315 315,680 910,1215 1215,1395 1395,1590
can infer all of these

68
00:02:01,470 --> 00:02:02,880
0,315 315,810 810,1200 1200,1305 1305,1410
very subtle cues| and it
|它一直延伸到道路上的行人，

69
00:02:02,880 --> 00:02:03,750
0,180 180,375 375,495 495,675 675,870
goes all the way to

70
00:02:03,750 --> 00:02:04,755
0,105 105,600 600,735 735,840 840,1005
the pedestrians on the road|
|

71
00:02:04,755 --> 00:02:06,195
0,165 165,360 360,695 835,1170 1170,1440
and even these even more
甚至红绿灯和场景中其他地方的更微妙的线索，

72
00:02:06,195 --> 00:02:07,935
0,345 345,815 985,1260 1260,1455 1455,1740
subtle cues in the traffic

73
00:02:07,935 --> 00:02:09,240
0,365 595,870 870,990 990,1140 1140,1305
lights and the rest of

74
00:02:09,240 --> 00:02:11,115
0,150 150,315 315,495 495,800 1480,1875
the scene {as,well -},| now
|现在，在场景中解释所有这些细节是一个非同寻常的挑战，

75
00:02:11,115 --> 00:02:12,380
0,395 415,675 675,825 825,990 990,1265
accounting for all of these

76
00:02:12,490 --> 00:02:13,815
0,380 380,620 620,770 770,1055 1055,1325
details in the scene is

77
00:02:13,815 --> 00:02:15,420
0,240 240,605 805,1170 1170,1440 1440,1605
an extraordinary challenge,| but we
|但作为人类，我们在一瞬间就做到了这一点，

78
00:02:15,420 --> 00:02:16,620
0,150 150,435 435,705 705,915 915,1200
as humans do this so

79
00:02:16,620 --> 00:02:18,360
0,750 750,1035 1035,1245 1245,1470 1470,1740
seamlessly within {a,split -} second,|
|

80
00:02:18,360 --> 00:02:19,500
0,240 240,480 480,705 705,900 900,1140
I probably put that frame
可能我把这个幻灯片的图片放上来，

81
00:02:19,500 --> 00:02:21,020
0,320 460,750 750,930 930,1170 1170,1520
up on the slide| and
|你们所以人一瞬间就可以推理出很多微秒的细节，

82
00:02:21,040 --> 00:02:21,960
0,275 275,410 410,545 545,725 725,920
all of you within a

83
00:02:21,960 --> 00:02:23,595
0,180 180,470 550,945 945,1320 1320,1635
split second could reason about

84
00:02:23,595 --> 00:02:24,980
0,225 225,375 375,570 570,1005 1005,1385
many of those subtle details|
|

85
00:02:25,180 --> 00:02:26,610
0,350 350,560 560,820 870,1220 1220,1430
without me {even,pointing -} them
在没有我指出的情况下，

86
00:02:26,610 --> 00:02:28,320
0,260 760,1080 1080,1260 1260,1470 1470,1710
out,| but the question of
|但今天课堂上的问题是，

87
00:02:28,320 --> 00:02:29,430
0,360 360,540 540,765 765,960 960,1110
today's class is,| how we
|我们如何构建机器学习和深度学习算法，

88
00:02:29,430 --> 00:02:31,020
0,150 150,440 910,1200 1200,1410 1410,1590
can build machine learning and

89
00:02:31,020 --> 00:02:32,740
0,135 135,410 490,1010 1300,1680
deep learning algorithms,| that
|可以实现对我们世界的同样类型和微妙的理解。

90
00:02:32,740 --> 00:02:34,260
0,195 195,495 495,780 780,1125 1125,1520
can achieve that same type

91
00:02:34,310 --> 00:02:36,895
0,400 780,1330 1650,2050 2160,2420 2420,2585
and subtle understanding of our

92
00:02:36,895 --> 00:02:39,450
0,305 1315,1605 1605,1800 1800,2105 2155,2555
world.| {And,deep -} learning in
|尤其是深度学习真正引领了这场计算机视觉革命，

93
00:02:39,470 --> 00:02:41,200
0,400 510,815 815,1085 1085,1445 1445,1730
particular is really leading this

94
00:02:41,200 --> 00:02:43,945
0,290 790,1190 1660,1980 1980,2300 2380,2745
revolution of computer vision| and
|并实现了计算机的现场定位，

95
00:02:43,945 --> 00:02:46,810
0,465 465,785 835,1230 1230,1625 2575,2865
achieving site of computers,| for
|例如，使机器人能够在其环境中捕捉到这些关键的视觉线索，

96
00:02:46,810 --> 00:02:49,030
0,290 520,900 900,1365 1365,1700 1960,2220
example, allowing robots to pick

97
00:02:49,030 --> 00:02:50,550
0,240 240,510 510,780 780,1140 1140,1520
up on these key visual

98
00:02:50,750 --> 00:02:53,425
0,530 530,755 755,995 995,1360 2280,2675
cues in their environment,| critical
|这对于真正与我们人类一起导航世界至关重要，

99
00:02:53,425 --> 00:02:54,990
0,270 270,495 495,1080 1080,1305 1305,1565
for really navigating the world

100
00:02:55,040 --> 00:02:56,580
0,335 335,670 720,1025 1025,1235 1235,1540
together with us as humans,|
|

101
00:02:56,960 --> 00:02:58,150
0,380 380,725 725,875 875,1070 1070,1190
these algorithms that you're going
你今天将要学习的这些算法已经变得如此主流，

102
00:02:58,150 --> 00:02:59,320
0,150 150,330 330,585 585,870 870,1170
to learn about today have

103
00:02:59,320 --> 00:03:00,925
0,315 315,600 600,1245 1245,1380 1380,1605
become so mainstreamed,| in fact,
|事实上，它们适用于你所有的智能手机，

104
00:03:00,925 --> 00:03:02,350
0,210 210,450 450,845 925,1230 1230,1425
that they're fitting on all

105
00:03:02,350 --> 00:03:03,850
0,150 150,330 330,890 1060,1335 1335,1500
of your smartphones,| in your
|放在你的口袋里，

106
00:03:03,850 --> 00:03:05,905
0,500 730,1130 1210,1545 1545,1785 1785,2055
pockets,| processing every single image
|处理你拍摄的每一张图像，

107
00:03:05,905 --> 00:03:07,390
0,225 225,390 390,695 835,1320 1320,1485
that you take,| enhancing those
|增强这些图像，检测人脸，等等。

108
00:03:07,390 --> 00:03:09,625
0,290 490,960 960,1250 1750,2055 2055,2235
images, detecting faces and so

109
00:03:09,625 --> 00:03:11,065
0,120 120,255 255,450 450,755 1165,1440
on and {so,forth -}.| And
|我们看到了一些令人兴奋的进步，

110
00:03:11,065 --> 00:03:12,565
0,195 195,345 345,570 570,905 925,1500
we're seeing some exciting advances

111
00:03:12,565 --> 00:03:13,465
0,375 375,495 495,600 600,735 735,900
ranging,| all the way from
|从生物学和医学，

112
00:03:13,465 --> 00:03:15,115
0,275 415,705 705,995 1165,1455 1455,1650
biology and medicine,| which we'll
|我们将在今天晚些时候讨论，

113
00:03:15,115 --> 00:03:15,955
0,150 150,315 315,420 420,555 555,840
talk about a bit later

114
00:03:15,955 --> 00:03:18,370
0,395 805,1035 1035,1515 1515,1805 2095,2415
today,| to autonomous driving and
|到自动驾驶和可访问性。

115
00:03:18,370 --> 00:03:20,360
0,590 730,1065 1065,1400
accessibility as well.|
|

116
00:03:20,630 --> 00:03:21,700
0,320 320,515 515,680 680,875 875,1070
And like I said,| deep
正如我所说的，|深度学习在过去十年左右的时间里像风暴一样席卷了整个领域，

117
00:03:21,700 --> 00:03:23,670
0,210 210,450 450,770 850,1250 1570,1970
learning has taken this field

118
00:03:23,690 --> 00:03:25,110
0,275 275,440 440,695 695,1040 1040,1420
as a whole by storm

119
00:03:25,250 --> 00:03:26,950
0,335 335,590 590,770 770,1030 1320,1700
in over the past decade

120
00:03:26,950 --> 00:03:28,090
0,270 270,555 555,825 825,960 960,1140
or so,| because of its
|因为它的能力就像我们昨天讨论的那样，

121
00:03:28,090 --> 00:03:29,725
0,320 730,1245 1245,1410 1410,1530 1530,1635
ability critically like we were

122
00:03:29,725 --> 00:03:31,470
0,225 225,465 465,755 1075,1410 1410,1745
talking about yesterday,| its ability
|它能够直接从原始数据和原始图像输入中学习它在环境中看到的东西，

123
00:03:31,550 --> 00:03:33,780
0,290 290,580 630,1030 1410,1810 1830,2230
to learn directly from raw

124
00:03:33,800 --> 00:03:35,280
0,400 420,695 695,890 890,1145 1145,1480
data and those raw image

125
00:03:35,540 --> 00:03:36,700
0,440 440,635 635,755 755,905 905,1160
inputs in what it sees

126
00:03:36,700 --> 00:03:38,940
0,210 210,420 420,770 1420,1820 1840,2240
in its environment| and learn
|并明确地学习如何执行，

127
00:03:39,140 --> 00:03:41,395
0,605 605,860 860,1115 1115,1450 1980,2255
explicitly how to perform,| {like,we
|就像我们昨天谈到的，

128
00:03:41,395 --> 00:03:42,910
0,150 150,360 360,570 570,845 1255,1515
-} talked about yesterday,| what
|环境中那些图像的特征提取，

129
00:03:42,910 --> 00:03:44,905
0,150 150,420 420,800 970,1670 1690,1995
is called feature extraction of

130
00:03:44,905 --> 00:03:46,590
0,210 210,515 895,1185 1185,1380 1380,1685
those images in the environment,|
|

131
00:03:46,700 --> 00:03:47,830
0,290 290,515 515,785 785,980 980,1130
and one example of that
其中一个例子是通过面部检测和识别，

132
00:03:47,830 --> 00:03:49,105
0,195 195,375 375,765 765,1140 1140,1275
is through facial detection and

133
00:03:49,105 --> 00:03:50,725
0,275 925,1215 1215,1380 1380,1515 1515,1620
recognition,| which all of you
|你们所有人都将在今天和明天的实验中练习，

134
00:03:50,725 --> 00:03:51,510
0,120 120,270 270,375 375,495 495,785
are going to get practice

135
00:03:51,680 --> 00:03:53,785
0,400 480,815 815,1355 1355,1595 1595,2105
with in today's and tomorrow's

136
00:03:53,785 --> 00:03:55,090
0,485 535,840 840,1020 1020,1170 1170,1305
labs| as part of the
|作为这门课总决赛的一部分。

137
00:03:55,090 --> 00:03:56,920
0,260 280,680 820,1220 1300,1605 1605,1830
grand final competition of {this,class

138
00:03:56,920 --> 00:03:59,470
0,320 1180,1580 1600,1965 1965,2280 2280,2550
-}.| Another really go to
|计算机视觉的另一个真正的例子是

139
00:03:59,470 --> 00:04:01,320
0,255 255,540 540,810 810,1130 1450,1850
example of computer vision is|
|

140
00:04:01,370 --> 00:04:02,935
0,290 290,755 755,1030 1110,1385 1385,1565
in autonomous driving and self
在自动驾驶和自动驾驶车辆中，

141
00:04:02,935 --> 00:04:04,285
0,285 285,665 775,1050 1050,1185 1185,1350
driving vehicles,| where we can
|我们可以将一幅图像作为输入，

142
00:04:04,285 --> 00:04:05,640
0,195 195,360 360,585 585,935 955,1355
take an image as input|
|

143
00:04:05,660 --> 00:04:07,075
0,275 275,550 690,1010 1010,1205 1205,1415
or maybe potentially a video
或者可能将视频作为输入，多幅图像，

144
00:04:07,075 --> 00:04:09,370
0,315 315,600 600,905 925,1325 1975,2295
as input, multiple images| and
|并处理所有这些数据，

145
00:04:09,370 --> 00:04:10,560
0,320 340,600 600,735 735,900 900,1190
process all of that data,|
|

146
00:04:10,700 --> 00:04:11,905
0,260 260,485 485,710 710,890 890,1205
so that we can train
这样我们就可以训练汽车学习，

147
00:04:11,905 --> 00:04:13,840
0,285 285,575 1045,1335 1335,1590 1590,1935
a car to learn| how
|如何驾驶方向盘或控制油门或执行制动命令，

148
00:04:13,840 --> 00:04:15,450
0,380 460,825 825,990 990,1230 1230,1610
to steer the wheel or

149
00:04:15,680 --> 00:04:17,725
0,305 305,500 500,905 905,1270 1410,2045
command a throttle or actuate

150
00:04:17,725 --> 00:04:19,860
0,255 255,720 720,1055 1375,1755 1755,2135
a braking command,| this entire
|这个整个控制系统，

151
00:04:19,940 --> 00:04:20,980
0,335 335,670
control system,|
|

152
00:04:20,980 --> 00:04:22,180
0,135 135,525 525,705 705,1050 1050,1200
the steering, the throttle, the
汽车的转向、油门和刹车可以端到端地执行，

153
00:04:22,180 --> 00:04:23,590
0,330 330,450 450,585 585,860 1120,1410
braking of a car can

154
00:04:23,590 --> 00:04:25,200
0,285 285,800 850,1185 1185,1365 1365,1610
be executed end to end,|
|

155
00:04:25,400 --> 00:04:26,755
0,395 395,695 695,965 965,1220 1220,1355
by taking as input the
方法是将车辆的图像和传感模式作为输入，

156
00:04:26,755 --> 00:04:28,860
0,245 595,870 870,1020 1020,1380 1380,2105
images and the sensing modalities

157
00:04:28,910 --> 00:04:30,340
0,275 275,410 410,670 840,1145 1145,1430
of the vehicle| and learning
|并学习如何预测这些驱动命令，

158
00:04:30,340 --> 00:04:31,920
0,270 270,435 435,690 690,990 990,1580
how to predict those actuation

159
00:04:31,970 --> 00:04:34,285
0,580 870,1270 1380,1715 1715,2015 2015,2315
commands,| {now\,,actually -} this end
|实际上，这种端到端的方法，

160
00:04:34,285 --> 00:04:35,500
0,180 180,405 405,735 735,1005 1005,1215
to end approach,| having a
|让一个神经网络来做所有这些事情，

161
00:04:35,500 --> 00:04:36,715
0,240 240,570 570,810 810,1050 1050,1215
single neural network do all

162
00:04:36,715 --> 00:04:38,485
0,195 195,485 685,1050 1050,1290 1290,1770
of this| is actually radically
|与绝大多数自动驾驶汽车公司完全不同，

163
00:04:38,485 --> 00:04:40,590
0,275 445,845 1015,1410 1410,1755 1755,2105
different than the vast majority

164
00:04:40,820 --> 00:04:42,430
0,260 260,740 740,995 995,1355 1355,1610
of autonomous vehicle companies,| like
|例如，如果你看一下 Waymo ，那是一种截然不同的方法，

165
00:04:42,430 --> 00:04:43,210
0,105 105,195 195,300 300,420 420,780
if you look at Waymo

166
00:04:43,210 --> 00:04:44,410
0,180 180,390 390,630 630,765 765,1200
for example, that's a radically

167
00:04:44,410 --> 00:04:45,895
0,240 240,620 850,1110 1110,1290 1290,1485
{different,approach -},| but we'll talk
|但我们将在今天的课上讨论这些方法，

168
00:04:45,895 --> 00:04:48,025
0,210 210,450 450,815 1405,1710 1710,2130
about those approaches in today's

169
00:04:48,025 --> 00:04:50,035
0,275 445,845 1285,1560 1560,1785 1785,2010
class| and in fact this
|事实上，这是我们在 MIT 建造的一辆车，

170
00:04:50,035 --> 00:04:51,570
0,195 195,390 390,540 540,815 1135,1535
is one of our vehicles

171
00:04:51,620 --> 00:04:53,130
0,260 260,455 455,590 590,880 1110,1510
that we've been building at

172
00:04:53,360 --> 00:04:54,910
0,400 690,995 995,1175 1175,1355 1355,1550
MIT| in my lab and
|在我的实验室里，就在这个房间上方几层，

173
00:04:54,910 --> 00:04:56,110
0,360 360,525 525,660 660,825 825,1200
cell just a few floors

174
00:04:56,110 --> 00:04:57,835
0,225 225,420 420,710 1150,1425 1425,1725
above this room| and we'll
|再次，我们将分享这项令人难以置信的工作的一些细节。

175
00:04:57,835 --> 00:04:59,095
0,335 505,825 825,990 990,1080 1080,1260
again share some of the

176
00:04:59,095 --> 00:05:00,450
0,240 240,420 420,630 630,960 960,1355
details on {this,incredible -} work.|
|

177
00:05:00,740 --> 00:05:01,660
0,245 245,365 365,515 515,650 650,920
But of course, it doesn't
当然，它并不止步于自动驾驶，

178
00:05:01,660 --> 00:05:02,970
0,195 195,405 405,585 585,1035 1035,1310
stop here with autonomous driving,|
|

179
00:05:03,230 --> 00:05:05,425
0,400 420,940 1380,1715 1715,1925 1925,2195
these algorithms directly the same
这些你们将在今天的课程中直接学习的相同算法，

180
00:05:05,425 --> 00:05:06,445
0,375 375,540 540,705 705,855 855,1020
algorithms that you'll learn about

181
00:05:06,445 --> 00:05:08,170
0,120 120,495 495,785 1225,1485 1485,1725
in today's class| can be
|可以一直扩展到影响医疗保健、医疗决策，

182
00:05:08,170 --> 00:05:09,550
0,380 430,720 720,870 870,1080 1080,1380
extended all the way to

183
00:05:09,550 --> 00:05:11,760
0,315 315,680 1090,1490 1540,1875 1875,2210
impact healthcare, medical decision making|
|

184
00:05:12,230 --> 00:05:13,930
0,290 290,580 720,1025 1025,1325 1325,1700
and finally even in these
最终甚至在这些无障碍应用程序中，

185
00:05:13,930 --> 00:05:15,960
0,620 790,1190 1240,1515 1515,1755 1755,2030
accessibility applications,| where we're seeing
|我们看到计算机视觉算法帮助视障人士，

186
00:05:16,670 --> 00:05:18,565
0,320 320,640 810,1310 1310,1670 1670,1895
computer vision {algorithms,helping -} the

187
00:05:18,565 --> 00:05:20,290
0,285 285,785 955,1290 1290,1515 1515,1725
visually impaired,| so for example
|举个例子，在这个项目中，

188
00:05:20,290 --> 00:05:21,020
0,180 180,330 330,620
in this project,|
|

189
00:05:21,020 --> 00:05:23,420
0,350 970,1370 1390,1790 1870,2175 2175,2400
researchers have built deep learning
研究人员已经建立了深度学习设备，可以检测步道，

190
00:05:23,420 --> 00:05:25,180
0,405 405,800 850,1140 1140,1395 1395,1760
enabled devices, that could detect

191
00:05:25,200 --> 00:05:27,785
0,580 1050,1310 1310,1570 1620,2060 2060,2585
trails,| so that visually impaired
|这样视力受损的跑步者就可以得到声音反馈，

192
00:05:27,785 --> 00:05:30,020
0,545 775,1110 1110,1395 1395,1740 1740,2235
runners could be provided audible

193
00:05:30,020 --> 00:05:31,460
0,290 670,915 915,1035 1035,1230 1230,1440
feedback,| so that they too
|这样他们也可以在外出跑步时导航。

194
00:05:31,460 --> 00:05:33,485
0,290 370,615 615,860 1090,1770 1770,2025
could, you know, navigate when

195
00:05:33,485 --> 00:05:34,330
0,150 150,270 270,420 420,585 585,845
they go out for runs.|
|

196
00:05:35,410 --> 00:05:36,630
0,400 450,725 725,890 890,1070 1070,1220
And like I said,| we
就像我说的，|我们经常把今天课程中将要讨论的许多任务视为理所当然的，

197
00:05:36,630 --> 00:05:38,085
0,260 340,735 735,1020 1020,1200 1200,1455
often take many of these

198
00:05:38,085 --> 00:05:39,240
0,365 445,705 705,900 900,1020 1020,1155
tasks that we're going to

199
00:05:39,240 --> 00:05:40,430
0,195 195,390 390,540 540,930 930,1190
talk about in today's lecture

200
00:05:40,450 --> 00:05:41,865
0,290 290,580 810,1100 1100,1265 1265,1415
for granted,| because we do
|因为我们在日常生活中无缝地完成了这些任务，

201
00:05:41,865 --> 00:05:43,410
0,195 195,420 420,1115 1165,1410 1410,1545
them so seamlessly in our

202
00:05:43,410 --> 00:05:45,170
0,180 180,315 315,450 450,740 1360,1760
day to day lives,| {but,the
|但今天这门课的问题是，

203
00:05:46,030 --> 00:05:47,540
0,305 305,575 575,830 830,1220 1220,1510
-} question of today's class

204
00:05:47,590 --> 00:05:49,245
0,305 305,530 530,710 710,970 1380,1655
is going to be,| at
|在它的核心，

205
00:05:49,245 --> 00:05:50,550
0,195 195,515 685,975 975,1140 1140,1305
its core,| how we can
|我们如何制造一台计算机来做这些不可思议的事情，

206
00:05:50,550 --> 00:05:51,990
0,210 210,450 450,770 1000,1275 1275,1440
build a computer to do

207
00:05:51,990 --> 00:05:53,340
0,210 210,495 495,780 780,1035 1035,1350
these same types of incredible

208
00:05:53,340 --> 00:05:54,255
0,270 270,465 465,630 630,735 735,915
things,| that all of us
|我们每天都认为理所当然的。

209
00:05:54,255 --> 00:05:55,695
0,240 240,420 420,695 985,1290 1290,1440
take for granted day to

210
00:05:55,695 --> 00:05:56,620
0,245
day.|
|

211
00:05:56,690 --> 00:05:58,045
0,305 305,610 720,1040 1040,1190 1190,1355
And specifically, we'll start with
具体地说，我们将从这个问题开始，

212
00:05:58,045 --> 00:06:00,745
0,180 180,480 480,875 1855,2255 2365,2700
this question of,| how does
|计算机到底是如何看东西的，

213
00:06:00,745 --> 00:06:02,410
0,270 270,585 585,965 1015,1380 1380,1665
a computer really see,| and
|更详细的是，

214
00:06:02,410 --> 00:06:03,640
0,270 270,615 615,915 915,1080 1080,1230
even more detailed than that

215
00:06:03,640 --> 00:06:05,140
0,210 210,530 700,990 990,1215 1215,1500
is,| how does a computer
|计算机是如何处理图像的，

216
00:06:05,140 --> 00:06:06,625
0,345 345,600 600,860 1030,1320 1320,1485
process an image,| if we
|如果我们认为，

217
00:06:06,625 --> 00:06:08,155
0,150 150,425 625,870 870,1115 1195,1530
think of,| you know, site
|[网站]是通过图像进入计算机的，

218
00:06:08,155 --> 00:06:09,400
0,270 270,540 540,795 795,1050 1050,1245
as coming to computers through

219
00:06:09,400 --> 00:06:11,080
0,260 820,1155 1155,1380 1380,1515 1515,1680
images,| then how can a
|那么计算机如何开始处理这些图像。

220
00:06:11,080 --> 00:06:12,150
0,210 210,435 435,645 645,795 795,1070
computer even start to process

221
00:06:12,440 --> 00:06:14,740
0,365 365,730 1620,1970 1970,2150 2150,2300
those images.| Well, to a
|对计算机来说，图像只是数字，

222
00:06:14,740 --> 00:06:16,530
0,270 270,620 700,1005 1005,1310 1390,1790
computer, images are just numbers,

223
00:06:16,910 --> 00:06:19,375
0,400 1110,1510 1710,2030 2030,2240 2240,2465
right,| and suppose, for example,
|例如，假设我们这里有一张 Abraham Lincoln 的照片，

224
00:06:19,375 --> 00:06:20,470
0,165 165,240 240,360 360,635 745,1095
we have a picture here

225
00:06:20,470 --> 00:06:22,340
0,255 255,675 675,950
of Abraham Lincoln,|
|

226
00:06:22,560 --> 00:06:24,125
0,305 305,545 545,880 1080,1385 1385,1565
okay, this picture is made
好的，这张图片是由所谓的像素组成的，

227
00:06:24,125 --> 00:06:24,905
0,150 150,330 330,495 495,615 615,780
up of what are called

228
00:06:24,905 --> 00:06:27,425
0,515 655,1055 1435,2025 2025,2310 2310,2520
pixels,| {every,pixel -} is just
|每个像素在这张图像中只是一个点，

229
00:06:27,425 --> 00:06:28,750
0,150 150,425 505,780 780,990 990,1325
a dot in this image,|
|

230
00:06:29,250 --> 00:06:30,155
0,275 275,455 455,620 620,755 755,905
and since this is a
因为这是一个灰度图像，

231
00:06:30,155 --> 00:06:31,340
0,225 225,450 450,720 720,1005 1005,1185
gray scale image,| each of
|所以每个像素都只是一个数字，

232
00:06:31,340 --> 00:06:33,290
0,210 210,740 1120,1485 1485,1740 1740,1950
these pixels is just a

233
00:06:33,290 --> 00:06:35,855
0,320 430,830 1390,1790 2170,2430 2430,2565
single number,| now we can
|现在我们可以将我们的图像表示为这个二维数字矩阵，

234
00:06:35,855 --> 00:06:37,280
0,275 385,645 645,885 885,1185 1185,1425
represent our image now as

235
00:06:37,280 --> 00:06:39,530
0,255 255,465 465,1160 1180,1950 1950,2250
this two dimensional matrix {of,numbers

236
00:06:39,530 --> 00:06:41,675
0,350 940,1340 1510,1815 1815,1995 1995,2145
-},| and because, like I
|因为，就像我说的，这是一幅灰度图像，

237
00:06:41,675 --> 00:06:42,320
0,135 135,255 255,360 360,465 465,645
said, this is a {gray,scale

238
00:06:42,320 --> 00:06:43,790
0,165 165,410 580,945 945,1350 1350,1470
-} image,| every pixel is
|每个像素只对应于该矩阵位置上的一个数字，

239
00:06:43,790 --> 00:06:44,950
0,450 450,555 555,690 690,870 870,1160
corresponding to just one number

240
00:06:45,120 --> 00:06:47,820
0,350 350,700 1260,1835 1835,2170
at that matrix location,|
|

241
00:06:47,860 --> 00:06:49,500
0,400 570,875 875,1085 1085,1355 1355,1640
now assume, for example, we
现在假设，例如，我们没有灰度图像，

242
00:06:49,500 --> 00:06:50,355
0,240 240,330 330,450 450,645 645,855
didn't have a gray scale

243
00:06:50,355 --> 00:06:51,195
0,225 225,435 435,540 540,660 660,840
image,| we had a color
|我们有一个彩色图像，它将是 RGB 图像，

244
00:06:51,195 --> 00:06:52,515
0,305 715,975 975,1095 1095,1200 1200,1320
image that would be an

245
00:06:52,515 --> 00:06:54,615
0,570 570,905 1405,1680 1680,1860 1860,2100
RGB image,| {so,now -} every
|所以现在每个像素将不只由一个数字组成，而是由三个数字组成，

246
00:06:54,615 --> 00:06:55,485
0,360 360,480 480,645 645,765 765,870
pixel is going to be

247
00:06:55,485 --> 00:06:56,505
0,360 360,570 570,720 720,870 870,1020
composed not just of one

248
00:06:56,505 --> 00:06:57,945
0,275 565,825 825,975 975,1155 1155,1440
number, but of three numbers,|
|

249
00:06:57,945 --> 00:06:58,665
0,225 225,300 300,435 435,585 585,720
so you can think of
所以你可以把它想象成三维矩阵，而不是二维矩阵，

250
00:06:58,665 --> 00:06:59,475
0,180 180,390 390,555 555,660 660,810
that as kind of a

251
00:06:59,475 --> 00:07:01,545
0,210 210,515 865,1625 1735,1980 1980,2070
{3D -} matrix instead of

252
00:07:01,545 --> 00:07:02,610
0,135 135,300 300,450 450,930 930,1065
a {2D -} matrix,| where
|你有三个二维矩阵堆叠在一起。

253
00:07:02,610 --> 00:07:04,485
0,260 430,720 720,1010 1090,1490 1600,1875
you almost have three two

254
00:07:04,485 --> 00:07:06,795
0,435 435,870 870,990 990,1235 1405,2310
dimensional matrix that are stacked

255
00:07:06,795 --> 00:07:07,640
0,135 135,315 315,450 450,570 570,845
on top of {each,other -}.|
|

256
00:07:08,410 --> 00:07:09,950
0,350 350,695 695,980 980,1205 1205,1540
So now with this basis
所以现在有了这个基本的图像数字表示的基础，

257
00:07:10,390 --> 00:07:13,245
0,400 630,1030 1320,1895 1895,2585 2585,2855
of basically numerical representations of

258
00:07:13,245 --> 00:07:14,580
0,275 295,555 555,735 735,1005 1005,1335
images,| we can start to
|我们可以开始考虑，

259
00:07:14,580 --> 00:07:16,100
0,315 315,630 630,960 960,1215 1215,1520
think about,| how we can
|我们如何或者我们可以建立什么类型的计算机视觉算法，

260
00:07:17,080 --> 00:07:18,525
0,290 290,545 545,890 890,1205 1205,1445
or what types of computer

261
00:07:18,525 --> 00:07:20,030
0,305 415,855 855,1050 1050,1215 1215,1505
vision algorithms we can build,|
|

262
00:07:20,200 --> 00:07:21,560
0,305 305,515 515,755 755,1025 1025,1360
that can take these systems
来将这些系统作为输入，以及它们可以执行什么操作。

263
00:07:21,640 --> 00:07:23,040
0,400 480,845 845,1100 1100,1265 1265,1400
as input and what {they,can

264
00:07:23,040 --> 00:07:24,900
0,210 210,560 640,1040 1270,1605 1605,1860
-} perform.| So the first
|所以我想跟你们谈的第一件事是，

265
00:07:24,900 --> 00:07:25,590
0,195 195,330 330,450 450,570 570,690
thing that I want to

266
00:07:25,590 --> 00:07:26,540
0,180 180,315 315,405 405,615 615,950
talk to you about is,|
|

267
00:07:26,920 --> 00:07:28,230
0,290 290,425 425,560 560,850 1050,1310
what kind of tasks do
我们到底想要训练这些系统来完成什么样的图像任务，

268
00:07:28,230 --> 00:07:29,205
0,105 105,285 285,525 525,735 735,975
we even want to train

269
00:07:29,205 --> 00:07:31,035
0,240 240,545 775,1170 1170,1545 1545,1830
these systems to complete with

270
00:07:31,035 --> 00:07:33,660
0,305 865,1265 1405,1995 1995,2315 2335,2625
images,| and broadly speaking there
|大体上讲，有两大类任务，

271
00:07:33,660 --> 00:07:35,145
0,270 270,600 600,885 885,1215 1215,1485
are two {broad,categories -} of

272
00:07:35,145 --> 00:07:36,240
0,270 270,555 555,765 765,960 960,1095
tasks,| we touched on this
|我们在昨天的课程中稍微谈到了这一点，

273
00:07:36,240 --> 00:07:37,320
0,120 120,270 270,420 420,540 540,1080
a little bit in yesterday's

274
00:07:37,320 --> 00:07:38,520
0,260 280,680 730,1005 1005,1125 1125,1200
lecture,| but just to be
|但为了在今天的课程中更具体一些，

275
00:07:38,520 --> 00:07:39,675
0,105 105,270 270,560 640,945 945,1155
a bit more concrete in

276
00:07:39,675 --> 00:07:41,390
0,375 375,635 895,1230 1230,1440 1440,1715
today's lecture,| those two tasks
|这两项任务要么是分类，要么是回归，

277
00:07:41,500 --> 00:07:43,910
0,335 335,635 635,1240 1380,1780 1860,2410
{are,either -} classification or regression,|
|

278
00:07:44,530 --> 00:07:47,900
0,395 395,790 1140,1720 2220,2620 2820,3370
now in regression, your prediction
对于回归，你的预测值会有一个连续的值，

279
00:07:47,920 --> 00:07:48,240
0,290
value

280
00:07:48,310 --> 00:07:49,370
0,275 275,425 425,575 575,755 755,1060
is going to take a

281
00:07:49,390 --> 00:07:51,480
0,400 720,1120 1440,1760 1760,1955 1955,2090
continuous value, right,| that could
|这可以是数轴上的任何实数，

282
00:07:51,480 --> 00:07:52,635
0,120 120,315 315,570 570,885 885,1155
be any real number on

283
00:07:52,635 --> 00:07:53,925
0,135 135,345 345,695 925,1170 1170,1290
the number line,| but in
|但在分类中，你的预测可能是 k 或 n 个不同的类别之一，

284
00:07:53,925 --> 00:07:56,235
0,515 1075,1380 1380,1800 1800,2085 2085,2310
classification, your prediction could take

285
00:07:56,235 --> 00:07:57,930
0,300 300,665 715,1080 1080,1320 1320,1695
one of, let's say, k

286
00:07:57,930 --> 00:08:00,315
0,270 270,560 640,1040 1060,1460 2110,2385
or n different classes,| {these,are
|这些是离散的不同类别。

287
00:08:00,315 --> 00:08:01,890
0,150 150,585 585,900 900,1295 1315,1575
-} discrete different classes.| So
|所以，让我们首先考虑图像分类的任务，

288
00:08:01,890 --> 00:08:03,630
0,270 270,510 510,830 1180,1485 1485,1740
let's consider first the task

289
00:08:03,630 --> 00:08:06,435
0,285 285,620 790,1430 2380,2640 2640,2805
of {image,classification -},| in this
|在这个任务中，我们想要为每个单独的图像预测一个单独的标签，

290
00:08:06,435 --> 00:08:07,520
0,240 240,450 450,615 615,795 795,1085
task we want to predict

291
00:08:07,690 --> 00:08:10,260
0,400 480,880 1200,1600 2040,2315 2315,2570
an individual label for every

292
00:08:10,260 --> 00:08:11,910
0,380 400,765 765,1065 1065,1350 1350,1650
single image| and this label
|我们预测的这个标签将是可以考虑的 n 个不同的可能标签之一，

293
00:08:11,910 --> 00:08:12,810
0,210 210,360 360,570 570,765 765,900
that we predict is going

294
00:08:12,810 --> 00:08:14,295
0,90 90,195 195,470 490,890 1120,1485
to be one of n

295
00:08:14,295 --> 00:08:16,230
0,365 535,935 985,1485 1485,1740 1740,1935
different possible labels that {could,be

296
00:08:16,230 --> 00:08:17,775
0,225 225,590 880,1155 1155,1320 1320,1545
-} considered,| so for example,
|例如，假设我们有一系列美国总统的图像，

297
00:08:17,775 --> 00:08:18,510
0,270 270,390 390,510 510,600 600,735
let's say we have a

298
00:08:18,510 --> 00:08:20,085
0,210 210,465 465,800 880,1200 1380,1575
bunch of images of US

299
00:08:20,085 --> 00:08:21,825
0,575 1045,1320 1320,1455 1455,1605 1605,1740
preidents| and we want to
|我们想要建立一个分类管道，

300
00:08:21,825 --> 00:08:23,340
0,135 135,300 300,785 835,1235 1255,1515
build a classification pipeline| to
|来告诉我们在屏幕上看到的这张图像中是哪位总统，

301
00:08:23,340 --> 00:08:24,600
0,120 120,300 300,570 570,920 970,1260
tell us which president is

302
00:08:24,600 --> 00:08:26,190
0,285 285,680 700,1050 1050,1365 1365,1590
in this particular image that

303
00:08:26,190 --> 00:08:27,710
0,165 165,470 760,1050 1050,1230 1230,1520
you see on the screen,|
|

304
00:08:29,420 --> 00:08:30,845
0,255 255,560 700,960 960,1140 1140,1425
the goal of our model
在这种情况下，我们模型的目标基本上是输出一个概率分数，

305
00:08:30,845 --> 00:08:31,955
0,240 240,435 435,690 690,915 915,1110
in this case is going

306
00:08:31,955 --> 00:08:33,800
0,165 165,425 715,1115 1165,1560 1560,1845
to be basically to output

307
00:08:33,800 --> 00:08:35,855
0,165 165,680 760,1160 1300,1575 1575,2055
a probability score,| {} probability
|这个图像包含这些不同总统之一的概率，

308
00:08:35,855 --> 00:08:37,580
0,315 315,525 525,810 810,1415 1435,1725
of this image containing one

309
00:08:37,580 --> 00:08:39,905
0,195 195,435 435,765 765,1430 2020,2325
of these different preidents,| and
|最大的分数最终将是

310
00:08:39,905 --> 00:08:41,210
0,165 165,425 685,975 975,1140 1140,1305
the maximum score is going

311
00:08:41,210 --> 00:08:42,140
0,120 120,270 270,570 570,795 795,930
to be ultimately the one|
|

312
00:08:42,140 --> 00:08:43,685
0,180 180,470 610,1010 1180,1425 1425,1545
that we infer to be
我们推断为图像中正确的总统的那个。

313
00:08:43,685 --> 00:08:44,885
0,150 150,345 345,885 885,1080 1080,1200
the correct preident in the

314
00:08:44,885 --> 00:08:45,760
0,245
image.|
|

315
00:08:46,080 --> 00:08:47,380
0,260 260,380 380,590 590,920 920,1300
So in order to correctly
所以，为了正确地执行这项任务，并正确地对这些图像进行分类，

316
00:08:47,610 --> 00:08:49,120
0,350 350,650 650,950 950,1205 1205,1510
perform this task and correctly

317
00:08:49,140 --> 00:08:52,480
0,760 1110,1475 1475,1840 2670,3005 3005,3340
classify these images,| our pipeline,
|我们的计算机视觉模型需要能够告诉我们

318
00:08:52,560 --> 00:08:54,245
0,365 365,635 635,890 890,1240 1290,1685
our computer vision model needs

319
00:08:54,245 --> 00:08:55,790
0,300 300,605 805,1125 1125,1320 1320,1545
the ability to be able

320
00:08:55,790 --> 00:08:56,960
0,255 255,435 435,705 705,990 990,1170
to tell us| what is
|这张特定的例如 Abraham Lincoln 图像的独特之处，

321
00:08:56,960 --> 00:08:59,320
0,290 700,1095 1095,1490 1570,1965 1965,2360
unique about this particular image

322
00:08:59,760 --> 00:09:01,540
0,395 395,890 890,1160 1160,1460 1460,1780
of Abraham Lincoln, for example,|
|

323
00:09:01,890 --> 00:09:03,940
0,400 630,950 950,1220 1220,1570 1650,2050
versus a different picture of
与 George Washington 的不同图像和 Obama 的不同图像。

324
00:09:04,020 --> 00:09:05,495
0,320 320,640 780,1115 1115,1295 1295,1475
George Washington versus a different

325
00:09:05,495 --> 00:09:07,690
0,315 315,695 835,1235 1585,1890 1890,2195
picture of Obama for example.|
|

326
00:09:08,310 --> 00:09:09,380
0,335 335,575 575,740 740,875 875,1070
{Now,another -} way to think
现在，考虑图像分类或图像处理的整个问题的另一种方式，

327
00:09:09,380 --> 00:09:10,820
0,225 225,450 450,675 675,980 1120,1440
about this whole problem of

328
00:09:10,820 --> 00:09:13,120
0,285 285,890 1030,1305 1305,1580 1900,2300
image classification or image processing|
|

329
00:09:14,070 --> 00:09:15,340
0,260 260,410 410,620 620,905 905,1270
at its high level is
从高层次上是，从特征的角度，

330
00:09:15,720 --> 00:09:17,705
0,335 335,605 605,875 875,1210 1620,1985
in terms of features,| or
|或者将这些视为你的数据的模式或特定类别的特征，

331
00:09:17,705 --> 00:09:18,755
0,225 225,360 360,510 510,750 750,1050
think of these as almost

332
00:09:18,755 --> 00:09:20,465
0,335 775,1080 1080,1290 1290,1485 1485,1710
patterns in your data or

333
00:09:20,465 --> 00:09:22,360
0,735 735,915 915,1110 1110,1445 1495,1895
characteristics of a {particular,class -},|
|

334
00:09:22,710 --> 00:09:24,820
0,320 320,850 960,1295 1295,1630 1710,2110
and classification then is simply
分类可以简单的完成，

335
00:09:24,840 --> 00:09:27,260
0,400 690,1090 1350,1955 1955,2225 2225,2420
done| by detecting all of
|通过检测数据中所有这些不同的模式，

336
00:09:27,260 --> 00:09:28,760
0,210 210,525 525,920 1030,1320 1320,1500
these different patterns in your

337
00:09:28,760 --> 00:09:31,550
0,290 550,930 930,1640 1930,2330 2440,2790
data| and identifying when certain
|并识别某些模式何时出现在其他模式上，

338
00:09:31,550 --> 00:09:33,730
0,350 400,800 940,1335 1335,1730 1780,2180
patterns occur {over,other -} patterns,|
|

339
00:09:34,350 --> 00:09:35,495
0,290 290,485 485,755 755,995 995,1145
so for example, if the
例如，如果某个特定类别的特征出现在图像中，

340
00:09:35,495 --> 00:09:37,360
0,275 295,585 585,875 895,1295 1465,1865
features of a particular class

341
00:09:37,650 --> 00:09:38,950
0,335 335,670 720,965 965,1055 1055,1300
are present in an image,|
|

342
00:09:39,300 --> 00:09:40,925
0,290 290,470 470,760 870,1270 1350,1625
then you might infer that
那么你可能会推断该图像属于该类别，

343
00:09:40,925 --> 00:09:42,155
0,165 165,405 405,660 660,930 930,1230
that image {is,of -} that

344
00:09:42,155 --> 00:09:43,160
0,255 255,465 465,645 645,840 840,1005
class,| so for example, if
|例如，如果你想要检测汽车，

345
00:09:43,160 --> 00:09:44,530
0,105 105,240 240,465 465,800 970,1370
you want to detect cars,|
|

346
00:09:44,670 --> 00:09:46,000
0,275 275,470 470,725 725,995 995,1330
you might look for patterns
你可能会在你的数据中查找车轮、车牌或前灯等数据模式，

347
00:09:46,080 --> 00:09:46,460
0,245 245,350
in your

348
00:09:46,460 --> 00:09:48,320
0,225 225,465 465,920 1120,1520 1540,1860
data, like wheels, license plates

349
00:09:48,320 --> 00:09:49,460
0,165 165,675 675,795 795,915 915,1140
or headlights,| {and,if -} those
|如果这些东西出现在你的图像中，

350
00:09:49,460 --> 00:09:50,525
0,240 240,435 435,705 705,945 945,1065
things are present in your

351
00:09:50,525 --> 00:09:51,890
0,245 595,870 870,1020 1020,1185 1185,1365
image,| then you can say
|那么你可以相当自信地说，

352
00:09:51,890 --> 00:09:53,290
0,210 210,465 465,690 690,980 1000,1400
with fairly high confidence that,|
|

353
00:09:53,370 --> 00:09:54,550
0,260 260,515 515,785 785,920 920,1180
your image of a car
你的图像是一辆汽车，而不是这些其他类别。

354
00:09:54,930 --> 00:09:56,255
0,400 600,860 860,995 995,1130 1130,1325
versus one of these other

355
00:09:56,255 --> 00:09:57,860
0,335 865,1125 1125,1260 1260,1455 1455,1605
categories.| So if we're building
|所以，如果我们构建一条计算机视觉管道，

356
00:09:57,860 --> 00:10:00,050
0,285 285,570 570,890 970,1370 1930,2190
a computer vision pipeline,| we
|我们有两个主要步骤需要考虑，

357
00:10:00,050 --> 00:10:01,295
0,225 225,465 465,690 690,975 975,1245
have two main steps really

358
00:10:01,295 --> 00:10:02,620
0,285 285,570 570,780 780,990 990,1325
{to,consider -},| The first step
|第一步是我们需要知道我们在数据中寻找什么特征或模式，

359
00:10:02,760 --> 00:10:03,770
0,290 290,470 470,635 635,830 830,1010
is that we need to

360
00:10:03,770 --> 00:10:05,375
0,260 430,780 780,1130 1150,1410 1410,1605
know what features or what

361
00:10:05,375 --> 00:10:06,920
0,335 445,780 780,990 990,1320 1320,1545
patterns we're looking for {in,our

362
00:10:06,920 --> 00:10:08,255
0,165 165,470 700,975 975,1110 1110,1335
-} data,| and the second
|第二步是我们需要检测这些模式，

363
00:10:08,255 --> 00:10:09,185
0,240 240,465 465,675 675,810 810,930
step is we need to

364
00:10:09,185 --> 00:10:11,240
0,245 625,1025 1045,1365 1365,1685 1765,2055
{then,detect -} those patterns,| once
|一旦我们检测到它们，

365
00:10:11,240 --> 00:10:12,125
0,210 210,435 435,615 615,750 750,885
we detect them,| we can
|我们就可以推断我们所在的类别。

366
00:10:12,125 --> 00:10:13,640
0,225 225,575 655,990 990,1275 1275,1515
then infer which class we're

367
00:10:13,640 --> 00:10:14,460
0,230
in.|
|

368
00:10:15,300 --> 00:10:16,250
0,275 275,440 440,605 605,755 755,950
Now, one way to solve
现在，解决这个问题的一种方法是利用我们特定领域的知识，

369
00:10:16,250 --> 00:10:17,530
0,210 210,405 405,570 570,830 880,1280
this is to leverage knowledge

370
00:10:17,760 --> 00:10:19,640
0,290 290,545 545,910 960,1360 1530,1880
about our particular field, right,|
|

371
00:10:19,640 --> 00:10:20,990
0,315 315,555 555,735 735,1005 1005,1350
so if we know something
所以，如果我们对我们的领域有所了解，

372
00:10:20,990 --> 00:10:22,150
0,255 255,405 405,630 630,870 870,1160
about our field,| for example
|例如关于人脸的知识，

373
00:10:22,680 --> 00:10:24,785
0,395 395,725 725,1060 1680,1955 1955,2105
about human faces,| we can
|我们可以用这些知识来定义我们的特征，

374
00:10:24,785 --> 00:10:26,080
0,165 165,345 345,630 630,960 960,1295
use that knowledge to define

375
00:10:26,310 --> 00:10:28,355
0,400 450,850 1410,1700 1700,1880 1880,2045
our features,| {what,makes -} up
|什么构成了一张脸，

376
00:10:28,355 --> 00:10:29,315
0,150 150,360 360,555 555,705 705,960
a face,| we know faces
|我们知道脸是由眼睛、鼻子和耳朵组成的，

377
00:10:29,315 --> 00:10:30,395
0,240 240,390 390,555 555,765 765,1080
are made up of eyes,

378
00:10:30,395 --> 00:10:31,930
0,480 480,645 645,930 930,1230 1230,1535
noses and ears, for example,|
|

379
00:10:32,340 --> 00:10:33,680
0,290 290,575 575,920 920,1160 1160,1340
we can define what each
我们可以定义每个组件的外观，

380
00:10:33,680 --> 00:10:35,200
0,165 165,440 490,885 885,1200 1200,1520
of those components look like,|
|

381
00:10:35,460 --> 00:10:37,520
0,350 350,740 740,890 890,1180 1800,2060
in defining {our,features -},| but
在定义我们的特征时，|但这种方法有一个很大的问题，

382
00:10:37,520 --> 00:10:38,630
0,150 150,240 240,405 405,710 790,1110
there's a big problem with

383
00:10:38,630 --> 00:10:40,330
0,320 430,780 780,1065 1065,1350 1350,1700
this approach,| and remember that
|记得图像就是这些数字的三维数组，

384
00:10:40,680 --> 00:10:42,935
0,400 840,1235 1235,1550 1550,1870 1950,2255
images are just these three

385
00:10:42,935 --> 00:10:46,085
0,725 1015,1530 1530,1755 1755,2075 2875,3150
dimensional {arrays,of -} numbers,| they
|即使在同一类型的物体中，它们也可能有很大的变化，

386
00:10:46,085 --> 00:10:47,105
0,135 135,270 270,450 450,735 735,1020
can have a lot of

387
00:10:47,105 --> 00:10:49,250
0,575 715,1115 1225,1605 1605,1890 1890,2145
variation even within the {same,type

388
00:10:49,250 --> 00:10:51,430
0,255 255,560 580,980 1420,1710 1710,2180
-} of object,| these variations
|这些变化可以包括任何东西，

389
00:10:51,510 --> 00:10:53,495
0,380 380,760 780,1180 1290,1595 1595,1985
can include really anything,| ranging
|从遮挡到照明，旋转、平移，在类别变动中。

390
00:10:53,495 --> 00:10:55,205
0,135 135,810 810,1065 1065,1440 1440,1710
from occlusions to variations in

391
00:10:55,205 --> 00:10:57,965
0,305 475,1085 1195,1805 2185,2535 2535,2760
lighting, rotations, translations into a

392
00:10:57,965 --> 00:10:59,460
0,225 225,785
class variation.|
|

393
00:10:59,780 --> 00:11:01,165
0,400 420,725 725,1010 1010,1250 1250,1385
And the problem here is,|
这里的问题是，|

394
00:11:01,165 --> 00:11:03,280
0,165 165,420 420,1025 1165,1565 1735,2115
that our classification pipeline needs
我们的分类流水线需要处理

395
00:11:03,280 --> 00:11:05,605
0,300 300,620 910,1215 1215,1520 2020,2325
the ability to handle| and
|所有这些不同类型的变化并保持不变的能力，

396
00:11:05,605 --> 00:11:07,495
0,210 210,1025 1165,1470 1470,1695 1695,1890
be invariant to all of

397
00:11:07,495 --> 00:11:09,150
0,195 195,510 510,870 870,1110 1110,1655
these different types of variations,|
|

398
00:11:09,920 --> 00:11:11,485
0,350 350,605 605,845 845,1180 1320,1565
while still being sensitive to
同时仍然对所有种类间变化敏感，

399
00:11:11,485 --> 00:11:12,870
0,120 120,285 285,555 555,935 985,1385
all of the inter class

400
00:11:12,950 --> 00:11:14,940
0,580 630,890 890,1310 1310,1640 1640,1990
variations,| the variations that occur
|不同种类之间发生的变化。

401
00:11:15,200 --> 00:11:17,080
0,320 320,640 690,1090
between different classes.|
|

402
00:11:17,210 --> 00:11:18,570
0,395 395,725 725,920 920,1070 1070,1360
Now, even though our pipeline
尽管我们的管道可以使用我们作为人类的特征，

403
00:11:19,070 --> 00:11:20,545
0,335 335,620 620,970 990,1295 1295,1475
could use features that we

404
00:11:20,545 --> 00:11:22,290
0,195 195,515 745,1005 1005,1265 1345,1745
as humans,| you know, define
|基于我们的一些先验知识，手动定义的特征，

405
00:11:22,460 --> 00:11:24,205
0,670 750,1100 1100,1370 1370,1595 1595,1745
manually define based on some

406
00:11:24,205 --> 00:11:26,130
0,105 105,270 270,525 525,875 1525,1925
of our prior knowledge,| the
|问题被分解为，

407
00:11:26,270 --> 00:11:28,525
0,395 395,770 770,1130 1130,1510 1950,2255
problem really breaks down,| in
|这些特征变得非常不健壮，

408
00:11:28,525 --> 00:11:31,200
0,305 475,855 855,1235 1975,2325 2325,2675
that these features, become very

409
00:11:31,490 --> 00:11:33,700
0,400 480,880 1140,1535 1535,1925 1925,2210
non robust,| when considering all
|当考虑到图像在现实世界中的大量不同变化时。

410
00:11:33,700 --> 00:11:34,825
0,150 150,315 315,615 615,930 930,1125
of these vast amounts of

411
00:11:34,825 --> 00:11:37,150
0,210 210,725 1045,1335 1335,1625 1945,2325
different variations that images take

412
00:11:37,150 --> 00:11:39,475
0,240 240,375 375,555 555,860 1930,2325
in the real world.| {So,in
|所以在实践中，就像我说的，

413
00:11:39,475 --> 00:11:41,635
0,300 300,605 1495,1785 1785,1950 1950,2160
-} practice, like I said,|
|

414
00:11:41,635 --> 00:11:43,030
0,335 475,945 945,1185 1185,1320 1320,1395
your algorithms need to be
你的算法需要能够承受所有这些不同类型的变化，

415
00:11:43,030 --> 00:11:44,425
0,150 150,330 330,860 910,1215 1215,1395
able to withstand all of

416
00:11:44,425 --> 00:11:45,685
0,165 165,405 405,645 645,810 810,1260
those different types of variations,|
|

417
00:11:45,685 --> 00:11:46,770
0,225 225,345 345,570 570,810 810,1085
and then the natural question
然后自然的问题是，

418
00:11:46,820 --> 00:11:48,010
0,305 305,560 560,830 830,1025 1025,1190
is that,| how can we
|我们如何建立一个计算机视觉算法来做到这一点，

419
00:11:48,010 --> 00:11:50,155
0,290 670,1005 1005,1245 1245,1550 1630,2145
build a computer vision algorithm

420
00:11:50,155 --> 00:11:51,750
0,270 270,435 435,705 705,1085 1195,1595
to do that| and still
|并仍然保持这种水平的健壮性，

421
00:11:51,830 --> 00:11:53,730
0,400 510,815 815,1025 1025,1280 1280,1900
maintain that level of robustness,|
|

422
00:11:53,960 --> 00:11:55,090
0,245 245,365 365,515 515,790 870,1130
and what we want is
我们想要的是一种提取特征的方法，

423
00:11:55,090 --> 00:11:56,340
0,120 120,300 300,600 600,915 915,1250
a way to extract features|
|

424
00:11:56,420 --> 00:11:58,945
0,400 630,920 920,1210 1560,1960 2190,2525
that can both detect those
既能检测数据中的那些特征，那些模式，

425
00:11:58,945 --> 00:12:01,900
0,335 1045,1410 1410,1775 2395,2745 2745,2955
features, those patterns in the

426
00:12:01,900 --> 00:12:04,290
0,260 910,1310 1420,1755 1755,2040 2040,2390
data,| and do so in
|又能以分层的方式做到这一点，

427
00:12:04,490 --> 00:12:06,745
0,275 275,935 935,1240 1680,1985 1985,2255
a {hierarchical,fashion -},| so going
|从头开始，从像素级别到具有语义意义的东西，

428
00:12:06,745 --> 00:12:08,215
0,300 300,480 480,725 1045,1320 1320,1470
all the way from the

429
00:12:08,215 --> 00:12:09,475
0,210 210,495 495,720 720,885 885,1260
ground up, from the pixel

430
00:12:09,475 --> 00:12:12,055
0,275 805,1205 1225,1620 1620,1965 1965,2580
level to something with semantic

431
00:12:12,055 --> 00:12:13,600
0,305 505,855 855,1095 1095,1335 1335,1545
meaning,| like for example, the
|例如，人脸的眼睛或鼻子。

432
00:12:13,600 --> 00:12:14,980
0,240 240,480 480,600 600,1100 1120,1380
eyes or the noses in

433
00:12:14,980 --> 00:12:16,660
0,135 135,360 360,710
the human face.|
|

434
00:12:16,930 --> 00:12:18,600
0,400 690,965 965,1240 1260,1535 1535,1670
Now, we learned in the
我们在上一节课中学到了，

435
00:12:18,600 --> 00:12:20,055
0,225 225,585 585,980 1030,1305 1305,1455
last class,| that we can
|我们可以使用神经网络来解决这类问题，

436
00:12:20,055 --> 00:12:21,585
0,210 210,495 495,755 835,1235 1285,1530
use neural networks exactly for

437
00:12:21,585 --> 00:12:23,445
0,150 150,345 345,525 525,815 1465,1860
this type of problem,| {neural,networks
|神经网络能够直接从数据中学习特征，

438
00:12:23,445 --> 00:12:24,890
0,255 255,525 525,810 810,1125 1125,1445
-} are capable of learning

439
00:12:25,420 --> 00:12:28,020
0,400 750,1150 1200,1490 1490,1780
features directly from data,|
|

440
00:12:28,170 --> 00:12:30,620
0,400 600,1000 1320,1700 1700,2080 2130,2450
and learn, most importantly, a
而且，最重要的是，学习一组分层的功能，

441
00:12:30,620 --> 00:12:32,660
0,705 705,930 930,1095 1095,1370 1660,2040
hierarchical set of features,| building
|在之前的功能基础上构建，

442
00:12:32,660 --> 00:12:34,190
0,315 315,555 555,825 825,1170 1170,1530
on top of previous features

443
00:12:34,190 --> 00:12:35,510
0,240 240,450 450,710 880,1140 1140,1320
that it's learned| to build
|构建越来越复杂的功能集。

444
00:12:35,510 --> 00:12:36,920
0,240 240,435 435,710 820,1170 1170,1410
more and more complex set

445
00:12:36,920 --> 00:12:39,300
0,180 180,470
of features.|
|

446
00:12:39,760 --> 00:12:41,140
0,400
Now,
现在，我们将确切地了解神经网络如何在图像领域做到这一点，

447
00:12:41,140 --> 00:12:42,330
0,300 300,405 405,555 555,810 810,1190
we're going to see exactly

448
00:12:42,380 --> 00:12:44,245
0,400 900,1250 1250,1475 1475,1715 1715,1865
how neural networks can do

449
00:12:44,245 --> 00:12:45,660
0,275 415,675 675,795 795,1035 1035,1415
this in the image domain,|
|

450
00:12:45,770 --> 00:12:46,980
0,320 320,515 515,680 680,890 890,1210
as part of this lecture,|
作为这节课的一部分，|

451
00:12:47,570 --> 00:12:49,420
0,320 320,640 780,1160 1160,1420 1500,1850
but specifically neural networks will
具体地说，神经网络将允许我们，

452
00:12:49,420 --> 00:12:51,250
0,285 285,620 1060,1365 1365,1605 1605,1830
allow us| to learn these
|从视觉数据中学习这些视觉特征，

453
00:12:51,250 --> 00:12:53,790
0,270 270,650 1210,1610 1630,2030 2140,2540
visual features from visual data,|
|

454
00:12:54,050 --> 00:12:55,840
0,335 335,665 665,965 965,1220 1220,1790
if we construct them cleverly,|
如果我们巧妙地构建这些视觉特征，|

455
00:12:55,840 --> 00:12:56,845
0,195 195,360 360,570 570,810 810,1005
{and,the -} key point here
这里的关键点是，

456
00:12:56,845 --> 00:12:58,290
0,150 150,405 405,660 660,935 1045,1445
is that,| actually the models
|我们在昨天的课程中了解到的模型和体系结构，

457
00:12:58,310 --> 00:12:59,680
0,260 260,365 365,1040 1040,1220 1220,1370
and the architectures that we

458
00:12:59,680 --> 00:13:01,170
0,240 240,495 495,675 675,1245 1245,1490
learned about in yesterday's lecture,|
|

459
00:13:01,610 --> 00:13:02,575
0,305 305,500 500,650 650,770 770,965
and so far in this
到目前为止，

460
00:13:02,575 --> 00:13:04,375
0,335 805,1185 1185,1365 1365,1545 1545,1800
course,| we'll see how they're
|我们将看到它们如何不适合或扩展到今天的图像问题域，

461
00:13:04,375 --> 00:13:06,790
0,210 210,515 685,1245 1245,1560 1560,2415
actually not suitable or extensible

462
00:13:06,790 --> 00:13:10,045
0,350 400,1070 1930,2190 2190,2450 2890,3255
to today's, you know, problem

463
00:13:10,045 --> 00:13:11,590
0,345 345,615 615,905 1075,1365 1365,1545
domain of images| and how
|以及我们如何更巧妙地构建和构造神经网络来克服这些问题。

464
00:13:11,590 --> 00:13:12,805
0,165 165,345 345,570 570,890 910,1215
we can build and construct

465
00:13:12,805 --> 00:13:13,885
0,240 240,510 510,765 765,900 900,1080
neural networks a bit more

466
00:13:13,885 --> 00:13:15,720
0,575 715,945 945,1175 1225,1530 1530,1835
cleverly to overcome those issues.|
|

467
00:13:15,980 --> 00:13:17,215
0,290 290,500 500,830 830,1025 1025,1235
So maybe let's start by
让我们先回顾一下我们在第一课中谈到的内容，

468
00:13:17,215 --> 00:13:19,255
0,725 1105,1395 1395,1560 1560,1785 1785,2040
revisiting what we talked about

469
00:13:19,255 --> 00:13:21,480
0,305 475,870 870,1265 1675,1950 1950,2225
in lecture one,| which was
|也就是我们学习的全连接网络，

470
00:13:21,560 --> 00:13:22,890
0,275 275,425 425,665 665,980 980,1330
where we learned about fully

471
00:13:23,000 --> 00:13:24,745
0,400 540,940 1140,1415 1415,1580 1580,1745
{connected,networks -},| now these were
|这些网络有多个隐藏层，

472
00:13:24,745 --> 00:13:26,610
0,275 445,845 1165,1410 1410,1560 1560,1865
networks that, you know, have

473
00:13:26,660 --> 00:13:28,270
0,400 450,770 770,1145 1145,1340 1340,1610
multiple hidden layers| and each
|在给定的隐藏层中的每个神经元都连接到它的前一层中的每个神经元，

474
00:13:28,270 --> 00:13:29,920
0,540 540,780 780,930 930,1220 1270,1650
neuron in a given hidden

475
00:13:29,920 --> 00:13:31,480
0,380 400,780 780,1110 1110,1320 1320,1560
layer is connected to every

476
00:13:31,480 --> 00:13:33,030
0,390 390,540 540,780 780,1155 1155,1550
neuron in {its,prior -} layer,|
|

477
00:13:33,500 --> 00:13:34,495
0,230 230,305 305,635 635,830 830,995
so it receives all of
所以，它接收作为这些完全连接的层的函数的所有先前层的输入，

478
00:13:34,495 --> 00:13:36,850
0,150 150,425 955,1505 1555,2070 2070,2355
the previous layers inputs as

479
00:13:36,850 --> 00:13:38,245
0,165 165,420 420,735 735,1050 1050,1395
a function of these fully

480
00:13:38,245 --> 00:13:39,580
0,330 330,845
connected layers,|
|

481
00:13:39,580 --> 00:13:40,435
0,240 240,450 450,585 585,720 720,855
now let's say that we
现在，让我们假设我们想要直接使用完全连接的网络，而不做任何修改，

482
00:13:40,435 --> 00:13:41,995
0,195 195,495 495,875 985,1320 1320,1560
want to directly, without any

483
00:13:41,995 --> 00:13:44,395
0,695 865,1265 1525,1800 1800,2040 2040,2400
modifications, use a fully connected

484
00:13:44,395 --> 00:13:45,925
0,390 390,675 675,960 960,1305 1305,1530
network,| like we learned about
|就像我们在第一课中了解到的那样，

485
00:13:45,925 --> 00:13:47,665
0,150 150,405 405,785 1285,1575 1575,1740
in lecture one,| with an
|使用图像处理管道，

486
00:13:47,665 --> 00:13:49,990
0,255 255,635 1075,1475 1705,2040 2040,2325
image processing pipeline,| {so,directly -}
|直接使用图像并将其提供给完全连接网络，

487
00:13:49,990 --> 00:13:51,925
0,345 345,645 645,950 1330,1650 1650,1935
taking an image and feeding

488
00:13:51,925 --> 00:13:52,930
0,150 150,300 300,435 435,690 690,1005
it to a fully connected

489
00:13:52,930 --> 00:13:54,490
0,350 790,1050 1050,1185 1185,1335 1335,1560
network,| could we do something
|我们能做些类似的事情吗。

490
00:13:54,490 --> 00:13:56,110
0,255 255,560 1060,1335 1335,1470 1470,1620
like that.| Actually in this
|事实上，在这种情况下，我们可以，

491
00:13:56,110 --> 00:13:57,325
0,210 210,480 480,830 850,1095 1095,1215
case, {we,could -},| the way
|我们要做的是，

492
00:13:57,325 --> 00:13:58,030
0,135 135,240 240,375 375,555 555,705
we would have to do

493
00:13:58,030 --> 00:13:59,650
0,165 165,470 730,1050 1050,1350 1350,1620
it is,| remember that because
|记住，因为我们的图像是一个二维数组，

494
00:13:59,650 --> 00:14:00,670
0,165 165,390 390,600 600,810 810,1020
our image is a two

495
00:14:00,670 --> 00:14:02,575
0,650 790,1190 1330,1590 1590,1740 1740,1905
dimensional array,| the first thing
|我们必须做的第一件事是将它折叠成一个一维数字序列，

496
00:14:02,575 --> 00:14:03,175
0,150 150,270 270,360 360,480 480,600
that we would have to

497
00:14:03,175 --> 00:14:04,195
0,105 105,285 285,630 630,870 870,1020
do is collapse that to

498
00:14:04,195 --> 00:14:05,425
0,105 105,255 255,735 735,990 990,1230
a one dimensional sequence of

499
00:14:05,425 --> 00:14:07,030
0,275 565,855 855,1035 1035,1290 1290,1605
numbers,| because a fully connected
|因为一个完全连接网络不是接受一个二维数组，

500
00:14:07,030 --> 00:14:08,935
0,350 730,1035 1035,1340 1360,1695 1695,1905
network is not taking in

501
00:14:08,935 --> 00:14:10,735
0,195 195,360 360,965 1255,1560 1560,1800
a two dimensional array,| it's
|而是接受一个一维序列。

502
00:14:10,735 --> 00:14:11,800
0,150 150,315 315,435 435,570 570,1065
taking in a one dimensional

503
00:14:11,800 --> 00:14:12,820
0,290
sequence.|
|

504
00:14:13,190 --> 00:14:14,200
0,320 320,515 515,695 695,875 875,1010
So the first thing that
所以，我们必须做的第一件事是，

505
00:14:14,200 --> 00:14:15,180
0,90 90,195 195,330 330,585 585,980
we have to do is,|
|

506
00:14:15,620 --> 00:14:17,430
0,605 605,920 920,1100 1100,1535 1535,1810
flatten that two dimensional array
将二维数组展平为像素值的向量，

507
00:14:17,660 --> 00:14:19,420
0,320 320,640 810,1220 1220,1445 1445,1760
to a vector of pixel

508
00:14:19,420 --> 00:14:20,590
0,260 280,585 585,795 795,1020 1020,1170
values| and feed that to
|并将其提供给我们的网络，

509
00:14:20,590 --> 00:14:22,470
0,120 120,410 1120,1395 1395,1575 1575,1880
our network,| {in,this -} case,
|在这种情况下，我们第一层中的每个神经元都连接到输入层中的所有神经元，

510
00:14:22,610 --> 00:14:24,685
0,400 480,1060 1170,1475 1475,1745 1745,2075
every neuron in our first

511
00:14:24,685 --> 00:14:27,160
0,365 895,1295 1525,1925 1975,2250 2250,2475
layer is connected to all

512
00:14:27,160 --> 00:14:29,280
0,530 700,1050 1050,1400 1420,1770 1770,2120
neurons in that input layer,

513
00:14:29,390 --> 00:14:30,580
0,320 320,500 500,665 665,890 890,1190
right,| so in that original
|所以，在原始图像展平后，

514
00:14:30,580 --> 00:14:32,845
0,380 430,975 975,1250 1750,2070 2070,2265
image flattened down,| we feed
|我们将所有这些像素提供给第一层，

515
00:14:32,845 --> 00:14:33,985
0,165 165,330 330,510 510,960 960,1140
all of those pixels to

516
00:14:33,985 --> 00:14:35,160
0,120 120,315 315,570 570,840 840,1175
the first layer,| and here
|在这里，你应该已经意识到一个非常重要的概念，

517
00:14:35,330 --> 00:14:37,150
0,275 275,485 485,770 770,1120 1560,1820
you should already appreciate the

518
00:14:37,150 --> 00:14:40,590
0,260 1150,1550 1660,2060 2500,2900 3040,3440
very important notion,| that every
|那就是真正定义我们图像的每一条空间信息，

519
00:14:40,640 --> 00:14:43,230
0,400 480,880 1020,1420 1590,2240 2240,2590
single piece of spatial information

520
00:14:43,340 --> 00:14:45,210
0,275 275,550 600,1000 1020,1420 1470,1870
that really defined our image|
|

521
00:14:45,470 --> 00:14:46,585
0,275 275,470 470,665 665,890 890,1115
that makes an image an
使图像成为图像的信息都已经完全消失了，

522
00:14:46,585 --> 00:14:48,940
0,275 625,1005 1005,1380 1380,1775 2035,2355
image is totally {lost,already -},|
|

523
00:14:48,940 --> 00:14:50,275
0,285 285,540 540,750 750,1080 1080,1335
before we've even started this
在我们开始这个问题之前，

524
00:14:50,275 --> 00:14:51,985
0,285 285,555 555,795 795,1325 1405,1710
problem,| because we've flattened that
|因为我们已经将二维图像展平为一维数组，

525
00:14:51,985 --> 00:14:53,380
0,165 165,660 660,930 930,1230 1230,1395
two dimensional image into a

526
00:14:53,380 --> 00:14:55,080
0,135 135,600 600,890 940,1365 1365,1700
one dimensional array,| we've completely
|所以我们已经完全摧毁了空间信息的所有概念。

527
00:14:55,190 --> 00:14:57,025
0,400 480,800 800,1085 1085,1340 1340,1835
destroyed all notion of spatial

528
00:14:57,025 --> 00:14:57,920
0,365
information.|
|

529
00:14:58,570 --> 00:15:00,285
0,365 365,650 650,970 1020,1400 1400,1715
And in addition, we really
此外，我们确实有大量的参数，

530
00:15:00,285 --> 00:15:02,190
0,285 285,635 925,1325 1375,1695 1695,1905
have an enormous number of

531
00:15:02,190 --> 00:15:04,515
0,530 1000,1400 1510,1830 1830,2085 2085,2325
parameters,| because this system is
|因为这个系统是完全连接的，

532
00:15:04,515 --> 00:15:05,925
0,270 270,635 685,990 990,1185 1185,1410
fully connected,| {take,for -} example,
|举个例子，在一个非常非常小的图像中，

533
00:15:05,925 --> 00:15:07,200
0,255 255,465 465,690 690,1005 1005,1275
in a very, very small

534
00:15:07,200 --> 00:15:08,715
0,290 580,840 840,960 960,1220 1240,1515
image,| which is even {100
|它甚至是 100 乘以 100 像素，

535
00:15:08,715 --> 00:15:10,200
0,275 415,675 675,780 780,975 975,1485
-} by {100 -} pixels,|
|

536
00:15:10,200 --> 00:15:11,445
0,270 270,435 435,765 765,1035 1035,1245
that's an incredibly small image
在今天的标准下，这是一个令人难以置信的小图像，

537
00:15:11,445 --> 00:15:13,155
0,225 225,570 570,845 1225,1485 1485,1710
in today's standards,| but that's
|但这将需要在第一层中的 10000 个神经元，

538
00:15:13,155 --> 00:15:15,090
0,105 105,240 240,450 450,1625 1645,1935
going {to,take} 10000 neurons just

539
00:15:15,090 --> 00:15:16,350
0,165 165,300 300,495 495,830 1000,1260
in the first layer,| which
|它将连接到，比如第二层中的 10000 个神经元，

540
00:15:16,350 --> 00:15:17,370
0,105 105,285 285,615 615,840 840,1020
will be connected to, {let's,say}

541
00:15:17,370 --> 00:15:18,840
0,120 120,945 945,1140 1140,1260 1260,1470
10000 neurons in the {second,layer

542
00:15:18,840 --> 00:15:20,685
0,350 850,1110 1110,1290 1290,1485 1485,1845
-},| the number of parameters
|仅在这一层中的参数数量就将是 10000 个平方参数，

543
00:15:20,685 --> 00:15:21,540
0,165 165,285 285,465 465,690 690,855
that you have just in

544
00:15:21,540 --> 00:15:22,530
0,135 135,315 315,570 570,810 810,990
that one layer alone is

545
00:15:22,530 --> 00:15:24,710
0,165 165,255 255,360 360,1370 1540,2180
{going,to} be 10,000 squared parameters,|
|

546
00:15:24,880 --> 00:15:25,860
0,335 335,455 455,560 560,710 710,980
it's going to {be,highly -}
这将是非常低效的，

547
00:15:25,860 --> 00:15:27,120
0,600 600,705 705,825 825,1050 1050,1260
inefficient,| you can imagine if
|你可以想象，如果您想要将此网络扩展到

548
00:15:27,120 --> 00:15:28,005
0,105 105,240 240,435 435,660 660,885
you want to scale this

549
00:15:28,005 --> 00:15:30,225
0,305 655,900 900,1080 1080,1415 1615,2220
network to| even a reasonably
|我们今天必须处理的合理大小的图像，

550
00:15:30,225 --> 00:15:31,200
0,225 225,510 510,750 750,855 855,975
sized image that we have

551
00:15:31,200 --> 00:15:32,415
0,150 150,300 300,540 540,855 855,1215
{to,deal -} with today,| so
|所以在实践中是不可行的，

552
00:15:32,415 --> 00:15:34,425
0,300 300,690 690,825 825,1115 1645,2010
not feasible in practice,| but
|相反，我们需要问问自己，

553
00:15:34,425 --> 00:15:35,660
0,365 445,705 705,855 855,990 990,1235
instead we need to ask

554
00:15:35,710 --> 00:15:36,830
0,335 335,545 545,680 680,830 830,1120
ourselves,| how we can build
|我们如何才能建立和维护一些非常独特的空间结构，

555
00:15:37,420 --> 00:15:39,075
0,290 290,580 1020,1295 1295,1445 1445,1655
and maintain some of that

556
00:15:39,075 --> 00:15:41,030
0,525 525,845 895,1395 1395,1650 1650,1955
spatial structure,| that's very unique
|这种空间结构在我们的输入中，

557
00:15:41,380 --> 00:15:43,665
0,335 335,670 1320,1655 1655,1940 1940,2285
about images here into our

558
00:15:43,665 --> 00:15:44,730
0,255 255,420 420,615 615,825 825,1065
input| and here into our
|以及最重要的是，在我们的模型中。

559
00:15:44,730 --> 00:15:46,620
0,320 370,750 750,1130
model most importantly.|
|

560
00:15:47,830 --> 00:15:49,170
0,290 290,440 440,575 575,850 960,1340
So to do this, let's
要做到这一点，让我们将二维图像表示为其原始形式，

561
00:15:49,170 --> 00:15:51,050
0,260 700,1100 1120,1395 1395,1575 1575,1880
represent our two {2D -}

562
00:15:51,730 --> 00:15:53,730
0,400 810,1160 1160,1475 1475,1775 1775,2000
as its original form,| as
|即一个二维数组，

563
00:15:53,730 --> 00:15:55,430
0,195 195,360 360,885 885,1250 1300,1700
a two dimensional array of

564
00:15:55,630 --> 00:15:57,720
0,400 1320,1595 1595,1745 1745,1910 1910,2090
numbers,| {one,way -} that we
|我们可以使用空间结构连接我们的输入的一种方式是，

565
00:15:57,720 --> 00:15:59,805
0,210 210,510 510,1190 1390,1770 1770,2085
can use spatial structure here

566
00:15:59,805 --> 00:16:02,325
0,725 1285,1590 1590,1895 1975,2295 2295,2520
inherent to our input is|
|

567
00:16:02,325 --> 00:16:03,705
0,255 255,600 600,885 885,1095 1095,1380
to connect what are called
将我们输入的这些部分连接到隐藏层中的神经元，

568
00:16:03,705 --> 00:16:05,900
0,365 415,765 765,1325 1645,1920 1920,2195
basically these patches of our

569
00:16:05,950 --> 00:16:07,920
0,400 900,1190 1190,1610 1610,1835 1835,1970
input to neurons in the

570
00:16:07,920 --> 00:16:09,260
0,195 195,525 525,810 810,1020 1020,1340
hidden layer,| so for example,
|例如，假设这里你可以看到的隐藏层中的每个神经元，

571
00:16:09,460 --> 00:16:11,295
0,365 365,640 870,1190 1190,1460 1460,1835
let's say that each neuron

572
00:16:11,295 --> 00:16:12,375
0,195 195,330 330,510 510,840 840,1080
in the hidden layer, that

573
00:16:12,375 --> 00:16:13,310
0,105 105,270 270,450 450,630 630,935
you can see here| only
|它们将只看到或做出反应，

574
00:16:13,390 --> 00:16:15,080
0,290 290,545 545,910 930,1310 1310,1690
is going to see or

575
00:16:15,430 --> 00:16:17,580
0,400 570,845 845,1040 1040,1360
respond| to a certain
|对于上一层中的一组或某一块神经元，

576
00:16:17,580 --> 00:16:19,310
0,380 460,860 970,1260 1260,1440 1440,1730
{} set or a certain

577
00:16:19,330 --> 00:16:21,150
0,400 480,860 860,1370 1370,1640 1640,1820
patch of neurons in the

578
00:16:21,150 --> 00:16:23,100
0,290 370,770 1330,1650 1650,1830 1830,1950
previous layer, right,| so you
|所以你也可以认为这是一个感受野，

579
00:16:23,100 --> 00:16:23,895
0,180 180,375 375,510 510,645 645,795
could also think of this

580
00:16:23,895 --> 00:16:25,425
0,225 225,570 570,810 810,1320 1320,1530
as almost a receptive field|
|

581
00:16:25,425 --> 00:16:27,390
0,240 240,545 775,1175 1315,1665 1665,1965
or what the single neuron
或者你下一层中的单个神经元在上一层中所能关注的，

582
00:16:27,390 --> 00:16:28,845
0,135 135,315 315,570 570,920 1090,1455
in your next layer can

583
00:16:28,845 --> 00:16:29,820
0,300 300,495 495,600 600,720 720,975
attend to in the previous

584
00:16:29,820 --> 00:16:30,960
0,255 255,405 405,570 570,825 825,1140
layer,| is not the entire
|不是整个图像，

585
00:16:30,960 --> 00:16:32,610
0,350 730,1005 1005,1215 1215,1455 1455,1650
image,| but rather a small
|而是你上一张图像中的一个小的感受野，

586
00:16:32,610 --> 00:16:34,200
0,540 540,830 850,1125 1125,1305 1305,1590
receptive field from your previous

587
00:16:34,200 --> 00:16:36,830
0,380 1210,1515 1515,1800 1800,2180 2230,2630
image,| {now,notice -} here how
|现在注意输入层的区域，

588
00:16:36,850 --> 00:16:38,760
0,305 305,610 1050,1355 1355,1625 1625,1910
the region of the input

589
00:16:38,760 --> 00:16:40,350
0,320 970,1245 1245,1350 1350,1470 1470,1590
layer,| which you can see
|你可以在左侧看到，

590
00:16:40,350 --> 00:16:41,235
0,90 90,210 210,375 375,600 600,885
on the left hand side

591
00:16:41,235 --> 00:16:44,330
0,365 1105,1805 1885,2235 2235,2565 2565,3095
here,| influences that single neuron
|如何影响右侧的单个神经元，

592
00:16:44,380 --> 00:16:45,345
0,290 290,425 425,575 575,785 785,965
on the right hand side|
|

593
00:16:45,345 --> 00:16:46,515
0,120 120,330 330,495 495,735 735,1170
and that's just one neuron
这只是下一层中的一个神经元，

594
00:16:46,515 --> 00:16:47,655
0,240 240,375 375,585 585,915 915,1140
in the next layer,| but
|当然，你可以想象在整个输入中定义这些连接，

595
00:16:47,655 --> 00:16:49,070
0,150 150,455 715,975 975,1125 1125,1415
of course you can imagine

596
00:16:49,360 --> 00:16:52,500
0,400 810,1370 1370,1690 1710,2110 2820,3140
basically defining these connections across

597
00:16:52,500 --> 00:16:53,925
0,180 180,435 435,810 810,1125 1125,1425
the whole input, right,| each
|每一次，你的输入上都有块，

598
00:16:53,925 --> 00:16:55,080
0,315 315,525 525,690 690,885 885,1155
time you have the single

599
00:16:55,080 --> 00:16:57,300
0,380 550,840 840,1110 1110,1490 1870,2220
patch on your input,| that
|对应于另一层上的单一神经元，

600
00:16:57,300 --> 00:16:59,180
0,735 735,885 885,1065 1065,1365 1365,1880
corresponds to a single neuron

601
00:16:59,290 --> 00:17:00,530
0,335 335,560 560,695 695,890 890,1240
output on the other layer,|
|

602
00:17:01,420 --> 00:17:02,280
0,245 245,350 350,515 515,695 695,860
and we can apply the
我们可以应用相同的原理，

603
00:17:02,280 --> 00:17:04,035
0,210 210,510 510,890 1090,1545 1545,1755
same principle| of connecting these
|将整个图像中的这些块连接到后续层中的单个神经元。

604
00:17:04,035 --> 00:17:06,560
0,575 1015,1395 1395,1775 1855,2190 2190,2525
patches across the entire image

605
00:17:06,910 --> 00:17:08,115
0,305 305,545 545,905 905,1055 1055,1205
to single neurons in the

606
00:17:08,115 --> 00:17:09,210
0,305 355,645 645,795 795,930 930,1095
subsequent layer,| {and,we -} do
|我们可以通过滑动块来做到这一点，

607
00:17:09,210 --> 00:17:10,770
0,210 210,465 465,800 910,1290 1290,1560
this by essentially sliding that

608
00:17:10,770 --> 00:17:11,900
0,320
patch,|
|

609
00:17:12,160 --> 00:17:13,980
0,440 440,590 590,1090 1170,1520 1520,1820
pixel by pixel across the
在输入图像上逐个像素，

610
00:17:13,980 --> 00:17:15,690
0,240 240,530 910,1185 1185,1455 1455,1710
input image| and we'll be
|我们将在输出层上使用另一幅图像进行响应。

611
00:17:15,690 --> 00:17:17,235
0,480 480,770 790,1035 1035,1245 1245,1545
responding with, you know, another

612
00:17:17,235 --> 00:17:18,920
0,335 625,885 885,1125 1125,1395 1395,1685
image on our output layer.|
|

613
00:17:20,010 --> 00:17:21,650
0,260 260,440 440,695 695,1030 1260,1640
In this way, we essentially
通过这种方式，我们基本上保留了我们输入所固有的所有非常关键和丰富的空间信息，

614
00:17:21,650 --> 00:17:23,230
0,510 510,780 780,990 990,1230 1230,1580
preserve all of that very

615
00:17:23,580 --> 00:17:26,020
0,380 380,755 755,1150 1380,2020 2040,2440
key and rich spatial information

616
00:17:26,310 --> 00:17:27,800
0,635 635,830 830,1025 1025,1280 1280,1490
inherent to our input,| {but,remember
|但请记住，这里的最终任务是，

617
00:17:27,800 --> 00:17:29,225
0,315 315,585 585,795 795,1095 1095,1425
-} that the ultimate task

618
00:17:29,225 --> 00:17:31,370
0,365 625,1025 1165,1440 1440,1715 1765,2145
here is,| not only to
|不仅仅是保存空间信息，

619
00:17:31,370 --> 00:17:33,110
0,360 360,795 795,1005 1005,1440 1440,1740
just preserve that spatial information,|
|

620
00:17:33,110 --> 00:17:34,415
0,225 225,405 405,645 645,960 960,1305
we want to ultimately learn
我们最终想要学习特征，学习这些模式，

621
00:17:34,415 --> 00:17:35,945
0,365 445,765 765,1005 1005,1305 1305,1530
features, learn those patterns,| so
|以便我们能够检测和分类这些图像，

622
00:17:35,945 --> 00:17:36,860
0,120 120,270 270,480 480,720 720,915
that we can detect and

623
00:17:36,860 --> 00:17:39,140
0,620 790,1065 1065,1340 1870,2145 2145,2280
classify these images,| and we
|我们可以通过[挥动]输入块之间的连接来做到这一点，

624
00:17:39,140 --> 00:17:40,510
0,135 135,285 285,510 510,810 810,1370
can do this by waving,

625
00:17:41,340 --> 00:17:43,310
0,530 530,740 740,1060 1410,1745 1745,1970
waving the connections between the

626
00:17:43,310 --> 00:17:45,640
0,560 880,1140 1140,1400 1420,1820
patches of our input,|
|

627
00:17:45,640 --> 00:17:47,400
0,315 315,710 760,1050 1050,1340 1360,1760
and, and in order to
而且，为了检测这些特定的特征是什么。

628
00:17:47,480 --> 00:17:49,470
0,400 600,845 845,1040 1040,1390 1590,1990
detect, you know, what those

629
00:17:49,820 --> 00:17:52,150
0,350 350,700 780,1180 1980,2225 2225,2330
certain features are.| {Let,me -}
|让我在这里举一个实际的例子，

630
00:17:52,150 --> 00:17:53,560
0,135 135,315 315,620 760,1110 1110,1410
give a practical example here,|
|

631
00:17:53,560 --> 00:17:55,200
0,240 240,390 390,555 555,860 1240,1640
and so in practice, this
所以在实践中，我描述的这个运算，我描述的这个块和滑动运算，

632
00:17:55,220 --> 00:17:57,145
0,400 480,740 740,1040 1040,1625 1625,1925
operation that I'm describing, this

633
00:17:57,145 --> 00:17:58,885
0,450 450,630 630,965 1045,1410 1410,1740
patching and sliding operation I'm

634
00:17:58,885 --> 00:18:00,745
0,485 595,945 945,1125 1125,1215 1215,1860
describing,| is actually a mathematical
|实际上是一个数据运算，以前被称为卷积，

635
00:18:00,745 --> 00:18:02,700
0,335 565,1050 1050,1215 1215,1425 1425,1955
operation formerly known as convolution,|
|

636
00:18:03,470 --> 00:18:04,600
0,305 305,500 500,725 725,920 920,1130
we first think about this
我们首先认为这是一个高水平，

637
00:18:04,600 --> 00:18:06,415
0,195 195,345 345,540 540,860 1180,1815
as a high level,| supposing
|假设我们有一个 4x4 像素的块，

638
00:18:06,415 --> 00:18:07,555
0,150 150,300 300,555 555,960 960,1140
that we have what's called

639
00:18:07,555 --> 00:18:09,490
0,195 195,495 495,810 810,1145 1345,1935
a four by four {pixel,patch

640
00:18:09,490 --> 00:18:10,915
0,380 820,1050 1050,1140 1140,1290 1290,1425
-},| so you can see
|所以你可以看到这个 4x4 像素的小块，

641
00:18:10,915 --> 00:18:11,905
0,150 150,330 330,480 480,645 645,990
this four by four pixel

642
00:18:11,905 --> 00:18:14,110
0,275 415,815 1135,1470 1470,1805 1945,2205
patch,| represented in red as
|在左手边用红色表示为一个红色的方框，

643
00:18:14,110 --> 00:18:15,025
0,135 135,315 315,585 585,810 810,915
a red box on the

644
00:18:15,025 --> 00:18:16,240
0,150 150,375 375,695
left hand side,|
|

645
00:18:16,250 --> 00:18:18,600
0,400 930,1370 1370,1690 1770,2060 2060,2350
and let's suppose, for example,|
让我们假设，例如，|

646
00:18:19,040 --> 00:18:19,840
0,290 290,425 425,515 515,635 635,800
since we have a four
因为我们有一个 4x4 的块，

647
00:18:19,840 --> 00:18:21,055
0,180 180,390 390,710 790,1065 1065,1215
by four patch,| this is
|这将由这一层中的 16 个不同的权重组成，

648
00:18:21,055 --> 00:18:22,980
0,195 195,510 510,905 1225,1575 1575,1925
going to consist of sixteen

649
00:18:23,300 --> 00:18:25,230
0,400 480,1060 1140,1430 1430,1625 1625,1930
different weights in this layer,|
|

650
00:18:26,000 --> 00:18:27,210
0,320 320,440 440,650 650,890 890,1210
we're going to apply this
我们将应用相同的 4x4 ，

651
00:18:27,230 --> 00:18:29,035
0,400 690,995 995,1220 1220,1490 1490,1805
same four by four,| let's
|让我们称这不再是一个块，

652
00:18:29,035 --> 00:18:29,850
0,135 135,285 285,420 420,540 540,815
call this not a patch

653
00:18:29,870 --> 00:18:31,330
0,290 290,515 515,680 680,875 875,1460
anymore,| let's use the terminology
|让我们使用术语 filter ，

654
00:18:31,330 --> 00:18:32,875
0,290 430,780 780,975 975,1215 1215,1545
filter,| we'll apply this same
|我们将应用相同的 4x4 filter 作为输入，

655
00:18:32,875 --> 00:18:35,245
0,270 270,450 450,690 690,1025 2095,2370
four by four filter in

656
00:18:35,245 --> 00:18:36,340
0,210 210,435 435,630 630,855 855,1095
the input,| and use the
|并使用该操作的结果来定义下一层中神经元的状态，

657
00:18:36,340 --> 00:18:37,735
0,210 210,360 360,600 600,980 1030,1395
result of that operation to

658
00:18:37,735 --> 00:18:39,655
0,365 505,870 870,1235 1465,1755 1755,1920
define the state of the

659
00:18:39,655 --> 00:18:41,010
0,450 450,705 705,825 825,1020 1020,1355
neuron in the next layer,|
|

660
00:18:41,390 --> 00:18:43,045
0,275 275,550 1050,1370 1370,1475 1475,1655
{and,now -} we're going to
现在我们要将 filter 向右移动两个像素，

661
00:18:43,045 --> 00:18:45,175
0,210 210,435 435,755 1345,1745 1765,2130
shift our filter by, let's

662
00:18:45,175 --> 00:18:46,420
0,195 195,420 420,900 900,1140 1140,1245
say, two pixels to the

663
00:18:46,420 --> 00:18:48,370
0,260 880,1280 1300,1665 1665,1785 1785,1950
right,| and that's going to
|这将定义未来层中相邻位置的下一个神经元，

664
00:18:48,370 --> 00:18:50,560
0,225 225,465 465,770 1030,1580 1930,2190
define the next neuron in

665
00:18:50,560 --> 00:18:52,320
0,90 90,675 675,1040 1210,1485 1485,1760
the adjacent location in the

666
00:18:52,430 --> 00:18:54,235
0,350 350,700 1230,1490 1490,1625 1625,1805
future layer,| and we keep
|我们继续这样做，

667
00:18:54,235 --> 00:18:55,030
0,210 210,390 390,510 510,615 615,795
doing this| and you can
|你可以看到在右手边，

668
00:18:55,030 --> 00:18:56,155
0,240 240,435 435,600 600,855 855,1125
see that on the right

669
00:18:56,155 --> 00:18:58,320
0,255 255,605 1075,1470 1470,1785 1785,2165
hand side,| you're sliding over
|你不仅滑过了输入图像，

670
00:18:58,340 --> 00:18:59,650
0,290 290,545 545,860 860,1070 1070,1310
not only the input image,|
|

671
00:18:59,650 --> 00:19:01,120
0,270 270,570 570,885 885,1215 1215,1470
but you're also sliding over
还滑过了第二层的输出神经元，

672
00:19:01,120 --> 00:19:02,995
0,320 340,630 630,1070 1240,1560 1560,1875
the output neurons in the

673
00:19:02,995 --> 00:19:03,910
0,300 300,510 510,675 675,795 795,915
{secondary,layer -},| and this is
|这就是我们如何开始在非常高的水平上考虑卷积。

674
00:19:03,910 --> 00:19:04,615
0,120 120,240 240,375 375,540 540,705
how we can start to

675
00:19:04,615 --> 00:19:06,880
0,210 210,545 1105,1775 1885,2145 2145,2265
think about convolution at a

676
00:19:06,880 --> 00:19:08,720
0,195 195,435 435,630 630,920
very, very high level.|
|

677
00:19:09,020 --> 00:19:11,280
0,400 510,845 845,1025 1025,1360 1860,2260
But you're probably wondering, right,|
但你可能想知道，|

678
00:19:11,750 --> 00:19:13,410
0,335 335,670 810,1085 1085,1220 1220,1660
not just how the convolution
不只是卷积运算是如何工作的，

679
00:19:13,610 --> 00:19:14,920
0,380 380,710 710,965 965,1145 1145,1310
operation works,| but I think
|我认为这里真正要缩小范围的主要事情是，

680
00:19:14,920 --> 00:19:15,985
0,165 165,375 375,630 630,885 885,1065
the main thing here to

681
00:19:15,985 --> 00:19:18,030
0,245 715,1080 1080,1365 1365,1665 1665,2045
really narrow down on is,|
|

682
00:19:18,260 --> 00:19:20,230
0,335 335,905 905,1265 1265,1630 1680,1970
how convolution allows us to
卷积如何让我们了解我们说讨论的数据中的这些特征，这些模式，

683
00:19:20,230 --> 00:19:21,900
0,285 285,645 645,1005 1005,1335 1335,1670
learn these features, these patterns

684
00:19:22,190 --> 00:19:23,110
0,275 275,410 410,605 605,800 800,920
in the data that we

685
00:19:23,110 --> 00:19:24,400
0,135 135,390 390,705 705,990 990,1290
were talking about,| because ultimately
|因为这最终是我们的最终目标，

686
00:19:24,400 --> 00:19:26,050
0,345 345,590 640,1040 1060,1380 1380,1650
that's our final goal,| that's
|这才是我们这门课的真正目标，就是提取这些模式。

687
00:19:26,050 --> 00:19:26,905
0,105 105,300 300,525 525,690 690,855
our real goal for this

688
00:19:26,905 --> 00:19:28,045
0,210 210,405 405,645 645,915 915,1140
class is to extract those

689
00:19:28,045 --> 00:19:29,665
0,305 895,1140 1140,1335 1335,1470 1470,1620
patterns.| {So,let's -} make this
|所以让我们通过一个具体的例子来让这个问题变得非常具体，

690
00:19:29,665 --> 00:19:31,140
0,275 325,600 600,765 765,1055 1075,1475
very concrete by walking through

691
00:19:31,430 --> 00:19:34,200
0,400 420,820 1350,1700 1700,2050 2370,2770
maybe a concrete example, right,|
|

692
00:19:35,230 --> 00:19:36,690
0,400 480,815 815,1025 1025,1250 1250,1460
so suppose, for example, we
例如，假设我们想要构建一个卷积算法，

693
00:19:36,690 --> 00:19:39,200
0,165 165,345 345,620 970,1370 1600,2510
want to build a convolutional

694
00:19:40,060 --> 00:19:42,500
0,550 840,1220 1220,1600
algorithm| to detect
|来检测或者对图像中的 X 进行分类，

695
00:19:42,500 --> 00:19:44,240
0,195 195,840 840,1095 1095,1370 1480,1740
or classify an X in

696
00:19:44,240 --> 00:19:45,440
0,105 105,350 640,915 915,1065 1065,1200
an image,| {this,is -} the
|这是图像中的字母 X ，

697
00:19:45,440 --> 00:19:46,390
0,180 180,435 435,615 615,705 705,950
letter X in an image,|
|

698
00:19:47,010 --> 00:19:49,190
0,400 690,1090 1140,1415 1415,1970 1970,2180
and here, for simplicity, let's
这里，为了简单，假设我们只有黑白图像，

699
00:19:49,190 --> 00:19:50,110
0,150 150,330 330,480 480,630 630,920
just say we have only

700
00:19:50,250 --> 00:19:51,725
0,290 290,440 440,575 575,850 1170,1475
black and white images, right,|
|

701
00:19:51,725 --> 00:19:53,240
0,165 165,360 360,905
so every pixel
所以在这张图像中的每一个像素将用 0 或 1 表示，

702
00:19:53,250 --> 00:19:54,200
0,260 260,395 395,605 605,815 815,950
in this image will be

703
00:19:54,200 --> 00:19:55,505
0,260 430,690 690,855 855,1050 1050,1305
represented by either a zero

704
00:19:55,505 --> 00:19:57,395
0,270 270,575 865,1125 1125,1695 1695,1890
or one,| for simplicity, there's
|为了简单，这张图中没有灰度级，

705
00:19:57,395 --> 00:19:58,775
0,135 135,360 360,695 955,1230 1230,1380
no gray scale in this

706
00:19:58,775 --> 00:20:00,410
0,275 685,1020 1020,1230 1230,1425 1425,1635
image| and actually here so
|所以我们把黑色表示为 -1 ，白色表示为 1 ，

707
00:20:00,410 --> 00:20:02,585
0,270 270,1010 1240,1590 1590,1860 1860,2175
we're representing black as {-1

708
00:20:02,585 --> 00:20:03,785
0,285 285,465 465,660 660,885 885,1200
-} and white as {1

709
00:20:03,785 --> 00:20:06,160
0,395 925,1200 1200,1350 1350,1925 1975,2375
-},| so to classify, we
|所以要分类，我们不能简单地比较左手边和右手边，

710
00:20:06,570 --> 00:20:08,390
0,400 420,820 990,1235 1235,1480 1500,1820
simply cannot, you know, compare

711
00:20:08,390 --> 00:20:09,575
0,180 180,345 345,570 570,890 940,1185
the left hand side to

712
00:20:09,575 --> 00:20:11,120
0,105 105,255 255,465 465,785 1165,1545
the right hand side, right,|
|

713
00:20:11,120 --> 00:20:12,880
0,380 550,810 810,945 945,1125 1125,1760
because these are both Xs,|
因为这两个都是 X ，|

714
00:20:13,080 --> 00:20:14,290
0,260 260,395 395,590 590,860 860,1210
{but,you -} can see that,|
但是你可以看到，|

715
00:20:15,030 --> 00:20:16,265
0,400 540,860 860,1040 1040,1145 1145,1235
because the one on the
因为右手边的那个稍微旋转了一下，

716
00:20:16,265 --> 00:20:17,140
0,120 120,285 285,420 420,570 570,875
right hand side is slightly

717
00:20:17,520 --> 00:20:19,400
0,610 810,1085 1085,1325 1325,1610 1610,1880
rotated to some degree,| it's
|它不会直接和左边的 X 对齐，

718
00:20:19,400 --> 00:20:20,420
0,150 150,330 330,480 480,720 720,1020
not going to directly align

719
00:20:20,420 --> 00:20:21,125
0,195 195,330 330,480 480,615 615,705
with the X on the

720
00:20:21,125 --> 00:20:22,025
0,135 135,315 315,495 495,705 705,900
left hand side,| even though
|尽管它是一个 X ，

721
00:20:22,025 --> 00:20:23,240
0,150 150,300 300,450 450,725 955,1215
it is an X,| we
|我们希望在这两个图像中都检测到 X ，

722
00:20:23,240 --> 00:20:24,725
0,165 165,470 640,960 960,1350 1350,1485
want to detect X in

723
00:20:24,725 --> 00:20:26,255
0,165 165,300 300,435 435,725 1195,1530
both of these images,| so
|所以我们需要考虑如何更巧妙地检测到定义 X 的那些特征。

724
00:20:26,255 --> 00:20:27,065
0,195 195,330 330,450 450,600 600,810
we need to think about

725
00:20:27,065 --> 00:20:28,040
0,180 180,330 330,525 525,750 750,975
how we can detect those

726
00:20:28,040 --> 00:20:29,240
0,320 340,660 660,855 855,990 990,1200
features that define an X

727
00:20:29,240 --> 00:20:30,905
0,210 210,345 345,525 525,1070 1420,1665
a bit {more,cleverly -}.| So
|所以让我们看看我们如何使用卷积来做到这一点，

728
00:20:30,905 --> 00:20:31,520
0,165 165,240 240,360 360,495 495,615
let's see how we can

729
00:20:31,520 --> 00:20:33,220
0,255 255,890 1030,1290 1290,1425 1425,1700
use convolutions {to,do -} that,|
|

730
00:20:33,690 --> 00:20:34,685
0,245 245,350 350,515 515,755 755,995
so in this case for
例如，在这种情况下，

731
00:20:34,685 --> 00:20:36,335
0,305 595,995 1015,1290 1290,1470 1470,1650
example,| instead we want our
|我们不是希望我们的模型逐块地比较这个 X 的图像，

732
00:20:36,335 --> 00:20:38,510
0,255 255,600 600,965 1165,1565 1855,2175
model to compare images of

733
00:20:38,510 --> 00:20:40,355
0,240 240,560 1060,1365 1365,1575 1575,1845
this X piece by piece

734
00:20:40,355 --> 00:20:42,680
0,315 315,615 615,900 900,1235 2065,2325
{or,patch -} by patch,| and
|我们寻找的重要块正是这些将定义我们的 X 的特征，

735
00:20:42,680 --> 00:20:44,390
0,195 195,525 525,1190 1270,1545 1545,1710
the important patches that we

736
00:20:44,390 --> 00:20:46,130
0,225 225,555 555,930 930,1310 1390,1740
look for are exactly these

737
00:20:46,130 --> 00:20:47,825
0,350 490,765 765,1005 1005,1370 1390,1695
features that will define our

738
00:20:47,825 --> 00:20:49,415
0,305 835,1095 1095,1215 1215,1350 1350,1590
X,| so if our model
|所以如果我们的模型能够在输入中找到这些大致相同位置的粗略特征块，

739
00:20:49,415 --> 00:20:51,040
0,270 270,540 540,855 855,1205 1225,1625
can find these rough feature

740
00:20:51,060 --> 00:20:53,260
0,670 1080,1480 1500,1760 1760,1910 1910,2200
patches roughly in the same

741
00:20:53,280 --> 00:20:53,620
0,290
position

742
00:20:53,810 --> 00:20:55,360
0,260 260,500 500,880 1050,1355 1355,1550
in our input,| then we
|我们可以确定，或者我们可以推断，

743
00:20:55,360 --> 00:20:56,635
0,255 255,615 615,885 885,1035 1035,1275
can determine or we can

744
00:20:56,635 --> 00:20:58,740
0,365 865,1265 1375,1710 1710,1875 1875,2105
infer,| that these two images
|这两个图像是相同类型或相同字母，

745
00:20:58,910 --> 00:21:01,440
0,400 510,800 800,965 965,1240 2130,2530
are of the same type

746
00:21:01,460 --> 00:21:03,265
0,275 275,425 425,650 650,1000 1440,1805
or the same letter, right,|
|

747
00:21:03,265 --> 00:21:04,090
0,225 225,330 330,435 435,570 570,825
it can get a lot
它可以比简单地测量这两张图像之间的相似性好得多，

748
00:21:04,090 --> 00:21:06,160
0,380 550,950 1120,1485 1485,1920 1920,2070
better than simply measuring the

749
00:21:06,160 --> 00:21:07,710
0,720 720,960 960,1155 1155,1305 1305,1550
similarity between these two images,|
|

750
00:21:08,000 --> 00:21:09,265
0,350 350,695 695,920 920,1115 1115,1265
because we're operating at the
因为我们是在块级别操作的，

751
00:21:09,265 --> 00:21:10,750
0,240 240,555 555,905 1015,1305 1305,1485
patch level,| {so,think -} of
|所以把每个块想象成一个微缩的图像，

752
00:21:10,750 --> 00:21:12,490
0,285 285,680 1060,1350 1350,1530 1530,1740
each patch almost like a

753
00:21:12,490 --> 00:21:14,485
0,495 495,770 1120,1425 1425,1680 1680,1995
miniature image, right,| a small
|一个小的二维值数组，

754
00:21:14,485 --> 00:21:16,470
0,225 225,810 810,1175 1435,1710 1710,1985
two dimensional array of values|
|

755
00:21:16,580 --> 00:21:17,880
0,260 260,395 395,545 545,755 755,1300
and we can use filters
我们可以使用 filter 来捕捉这些小块或小图像何时出现，

756
00:21:18,080 --> 00:21:19,675
0,305 305,500 500,755 755,1120 1260,1595
to pick up on when

757
00:21:19,675 --> 00:21:22,195
0,335 1405,1725 1725,2115 2115,2295 2295,2520
these small patches or small

758
00:21:22,195 --> 00:21:24,700
0,305 835,1235 1795,2175 2175,2400 2400,2505
images occur,| so in the
|所以在 X 的情况下，

759
00:21:24,700 --> 00:21:26,310
0,150 150,315 315,855 855,1080 1080,1610
case of Xs,| these filters
|这些 filter 可能代表语义的东西，

760
00:21:26,330 --> 00:21:29,140
0,320 320,640 1470,2135 2135,2440 2490,2810
may represent semantic things,| for
|例如对角线或交叉点，

761
00:21:29,140 --> 00:21:31,645
0,320 520,780 780,1305 1305,1580 2140,2505
example the diagonal lines or

762
00:21:31,645 --> 00:21:33,310
0,285 285,810 810,1065 1065,1385 1405,1665
the crossings,| that capture all
|它们捕捉到了 X 的所有重要特征，

763
00:21:33,310 --> 00:21:34,930
0,135 135,330 330,585 585,1350 1350,1620
of the important characteristics of

764
00:21:34,930 --> 00:21:37,320
0,165 165,410 1330,1725 1725,2100 2100,2390
the X,| so we'll probably
|所以我们可能会在 X 的任何图像中捕捉到我们字母的臂部和中心的这些特征，

765
00:21:37,340 --> 00:21:39,805
0,400 630,1030 1140,1540 1980,2285 2285,2465
capture these features in the

766
00:21:39,805 --> 00:21:41,605
0,275 475,750 750,915 915,1205 1525,1800
arms and the center of

767
00:21:41,605 --> 00:21:43,740
0,195 195,515 1345,1635 1635,1830 1830,2135
our letter in any image

768
00:21:43,790 --> 00:21:45,145
0,260 260,365 365,610 690,1070 1070,1355
of an X,| regardless of
|无论图像如何平移或旋转等等，

769
00:21:45,145 --> 00:21:46,555
0,210 210,390 390,615 615,965 1165,1410
how that image is, you

770
00:21:46,555 --> 00:21:48,400
0,245 325,885 885,1095 1095,1575 1575,1845
know, translated or rotated or

771
00:21:48,400 --> 00:21:49,795
0,165 165,440 670,960 960,1170 1170,1395
{so,on -},| and note that
|注意，即使在这些较小的矩阵中，

772
00:21:49,795 --> 00:21:51,870
0,270 270,525 525,780 780,1145 1285,2075
even in these smaller matrices,

773
00:21:52,100 --> 00:21:53,660
0,380 380,650 650,940
right,| these are
|这些是权重的 filter ，

774
00:21:53,790 --> 00:21:55,535
0,560 560,860 860,1340 1340,1580 1580,1745
filters of weights, right,| {these,are
|这些也只是这些块中每个像素的数值，

775
00:21:55,535 --> 00:21:57,070
0,210 210,420 420,690 690,1260 1260,1535
-} also just numerical values

776
00:21:57,390 --> 00:21:59,800
0,400 1140,1490 1490,1925 1925,2105 2105,2410
of each pixel in these

777
00:21:59,970 --> 00:22:01,535
0,365 365,920 920,1115 1115,1340 1340,1565
many patches| is simply just
|只是一个简单的数值，

778
00:22:01,535 --> 00:22:02,840
0,180 180,585 585,845 895,1125 1125,1305
a {numerical,value -},| there are
|在某种效果上也有图像，

779
00:22:02,840 --> 00:22:04,630
0,225 225,500 910,1215 1215,1455 1455,1790
also images in some effect,

780
00:22:04,950 --> 00:22:07,025
0,400 660,1060 1230,1550 1550,1850 1850,2075
right,| and all that's really
|而所有这些都是这个问题留下的，

781
00:22:07,025 --> 00:22:09,260
0,365 565,870 870,1110 1110,1445 1975,2235
left {in,this -} problem,| and
|在我们讨论的这个想法中，

782
00:22:09,260 --> 00:22:11,090
0,150 150,440 940,1305 1305,1545 1545,1830
in this idea that we're

783
00:22:11,090 --> 00:22:13,420
0,320 850,1250 1390,1725 1725,1995 1995,2330
discussing| is to define that
|我们定义了一种操作，

784
00:22:13,470 --> 00:22:15,575
0,400 960,1295 1295,1550 1550,1820 1820,2105
operation,| that can take these
|它可以获取这些微小的块，

785
00:22:15,575 --> 00:22:17,950
0,465 465,995 1315,1665 1665,1995 1995,2375
miniature patches| and try to
|并尝试提取，检测，

786
00:22:18,000 --> 00:22:19,450
0,275 275,550 570,815 815,1055 1055,1450
pick up, you know, detect,|
|

787
00:22:19,470 --> 00:22:21,125
0,305 305,500 500,920 920,1270 1380,1655
when those patches occur in
这些块何时出现在图像中，以及何时可能不出现。

788
00:22:21,125 --> 00:22:22,175
0,150 150,390 390,660 660,840 840,1050
your image and when they

789
00:22:22,175 --> 00:22:23,500
0,255 255,585 585,905
maybe don't occur.|
|

790
00:22:24,730 --> 00:22:26,115
0,400 420,740 740,950 950,1160 1160,1385
And that brings us right
这就把我们带回了卷积的概念，

791
00:22:26,115 --> 00:22:27,735
0,305 355,690 690,945 945,1260 1260,1620
back to this notion of

792
00:22:27,735 --> 00:22:29,805
0,635 745,1020 1020,1485 1485,1770 1770,2070
convolution,| {so,convolution -} is exactly
|所以卷积正是可以解决这个问题的运算，

793
00:22:29,805 --> 00:22:31,245
0,345 345,725 775,1050 1050,1215 1215,1440
that operation that will solve

794
00:22:31,245 --> 00:22:33,780
0,240 240,545 985,1650 1650,2225 2275,2535
that problem,| convolution preserves all
|卷积保存输入数据的空间信息，

795
00:22:33,780 --> 00:22:35,355
0,135 135,315 315,825 825,1190 1300,1575
of that spatial information in

796
00:22:35,355 --> 00:22:37,335
0,275 325,725 895,1230 1230,1565 1615,1980
our input| by learning image
|通过学习保存在输入数据中小的方块区域中的图像特征。

797
00:22:37,335 --> 00:22:39,555
0,365 565,900 900,1235 1255,1655 1795,2220
features in those smaller squares

798
00:22:39,555 --> 00:22:41,640
0,285 285,575 1105,1425 1425,1830 1830,2085
of regions that preserve our

799
00:22:41,640 --> 00:22:43,185
0,240 240,495 495,860 1120,1410 1410,1545
{input,data -}.| So just to
|所以，为了给出另一个具体的例子来执行这个操作，

800
00:22:43,185 --> 00:22:45,450
0,195 195,545 745,1080 1080,1415 1885,2265
give another concrete example to

801
00:22:45,450 --> 00:22:47,940
0,330 330,680 880,1280 1840,2220 2220,2490
perform this operation,| we need
|我们需要进行逐个元素的乘法，

802
00:22:47,940 --> 00:22:49,400
0,165 165,440 460,735 735,1010 1060,1460
to do an element wise

803
00:22:49,840 --> 00:22:51,975
0,790 900,1220 1220,1415 1415,1640 1640,2135
multiplication,| between the filter matrix,
|在 filter 矩阵，那些小块以及输入图像的块之间，

804
00:22:51,975 --> 00:22:54,315
0,225 225,735 735,1265 1795,2115 2115,2340
those miniature patches, as well

805
00:22:54,315 --> 00:22:56,060
0,305 445,825 825,1205 1225,1485 1485,1745
as the patch of our

806
00:22:56,380 --> 00:22:57,420
0,260 260,500 500,785 785,935 935,1040
input image, right,| so you
|所以你有两个块，

807
00:22:57,420 --> 00:22:58,665
0,195 195,530 580,840 840,1035 1035,1245
have basically think {of,two -}

808
00:22:58,665 --> 00:23:00,090
0,435 435,615 615,810 810,1125 1125,1425
patches,| you have the weight
|你有权重矩阵块，你想要检测的东西，你可以在这里的左上角看到，

809
00:23:00,090 --> 00:23:01,830
0,555 555,890 1090,1380 1380,1560 1560,1740
matrix patch, the thing that

810
00:23:01,830 --> 00:23:02,970
0,165 165,345 345,585 585,900 900,1140
you want to detect, which

811
00:23:02,970 --> 00:23:03,615
0,120 120,285 285,435 435,525 525,645
you can see on the

812
00:23:03,615 --> 00:23:05,265
0,210 210,450 450,675 675,995 1375,1650
{top,left -} hand here,| and
|你还有第二个块，这是你希望在输入图像中与之进行比较的东西，

813
00:23:05,265 --> 00:23:06,645
0,225 225,450 450,660 660,995 1075,1380
you also have the secondary

814
00:23:06,645 --> 00:23:07,725
0,305 355,645 645,810 810,945 945,1080
patch, which is the thing

815
00:23:07,725 --> 00:23:09,150
0,150 150,405 405,750 750,1080 1080,1425
that you are looking to

816
00:23:09,150 --> 00:23:10,635
0,255 255,435 435,740 970,1245 1245,1485
compare it {against,in -} your

817
00:23:10,635 --> 00:23:12,345
0,210 210,455 1075,1380 1380,1530 1530,1710
input image,| and the question
|问题是，

818
00:23:12,345 --> 00:23:13,860
0,225 225,515 715,1035 1035,1290 1290,1515
is how,| how similar are
|你在它们之间观察到的这两块有多相似，

819
00:23:13,860 --> 00:23:15,360
0,225 225,435 435,980 1030,1290 1290,1500
these two patches that you

820
00:23:15,360 --> 00:23:18,060
0,350 790,1095 1095,1400 2050,2430 2430,2700
observe between them,| so for
|例如，这会产生一个 3x3 的矩阵，

821
00:23:18,060 --> 00:23:20,840
0,290 790,1190 1690,2090 2230,2505 2505,2780
example, this results in a

822
00:23:20,980 --> 00:23:22,200
0,290 290,485 485,650 650,1085 1085,1220
three by three matrix,| because
|因为你要在两个小的 3x3 矩阵进行逐个元素的乘法，

823
00:23:22,200 --> 00:23:23,220
0,180 180,270 270,405 405,675 675,1020
you're doing an element wise

824
00:23:23,220 --> 00:23:25,140
0,680 940,1275 1275,1605 1605,1890
multiplication between two small

825
00:23:25,140 --> 00:23:26,565
0,180 180,330 330,480 480,1100 1120,1425
three by three matrices,| you're
|你会得到另外一个 3x3 的矩阵，

826
00:23:26,565 --> 00:23:27,440
0,90 90,210 210,330 330,540 540,875
going to be left with

827
00:23:27,670 --> 00:23:29,360
0,400 450,725 725,890 890,1055 1055,1690
another three by three matrix,|
|

828
00:23:29,920 --> 00:23:31,035
0,260 260,440 440,710 710,950 950,1115
{in,this -} case, all of
在这种情况下，所有的矩阵，

829
00:23:31,035 --> 00:23:32,055
0,120 120,600 600,765 765,900 900,1020
the matrix,| all of the
|这个结果矩阵的所有元素都是 1 ，

830
00:23:32,055 --> 00:23:34,125
0,245 535,825 825,1095 1095,1455 1455,2070
elements of this resulting matrix,

831
00:23:34,125 --> 00:23:35,600
0,135 135,285 285,465 465,755 1075,1475
you can see here are

832
00:23:35,680 --> 00:23:38,120
0,400 930,1205 1205,1355 1355,1630 2040,2440
ones,| because in every location
|因为在 filter 中的每个位置，在图像块中的每个位置，都是匹配的，

833
00:23:38,590 --> 00:23:40,280
0,400 480,755 755,1030 1110,1400 1400,1690
in the filter and every

834
00:23:40,300 --> 00:23:42,860
0,400 420,820 870,1270 1650,2050 2160,2560
location in the image patch

835
00:23:43,150 --> 00:23:44,685
0,260 260,425 425,730 780,1370 1370,1535
we are {perfectly,matching -},| so
|所以，当我们做逐个像素乘法时，我们全部得到 1 ，

836
00:23:44,685 --> 00:23:45,390
0,105 105,225 225,345 345,480 480,705
when we do that element

837
00:23:45,390 --> 00:23:46,970
0,270 270,885 885,1125 1125,1275 1275,1580
wise multiplication, we {get,ones -}

838
00:23:47,110 --> 00:23:49,035
0,400 1080,1325 1325,1505 1505,1745 1745,1925
everywhere,| the last step is
|最后一步是我们要累加这个矩阵的结果，逐个像素乘法，

839
00:23:49,035 --> 00:23:49,755
0,135 135,255 255,390 390,555 555,720
that we need to sum

840
00:23:49,755 --> 00:23:51,380
0,275 535,885 885,1155 1155,1350 1350,1625
up the result of that

841
00:23:51,520 --> 00:23:53,810
0,670 780,1085 1085,1340 1340,1610 1610,2290
matrix, {that,element -} wise multiplication,|
|

842
00:23:54,580 --> 00:23:56,265
0,290 290,515 515,785 785,1120 1320,1685
and the result is let's
在这种情况下，结果是 9 ，

843
00:23:56,265 --> 00:23:57,440
0,165 165,450 450,690 690,870 870,1175
say 9, in this case,|
|

844
00:23:58,060 --> 00:23:58,950
0,245 245,350 350,470 470,650 650,890
everything was a one, it's
每个都是 1 ，它是一个 3x3 矩阵，所以结果是 9 。

845
00:23:58,950 --> 00:23:59,910
0,90 90,225 225,375 375,525 525,960
a three by three matrix,

846
00:23:59,910 --> 00:24:00,890
0,120 120,285 285,480 480,675 675,980
so the result is 9.|
|

847
00:24:02,640 --> 00:24:03,960
0,400
Now,
现在，让我们再考虑一个例子，

848
00:24:04,090 --> 00:24:05,630
0,425 425,725 725,995 995,1220 1220,1540
let's consider one more example,

849
00:24:05,830 --> 00:24:07,425
0,395 395,695 695,860 860,1120 1320,1595
right,| now we have this
|现在我们有这张绿色的图像，

850
00:24:07,425 --> 00:24:08,850
0,255 255,570 570,905 985,1275 1275,1425
image in green,| and we
|我们想要检测黄色的 filter ，

851
00:24:08,850 --> 00:24:10,280
0,150 150,390 390,740 790,1110 1110,1430
want to detect this filter

852
00:24:10,810 --> 00:24:12,420
0,305 305,610
in yellow,|
|

853
00:24:12,640 --> 00:24:14,060
0,320 320,500 500,650 650,800 800,1420
suppose we want to compute
假设我们想用这个 3x3 filter 来计算这个 5x5 图像的卷积，

854
00:24:14,170 --> 00:24:15,780
0,290 290,850 930,1220 1220,1400 1400,1610
the convolution of this {5x5

855
00:24:15,780 --> 00:24:17,145
0,195 195,360 360,650 820,1125 1125,1365
- -} image with this

856
00:24:17,145 --> 00:24:19,110
0,210 210,405 405,720 720,1115 1705,1965
{3x3 - -} filter,| {to,do
|要做到这一点，

857
00:24:19,110 --> 00:24:19,995
0,120 120,330 330,525 525,660 660,885
-} this,| we need to
|我们需要覆盖整个图像，

858
00:24:19,995 --> 00:24:22,545
0,335 355,755 865,1265 1345,2075 2275,2550
cover basically the entirety of

859
00:24:22,545 --> 00:24:24,540
0,195 195,515 745,1145 1285,1710 1710,1995
our image| by sliding over
|通过逐段滑动这个 filter ，

860
00:24:24,540 --> 00:24:25,875
0,210 210,500 670,960 960,1155 1155,1335
this filter piece by piece|
|

861
00:24:25,875 --> 00:24:27,930
0,165 165,635 835,1110 1110,1830 1830,2055
and comparing the similarity or
并比较整个图像中这个 filter 的相似性或卷积，

862
00:24:27,930 --> 00:24:29,720
0,165 165,680 940,1260 1260,1485 1485,1790
the convolution of this filter

863
00:24:29,860 --> 00:24:31,680
0,335 335,590 590,860 860,1210
across the entire image,|
|

864
00:24:32,180 --> 00:24:33,385
0,275 275,425 425,590 590,860 860,1205
and we do that, again
再次，我们通过同样的机制做到这一点，

865
00:24:33,385 --> 00:24:34,930
0,240 240,390 390,585 585,905 1255,1545
through the same mechanism,| {at,every
|在每个位置，

866
00:24:34,930 --> 00:24:36,430
0,290 370,770 820,1095 1095,1380 1380,1500
-} location,| we compute an
|我们计算那个块和在图片中位置的逐个元素乘法，

867
00:24:36,430 --> 00:24:38,365
0,225 225,590 730,1455 1455,1710 1710,1935
element wise multiplication of that

868
00:24:38,365 --> 00:24:39,700
0,335 415,690 690,930 930,1185 1185,1335
patch with that location on

869
00:24:39,700 --> 00:24:40,885
0,90 90,320 520,810 810,1005 1005,1185
the image,| add up all
|将所有结果条目相加，

870
00:24:40,885 --> 00:24:42,720
0,150 150,345 345,615 615,1145 1435,1835
of the resulting entries| and
|并将其传递到下一层。

871
00:24:43,100 --> 00:24:44,605
0,335 335,635 635,995 995,1280 1280,1505
pass that to our next

872
00:24:44,605 --> 00:24:46,150
0,335 715,960 960,1155 1155,1350 1350,1545
layer.| So let's walk through
|所以让我们来看一下，

873
00:24:46,150 --> 00:24:47,350
0,210 210,540 540,885 885,1035 1035,1200
it,| first, let's start off
|首先，让我们从左上角开始，

874
00:24:47,350 --> 00:24:48,610
0,150 150,255 255,500 640,960 960,1260
in the upper left {hand,corner

875
00:24:48,610 --> 00:24:50,610
0,380 940,1260 1260,1485 1485,1695 1695,2000
-},| we place our filter
|我们将 filter 放置在图像的左上角，

876
00:24:50,660 --> 00:24:52,300
0,400 750,995 995,1175 1175,1415 1415,1640
over the upper left hand

877
00:24:52,300 --> 00:24:53,665
0,255 255,450 450,600 600,890 1120,1365
corner {of,our -} image,| we
|我们对元素进行相乘，

878
00:24:53,665 --> 00:24:55,015
0,225 225,525 525,1035 1035,1215 1215,1350
element wise multiply,| we add
|我们将所有结果相加，

879
00:24:55,015 --> 00:24:56,740
0,135 135,300 300,605 1045,1445 1465,1725
up all the results| and
|我们得到 4 ，

880
00:24:56,740 --> 00:24:57,895
0,105 105,255 255,560 700,960 960,1155
we get 4| and that
|这个 4 将被放入下一层，

881
00:24:57,895 --> 00:24:58,945
0,240 240,420 420,600 600,780 780,1050
four is going to be

882
00:24:58,945 --> 00:25:00,930
0,395 535,870 870,1095 1095,1385 1585,1985
placed {into,the -} next layer,|
|

883
00:25:01,250 --> 00:25:03,040
0,290 290,530 530,880 1080,1475 1475,1790
this next layer again is
下一层同样是另一幅图像，

884
00:25:03,040 --> 00:25:04,810
0,240 240,560 970,1215 1215,1485 1485,1770
another image,| but it's determined
|但它被确定为我们卷积运算的结果，

885
00:25:04,810 --> 00:25:06,190
0,255 255,495 495,830 1000,1245 1245,1380
as the {result,of -} our

886
00:25:06,190 --> 00:25:08,695
0,530 580,980 1600,1965 1965,2250 2250,2505
convolution operation,| we slide over
|我们滑动那个 filter 到下一个位置，

887
00:25:08,695 --> 00:25:09,810
0,240 240,540 540,780 780,870 870,1115
{that,filter -} to the next

888
00:25:09,830 --> 00:25:11,820
0,400 600,845 845,1085 1085,1480 1590,1990
location,| the next location provides
|下一个位置提供了图像中的下一个值，

889
00:25:11,990 --> 00:25:13,510
0,290 290,530 530,880 1110,1370 1370,1520
the next value in our

890
00:25:13,510 --> 00:25:14,740
0,240 240,450 450,615 615,885 885,1230
image| and we keep repeating
|我们可以一遍又一遍地重复这个过程，

891
00:25:14,740 --> 00:25:16,135
0,165 165,470 610,975 975,1200 1200,1395
this process over and over

892
00:25:16,135 --> 00:25:17,860
0,180 180,420 420,815 1195,1485 1485,1725
and over again| until we've
|直到我们的 filter 覆盖了整个图像，

893
00:25:17,860 --> 00:25:19,810
0,260 700,1020 1020,1320 1320,1650 1650,1950
{covered,our -} filter over the

894
00:25:19,810 --> 00:25:21,550
0,270 270,590 1090,1365 1365,1530 1530,1740
entire image,| and as a
|我们还完整地填写了输出特征图的结果，

895
00:25:21,550 --> 00:25:23,815
0,315 315,795 795,1190 1600,1980 1980,2265
result, we've also completely filled

896
00:25:23,815 --> 00:25:25,290
0,240 240,540 540,905 955,1215 1215,1475
{out,the -} result of our

897
00:25:25,760 --> 00:25:27,400
0,335 335,635 635,1000 1110,1430 1430,1640
output feature map,| the output
|输出特征图就是，你可以认为是，

898
00:25:27,400 --> 00:25:28,510
0,225 225,465 465,660 660,900 900,1110
feature map is basically what

899
00:25:28,510 --> 00:25:29,370
0,120 120,270 270,435 435,585 585,860
you can think of is|
|

900
00:25:29,480 --> 00:25:31,255
0,350 350,680 680,1190 1190,1490 1490,1775
how closely aligned our filter
我们的 filter 每个元素与输入图像中每个位置的匹配程度有多高。

901
00:25:31,255 --> 00:25:32,500
0,365 595,855 855,1115
is to every

902
00:25:32,500 --> 00:25:33,960
0,380 460,720 720,960 960,1200 1200,1460
location in our input image.|
|

903
00:25:35,810 --> 00:25:37,210
0,400 510,800 800,965 965,1265 1265,1400
So now that we've kind
现在我们已经了解了定义卷积运算的机制，

904
00:25:37,210 --> 00:25:38,190
0,120 120,360 360,570 570,705 705,980
of gone through the mechanism

905
00:25:38,570 --> 00:25:40,300
0,320 320,770 770,1130 1130,1475 1475,1730
that defines this operation of

906
00:25:40,300 --> 00:25:42,655
0,560 1180,1575 1575,1740 1740,1995 1995,2355
convolution,| let's see how different
|让我们看看如何使用不同的 filter ，

907
00:25:42,655 --> 00:25:45,120
0,635 1165,1485 1485,1695 1695,1985 2065,2465
filters| could be used to
|来检测数据中的不同类型的模式，

908
00:25:45,230 --> 00:25:46,740
0,380 380,725 725,995 995,1205 1205,1510
detect different types of patterns

909
00:25:47,240 --> 00:25:48,910
0,260 260,425 425,730 1200,1490 1490,1670
in our data,| {so,for -}
|举个例子，让我们来看看这张女性的脸部照片，

910
00:25:48,910 --> 00:25:50,095
0,225 225,510 510,660 660,900 900,1185
example, let's take this picture

911
00:25:50,095 --> 00:25:52,530
0,195 195,330 330,750 750,1025 2035,2435
of a woman's face| and
|以及对这张照片应用三种不同滤镜的结果，

912
00:25:52,550 --> 00:25:54,595
0,395 395,695 695,1000 1140,1540 1710,2045
the output of applying three

913
00:25:54,595 --> 00:25:56,935
0,335 385,765 765,1035 1035,1535 2005,2340
different types of filters to

914
00:25:56,935 --> 00:25:58,270
0,300 300,665 805,1095 1095,1230 1230,1335
this picture, right,| so you
|这样你就可以看到准确的 filter ，

915
00:25:58,270 --> 00:25:59,640
0,150 150,330 330,620 640,1005 1005,1370
can see the exact filter,|
|

916
00:26:00,200 --> 00:26:01,240
0,290 290,470 470,695 695,860 860,1040
they're all {3x3 - -}
它们都是 3x3 的 filter ，

917
00:26:01,240 --> 00:26:02,605
0,375 375,570 570,780 780,1005 1005,1365
filters,| so the exact filters
|所以你可以在相应的右下角看到准确的 filter ，

918
00:26:02,605 --> 00:26:03,295
0,120 120,270 270,435 435,555 555,690
you can see on the

919
00:26:03,295 --> 00:26:04,870
0,240 240,495 495,705 705,1025 1285,1575
bottom right hand corner of

920
00:26:04,870 --> 00:26:06,985
0,165 165,855 855,1190 1630,1905 1905,2115
the {corresponding,face -},| and by
|通过应用这三种不同的 filter ，

921
00:26:06,985 --> 00:26:08,380
0,270 270,540 540,750 750,960 960,1395
applying these three different filters,|
|

922
00:26:08,380 --> 00:26:09,145
0,135 135,285 285,465 465,615 615,765
you can see how we
你可以看到我们如何实现截然不同的结果，

923
00:26:09,145 --> 00:26:11,035
0,210 210,545 565,1215 1215,1475 1585,1890
can achieve {drastically,different -} results,|
|

924
00:26:11,035 --> 00:26:12,415
0,210 210,465 465,750 750,1085 1105,1380
and simply by changing the
通过简单地改变这些 3x3 矩阵中权重，

925
00:26:12,415 --> 00:26:14,245
0,515 775,1035 1035,1215 1215,1530 1530,1830
weights that are present in

926
00:26:14,245 --> 00:26:15,690
0,255 255,480 480,660 660,825 825,1445
these {3x3 - -} matrices,|
|

927
00:26:15,950 --> 00:26:17,310
0,260 260,425 425,605 605,755 755,1360
you can see the variability
你就可以看到我们可以检测到的不同类型特征的可变性，

928
00:26:17,360 --> 00:26:18,600
0,290 290,530 530,770 770,950 950,1240
of different types of features

929
00:26:19,040 --> 00:26:20,410
0,275 275,440 440,680 680,1030 1080,1370
{that,we -} can detect,| so
|例如，我们可以设计 filter 来锐化图像，

930
00:26:20,410 --> 00:26:21,565
0,180 180,420 420,645 645,870 870,1155
for example, we can design

931
00:26:21,565 --> 00:26:24,145
0,545 925,1325 1465,1865 1975,2445 2445,2580
filters that can sharpen an

932
00:26:24,145 --> 00:26:26,790
0,275 505,905 1075,1455 1455,2025 2025,2645
image,| make the edges sharper
|使图像中的边缘更清晰，

933
00:26:26,870 --> 00:26:28,165
0,305 305,470 470,730 840,1115 1115,1295
in the image,| we can
|我们可以设计 filter 来提取边缘，

934
00:26:28,165 --> 00:26:29,550
0,225 225,615 615,780 780,1020 1020,1385
design {filters,that -} will extract

935
00:26:29,720 --> 00:26:32,230
0,580 1020,1295 1295,1570 1740,2140 2160,2510
edges,| we can do stronger
|我们可以进行更强的边缘检测，

936
00:26:32,230 --> 00:26:34,015
0,330 330,750 750,960 960,1260 1260,1785
edge detection| by again modifying
|通过再次修改所有这些 filter 中的权重。

937
00:26:34,015 --> 00:26:35,125
0,180 180,585 585,840 840,975 975,1110
the weights in all of

938
00:26:35,125 --> 00:26:35,920
0,195 195,695
those filters.|
|

939
00:26:37,420 --> 00:26:38,430
0,260 260,380 380,560 560,785 785,1010
So I hope now that
所以现在我希望你们所有人都能体会到这些力量，

940
00:26:38,430 --> 00:26:39,150
0,180 180,300 300,450 450,600 600,720
all of you can kind

941
00:26:39,150 --> 00:26:40,730
0,180 180,500 550,855 855,1160 1180,1580
of appreciate the power of,|
|

942
00:26:40,780 --> 00:26:42,260
0,260 260,455 455,740 740,1085 1085,1480
you know, number one is
首先，是这些 filter 操作，

943
00:26:42,370 --> 00:26:43,875
0,320 320,785 785,1100 1100,1355 1355,1505
these filtering operations| and how
|以及我们数学地定义它们，

944
00:26:43,875 --> 00:26:46,160
0,165 165,390 390,645 645,965 1585,2285
we can define them mathematically|
|

945
00:26:46,300 --> 00:26:47,510
0,260 260,410 410,605 605,860 860,1210
in the form of these
在这些更小的基于块的运算和矩阵的形式，

946
00:26:47,830 --> 00:26:50,370
0,400 750,1130 1130,1510 1740,2140 2250,2540
smaller patch based operations and

947
00:26:50,370 --> 00:26:51,825
0,650 790,1050 1050,1170 1170,1275 1275,1455
matrices,| that we can then
|然后我们可以在图像上滑动，

948
00:26:51,825 --> 00:26:53,475
0,225 225,450 450,645 645,905 1375,1650
slide over an image,| {and,these
|这些概念是如此强大，

949
00:26:53,475 --> 00:26:55,190
0,210 210,915 915,1155 1155,1410 1410,1715
-} concepts are so powerful,|
|

950
00:26:55,270 --> 00:26:56,685
0,290 290,515 515,770 770,995 995,1415
because number one, they preserve
因为首先，它们保留了我们原始输入的空间信息，

951
00:26:56,685 --> 00:26:58,695
0,210 210,690 690,1055 1525,1800 1800,2010
the spatial information of our

952
00:26:58,695 --> 00:27:01,440
0,335 565,965 1225,1625 1675,2075 2365,2745
original input,| while still performing
|同时仍在执行特征提取，

953
00:27:01,440 --> 00:27:03,520
0,345 345,710 730,1430
this feature extraction,|
|

954
00:27:03,520 --> 00:27:04,830
0,320 340,615 615,795 795,1005 1005,1310
now you can think of,|
现在，你可以想到，|

955
00:27:05,000 --> 00:27:06,970
0,275 275,550 570,1205 1205,1520 1520,1970
instead of defining those filters,
不是像我们在上一张幻灯片中所说的那样定义这些 filter ，

956
00:27:06,970 --> 00:27:07,765
0,180 180,360 360,525 525,660 660,795
like we said on the

957
00:27:07,765 --> 00:27:08,965
0,255 255,615 615,855 855,990 990,1200
previous slide,| what if we
|如果我们尝试学习它们，

958
00:27:08,965 --> 00:27:10,200
0,225 225,375 375,555 555,855 855,1235
tried to learn them,| and
|记住，这些 filter 是我们数据中重要模式的某种代理，

959
00:27:10,220 --> 00:27:11,650
0,395 395,710 710,890 890,1055 1055,1430
remember again, that those filters

960
00:27:11,650 --> 00:27:13,435
0,290 400,645 645,810 810,1425 1425,1785
are kind of proxies for

961
00:27:13,435 --> 00:27:14,605
0,300 300,600 600,810 810,945 945,1170
important patterns in our data,|
|

962
00:27:14,605 --> 00:27:15,715
0,180 180,390 390,690 690,885 885,1110
{so,our -} neural network could
所以我们的神经网络可以尝试学习那些小块 filter 中的元素，

963
00:27:15,715 --> 00:27:17,850
0,165 165,360 360,665 1165,1565 1735,2135
try to learn those elements

964
00:27:18,050 --> 00:27:19,770
0,290 290,545 545,860 860,1160 1160,1720
of those small patch filters|
|

965
00:27:19,970 --> 00:27:21,550
0,350 350,850 930,1205 1205,1340 1340,1580
as weights in the neural
作为神经网络中的权重，

966
00:27:21,550 --> 00:27:23,200
0,240 240,570 570,885 885,1250 1330,1650
network| and learning those would
|学习这些元素本质上等同于

967
00:27:23,200 --> 00:27:25,930
0,320 910,1370 1780,2180 2200,2520 2520,2730
essentially equate to| picking up
|挑选和学习定义一个类别与另一个类别的模式。

968
00:27:25,930 --> 00:27:27,160
0,195 195,435 435,660 660,915 915,1230
and learning the patterns that

969
00:27:27,160 --> 00:27:28,975
0,350 460,795 795,1095 1095,1460 1480,1815
define one class versus another

970
00:27:28,975 --> 00:27:29,960
0,335
class.|
|

971
00:27:30,900 --> 00:27:32,950
0,400 1140,1415 1415,1550 1550,1775 1775,2050
And now that we've gotten
现在我们已经得到了这次行动和理解，

972
00:27:33,030 --> 00:27:34,835
0,400 450,850 960,1220 1220,1475 1475,1805
this operation and this understanding

973
00:27:34,835 --> 00:27:36,035
0,240 240,450 450,755 805,1065 1065,1200
under our belt,| we can
|我们可以进一步推进这一步，

974
00:27:36,035 --> 00:27:37,180
0,165 165,345 345,570 570,825 825,1145
take this one step further,|
|

975
00:27:37,470 --> 00:27:39,080
0,260 260,520 600,905 905,1145 1145,1610
{we,can -} take this singular
我们可以利用这种奇异的卷积运算，

976
00:27:39,080 --> 00:27:41,165
0,590 670,1070 1390,1710 1710,1920 1920,2085
convolution operation| and start to
|并开始考虑如何在这种运算的基础上构建完整的层，卷积层，

977
00:27:41,165 --> 00:27:41,945
0,165 165,345 345,510 510,645 645,780
think about how we can

978
00:27:41,945 --> 00:27:45,140
0,275 295,695 715,1295 1885,2700 2700,3195
build entire layers, convolutional layers,

979
00:27:45,140 --> 00:27:46,475
0,255 255,405 405,675 675,1070 1090,1335
out of this operation,| so
|这样我们就可以开始想象卷积网络和神经网络。

980
00:27:46,475 --> 00:27:47,300
0,135 135,285 285,480 480,705 705,825
that we can start to

981
00:27:47,300 --> 00:27:49,670
0,180 180,530 580,1395 1395,1760 2080,2370
even imagine convolutional networks and

982
00:27:49,670 --> 00:27:50,860
0,240 240,500
neural networks.|
|

983
00:27:50,990 --> 00:27:52,180
0,400 450,740 740,950 950,1055 1055,1190
And first we'll take a
首先，我们来看看所谓的，

984
00:27:52,180 --> 00:27:53,170
0,150 150,405 405,615 615,795 795,990
look at, you know, what

985
00:27:53,170 --> 00:27:56,215
0,150 150,440 1810,2210 2470,2760 2760,3045
are called,| well, what you
|通过创建卷积层和卷积网络，你最终创建的就是，

986
00:27:56,215 --> 00:27:58,180
0,375 375,750 750,1065 1065,1350 1350,1965
ultimately create by creating convolutional

987
00:27:58,180 --> 00:28:00,085
0,255 255,420 420,1130 1270,1650 1650,1905
layers and convolutional networks is|
|

988
00:28:00,085 --> 00:28:01,240
0,255 255,375 375,525 525,945 945,1155
what's called a CNN, a
所谓的 CNN ，卷积神经网络。

989
00:28:01,240 --> 00:28:03,445
0,570 570,810 810,1070 1780,2025 2025,2205
convolutional neural network.| {And,that's -}
|这将是今天课程的核心架构，

990
00:28:03,445 --> 00:28:04,090
0,120 120,225 225,315 315,465 465,645
going to be the core

991
00:28:04,090 --> 00:28:06,865
0,290 1000,1335 1335,1815 1815,2120 2530,2775
architecture of today's class,| so
|让我们考虑一个非常简单的 CNN ，

992
00:28:06,865 --> 00:28:08,280
0,255 255,540 540,810 810,1065 1065,1415
let's consider a very simple

993
00:28:08,330 --> 00:28:10,080
0,515 515,710 710,935 935,1300 1350,1750
CNN,| that was designed for
|它是为图像分类而设计的，

994
00:28:10,190 --> 00:28:12,535
0,365 365,1000 1560,1820 1820,2045 2045,2345
{image,classification -},| the task here
|这里的任务是直接从原始数据中学习特征，

995
00:28:12,535 --> 00:28:13,525
0,240 240,450 450,630 630,810 810,990
again is to learn the

996
00:28:13,525 --> 00:28:16,060
0,275 835,1235 1555,1955 1975,2265 2265,2535
features directly from the raw

997
00:28:16,060 --> 00:28:18,100
0,380 1060,1365 1365,1605 1605,1830 1830,2040
data| and use these learned
|并使用这些学习的特征对我们想要执行的目标检测任务进行分类。

998
00:28:18,100 --> 00:28:20,800
0,320 730,1110 1110,1730 1960,2340 2340,2700
features for classification towards some

999
00:28:20,800 --> 00:28:22,300
0,345 345,660 660,945 945,1380 1380,1500
task of object detection that

1000
00:28:22,300 --> 00:28:23,620
0,135 135,330 330,600 600,950
we want to perform.|
|

1001
00:28:24,650 --> 00:28:25,705
0,290 290,410 410,560 560,770 770,1055
Now there are three main
现在 CNN 有三个主要操作，

1002
00:28:25,705 --> 00:28:27,790
0,395 805,1065 1065,1215 1215,1745 1825,2085
operations to a CNN| and
|我们将在这里一步一步地介绍它们，

1003
00:28:27,790 --> 00:28:28,750
0,165 165,330 330,525 525,735 735,960
we'll go through them step

1004
00:28:28,750 --> 00:28:30,055
0,240 240,510 510,830 880,1140 1140,1305
by step here,| but then
|但在这节课的其余部分，我们会深入到每一个操作。

1005
00:28:30,055 --> 00:28:31,525
0,210 210,515 685,1050 1050,1305 1305,1470
go deeper into each of

1006
00:28:31,525 --> 00:28:34,060
0,275 505,905 1045,1410 1410,1775 2275,2535
them in the remainder of

1007
00:28:34,060 --> 00:28:34,960
0,180 180,435 435,615 615,720 720,900
this class.| {So,the -} first
|所以第一步是卷积，

1008
00:28:34,960 --> 00:28:36,775
0,320 370,675 675,1220 1300,1575 1575,1815
step is convolutions,| which we've
|我们在今天的课上已经看过很多，

1009
00:28:36,775 --> 00:28:37,825
0,210 210,435 435,615 615,795 795,1050
already seen a lot of

1010
00:28:37,825 --> 00:28:40,380
0,285 285,810 810,1205 1255,1655 1915,2555
in today's class already,| convolutions
|卷积是用来生成这些特征图的，

1011
00:28:40,820 --> 00:28:42,445
0,290 290,545 545,815 815,1120 1290,1625
are used to generate these

1012
00:28:42,445 --> 00:28:43,765
0,300 300,665 775,1020 1020,1155 1155,1320
feature maps,| so they take
|所以它们让我们输入先前的图像，

1013
00:28:43,765 --> 00:28:45,025
0,240 240,510 510,735 735,960 960,1260
us input both the previous

1014
00:28:45,025 --> 00:28:47,365
0,395 835,1170 1170,1410 1410,1715 2005,2340
image| as well as some
|以及它们想要检测的一些 filter ，

1015
00:28:47,365 --> 00:28:48,640
0,335 415,720 720,885 885,1050 1050,1275
filter that they want to

1016
00:28:48,640 --> 00:28:49,990
0,300 300,540 540,800 820,1140 1140,1350
detect,| and they output a
|并输出 filter 与原始图像关系的特征图。

1017
00:28:49,990 --> 00:28:51,700
0,240 240,510 510,830 910,1310 1390,1710
feature map of how this

1018
00:28:51,700 --> 00:28:54,565
0,320 340,740 1660,2060 2380,2685 2685,2865
filter is related to the

1019
00:28:54,565 --> 00:28:55,700
0,225 225,575
original image.|
|

1020
00:28:55,710 --> 00:28:57,395
0,275 275,515 515,880 930,1330 1380,1685
The second step is like
第二步像昨天一样，

1021
00:28:57,395 --> 00:28:59,555
0,305 625,960 960,1155 1155,1980 1980,2160
yesterday,| {applying,a -} non-linearity to
|对这些特征映射的结果应用非线性函数，

1022
00:28:59,555 --> 00:29:00,680
0,180 180,435 435,645 645,855 855,1125
the result of these feature

1023
00:29:00,680 --> 00:29:03,395
0,350 730,1035 1035,1800 1800,2085 2085,2715
maps,| that injects some non-linear
|向我们的神经网络注入一些非线性激活，

1024
00:29:03,395 --> 00:29:05,140
0,515 865,1095 1095,1230 1230,1485 1485,1745
activations to our neural networks,|
|

1025
00:29:05,250 --> 00:29:06,275
0,335 335,545 545,680 680,845 845,1025
allows it to deal with
使其能够处理非线性数据。

1026
00:29:06,275 --> 00:29:08,975
0,480 480,785 1615,1980 1980,2345 2365,2700
non-linear data.| Third step is
|第三步是池化，

1027
00:29:08,975 --> 00:29:10,580
0,485 595,870 870,1065 1065,1350 1350,1605
pooling,| which is essentially a
|这本质上是一种向下采样操作，

1028
00:29:10,580 --> 00:29:12,755
0,240 240,795 795,1190 1660,1965 1965,2175
down sampling operation| to allow
|允许我们的图像，允许我们的网络处理越来越大规模的图像，

1029
00:29:12,755 --> 00:29:14,620
0,180 180,455 1105,1395 1395,1575 1575,1865
our images, allow our networks

1030
00:29:14,700 --> 00:29:15,935
0,275 275,410 410,590 590,910 930,1235
to deal with larger and

1031
00:29:15,935 --> 00:29:18,370
0,305 355,645 645,935 1135,1500 1500,2435
larger scale images| by progressively
|通过逐渐缩小其大小，

1032
00:29:18,390 --> 00:29:20,225
0,400 420,905 905,1145 1145,1480 1590,1835
down scaling their size,| so
|以便我们的 filter 可以在感受野内逐渐增长。

1033
00:29:20,225 --> 00:29:21,800
0,105 105,270 270,690 690,900 900,1575
that our filters can progressively

1034
00:29:21,800 --> 00:29:24,300
0,320 700,1035 1035,1575 1575,1850
grow in receptive field.|
|

1035
00:29:25,000 --> 00:29:27,015
0,305 305,610 1110,1610 1610,1820 1820,2015
And finally, feeding all of
最后，将所有这些结果特征提供给某个神经网络，

1036
00:29:27,015 --> 00:29:29,520
0,305 565,965 985,1385 1885,2205 2205,2505
these resulting features to some

1037
00:29:29,520 --> 00:29:32,100
0,360 360,620 1360,1760 1810,2210 2290,2580
neural network| to infer the
|以推断种类分数，

1038
00:29:32,100 --> 00:29:33,900
0,270 270,650 910,1310 1360,1635 1635,1800
class scores,| {now\,,by -} the
|当我们到达这个完全连接的层时，

1039
00:29:33,900 --> 00:29:34,755
0,195 195,375 375,525 525,705 705,855
time that we get to

1040
00:29:34,755 --> 00:29:37,995
0,245 925,1305 1305,1685 2065,2465 2875,3240
this fully connected layer,| remember
|记住我们已经提取了我们的特征，

1041
00:29:37,995 --> 00:29:39,360
0,270 270,555 555,810 810,1230 1230,1365
that we've already extracted our

1042
00:29:39,360 --> 00:29:41,145
0,290 820,1170 1170,1470 1470,1665 1665,1785
features| and essentially you can
|你可以认为这不再是一个二维图像，

1043
00:29:41,145 --> 00:29:42,195
0,165 165,345 345,555 555,765 765,1050
think of this no longer

1044
00:29:42,195 --> 00:29:43,670
0,360 360,600 600,735 735,1185 1185,1475
being a two dimensional image,|
|

1045
00:29:44,050 --> 00:29:45,270
0,260 260,395 395,620 620,965 965,1220
we can now use the
我们现在可以使用我们在第一课中学到的方法，

1046
00:29:45,270 --> 00:29:46,460
0,255 255,510 510,645 645,855 855,1190
methods that we learned about

1047
00:29:46,630 --> 00:29:48,290
0,275 275,545 545,940 990,1325 1325,1660
in lecture one| to directly
|直接获取神经网络检测到的学习特征，

1048
00:29:48,340 --> 00:29:50,385
0,320 320,545 545,785 785,1120 1770,2045
take those learned features that

1049
00:29:50,385 --> 00:29:52,040
0,120 120,345 345,570 570,935 1045,1655
the neural network has detected|
|

1050
00:29:52,420 --> 00:29:54,195
0,400 600,1000 1020,1355 1355,1565 1565,1775
and infer based on those
并根据这些学习特征，

1051
00:29:54,195 --> 00:29:55,485
0,255 255,555 555,795 795,975 975,1290
learned features| and based on
|根据它们是否被检测到，

1052
00:29:55,485 --> 00:29:56,745
0,375 375,630 630,840 840,1170 1170,1260
if they were detected or

1053
00:29:56,745 --> 00:29:57,930
0,105 105,225 225,360 360,635 865,1185
if they were not,| what
|我们所在的种类。

1054
00:29:57,930 --> 00:29:59,100
0,255 255,525 525,770
class we're in.|
|

1055
00:29:59,890 --> 00:30:01,875
0,380 380,680 680,1120 1260,1660 1680,1985
So now let's basically just
现在让我们更详细地逐一介绍这些操作，

1056
00:30:01,875 --> 00:30:02,940
0,255 255,495 495,660 660,810 810,1065
go through each of these

1057
00:30:02,940 --> 00:30:04,305
0,380 550,840 840,1020 1020,1215 1215,1365
operations one by one in

1058
00:30:04,305 --> 00:30:05,565
0,120 120,270 270,545 595,975 975,1260
a bit more detail| and
|看看我们如何才能建立起 CNN 的这个非常基本的架构。

1059
00:30:05,565 --> 00:30:06,920
0,180 180,345 345,510 510,785 955,1355
see how we could even

1060
00:30:07,210 --> 00:30:08,670
0,290 290,580 630,920 920,1145 1145,1460
build up this very basic

1061
00:30:08,670 --> 00:30:10,660
0,380 520,765 765,900 900,1400
architecture of a CNN.|
|

1062
00:30:10,910 --> 00:30:12,430
0,275 275,515 515,980 980,1265 1265,1520
So first, let's go back
所以首先，让我们回到过去，再考虑一次卷积运算，

1063
00:30:12,430 --> 00:30:13,765
0,350 400,705 705,885 885,1080 1080,1335
and consider one more time

1064
00:30:13,765 --> 00:30:15,690
0,225 225,695 775,1175 1255,1650 1650,1925
the convolution operation,| that's central,
|这是 CNN 的核心，

1065
00:30:16,130 --> 00:30:17,850
0,395 395,740 740,1010 1010,1220 1220,1720
central core to the CNN,|
|

1066
00:30:18,500 --> 00:30:20,245
0,275 275,455 455,760 1020,1370 1370,1745
{and,as -} before, each neuron
和以前一样，这个隐藏层中的每个神经元都将，

1067
00:30:20,245 --> 00:30:21,400
0,195 195,420 420,690 690,945 945,1155
in this hidden layer is

1068
00:30:21,400 --> 00:30:23,380
0,180 180,300 300,530 700,1250 1630,1980
going to be| computed as
|被计算为其输入的加权和，

1069
00:30:23,380 --> 00:30:24,870
0,225 225,630 630,945 945,1215 1215,1490
a weighted sum of its

1070
00:30:24,950 --> 00:30:27,805
0,520 1170,1490 1490,1655 1655,2080 2460,2855
inputs,| applying a bias and
|使用偏置和非线性激活，

1071
00:30:27,805 --> 00:30:29,980
0,485 565,885 885,1065 1065,1905 1905,2175
activating with a non-linearity,| should
|听起来应该和昨天课程的第一课非常相似，

1072
00:30:29,980 --> 00:30:31,330
0,210 210,480 480,830 850,1110 1110,1350
sound very similar to lecture

1073
00:30:31,330 --> 00:30:33,760
0,380 640,1040 1120,1875 1875,2145 2145,2430
one in yesterday's class,| but
|但现在不同的是，

1074
00:30:33,760 --> 00:30:35,320
0,240 240,560 940,1230 1230,1440 1440,1560
except now,| when we're going
|当我们要做第一步的时候，

1075
00:30:35,320 --> 00:30:36,510
0,165 165,315 315,525 525,825 825,1190
to do that first step,|
|

1076
00:30:37,040 --> 00:30:38,125
0,260 260,410 410,590 590,845 845,1085
instead of just doing a
我们不是只用我们的权重做点积，

1077
00:30:38,125 --> 00:30:39,400
0,210 210,525 525,750 750,885 885,1275
dot product with our weights,|
|

1078
00:30:39,400 --> 00:30:40,285
0,255 255,390 390,585 585,750 750,885
we're going to apply a
而是用我们的权重进行卷积，

1079
00:30:40,285 --> 00:30:41,935
0,455 745,1005 1005,1155 1155,1470 1470,1650
convolution with our weights,| which
|这就是简单的逐个元素乘法和加法，

1080
00:30:41,935 --> 00:30:42,970
0,135 135,330 330,540 540,765 765,1035
is simply that element wise

1081
00:30:42,970 --> 00:30:44,840
0,680 790,1095 1095,1400
multiplication and addition,

1082
00:30:44,970 --> 00:30:46,600
0,320 320,545 545,770 770,1150 1230,1630
right,| {and,that -} sliding operation.|
|还有那个滑动操作。|

1083
00:30:47,370 --> 00:30:49,780
0,400 510,1060 1140,1540 1560,1960 2010,2410
Now, what's really special here|
这里真正特别的是，|

1084
00:30:49,890 --> 00:30:50,825
0,275 275,410 410,560 560,755 755,935
and what I really want
我真正想强调的是本地连接，

1085
00:30:50,825 --> 00:30:52,990
0,225 225,575 985,1385 1585,1875 1875,2165
to stress is the {local,connectivity

1086
00:30:53,160 --> 00:30:56,240
0,610 990,1390 1440,1840 1860,2470 2730,3080
-},| every single neuron in
|这一隐藏层中的每一个神经元只能看到

1087
00:30:56,240 --> 00:30:58,450
0,285 285,570 570,920 1180,1580 1810,2210
this hidden layer only sees|
|

1088
00:30:58,590 --> 00:31:01,030
0,400 540,940 1020,1420 1500,1900 1920,2440
a certain patch of inputs
其前一层中的某一块输入，

1089
00:31:01,050 --> 00:31:02,270
0,260 260,440 440,740 740,1055 1055,1220
in {its,previous -} layer,| so
|所以，如果我只指向输出层的这一个神经元，

1090
00:31:02,270 --> 00:31:03,185
0,75 75,255 255,495 495,705 705,915
if I point at just

1091
00:31:03,185 --> 00:31:04,775
0,210 210,435 435,905 1075,1350 1350,1590
this one neuron in the

1092
00:31:04,775 --> 00:31:06,970
0,255 255,545 1045,1395 1395,1845 1845,2195
output layer,| this neuron only
|这个神经元只能看到这个红色方块的输入，

1093
00:31:07,260 --> 00:31:09,740
0,400 1050,1430 1430,1925 1925,2240 2240,2480
sees the inputs {at,this -}

1094
00:31:09,740 --> 00:31:10,940
0,285 285,525 525,660 660,990 990,1200
red square,| it doesn't see
|它看不到图像其余部分中的任何其他输入，

1095
00:31:10,940 --> 00:31:12,215
0,210 210,420 420,555 555,800 850,1275
any of the other inputs

1096
00:31:12,215 --> 00:31:13,010
0,180 180,315 315,495 495,690 690,795
in the rest of the

1097
00:31:13,010 --> 00:31:13,960
0,230
image,|
|

1098
00:31:14,030 --> 00:31:15,400
0,260 260,500 500,755 755,1130 1130,1370
and that's really important to
能够将这些模型缩放到超大规模图像中，这一点非常重要，

1099
00:31:15,400 --> 00:31:16,660
0,120 120,375 375,720 720,1005 1005,1260
be able to scale these

1100
00:31:16,660 --> 00:31:18,685
0,320 820,1095 1095,1320 1320,1665 1665,2025
models to very large scale

1101
00:31:18,685 --> 00:31:20,320
0,365 835,1110 1110,1245 1245,1380 1380,1635
images,| {now\,,you -} can imagine
|现在你可以想象，

1102
00:31:20,320 --> 00:31:21,325
0,270 270,465 465,630 630,795 795,1005
that,| as you go deeper
|随着你越来越深入你的网络，

1103
00:31:21,325 --> 00:31:22,350
0,180 180,360 360,570 570,750 750,1025
and deeper into your network,|
|

1104
00:31:22,730 --> 00:31:24,880
0,400 1020,1325 1325,1520 1520,1790 1790,2150
eventually, because the next layer
最终，因为在下一层，你将关注更大的块，

1105
00:31:24,880 --> 00:31:25,930
0,300 300,435 435,660 660,900 900,1050
you're going to attend to

1106
00:31:25,930 --> 00:31:27,535
0,105 105,380 430,830 1180,1455 1455,1605
a larger patch,| and that
|这将不仅包括这个红色方块的数据，

1107
00:31:27,535 --> 00:31:29,200
0,255 255,635 745,1145 1195,1485 1485,1665
will include data from not

1108
00:31:29,200 --> 00:31:31,080
0,290 370,705 705,990 990,1340 1480,1880
only this red square,| but
|而且还包括一个你可以想象到的更大的红色方块。

1109
00:31:31,100 --> 00:31:32,740
0,400 480,755 755,965 965,1295 1295,1640
effectively a much larger red

1110
00:31:32,740 --> 00:31:34,075
0,270 270,450 450,645 645,960 960,1335
square that you could imagine

1111
00:31:34,075 --> 00:31:35,040
0,395
there.|
|

1112
00:31:36,780 --> 00:31:38,600
0,365 365,755 755,1085 1085,1480 1530,1820
Now let's define this actual
现在让我们定义一下进程的实际计算，

1113
00:31:38,600 --> 00:31:41,015
0,585 585,1130 1150,1500 1500,1850 2170,2415
computation that's going on,| for
|对于隐藏层中的神经元，

1114
00:31:41,015 --> 00:31:42,170
0,120 120,450 450,675 675,900 900,1155
a neuron in a hidden

1115
00:31:42,170 --> 00:31:43,715
0,330 330,710 730,1095 1095,1275 1275,1545
layer,| {its,inputs -} are those
|它的输入是那些在上一层落在它的块中的神经元，

1116
00:31:43,715 --> 00:31:45,860
0,515 805,1140 1140,1410 1410,1745 1765,2145
neurons that fell within its

1117
00:31:45,860 --> 00:31:47,740
0,380 820,1080 1080,1215 1215,1485 1485,1880
patch in the {previous,layer -},|
|

1118
00:31:47,880 --> 00:31:49,415
0,260 260,440 440,665 665,965 965,1535
we can apply this matrix
我们可以应用这个权重矩阵，表示为 4x4 的 filter ，

1119
00:31:49,415 --> 00:31:51,395
0,165 165,665 955,1290 1290,1710 1710,1980
of weights here, denoted as

1120
00:31:51,395 --> 00:31:52,685
0,305 325,615 615,795 795,1005 1005,1290
a {4x4 - -} filter,|
|

1121
00:31:52,685 --> 00:31:53,435
0,210 210,315 315,480 480,630 630,750
that you can see on
你可以在左侧看到，

1122
00:31:53,435 --> 00:31:54,875
0,120 120,270 270,510 510,845 1195,1440
the {left,hand -} side,| and
|在这种情况下，我们进行按元素相乘，

1123
00:31:54,875 --> 00:31:55,775
0,105 105,285 285,540 540,765 765,900
in this case we do

1124
00:31:55,775 --> 00:31:58,295
0,135 135,420 420,735 735,1385 2245,2520
{an,element -} wise multiplication,| we
|我们累加输出，应用偏置，然后添加非线性，

1125
00:31:58,295 --> 00:31:59,765
0,195 195,465 465,930 930,1260 1260,1470
add the outputs, we apply

1126
00:31:59,765 --> 00:32:01,025
0,135 135,575 715,990 990,1110 1110,1260
a bias and we add

1127
00:32:01,025 --> 00:32:02,500
0,195 195,390 390,1115
that {non-linearity -},|
|

1128
00:32:02,880 --> 00:32:04,505
0,365 365,620 620,905 905,1210 1290,1625
that's the the core steps
这是所有神经网络的核心步骤，

1129
00:32:04,505 --> 00:32:05,645
0,210 210,375 375,585 585,840 840,1140
that we take in really

1130
00:32:05,645 --> 00:32:06,845
0,270 270,465 465,660 660,930 930,1200
all of these neural networks,|
|

1131
00:32:06,845 --> 00:32:07,985
0,255 255,435 435,645 645,930 930,1140
that you're learning about in
我们在今天和本周的课程中学到的。

1132
00:32:07,985 --> 00:32:10,120
0,545 625,1025 1165,1470 1470,1830 1830,2135
today's and this week's class,

1133
00:32:10,320 --> 00:32:12,305
0,245 245,335 335,580 1320,1685 1685,1985
to be honest.| {Now,remember -}
|记住这个按元素的乘法和加法运算，

1134
00:32:12,305 --> 00:32:14,675
0,240 240,545 985,1385 1405,1710 1710,2370
that this element wise multiplication

1135
00:32:14,675 --> 00:32:16,660
0,330 330,665 745,1145 1285,1680 1680,1985
and addition operation,| that's sliding
|这是滑动运算，这叫做卷积，

1136
00:32:16,710 --> 00:32:18,455
0,400 480,860 860,1040 1040,1520 1520,1745
operation, that's called convolution| and
|这是这些层的基础。

1137
00:32:18,455 --> 00:32:20,080
0,225 225,345 345,605 955,1290 1290,1625
that's the basis of these

1138
00:32:20,190 --> 00:32:22,220
0,580 990,1235 1235,1370 1370,1730 1730,2030
layers.| So that defines how
|所以这定义了卷积层中的神经元是如何连接的，

1139
00:32:22,220 --> 00:32:24,610
0,530 580,945 945,1665 1665,2055 2055,2390
neurons in convolutional layers are

1140
00:32:24,750 --> 00:32:26,770
0,380 380,620 620,800 800,1300 1350,2020
connected,| how they're mathematically formulated,|
|它们是如何数学表达的，|

1141
00:32:27,180 --> 00:32:29,390
0,275 275,550 630,995 995,1360 1380,2210
but within a single convolutional
但在一个卷积层内，

1142
00:32:29,390 --> 00:32:30,820
0,240 240,630 630,840 840,1065 1065,1430
layer,| it's also really important
|了解这一点也非常重要，

1143
00:32:31,320 --> 00:32:33,605
0,400 660,1060 1500,1805 1805,2015 2015,2285
to understand,| that a single
|单个层实际上可以尝试检测多组 filter ，

1144
00:32:33,605 --> 00:32:35,195
0,365 415,815 895,1185 1185,1365 1365,1590
layer could actually try to

1145
00:32:35,195 --> 00:32:37,480
0,335 505,905 1105,1485 1485,1770 1770,2285
detect multiple sets {of,filters -},|
|

1146
00:32:37,770 --> 00:32:38,900
0,305 305,485 485,665 665,890 890,1130
maybe you want to detect
也许你想在一个图像中检测多个特征，而不只是一个特征，

1147
00:32:38,900 --> 00:32:40,660
0,210 210,405 405,710 910,1310 1360,1760
in one image multiple features,

1148
00:32:40,710 --> 00:32:42,160
0,290 290,500 500,725 725,1030 1050,1450
not {just,one -} feature,| but
|但是，如果你在检测人脸，

1149
00:32:42,300 --> 00:32:43,580
0,245 245,490 720,995 995,1115 1115,1280
you know if you are

1150
00:32:43,580 --> 00:32:44,870
0,360 360,645 645,900 900,1095 1095,1290
detecting faces,| you don't only
|你不仅仅想检测眼睛，

1151
00:32:44,870 --> 00:32:46,520
0,240 240,465 465,800 880,1280 1390,1650
want to detect eyes,| you
|你想要检测眼睛，鼻子，嘴巴，耳朵，

1152
00:32:46,520 --> 00:32:47,920
0,135 135,360 360,710 910,1155 1155,1400
want to detect, you know

1153
00:32:48,030 --> 00:32:50,330
0,400 480,1040 1040,1445 1445,1750 2010,2300
{eyes\,,noses -}, mouths, ears,| all
|所有这些都是定义人脸的关键模式，

1154
00:32:50,330 --> 00:32:51,550
0,165 165,345 345,540 540,825 825,1220
of those things are critical

1155
00:32:51,570 --> 00:32:53,650
0,400 780,1145 1145,1505 1505,1790 1790,2080
patterns that define a face|
|

1156
00:32:53,700 --> 00:32:54,950
0,305 305,485 485,635 635,800 800,1250
and can {help,you -} classify
可以帮助你对人脸进行分类。

1157
00:32:54,950 --> 00:32:57,440
0,135 135,410 1630,2030 2080,2355 2355,2490
a face.| So what we
|所以我们需要考虑的实际上是，

1158
00:32:57,440 --> 00:32:58,540
0,120 120,255 255,420 420,705 705,1100
need to think of is

1159
00:32:58,560 --> 00:33:01,030
0,400 450,1060 1140,1540 1890,2180 2180,2470
actually,| convolution operations that can
|卷积运算可以输出大量不同的图像，

1160
00:33:01,050 --> 00:33:02,660
0,305 305,515 515,820 1080,1370 1370,1610
output a volume of different

1161
00:33:02,660 --> 00:33:03,260
0,350
images,

1162
00:33:03,630 --> 00:33:05,855
0,400 480,880 1170,1600 1710,2000 2000,2225
right,| {every,slice -} of this
|这个[体积]的每个切片代表了一个不同的 filter ，

1163
00:33:05,855 --> 00:33:08,200
0,335 595,995 1135,1830 1830,2040 2040,2345
volume effectively denotes a different

1164
00:33:08,250 --> 00:33:10,120
0,400 660,935 935,1070 1070,1330 1470,1870
filter,| that can be identified
|可以在我们的原始输入中识别出来，

1165
00:33:10,500 --> 00:33:12,650
0,365 365,665 665,1000 1260,1660 1830,2150
in our original input,| and
|每个 filter 也将与我们图像中的特定图案或特征相对应，

1166
00:33:12,650 --> 00:33:13,925
0,225 225,405 405,600 600,1050 1050,1275
each of those filters is

1167
00:33:13,925 --> 00:33:16,100
0,180 180,485 625,1025 1195,1595 1945,2175
going to basically correspond to

1168
00:33:16,100 --> 00:33:17,770
0,135 135,440 550,950 1060,1365 1365,1670
a specific pattern or feature

1169
00:33:18,000 --> 00:33:19,300
0,275 275,500 500,785 785,1010 1010,1300
in our image as well,|
|

1170
00:33:20,050 --> 00:33:21,360
0,275 275,410 410,590 590,910 1020,1310
think of the connections in
再次想想这些神经元中的连接，它们的感受野，

1171
00:33:21,360 --> 00:33:23,055
0,195 195,590 910,1215 1215,1425 1425,1695
these neurons, in terms of,

1172
00:33:23,055 --> 00:33:24,435
0,210 210,375 375,600 600,1125 1125,1380
you know, their receptive field

1173
00:33:24,435 --> 00:33:27,015
0,330 330,695 1165,1545 1545,1925 2185,2580
{once,again -},| the locations within
|它们在上一层中连接到的该节点的输入内的位置，

1174
00:33:27,015 --> 00:33:29,220
0,390 390,785 1015,1350 1350,1650 1650,2205
the input of that node

1175
00:33:29,220 --> 00:33:30,285
0,225 225,360 360,555 555,855 855,1065
that they were connected to

1176
00:33:30,285 --> 00:33:32,250
0,135 135,425 445,845 865,1265 1645,1965
in the previous layer,| these
|这些参数真正定义了，

1177
00:33:32,250 --> 00:33:34,980
0,540 540,920 1000,1400 2230,2535 2535,2730
parameters really define,| what I
|我喜欢认为的信息的空间排列，

1178
00:33:34,980 --> 00:33:35,880
0,195 195,375 375,540 540,690 690,900
like to think of as

1179
00:33:35,880 --> 00:33:38,120
0,285 285,840 840,1410 1410,1730 1840,2240
the spatial arrangement of information,|
|

1180
00:33:38,530 --> 00:33:40,580
0,335 335,970 1230,1595 1595,1805 1805,2050
that propagates throughout the network
它在整个网络中传播，特别是在卷积层中传播。

1181
00:33:40,660 --> 00:33:42,330
0,305 305,560 560,770 770,1355 1355,1670
and throughout the convolutional layers

1182
00:33:42,330 --> 00:33:43,580
0,255 255,620
in particular.|
|

1183
00:33:44,060 --> 00:33:45,940
0,400 1050,1325 1325,1505 1505,1685 1685,1880
Now, I think just to
现在，我想总结一下我们所看到的，

1184
00:33:45,940 --> 00:33:47,530
0,525 525,765 765,1065 1065,1320 1320,1590
summarize what we've seen| and
|以及这些类型的神经网络中的连接是如何定义的，

1185
00:33:47,530 --> 00:33:49,705
0,320 340,740 1330,1650 1650,1905 1905,2175
how connections in these types

1186
00:33:49,705 --> 00:33:51,480
0,335 415,780 780,1055 1105,1440 1440,1775
of neural networks are defined,|
|

1187
00:33:52,130 --> 00:33:54,690
0,400 930,1265 1265,1510 1530,1930 2160,2560
and let's say how the,
让我们来看看卷积网络的输出是怎样的，

1188
00:33:55,850 --> 00:33:57,205
0,275 275,545 545,860 860,1070 1070,1355
how the output of a

1189
00:33:57,205 --> 00:33:58,800
0,660 660,915 915,1155 1155,1305 1305,1595
convolutional network is a volume,|
|

1190
00:33:59,750 --> 00:34:00,790
0,245 245,425 425,665 665,860 860,1040
we are well on our
我们正在朝着真正理解卷积神经网络并定义它们的方向前进，

1191
00:34:00,790 --> 00:34:02,275
0,285 285,525 525,770 850,1245 1245,1485
way to really understanding, you

1192
00:34:02,275 --> 00:34:04,590
0,245 265,960 960,1200 1200,1475 1915,2315
know, convolutional neural networks and

1193
00:34:04,760 --> 00:34:06,450
0,455 455,730 810,1115 1115,1415 1415,1690
defining them, right,| that's the,
|这就是，我们刚刚谈到的是 CNN 的主要组成部分，

1194
00:34:06,830 --> 00:34:07,840
0,275 275,425 425,590 590,800 800,1010
what we just covered is

1195
00:34:07,840 --> 00:34:09,280
0,225 225,420 420,680 700,1100 1120,1440
really the main component of

1196
00:34:09,280 --> 00:34:11,830
0,530 970,1320 1320,1455 1455,2180 2200,2550
CNNs,| that's the convolutional operation
|这就是定义这些卷积层的卷积运算，

1197
00:34:11,830 --> 00:34:13,540
0,225 225,540 540,765 765,1380 1380,1710
that defines these convolutional layers,|
|

1198
00:34:13,540 --> 00:34:15,475
0,225 225,525 525,920 1300,1650 1650,1935
{the,remaining -} steps are very
剩下的步骤也非常关键，

1199
00:34:15,475 --> 00:34:17,035
0,270 270,510 510,720 720,1025 1285,1560
critical as well,| {but,I -}
|但我想暂停一下，

1200
00:34:17,035 --> 00:34:17,980
0,120 120,225 225,450 450,765 765,945
want to maybe pause for

1201
00:34:17,980 --> 00:34:18,775
0,90 90,270 270,450 450,615 615,795
a second| and make sure
|确保每个人在卷积运算和卷积层的定义上都是一致的。

1202
00:34:18,775 --> 00:34:19,690
0,240 240,525 525,600 600,720 720,915
that everyone's on the same

1203
00:34:19,690 --> 00:34:21,480
0,320 370,630 630,765 765,1410 1410,1790
page with the convolutional operation

1204
00:34:21,500 --> 00:34:23,545
0,400 570,845 845,1120 1170,1445 1445,2045
and the definition of convolutional

1205
00:34:23,545 --> 00:34:24,400
0,425
layers.|
|

1206
00:34:28,420 --> 00:34:30,765
0,400 450,850 1380,1780 1890,2150 2150,2345
Awesome.| {Okay\,,so -} the next
太棒了。|好的，这里的下一步是，

1207
00:34:30,765 --> 00:34:31,815
0,240 240,435 435,615 615,810 810,1050
step here is| to take
|提取我们的卷积层提取的那些结果特征图，

1208
00:34:31,815 --> 00:34:33,900
0,335 535,935 985,1350 1350,1715 1825,2085
those resulting feature maps that

1209
00:34:33,900 --> 00:34:36,165
0,165 165,795 795,1190 1240,1640 1930,2265
our convolutional layers extract| and
|并将非线性应用于卷积层的输出体积，

1210
00:34:36,165 --> 00:34:37,575
0,225 225,375 375,525 525,1215 1215,1410
apply a {non-linearity -} to

1211
00:34:37,575 --> 00:34:39,495
0,225 225,510 510,815 1465,1755 1755,1920
the output volume of the

1212
00:34:39,495 --> 00:34:40,980
0,585 585,825 825,1080 1080,1245 1245,1485
convolutional layer,| so as we
|所以正如我们在第一节课中讨论的，

1213
00:34:40,980 --> 00:34:42,020
0,255 255,420 420,540 540,720 720,1040
discussed in the first lecture,|
|

1214
00:34:42,220 --> 00:34:43,950
0,335 335,560 560,1325 1325,1490 1490,1730
applying these non-linearities is really
应用这些非线性是非常关键的，

1215
00:34:43,950 --> 00:34:45,735
0,350 520,920 970,1275 1275,1515 1515,1785
critical,| because it allows us
|因为它允许我们处理非线性数据，

1216
00:34:45,735 --> 00:34:47,625
0,210 210,435 435,785 955,1605 1605,1890
to deal with non-linear data|
|

1217
00:34:47,625 --> 00:34:49,005
0,330 330,615 615,885 885,1125 1125,1380
and because image data in
而且因为图像数据特别是高度非线性的，

1218
00:34:49,005 --> 00:34:52,095
0,365 715,1110 1110,1505 1615,2375 2695,3090
particular is extremely non-linear,| that's
|这是卷积神经网络在实践中实际运行的关键组成部分，

1219
00:34:52,095 --> 00:34:53,720
0,245 715,975 975,1095 1095,1290 1290,1625
a, you know, a critical

1220
00:34:54,040 --> 00:34:56,595
0,400 480,880 900,1235 1235,1570 1740,2555
component of what makes convolutional

1221
00:34:56,595 --> 00:34:58,710
0,315 315,575 745,1110 1110,1475 1795,2115
neural networks actually operational in

1222
00:34:58,710 --> 00:35:00,240
0,320
practice,|
|

1223
00:35:00,280 --> 00:35:02,235
0,380 380,760 810,1130 1130,1730 1730,1955
in particular, for convolutional neural
特别是，对于卷积神经网络，

1224
00:35:02,235 --> 00:35:04,020
0,275 415,765 765,1200 1200,1545 1545,1785
networks,| the activation function that
|对于这些模型来说，非常常见的激活函数是 ReLU 激活函数，

1225
00:35:04,020 --> 00:35:06,135
0,260 580,945 945,1310 1330,1730 1840,2115
is really, really common for

1226
00:35:06,135 --> 00:35:08,000
0,225 225,575 1045,1380 1380,1590 1590,1865
these models is the ReLU

1227
00:35:08,350 --> 00:35:10,920
0,470 470,850 1950,2225 2225,2405 2405,2570
activation function,| {we,talked -} a
|我们在昨天的第一课和第二课中谈到了这一点，

1228
00:35:10,920 --> 00:35:11,760
0,150 150,330 330,510 510,690 690,840
little bit about this in

1229
00:35:11,760 --> 00:35:13,130
0,225 225,480 480,690 690,990 990,1370
lecture one and two yesterday,|
|

1230
00:35:13,420 --> 00:35:14,910
0,260 260,520 570,995 995,1295 1295,1490
the ReLU activation function, you
ReLU 激活函数，你可以在右侧看到它，

1231
00:35:14,910 --> 00:35:15,660
0,180 180,345 345,465 465,615 615,750
can see it on the

1232
00:35:15,660 --> 00:35:17,175
0,165 165,390 390,710 1090,1365 1365,1515
right {hand,side -},| think of
|可以将此函数视为逐个像素的操作，

1233
00:35:17,175 --> 00:35:18,390
0,180 180,465 465,705 705,870 870,1215
this function as a pixel

1234
00:35:18,390 --> 00:35:20,280
0,150 150,570 570,920 940,1335 1335,1890
by pixel operation,| that replaces
|该操作将所有负值替换为零，

1235
00:35:20,280 --> 00:35:22,470
0,315 315,645 645,975 975,1370 1870,2190
basically all negative {values,with -}

1236
00:35:22,470 --> 00:35:23,850
0,320 370,660 660,855 855,1080 1080,1380
zero,| it keeps all positive
|它使所有正值保持不变，

1237
00:35:23,850 --> 00:35:25,590
0,360 360,630 630,920 1240,1560 1560,1740
values the same,| it's the
|当一个值是正值时，它是恒等式函数，

1238
00:35:25,590 --> 00:35:27,015
0,315 315,710 850,1110 1110,1230 1230,1425
identity function when a value

1239
00:35:27,015 --> 00:35:28,530
0,225 225,515 955,1215 1215,1320 1320,1515
is positive,| but when it's
|但当它是负值时，它把一切都压回到零，

1240
00:35:28,530 --> 00:35:30,690
0,260 580,840 840,1100 1210,1875 1875,2160
negative, it basically squashes everything

1241
00:35:30,690 --> 00:35:31,900
0,180 180,360 360,510 510,770
back up to zero,|
|

1242
00:35:32,430 --> 00:35:33,350
0,260 260,380 380,560 560,770 770,920
think of this almost as
这可以看作是一个阈值函数，

1243
00:35:33,350 --> 00:35:34,985
0,120 120,630 630,900 900,1185 1185,1635
a thresholding function, right,| thresholds
|阈值是任何等于零，任何小于零的值都会回到零，

1244
00:35:34,985 --> 00:35:36,740
0,245 655,915 915,1080 1080,1385 1435,1755
is everything at zero, anything

1245
00:35:36,740 --> 00:35:37,835
0,210 210,375 375,600 600,885 885,1095
less than zero comes back

1246
00:35:37,835 --> 00:35:39,470
0,165 165,330 330,605 1045,1350 1350,1635
up {to,zero -},| so negative
|所以这里的负值表示卷积中的负检测，

1247
00:35:39,470 --> 00:35:43,580
0,380 520,920 1240,1640 3310,3710 3790,4110
values here indicate basically a

1248
00:35:43,580 --> 00:35:46,490
0,320 550,1140 1140,1440 1440,2060 2650,2910
negative detection in convolution,| that
|你可能只想说没有检测到，

1249
00:35:46,490 --> 00:35:47,240
0,135 135,300 300,480 480,615 615,750
you may want to just

1250
00:35:47,240 --> 00:35:49,055
0,210 210,525 525,870 870,1460 1510,1815
say was no detection, right,|
|

1251
00:35:49,055 --> 00:35:49,760
0,165 165,285 285,420 420,570 570,705
and you can think of
你可以认为这是一种直观的理解机制，

1252
00:35:49,760 --> 00:35:50,420
0,150 150,330 330,480 480,585 585,660
that as kind of an

1253
00:35:50,420 --> 00:35:53,165
0,680 1060,1460 1630,2025 2025,2415 2415,2745
intuitive mechanism for understanding| why
|为什么 ReLU 激活函数在卷积神经网络中如此受欢迎，

1254
00:35:53,165 --> 00:35:54,740
0,195 195,540 540,945 945,1305 1305,1575
the ReLU activation functions is

1255
00:35:54,740 --> 00:35:56,870
0,180 180,470 1030,1320 1320,1905 1905,2130
so popular in {convolutional,neural -}

1256
00:35:56,870 --> 00:35:58,900
0,260 310,555 555,800 1540,1785 1785,2030
networks,| the other, the other
|另一种流行的看法是， ReLU 激活函数，

1257
00:35:59,430 --> 00:36:01,900
0,400 480,880 900,1220 1220,1540 1920,2470
popular belief is that ReLU

1258
00:36:02,100 --> 00:36:03,680
0,440 440,820 1110,1340 1340,1490 1490,1580
activation functions,| well, {it's,not -}
|好的，这不是一种[信仰]，

1259
00:36:03,680 --> 00:36:04,835
0,165 165,390 390,585 585,840 840,1155
a belief,| they are extremely
|它们非常容易计算，而且计算效率很高，

1260
00:36:04,835 --> 00:36:06,110
0,300 300,510 510,885 885,1095 1095,1275
easy to compute and they're

1261
00:36:06,110 --> 00:36:08,050
0,165 165,470 520,810 810,1620 1620,1940
{very,easy -} and computationally efficient,|
|

1262
00:36:08,310 --> 00:36:09,860
0,305 305,665 665,845 845,1085 1085,1550
their gradients are very cleanly
它们的渐变被非常清晰地定义，

1263
00:36:09,860 --> 00:36:11,330
0,270 270,510 510,1065 1065,1350 1350,1470
defined,| they're constants except for
|除了分段非线性外，它们都是常量，

1264
00:36:11,330 --> 00:36:14,380
0,105 105,285 285,590 1600,2000 2080,3050
a piece wise {non-nonlinearity -},|
|

1265
00:36:15,210 --> 00:36:16,070
0,245 245,350 350,500 500,665 665,860
so that makes them very
这使得它们在这些领域非常受欢迎。

1266
00:36:16,070 --> 00:36:18,120
0,320 490,765 765,945 945,1370
popular for these domains.|
|

1267
00:36:19,810 --> 00:36:21,140
0,335 335,515 515,695 695,980 980,1330
Now the next key operation
现在， CNN 的下一个关键操作是池化，

1268
00:36:21,340 --> 00:36:23,160
0,260 260,455 455,1000 1050,1450 1530,1820
in a CNN is that

1269
00:36:23,160 --> 00:36:24,870
0,225 225,710 970,1290 1290,1605 1605,1710
of pooling,| {now,pooling -} is
|池化是一项处于其核心的操作，

1270
00:36:24,870 --> 00:36:26,550
0,210 210,555 555,825 825,1100 1420,1680
an operation that is at

1271
00:36:26,550 --> 00:36:27,900
0,210 210,495 495,795 795,1095 1095,1350
its core,| it serves one
|它只有一个目的，

1272
00:36:27,900 --> 00:36:29,055
0,320 340,600 600,735 735,915 915,1155
purpose| and that is to
|那就是逐渐降低图像的维度，

1273
00:36:29,055 --> 00:36:31,155
0,270 270,465 465,1445 1585,1905 1905,2100
reduce the dimensionality of the

1274
00:36:31,155 --> 00:36:32,745
0,275 415,1185 1185,1320 1320,1440 1440,1590
image| progressively as you go
|随着你在卷积层中越走越深，

1275
00:36:32,745 --> 00:36:34,320
0,240 240,465 465,755 1105,1395 1395,1575
deeper and deeper through your

1276
00:36:34,320 --> 00:36:36,620
0,780 780,1340 1660,1920 1920,2040 2040,2300
{convolutional,layers -},| now you can
|现在你可以真正开始思考这一点，

1277
00:36:36,850 --> 00:36:38,310
0,400 420,725 725,890 890,1145 1145,1460
really start to reason about

1278
00:36:38,310 --> 00:36:40,310
0,240 240,540 540,920 1420,1710 1710,2000
this,| is that when you
|当你降低你的特征的维度时，

1279
00:36:40,390 --> 00:36:42,015
0,320 320,500 500,1265 1265,1415 1415,1625
decrease the dimensionality of your

1280
00:36:42,015 --> 00:36:45,045
0,335 745,1185 1185,1535 2095,2495 2755,3030
features,| you're effectively increasing the
|你现在是在增加你的过滤器的维度，

1281
00:36:45,045 --> 00:36:47,355
0,935 955,1230 1230,1425 1425,1955 2005,2310
dimensionality of your filters, right,|
|

1282
00:36:47,355 --> 00:36:48,600
0,225 225,420 420,690 690,1035 1035,1245
now because every filter that
因为你滑过较小图像的每个 filter

1283
00:36:48,600 --> 00:36:49,755
0,180 180,390 390,615 615,885 885,1155
you slide over a smaller

1284
00:36:49,755 --> 00:36:51,710
0,335 655,975 975,1515 1515,1680 1680,1955
image| is capturing a larger
|都捕捉到了以前在该网络中发生的更大的感受野，

1285
00:36:52,090 --> 00:36:54,260
0,665 665,970 1080,1430 1430,1775 1775,2170
receptive field that occurred previously

1286
00:36:54,700 --> 00:36:56,505
0,275 275,470 470,790 1500,1730 1730,1805
{in,that -} network,| so a
|所以，一种非常常见的池化技术就是

1287
00:36:56,505 --> 00:36:58,365
0,180 180,480 480,845 1195,1500 1500,1860
very common technique for pooling

1288
00:36:58,365 --> 00:36:59,910
0,150 150,405 405,615 615,965 1105,1545
is| what's called maximum pooling
|所谓的最大池化或简称 max 池化，

1289
00:36:59,910 --> 00:37:01,190
0,180 180,435 435,810 810,990 990,1280
or {max,pooling -} for short,|
|

1290
00:37:01,570 --> 00:37:03,285
0,350 350,650 650,910 960,1360 1470,1715
max pooling is exactly, you
max 池化就是，它听起来像什么，

1291
00:37:03,285 --> 00:37:04,335
0,150 150,300 300,465 465,735 735,1050
{know,what -} it sounds like,|
|

1292
00:37:04,335 --> 00:37:05,715
0,225 225,405 405,690 690,1170 1170,1380
so it {basically,operates -} with
所以它基本上是用这些小块来运作的，

1293
00:37:05,715 --> 00:37:07,470
0,210 210,450 450,900 900,1235 1405,1755
these small {patches,again -},| that
|滑过一张图像，

1294
00:37:07,470 --> 00:37:09,110
0,255 255,465 465,615 615,860 1240,1640
slide over an image,| but
|但不是进行卷积运算，

1295
00:37:09,190 --> 00:37:10,520
0,245 245,395 395,620 620,830 830,1330
instead of doing this convolution

1296
00:37:10,570 --> 00:37:12,315
0,400 720,1010 1010,1220 1220,1595 1595,1745
operation,| what these patches will
|这些块简单地获取块位置的最大值，

1297
00:37:12,315 --> 00:37:13,170
0,135 135,270 270,495 495,720 720,855
do is {simply,take -} the

1298
00:37:13,170 --> 00:37:15,170
0,260 700,1005 1005,1230 1230,1550 1600,2000
maximum of that patch location,|
|

1299
00:37:15,520 --> 00:37:16,590
0,380 380,620 620,755 755,905 905,1070
so think of this as
所以，这可以把这看作是位置的激活的最大值，

1300
00:37:16,590 --> 00:37:18,140
0,165 165,420 420,890 1030,1290 1290,1550
kind of activating the maximum

1301
00:37:18,190 --> 00:37:20,030
0,350 350,590 590,860 860,1240 1440,1840
value that comes from that

1302
00:37:20,100 --> 00:37:21,420
0,320
location,|
|

1303
00:37:21,520 --> 00:37:23,510
0,365 365,875 875,1190 1190,1415 1415,1990
and propagating only the maximums,|
并且只传播最大值，|

1304
00:37:24,220 --> 00:37:25,455
0,395 395,695 695,875 875,1010 1010,1235
{I,encourage -} all of you
我鼓励大家想一想其他方法，

1305
00:37:25,455 --> 00:37:26,595
0,240 240,435 435,645 645,870 870,1140
actually to think of maybe

1306
00:37:26,595 --> 00:37:28,230
0,495 495,750 750,1115 1195,1470 1470,1635
brainstorm other ways,| that we
|让我们可以执行比 max 池化更好的池化操作，

1307
00:37:28,230 --> 00:37:30,540
0,270 270,650 850,1250 1300,1700 1810,2310
could perform even better pooling

1308
00:37:30,540 --> 00:37:32,250
0,350 370,675 675,945 945,1460 1480,1710
operations than max pooling,| there
|有很多常见的方法，

1309
00:37:32,250 --> 00:37:33,615
0,90 90,285 285,620 700,1100 1120,1365
are many common ways,| but
|但你可以想出一些，

1310
00:37:33,615 --> 00:37:35,325
0,90 90,210 210,360 360,635 1375,1710
you could think of some,|
|

1311
00:37:35,325 --> 00:37:36,960
0,240 240,510 510,840 840,1205 1285,1635
for example, or are mean
例如，或者是算术平均池化或平均池化，

1312
00:37:36,960 --> 00:37:38,835
0,405 405,600 600,890 940,1490 1570,1875
pooling or {average,pooling -},| maybe
|也许你不想只取最大值，

1313
00:37:38,835 --> 00:37:39,690
0,180 180,405 405,540 540,705 705,855
you don't want to just

1314
00:37:39,690 --> 00:37:40,940
0,165 165,300 300,560 700,975 975,1250
{take,the -} maximum,| you could
|你可以将所有这些像素的平均值折叠为结果中的单个值，

1315
00:37:41,320 --> 00:37:43,515
0,520 660,1060 1140,1385 1385,1630 1920,2195
collapse basically the average of

1316
00:37:43,515 --> 00:37:45,300
0,150 150,300 300,575 955,1500 1500,1785
all of these pixels into

1317
00:37:45,300 --> 00:37:46,845
0,350 400,720 720,915 915,1185 1185,1545
your into a single value

1318
00:37:46,845 --> 00:37:48,420
0,270 270,575 685,1085
in the result,|
|

1319
00:37:48,850 --> 00:37:49,725
0,275 275,425 425,560 560,680 680,875
but these are the key
但这些都是卷积神经网络的核心关键操作，

1320
00:37:49,725 --> 00:37:52,100
0,335 505,885 885,1725 1725,2100 2100,2375
operations of convolutional neural networks

1321
00:37:52,150 --> 00:37:53,415
0,290 290,500 500,800 800,1070 1070,1265
at their core,| and now
|现在我们准备开始将它们组合在一起，

1322
00:37:53,415 --> 00:37:54,870
0,240 240,480 480,720 720,965 1135,1455
we're ready to really start

1323
00:37:54,870 --> 00:37:56,340
0,285 285,525 525,795 795,1170 1170,1470
to put them together| and
|从头到尾形成和构建 CNN ，

1324
00:37:56,340 --> 00:37:59,550
0,320 940,1340 1480,1880 2320,2940 2940,3210
form and construct CNN all

1325
00:37:59,550 --> 00:38:00,300
0,120 120,255 255,420 420,540 540,750
the way from the ground

1326
00:38:00,300 --> 00:38:01,935
0,270 270,465 465,675 675,1220 1360,1635
up,| {and,with -} CNNs, we
|有了 CNN ，我们可以一个接一个地对这些操作进行分层，

1327
00:38:01,935 --> 00:38:03,675
0,180 180,465 465,840 840,1235 1465,1740
can layer these operations one

1328
00:38:03,675 --> 00:38:05,325
0,225 225,420 420,665 1075,1410 1410,1650
after the other,| starting first
|首先从卷积非线性开始，

1329
00:38:05,325 --> 00:38:07,710
0,195 195,695 1165,2070 2070,2205 2205,2385
with convolutions non-linearities| and then
|然后池化并反复重复这些操作，

1330
00:38:07,710 --> 00:38:09,615
0,470 610,1005 1005,1395 1395,1620 1620,1905
pooling and repeating these over

1331
00:38:09,615 --> 00:38:10,995
0,180 180,425 445,810 810,1080 1080,1380
and over again| to learn
|以了解这些特征的层次结构，

1332
00:38:10,995 --> 00:38:13,110
0,345 345,1085 1165,1470 1470,1775 1855,2115
these hierarchies of features,| and
|这就是我们如何获得这样的图片，

1333
00:38:13,110 --> 00:38:14,450
0,240 240,495 495,735 735,870 870,1340
that's exactly how we obtained

1334
00:38:14,650 --> 00:38:15,885
0,365 365,620 620,830 830,1025 1025,1235
pictures like this,| which we
|我们昨天的课程一开始就是这样，

1335
00:38:15,885 --> 00:38:18,045
0,300 300,975 975,1235 1255,1655 1855,2160
started yesterday's lecture with,| and
|学习这些特征的分层分解，

1336
00:38:18,045 --> 00:38:20,430
0,285 285,600 600,1320 1320,2075 2095,2385
learning these hierarchical decompositions of

1337
00:38:20,430 --> 00:38:22,575
0,290 580,885 885,1620 1620,1995 1995,2145
features| by progressively stacking and
|通过逐步堆叠和堆叠这些 filter ，

1338
00:38:22,575 --> 00:38:24,450
0,545 595,915 915,1395 1395,1665 1665,1875
stacking these filters on top

1339
00:38:24,450 --> 00:38:25,860
0,135 135,270 270,560 580,980 1030,1410
of {each,other -},| each filter
|每个 filter 可以使用它学习到的所有以前的 filter 。

1340
00:38:25,860 --> 00:38:26,805
0,240 240,405 405,645 645,825 825,945
could then use all of

1341
00:38:26,805 --> 00:38:28,470
0,150 150,390 390,965 1225,1485 1485,1665
the previous filters that it

1342
00:38:28,470 --> 00:38:30,060
0,225 225,530
had learned.|
|

1343
00:38:30,780 --> 00:38:32,105
0,275 275,440 440,830 830,1100 1100,1325
So a CNN built for
所以，为图像分类而建立的 CNN 可以分为两个部分，

1344
00:38:32,105 --> 00:38:34,940
0,255 255,875 2065,2385 2385,2610 2610,2835
image classification can be really

1345
00:38:34,940 --> 00:38:36,220
0,255 255,480 480,735 735,990 990,1280
broken down into two parts,|
|

1346
00:38:36,240 --> 00:38:37,760
0,400 510,815 815,980 980,1205 1205,1520
{first,is -} the feature learning
首先是特征学习管道，

1347
00:38:37,760 --> 00:38:39,575
0,350 400,800 850,1170 1170,1490 1540,1815
pipeline,| which we learn the
|我们学习我们想要检测的特征，

1348
00:38:39,575 --> 00:38:40,925
0,275 505,780 780,930 930,1110 1110,1350
features that we want to

1349
00:38:40,925 --> 00:38:42,245
0,335 505,780 780,915 915,1065 1065,1320
detect,| and then the second
|然后第二部分是检测这些特征并进行分类。

1350
00:38:42,245 --> 00:38:44,135
0,285 285,605 625,1005 1005,1575 1575,1890
part is actually detecting those

1351
00:38:44,135 --> 00:38:45,790
0,335 475,750 750,945 945,1125 1125,1655
features and doing the classification.|
|

1352
00:38:48,010 --> 00:38:50,280
0,400 780,1085 1085,1715 1715,1910 1910,2270
Now the convolutional and pooling
现在，从模型的第一部分输出的卷积层和池化层，

1353
00:38:50,280 --> 00:38:52,590
0,470 760,1160 1660,1935 1935,2100 2100,2310
layers output from the first

1354
00:38:52,590 --> 00:38:54,045
0,195 195,330 330,480 480,770 1150,1455
part of that model,| {the,goal
|这些卷积和池化层的目标是，

1355
00:38:54,045 --> 00:38:55,350
0,225 225,435 435,645 645,1185 1185,1305
-} of those convolutional and

1356
00:38:55,350 --> 00:38:56,780
0,285 285,525 525,690 690,980 1030,1430
pooling layers is| to output
|输出从我们的输入中提取的高级特征，

1357
00:38:57,160 --> 00:38:59,330
0,305 305,515 515,770 770,1120 1770,2170
the high level features that

1358
00:38:59,470 --> 00:39:01,215
0,395 395,950 950,1115 1115,1390 1410,1745
are extracted from our input,|
|

1359
00:39:01,215 --> 00:39:02,655
0,195 195,330 330,555 555,905 1165,1440
but the next step is
但下一步是使用这些特征并检测它们的存在，

1360
00:39:02,655 --> 00:39:03,860
0,195 195,405 405,630 630,885 885,1205
to actually use those features

1361
00:39:04,120 --> 00:39:05,595
0,395 395,710 710,935 935,1235 1235,1475
and detect their presence| in
|以便对图像进行分类。

1362
00:39:05,595 --> 00:39:06,710
0,165 165,345 345,735 735,870 870,1115
order to classify {the,image -}.|
|

1363
00:39:07,300 --> 00:39:08,330
0,245 245,335 335,500 500,725 725,1030
So we can feed these
所以，我们可以将这些输出特征输入到我们在第一课中学到的完全连接层中，

1364
00:39:08,380 --> 00:39:11,580
0,320 320,640 840,1240 2580,2900 2900,3200
output features into the fully

1365
00:39:11,580 --> 00:39:13,050
0,360 360,870 870,1110 1110,1245 1245,1470
connected layers that we learned

1366
00:39:13,050 --> 00:39:14,280
0,225 225,375 375,615 615,960 960,1230
about in lecture one,| because
|因为这些现在只是一个一维特征数组，

1367
00:39:14,280 --> 00:39:15,435
0,165 165,315 315,495 495,800 880,1155
these are now just a

1368
00:39:15,435 --> 00:39:17,450
0,165 165,720 720,1055 1405,1710 1710,2015
one dimensional array of features|
|

1369
00:39:17,980 --> 00:39:18,915
0,260 260,380 380,515 515,695 695,935
and we can use those
我们可以使用这些特征来检测我们所在的类别，

1370
00:39:18,915 --> 00:39:20,565
0,270 270,605 775,1020 1020,1265 1315,1650
to detect, you know, what

1371
00:39:20,565 --> 00:39:22,050
0,270 270,495 495,705 705,1085 1225,1485
class we're in,| and we
|我们可以使用一个称为 softmax 函数的函数来实现这一点，

1372
00:39:22,050 --> 00:39:23,060
0,135 135,300 300,525 525,735 735,1010
can do this by using

1373
00:39:23,170 --> 00:39:25,545
0,290 290,580 1230,1610 1610,1865 1865,2375
a function called a softmax

1374
00:39:25,545 --> 00:39:26,640
0,365
function,|
|

1375
00:39:26,640 --> 00:39:27,315
0,180 180,315 315,450 450,555 555,675
you can think of a
你可以将 softmax 函数看作是一个简单的正规化函数，

1376
00:39:27,315 --> 00:39:28,550
0,195 195,435 435,705 705,945 945,1235
{softmax -} function as simply

1377
00:39:28,810 --> 00:39:30,780
0,275 275,815 815,1210 1290,1685 1685,1970
a normalizing function,| whose output
|其输出表示绝对概率分布的输出，

1378
00:39:30,780 --> 00:39:33,405
0,290 1180,1485 1485,1665 1665,1905 1905,2625
represents that of a categorical

1379
00:39:33,405 --> 00:39:35,325
0,485 625,1025 1255,1545 1545,1740 1740,1920
probability distribution,| {so,another -} way
|所以，另一种考虑方式是，

1380
00:39:35,325 --> 00:39:36,810
0,165 165,330 330,465 465,725 1195,1485
to think of this is|
|

1381
00:39:36,810 --> 00:39:37,740
0,270 270,510 510,615 615,735 735,930
basically if you have an
如果你有一个数字数组，你想要折叠，

1382
00:39:37,740 --> 00:39:39,210
0,210 210,390 390,680 1030,1290 1290,1470
array of numbers, you want

1383
00:39:39,210 --> 00:39:40,650
0,210 210,650 790,1065 1065,1215 1215,1440
to collapse| and those numbers
|这些数字可以是任何实数形式，

1384
00:39:40,650 --> 00:39:41,700
0,210 210,345 345,555 555,780 780,1050
could take any real number

1385
00:39:41,700 --> 00:39:43,155
0,380 640,885 885,1005 1005,1155 1155,1455
form,| you want to collapse
|你想把它压缩成某种概率分布，

1386
00:39:43,155 --> 00:39:44,835
0,195 195,450 450,735 735,1205 1285,1680
that into some {probability,distribution -},|
|

1387
00:39:44,835 --> 00:39:46,880
0,545 655,1035 1035,1320 1320,1625 1645,2045
probability distribution has several properties,|
概率分布有几个属性，|

1388
00:39:47,620 --> 00:39:48,840
0,575 575,770 770,950 950,1085 1085,1220
namely that all of its
即它的所有值都必须求和为 1 ，

1389
00:39:48,840 --> 00:39:49,860
0,285 285,555 555,690 690,855 855,1020
values have to sum to

1390
00:39:49,860 --> 00:39:51,405
0,260 610,990 990,1245 1245,1410 1410,1545
one,| it always has to
|它也必须始终介于 0 和 1 之间，

1391
00:39:51,405 --> 00:39:52,395
0,180 180,420 420,600 600,750 750,990
be between zero and {one,as

1392
00:39:52,395 --> 00:39:54,645
0,285 285,605 1075,1470 1470,1865 1945,2250
-} well,| so maintaining those
|所以，维护这两个属性是 softmax 操作所做的事情，

1393
00:39:54,645 --> 00:39:56,205
0,195 195,485 805,1110 1110,1290 1290,1560
two properties is what {a,softmax

1394
00:39:56,205 --> 00:39:57,795
0,585 585,945 945,1260 1260,1455 1455,1590
-} operation does,| you can
|你可以在这里看到它的方程式，

1395
00:39:57,795 --> 00:39:58,970
0,105 105,270 270,570 570,855 855,1175
{see,its -} equation right here,|
|

1396
00:39:59,140 --> 00:40:01,080
0,305 305,610 750,1150 1170,1570 1650,1940
it effectively just makes everything
它有效地使一切都是正的，

1397
00:40:01,080 --> 00:40:02,630
0,290 430,690 690,810 810,975 975,1550
positive| and then it normalizes
|然后它使结果在彼此之间正规化，

1398
00:40:02,740 --> 00:40:04,290
0,335 335,670 900,1190 1190,1355 1355,1550
the result across each other|
|

1399
00:40:04,290 --> 00:40:05,595
0,165 165,315 315,795 795,1095 1095,1305
and that maintains those two
这保持了我刚才提到的两个属性。

1400
00:40:05,595 --> 00:40:06,740
0,275 295,540 540,675 675,855 855,1145
properties that I just mentioned.|
|

1401
00:40:08,570 --> 00:40:09,790
0,350 350,635 635,935 935,1085 1085,1220
Great, so let's put all
好的，让我们把所有这些放在一起，

1402
00:40:09,790 --> 00:40:11,155
0,120 120,330 330,680 700,1080 1080,1365
of this together| and actually
|看看我们如何编写我们的第一个卷积神经网络，

1403
00:40:11,155 --> 00:40:12,360
0,195 195,375 375,555 555,825 825,1205
see how we could program

1404
00:40:12,440 --> 00:40:14,640
0,320 320,590 590,1450 1560,1925 1925,2200
our first convolutional neural network|
|

1405
00:40:14,660 --> 00:40:16,470
0,290 290,410 410,640 690,1090 1410,1810
end to end entirely from
完全从头开始。

1406
00:40:16,580 --> 00:40:18,300
0,380 380,760 960,1310 1310,1445 1445,1720
scratch.| {So,let's -} start by
|所以，让我们首先定义我们的特征提取头，

1407
00:40:18,350 --> 00:40:21,085
0,620 620,1205 1205,1595 1595,1990 2130,2735
firstly defining our feature extraction

1408
00:40:21,085 --> 00:40:23,515
0,335 1195,1595 1675,2025 2025,2265 2265,2430
head,| which starts with a
|它从卷积层开始，

1409
00:40:23,515 --> 00:40:26,190
0,570 570,845 895,1295 1435,1835 1945,2675
convolutional layer| and here 32
|这里是 32 个 filter 或 32 个特征，

1410
00:40:26,240 --> 00:40:28,060
0,485 485,680 680,1160 1160,1450 1560,1820
filters or 32 features,| you
|你可以想象这第一层，这第一层的结果是学习，

1411
00:40:28,060 --> 00:40:29,590
0,150 150,440 790,1065 1065,1260 1260,1530
can imagine that this first

1412
00:40:29,590 --> 00:40:30,760
0,345 345,660 660,870 870,1020 1020,1170
layer, the result of this

1413
00:40:30,760 --> 00:40:31,770
0,195 195,375 375,540 540,720 720,1010
first layer is to learn,|
|

1414
00:40:31,850 --> 00:40:33,760
0,365 365,730 780,1180 1380,1670 1670,1910
not one filter, not one
不是一个 filter ，不是我们图像中的一个模式，而是 32 个模式，

1415
00:40:33,760 --> 00:40:35,455
0,350 610,870 870,1005 1005,1280 1390,1695
pattern in our image, but

1416
00:40:35,455 --> 00:40:37,390
0,540 540,815 1255,1560 1560,1725 1725,1935
{32,patterns -},| okay, So those
|好的，所以这 32 个结果将被传递到池化层，

1417
00:40:37,390 --> 00:40:39,385
0,675 675,1040 1390,1680 1680,1860 1860,1995
32 results are going to

1418
00:40:39,385 --> 00:40:41,080
0,135 135,345 345,665 1135,1470 1470,1695
then be passed to a

1419
00:40:41,080 --> 00:40:43,270
0,300 300,590 880,1170 1170,1460 1840,2190
pooling layer| and then passed
|然后传递到下一组卷积运算，

1420
00:40:43,270 --> 00:40:44,665
0,285 285,510 510,705 705,1005 1005,1395
on to the next set

1421
00:40:44,665 --> 00:40:47,410
0,300 300,1085 1315,1715 2275,2550 2550,2745
{of,convolutional -} operations,| the next
|下一组卷积运算现在将包含 64 个特征，

1422
00:40:47,410 --> 00:40:48,985
0,165 165,285 285,840 840,1160 1240,1575
set of convolutional operations now

1423
00:40:48,985 --> 00:40:50,890
0,315 315,675 675,1290 1290,1590 1590,1905
will contain 64 features| will
|将逐步增长和扩展我们在这张图像中识别的模式集，

1424
00:40:50,890 --> 00:40:53,850
0,285 285,1155 1155,1460 2020,2420 2560,2960
keep progressively growing and expanding

1425
00:40:54,050 --> 00:40:55,585
0,350 350,575 575,755 755,1060 1260,1535
our set of patterns that

1426
00:40:55,585 --> 00:40:57,000
0,255 255,765 765,1005 1005,1155 1155,1415
we're identifying in this image,|
|

1427
00:40:58,330 --> 00:41:00,195
0,400 480,740 740,905 905,1210 1290,1865
next, we can finally flatten
接下来，我们最终可以扁平化那些我们已经识别的结果特征，

1428
00:41:00,195 --> 00:41:02,030
0,365 415,815 835,1215 1215,1470 1470,1835
those resulting features that we've

1429
00:41:02,110 --> 00:41:03,885
0,400 750,1115 1115,1400 1400,1610 1610,1775
identified| and feed all of
|并将所有这些都提供给我们的紧密层，

1430
00:41:03,885 --> 00:41:05,235
0,240 240,525 525,780 780,1050 1050,1350
this through {our,dense -} layers,|
|

1431
00:41:05,235 --> 00:41:06,450
0,165 165,420 420,735 735,1080 1080,1215
are fully connected layers that
也就是我们在第一课中学到的完全相连的层，

1432
00:41:06,450 --> 00:41:07,305
0,105 105,285 285,480 480,615 615,855
we learned about in lecture

1433
00:41:07,305 --> 00:41:08,865
0,365 685,990 990,1200 1200,1380 1380,1560
one,| these will allow us
|这将使我们能够预测最终，比如 10 个种类，

1434
00:41:08,865 --> 00:41:10,605
0,180 180,455 625,960 960,1295 1405,1740
to predict those final, let's

1435
00:41:10,605 --> 00:41:11,865
0,240 240,570 570,900 900,1155 1155,1260
say ten classes,| if we
|如果我们的图像中有十个不同的最终可能种类，

1436
00:41:11,865 --> 00:41:14,060
0,135 135,360 360,660 660,1025 1795,2195
have ten different final possible

1437
00:41:14,110 --> 00:41:15,680
0,395 395,635 635,740 740,1000 1170,1570
classes in our image,| this
|这一层将说明这一点，

1438
00:41:15,790 --> 00:41:17,370
0,400 450,850 870,1220 1220,1430 1430,1580
layer will account for that|
|

1439
00:41:17,370 --> 00:41:19,100
0,290 580,855 855,1035 1035,1335 1335,1730
and allow us to output
并允许我们使用 softmax 输出这十个种类的概率分布。

1440
00:41:19,180 --> 00:41:21,560
0,365 365,1000 1080,1340 1340,1870 1980,2380
using softmax the probability distribution

1441
00:41:21,790 --> 00:41:23,620
0,335 335,590 590,830 830,1150
across those ten classes.|
|

1442
00:41:24,910 --> 00:41:25,905
0,275 275,425 425,590 590,815 815,995
So, so far we've talked
到目前为止，我们已经讨论了，

1443
00:41:25,905 --> 00:41:28,095
0,335 955,1260 1260,1455 1455,1745 1855,2190
about,| how we can, let's
|如何使用 CNN 来执行图像分类任务，

1444
00:41:28,095 --> 00:41:30,150
0,180 180,495 495,1115 1225,1625 1735,2055
say use CNNs to perform

1445
00:41:30,150 --> 00:41:32,040
0,270 270,860 940,1340 1420,1710 1710,1890
image classification tasks,| {but,in -}
|但实际上，我在今天的课程中特别想强调的一件事是，

1446
00:41:32,040 --> 00:41:33,450
0,290 730,1005 1005,1125 1125,1245 1245,1410
reality, one thing I really

1447
00:41:33,450 --> 00:41:34,650
0,150 150,345 345,615 615,825 825,1200
want to stress in today's

1448
00:41:34,650 --> 00:41:36,080
0,290 370,720 720,1005 1005,1185 1185,1430
class, especially towards the end,|
|

1449
00:41:36,370 --> 00:41:37,850
0,275 275,455 455,760 840,1160 1160,1480
is that this same architecture
我们到目前为止讨论的相同的体系结构和相同的构建块是可扩展的，

1450
00:41:38,230 --> 00:41:39,600
0,335 335,590 590,890 890,1190 1190,1370
and same building blocks that

1451
00:41:39,600 --> 00:41:40,730
0,195 195,390 390,645 645,840 840,1130
we've talked about so far

1452
00:41:40,990 --> 00:41:43,860
0,335 335,1210 1290,1690 2160,2555 2555,2870
are extensible| and they extend
|它们可以扩展到我们可以想象的许多不同的应用程序和模型类型，

1453
00:41:43,860 --> 00:41:46,220
0,255 255,495 495,735 735,1070 1960,2360
to so many different applications

1454
00:41:46,360 --> 00:41:48,810
0,400 510,905 905,1300 1560,1960 2160,2450
and model types, that we

1455
00:41:48,810 --> 00:41:50,240
0,180 180,470 610,915 915,1125 1125,1430
can imagine,| so for example,
|例如，当我们考虑 CNN 进行分类时，

1456
00:41:50,530 --> 00:41:52,550
0,305 305,610 630,1030 1140,1460 1460,2020
when we considered the CNN

1457
00:41:52,600 --> 00:41:54,675
0,305 305,880 1380,1670 1670,1880 1880,2075
for classification,| we saw that
|我们看到它实际上有两个部分，

1458
00:41:54,675 --> 00:41:55,910
0,165 165,390 390,690 690,945 945,1235
it really had {two,parts -},|
|

1459
00:41:56,140 --> 00:41:57,380
0,260 260,455 455,695 695,920 920,1240
the first part being feature
第一部分是特征提取，学习要寻找什么特征，

1460
00:41:57,460 --> 00:41:59,085
0,590 590,905 905,1145 1145,1400 1400,1625
extraction, learning what features to

1461
00:41:59,085 --> 00:42:00,240
0,150 150,455 535,795 795,930 930,1155
look for,| and the second
|第二部分是这些特征的分类和检测。

1462
00:42:00,240 --> 00:42:02,400
0,350 490,890 1030,1290 1290,1790 1810,2160
part being the classification, the

1463
00:42:02,400 --> 00:42:04,320
0,495 495,675 675,870 870,1190
detection of those features.|
|

1464
00:42:05,060 --> 00:42:06,700
0,350 350,590 590,830 830,1055 1055,1640
Now, what makes a convolutional
现在，卷积神经网络真正强大的地方是，

1465
00:42:06,700 --> 00:42:08,610
0,255 255,530 790,1170 1170,1530 1530,1910
neural network really, really powerful

1466
00:42:09,500 --> 00:42:13,375
0,400 1080,1480 1590,1990 2910,3310 3570,3875
is| exactly the observation that
|因为观察到特征学习部分，

1467
00:42:13,375 --> 00:42:15,295
0,210 210,515 565,965 1075,1475 1555,1920
the feature learning part,| this
|神经网络的第一部分是非常灵活的，

1468
00:42:15,295 --> 00:42:16,570
0,345 345,645 645,870 870,1035 1035,1275
first part of the neural

1469
00:42:16,570 --> 00:42:20,290
0,260 1450,1850 2320,2720 2770,3170 3430,3720
network is extremely flexible,| {you,can
|你可以取走神经网络的第一部分，

1470
00:42:20,290 --> 00:42:21,535
0,195 195,420 420,675 675,975 975,1245
-} take that first part

1471
00:42:21,535 --> 00:42:23,035
0,180 180,315 315,555 555,815 1195,1500
of the neural network,| chop
|砍掉它后面的部分，

1472
00:42:23,035 --> 00:42:24,360
0,225 225,450 450,660 660,945 945,1325
off what comes after it,|
|

1473
00:42:24,440 --> 00:42:25,435
0,305 305,485 485,635 635,815 815,995
and put a bunch of
然后把一堆不同的头放进后面的部分，

1474
00:42:25,435 --> 00:42:27,265
0,270 270,665 955,1355 1405,1680 1680,1830
different heads into the part

1475
00:42:27,265 --> 00:42:28,420
0,150 150,315 315,605 685,975 975,1155
that comes after,| the goal
|第一部分的目标是提取这些特征，

1476
00:42:28,420 --> 00:42:29,590
0,180 180,345 345,585 585,900 900,1170
of the first part is

1477
00:42:29,590 --> 00:42:31,420
0,315 315,645 645,885 885,1190 1540,1830
to extract {those,features -},| what
|如何处理这些特性完全由你自己决定，

1478
00:42:31,420 --> 00:42:32,430
0,210 210,405 405,570 570,735 735,1010
you do with the features

1479
00:42:32,720 --> 00:42:33,880
0,350 350,635 635,860 860,980 980,1160
is entirely up to you,|
|

1480
00:42:33,880 --> 00:42:35,100
0,210 210,330 330,585 585,900 900,1220
but you can still leverage
但你仍然可以利用第一部分的灵活性和强大功能来学习所有这些核心特性，

1481
00:42:35,180 --> 00:42:36,700
0,275 275,550 960,1205 1205,1325 1325,1520
the flexibility and the power

1482
00:42:36,700 --> 00:42:38,155
0,180 180,315 315,525 525,860 1090,1455
of the first part to

1483
00:42:38,155 --> 00:42:39,520
0,300 300,495 495,645 645,935 1045,1365
learn all of {those,core -}

1484
00:42:39,520 --> 00:42:41,290
0,320 340,615 615,795 795,1100 1450,1770
features,| so for example, that
|例如，这部分将寻找所有不同的图像分类域，

1485
00:42:41,290 --> 00:42:42,835
0,320 370,690 690,975 975,1320 1320,1545
portion will look for, you

1486
00:42:42,835 --> 00:42:43,630
0,120 120,270 270,435 435,585 585,795
know, all of the {different,image

1487
00:42:43,630 --> 00:42:46,465
0,350 370,1010 1510,2090 2260,2565 2565,2835
-} classification domains,| that future
|在我们提取特征之后的部分，

1488
00:42:46,465 --> 00:42:47,995
0,360 360,690 690,1050 1050,1410 1410,1530
portion after you've extracted the

1489
00:42:47,995 --> 00:42:49,735
0,275 355,755 985,1230 1230,1425 1425,1740
features,| or we can also
|我们也可以引入新体系结构，

1490
00:42:49,735 --> 00:42:51,565
0,365 385,675 675,1365 1365,1605 1605,1830
introduce new architectures,| that take
|获取这些特征并执行分割或图像说明之类的任务，

1491
00:42:51,565 --> 00:42:53,620
0,210 210,515 925,1215 1215,1505 1735,2055
those features and maybe perform

1492
00:42:53,620 --> 00:42:55,495
0,320 340,660 660,1310 1330,1620 1620,1875
tasks like segmentation or image

1493
00:42:55,495 --> 00:42:56,730
0,570 570,735 735,885 885,990 990,1235
captioning,| like we saw in
|像我们在昨天课程中看到的。

1494
00:42:56,810 --> 00:42:58,180
0,665 665,910
yesterday's lecture.|
|

1495
00:42:59,230 --> 00:43:00,255
0,275 275,425 425,575 575,755 755,1025
So in the case of
例如，在分类的情况下，

1496
00:43:00,255 --> 00:43:02,415
0,605 835,1140 1140,1445 1645,1950 1950,2160
classification, for example,| just to
|仅将分类故事联系起来，

1497
00:43:02,415 --> 00:43:03,980
0,195 195,420 420,755 805,1065 1065,1565
tie up the, the classification

1498
00:43:04,150 --> 00:43:06,240
0,400 660,995 995,1115 1115,1390 1710,2090
story,| there's a significant impact
|这在医疗保健、医疗决策等领域产生重大影响，

1499
00:43:06,240 --> 00:43:08,300
0,285 285,690 690,1020 1020,1400 1660,2060
in domains like healthcare, medical

1500
00:43:08,350 --> 00:43:09,645
0,350 350,680 680,935 935,1085 1085,1295
decision making,| where deep learning
|深度学习模型被应用于医学扫描分析，

1501
00:43:09,645 --> 00:43:10,860
0,285 285,510 510,720 720,1005 1005,1215
models are being applied to

1502
00:43:10,860 --> 00:43:13,250
0,150 150,440 1030,1430 1480,1880 1960,2390
the analysis of medical scans|
|

1503
00:43:13,720 --> 00:43:15,480
0,290 290,485 485,790 870,1270 1470,1760
across a whole host of
通过一系列不同的医学图像。

1504
00:43:15,480 --> 00:43:17,660
0,240 240,540 540,1160
different medical imagery.|
|

1505
00:43:18,530 --> 00:43:20,670
0,395 395,1000 1230,1550 1550,1805 1805,2140
Now classification tells us basically
现在，分类告诉我们对图像所包含内容的离散预测，

1506
00:43:20,960 --> 00:43:24,550
0,400 900,1630 1890,2440 2700,3100 3300,3590
a discrete prediction of what

1507
00:43:24,550 --> 00:43:26,080
0,165 165,440 580,980 1150,1410 1410,1530
our image contains,| but we
|但我们也可以更深入地研究这个问题，

1508
00:43:26,080 --> 00:43:27,240
0,195 195,390 390,540 540,795 795,1160
can actually go much deeper

1509
00:43:27,500 --> 00:43:29,190
0,365 365,650 650,970 1050,1370 1370,1690
into this problem as well,|
|

1510
00:43:29,270 --> 00:43:31,165
0,275 275,455 455,760 1170,1570 1590,1895
{so,for -} example, imagine that
比如，想象一下，我们不仅要识别这张图片是一辆出租车，

1511
00:43:31,165 --> 00:43:32,370
0,240 240,435 435,735 735,945 945,1205
we're not trying to only

1512
00:43:32,720 --> 00:43:34,255
0,380 380,650 650,845 845,1150 1260,1535
identify that this image is

1513
00:43:34,255 --> 00:43:35,310
0,120 120,365 385,645 645,780 780,1055
an image of a taxi,|
|

1514
00:43:35,630 --> 00:43:36,690
0,275 275,410 410,575 575,770 770,1060
which you can see here,|
你可以在这里看到，|

1515
00:43:37,070 --> 00:43:38,950
0,400 420,710 710,950 950,1300 1530,1880
but also more importantly,| maybe
更重要的是，|我们可能希望我们的神经网络不仅能告诉我们这是一辆出租车，

1516
00:43:38,950 --> 00:43:40,410
0,225 225,435 435,770 850,1200 1200,1460
we want our neural network

1517
00:43:40,430 --> 00:43:42,100
0,275 275,425 425,700 1170,1460 1460,1670
to tell us not only

1518
00:43:42,100 --> 00:43:42,990
0,210 210,360 360,480 480,615 615,890
that this is a taxi,|
|

1519
00:43:43,220 --> 00:43:45,450
0,400 630,890 890,1100 1100,1450 1830,2230
but identify and draw a
还能识别并在出租车的这个位置上画一个特定的边界框，

1520
00:43:45,530 --> 00:43:47,880
0,400 540,1100 1100,1390 1590,1970 1970,2350
specific bounding box over this

1521
00:43:48,170 --> 00:43:49,660
0,400 450,710 710,845 845,1120 1230,1490
location of the taxi,| so
|所以这是一个两阶段的问题，

1522
00:43:49,660 --> 00:43:50,365
0,120 120,255 255,405 405,540 540,705
this is kind of a

1523
00:43:50,365 --> 00:43:52,015
0,195 195,435 435,785 1075,1410 1410,1650
two {phase,problem -},| number one
|第一，我们需要画一个盒子，

1524
00:43:52,015 --> 00:43:52,885
0,180 180,330 330,465 465,645 645,870
is that we need to

1525
00:43:52,885 --> 00:43:54,430
0,165 165,285 285,545 1105,1350 1350,1545
draw a box| and number
|第二，我们需要对盒子里的东西进行分类，

1526
00:43:54,430 --> 00:43:55,135
0,195 195,315 315,450 450,570 570,705
two is we need to

1527
00:43:55,135 --> 00:43:56,305
0,540 540,735 735,855 855,990 990,1170
classify what was {in,that -}

1528
00:43:56,305 --> 00:43:57,880
0,305 685,930 930,1125 1125,1320 1320,1575
box,| so it's {both,a -}
|所以这是一个回归问题，

1529
00:43:57,880 --> 00:43:59,830
0,465 465,860 1210,1545 1545,1785 1785,1950
regression problem,| where is the
|盒子在哪里，

1530
00:43:59,830 --> 00:44:01,950
0,260 880,1200 1200,1380 1380,1725 1725,2120
box,| that's a continuous problem
|这是一个连续的问题，也是一个在盒子里的分类问题。

1531
00:44:02,180 --> 00:44:03,750
0,320 320,515 515,680 680,950 950,1570
as well as a classification

1532
00:44:03,800 --> 00:44:05,245
0,350 350,700 720,995 995,1205 1205,1445
problem is {what,is -} in

1533
00:44:05,245 --> 00:44:07,300
0,195 195,485 1105,1505 1525,1875 1875,2055
that box.| Now that's a
|这是一个比我们在今天的课程中所讨论的要困难得多的问题，

1534
00:44:07,300 --> 00:44:09,330
0,300 300,555 555,825 825,1190 1630,2030
much, much harder problem than

1535
00:44:09,500 --> 00:44:10,585
0,275 275,515 515,695 695,920 920,1085
what we've covered so far

1536
00:44:10,585 --> 00:44:11,940
0,135 135,240 240,465 465,845 955,1355
in the lecture today,| because
|因为在我们的场景中可能有很多物体，

1537
00:44:12,080 --> 00:44:14,305
0,400 900,1205 1205,1460 1460,1810 1890,2225
potentially there are many objects

1538
00:44:14,305 --> 00:44:15,220
0,180 180,330 330,540 540,735 735,915
in {our,scene -},| not just
|而不仅仅是一个物体，

1539
00:44:15,220 --> 00:44:16,540
0,285 285,680 910,1140 1140,1215 1215,1320
one object,| so we have
|所以我们必须解释这样一个事实，

1540
00:44:16,540 --> 00:44:17,635
0,180 180,435 435,660 660,855 855,1095
to account for this fact,|
|

1541
00:44:17,635 --> 00:44:18,940
0,335 445,845 865,1265
that maybe our
也许我们可以约束任意多个对象。

1542
00:44:18,980 --> 00:44:21,540
0,305 305,740 740,1445 1445,1810 2160,2560
could constrain arbitrarily many objects.|
|

1543
00:44:23,440 --> 00:44:25,395
0,400 870,1160 1160,1450 1470,1790 1790,1955
Now our network needs to
现在我们的网络需要灵活到那个程度，

1544
00:44:25,395 --> 00:44:27,950
0,240 240,635 1075,1475 1735,2135 2155,2555
be flexible to that degree,|
|

1545
00:44:28,150 --> 00:44:29,145
0,305 305,455 455,545 545,710 710,995
needs to be able to
需要能够推断场景中对象的动态数量，

1546
00:44:29,145 --> 00:44:31,160
0,345 345,720 720,1115 1165,1565 1615,2015
infer a dynamic number of

1547
00:44:31,240 --> 00:44:32,310
0,320 320,500 500,650 650,875 875,1070
objects in the scene,| {and,if
|如果场景只是一辆出租车，

1548
00:44:32,310 --> 00:44:33,960
0,150 150,330 330,620 910,1305 1305,1650
-} the scene is only

1549
00:44:33,960 --> 00:44:34,920
0,195 195,315 315,570 570,810 810,960
of a taxi,| then it
|那么它应该只输出一个边界框，

1550
00:44:34,920 --> 00:44:36,120
0,150 150,410 550,900 900,1095 1095,1200
should only output, you know,

1551
00:44:36,120 --> 00:44:37,725
0,165 165,390 390,825 825,1130 1360,1605
that one bounding box,| but
|但另一方面，如果图像有许多对象，甚至可能是不同类别的对象，

1552
00:44:37,725 --> 00:44:39,135
0,120 120,225 225,375 375,695 1075,1410
on the other hand, if

1553
00:44:39,135 --> 00:44:41,210
0,180 180,375 375,725 925,1325 1675,2075
the image has many objects,

1554
00:44:41,620 --> 00:44:43,430
0,400 450,815 815,1115 1115,1430 1430,1810
potentially even of different classes,|
|

1555
00:44:43,480 --> 00:44:44,880
0,260 260,395 395,530 530,790 1110,1400
we need a model that
我们需要一个模型来为这些不同的示例中的每一个绘制一个边界框，

1556
00:44:44,880 --> 00:44:46,010
0,180 180,330 330,450 450,825 825,1130
can draw a bounding box

1557
00:44:46,120 --> 00:44:47,660
0,275 275,470 470,680 680,970 1140,1540
for each of these different

1558
00:44:47,860 --> 00:44:50,180
0,400 1080,1430 1430,1670 1670,1940 1940,2320
examples,| as well as associate
|并将它们的预测分类标签独立地与每个示例相关联。

1559
00:44:50,230 --> 00:44:53,190
0,365 365,1000 1020,1660 1740,2320 2700,2960
their predicted classification labels to

1560
00:44:53,190 --> 00:44:54,840
0,180 180,500 580,1340
each one independently.|
|

1561
00:44:56,230 --> 00:44:58,080
0,395 395,665 665,940 1290,1595 1595,1850
Now this is actually quite
实际上，这相当复杂，

1562
00:44:58,080 --> 00:45:00,120
0,350 550,855 855,1160 1450,1785 1785,2040
complicated in practice,| because those
|因为这些方框可以在图像中的任何位置，

1563
00:45:00,120 --> 00:45:01,305
0,320 370,645 645,870 870,1065 1065,1185
boxes can be anywhere in

1564
00:45:01,305 --> 00:45:02,805
0,105 105,335 625,945 945,1080 1080,1500
the image,| there's no constraints
|盒子放在哪里没有限制，

1565
00:45:02,805 --> 00:45:03,765
0,120 120,285 285,420 420,665 685,960
on where the boxes can

1566
00:45:03,765 --> 00:45:05,550
0,275 625,1025 1135,1410 1410,1605 1605,1785
be| and they can also
|它们也可以有不同的大小，

1567
00:45:05,550 --> 00:45:06,980
0,105 105,225 225,480 480,890 1030,1430
be of different sizes,| {they,can
|它们也可以具有不同的比率，

1568
00:45:07,030 --> 00:45:08,030
0,245 245,440 440,620 620,725 725,1000
-} be also of different

1569
00:45:08,170 --> 00:45:09,705
0,670 750,1025 1025,1160 1160,1310 1310,1535
ratios,| some can be tall,
|有些可能很高，有些可能很宽。

1570
00:45:09,705 --> 00:45:11,400
0,225 225,375 375,510 510,785 1285,1695
some can be wide.| Let's
|首先，让我们考虑一种非常幼稚的方式来做这件事，

1571
00:45:11,400 --> 00:45:12,945
0,210 210,360 360,620 730,1275 1275,1545
consider a very naive way

1572
00:45:12,945 --> 00:45:14,895
0,180 180,405 405,755 1015,1415 1585,1950
of {doing,this -} first,| let's
|让我们在该图像上的某个位置放置一个随机框，

1573
00:45:14,895 --> 00:45:15,870
0,150 150,300 300,510 510,765 765,975
take our image and start

1574
00:45:15,870 --> 00:45:17,690
0,195 195,710 850,1140 1140,1425 1425,1820
by placing a random box

1575
00:45:18,040 --> 00:45:19,680
0,395 395,710 710,935 935,1240 1350,1640
somewhere {on,that -} image,| for
|例如，我们只需随机选择一个位置，一个随机大小，

1576
00:45:19,680 --> 00:45:20,805
0,290 460,720 720,870 870,1020 1020,1125
example, we just pick a

1577
00:45:20,805 --> 00:45:22,340
0,245 325,675 675,885 885,1140 1140,1535
random location, a random size,|
|

1578
00:45:22,690 --> 00:45:23,790
0,335 335,470 470,620 620,830 830,1100
we'll place {a,box -} right
我们就会在那里放置一个盒子，

1579
00:45:23,790 --> 00:45:26,070
0,320 1000,1400 1510,1905 1905,2160 2160,2280
there,| this box, like I
|这个盒子，就像我说的，有一个随机的位置，随机的大小，

1580
00:45:26,070 --> 00:45:27,290
0,150 150,300 300,435 435,710 820,1220
said, has {a,random -} location,

1581
00:45:27,310 --> 00:45:28,665
0,400 420,820 840,1085 1085,1190 1190,1355
random size,| then we can
|然后我们可以拿起那个盒子，

1582
00:45:28,665 --> 00:45:30,080
0,210 210,405 405,695 835,1125 1125,1415
take that box| and only
|只把那个随机的盒子送入我们的卷积神经网络，

1583
00:45:30,100 --> 00:45:31,950
0,335 335,575 575,875 875,1270 1530,1850
feed that random box through

1584
00:45:31,950 --> 00:45:33,465
0,240 240,825 825,1050 1050,1275 1275,1515
our convolutional neural network| which
|它被训练来进行分类，只是分类，

1585
00:45:33,465 --> 00:45:34,910
0,165 165,360 360,525 525,785 805,1445
is trained to do classification,

1586
00:45:35,410 --> 00:45:37,940
0,305 305,850
just classification,|
|

1587
00:45:37,950 --> 00:45:39,755
0,400 870,1160 1160,1370 1370,1550 1550,1805
and this neural network can
这个神经网络可以检测到，

1588
00:45:39,755 --> 00:45:41,030
0,270 270,495 495,720 720,1005 1005,1275
detect,| well, number one, is
|第一，盒子里有没有一类物体，

1589
00:45:41,030 --> 00:45:42,635
0,195 195,375 375,680 850,1245 1245,1605
there a class of object

1590
00:45:42,635 --> 00:45:44,105
0,270 270,480 480,785 895,1215 1215,1470
in that box or not,|
|

1591
00:45:44,105 --> 00:45:45,200
0,195 195,360 360,570 570,795 795,1095
and if so, what class
如果是的话，是什么种类，

1592
00:45:45,200 --> 00:45:46,610
0,255 255,530 760,1020 1020,1215 1215,1410
is it,| and then what
|然后我们可以做的是，

1593
00:45:46,610 --> 00:45:47,150
0,120 120,225 225,315 315,420 420,540
we could do is,| we
|我们可以一遍又一遍地重复这个过程，

1594
00:45:47,150 --> 00:45:48,730
0,120 120,300 300,615 615,1160 1180,1580
could just keep repeating this

1595
00:45:48,780 --> 00:45:50,375
0,400 510,845 845,1025 1025,1265 1265,1595
process over and over again,|
|

1596
00:45:50,375 --> 00:45:51,425
0,210 210,390 390,570 570,765 765,1050
for all of these random
对我们图像中的所有这些随机框，很多随机框，

1597
00:45:51,425 --> 00:45:53,345
0,365 745,1005 1005,1125 1125,1385 1675,1920
boxes in our image, you

1598
00:45:53,345 --> 00:45:55,235
0,195 195,495 495,845 925,1695 1695,1890
know, many, many instances of

1599
00:45:55,235 --> 00:45:56,660
0,225 225,555 555,825 825,1020 1020,1425
random boxes,| {we,keep -} sampling
|我们不断地采样一个新的盒子，

1600
00:45:56,660 --> 00:45:58,205
0,120 120,300 300,620 1120,1395 1395,1545
a new box,| feed it
|通过我们的卷积神经网络输入它，

1601
00:45:58,205 --> 00:45:59,510
0,135 135,300 300,855 855,1080 1080,1305
through our convolutional neural network|
|

1602
00:45:59,510 --> 00:46:00,650
0,195 195,360 360,600 600,900 900,1140
and ask this question what
然后问这个问题，盒子里是什么，

1603
00:46:00,650 --> 00:46:01,490
0,120 120,240 240,360 360,585 585,840
was in the box,| if
|如果里面有什么东西，那是什么，

1604
00:46:01,490 --> 00:46:02,450
0,150 150,315 315,555 555,780 780,960
there was something in there,

1605
00:46:02,450 --> 00:46:04,340
0,180 180,470 730,1005 1005,1280 1630,1890
then what is it,| and
|我们继续前进，直到我们用尽了图像中所有的方框。

1606
00:46:04,340 --> 00:46:05,540
0,120 120,285 285,510 510,830 880,1200
we keep moving on until

1607
00:46:05,540 --> 00:46:07,340
0,320 490,765 765,915 915,1155 1155,1800
we kind of have exhausted

1608
00:46:07,340 --> 00:46:08,495
0,270 270,465 465,705 705,900 900,1155
all of the the boxes

1609
00:46:08,495 --> 00:46:10,100
0,270 270,375 375,605 1255,1500 1500,1605
in {the,image -}.| But the
|但这里的问题是，

1610
00:46:10,100 --> 00:46:10,865
0,195 195,375 375,495 495,645 645,765
problem here is that,| there
|我们必须处理的潜在输入太多了，

1611
00:46:10,865 --> 00:46:11,860
0,105 105,315 315,585 585,750 750,995
are just way too many

1612
00:46:12,270 --> 00:46:13,670
0,400 630,1025 1025,1175 1175,1295 1295,1400
potential inputs that we would

1613
00:46:13,670 --> 00:46:14,555
0,150 150,330 330,525 525,720 720,885
have {to,deal -} with,| this
|这对于在实时系统中运行是完全不切实际的，

1614
00:46:14,555 --> 00:46:16,430
0,150 150,330 330,585 585,1445 1615,1875
would be totally impractical to

1615
00:46:16,430 --> 00:46:18,380
0,210 210,560 1240,1515 1515,1710 1710,1950
run in {a,real -} time

1616
00:46:18,380 --> 00:46:20,105
0,320 490,795 795,1065 1065,1320 1320,1725
system,| {for,example, -} with today's
|例如，用今天的计算机，

1617
00:46:20,105 --> 00:46:21,635
0,425 505,825 825,1050 1050,1290 1290,1530
compute,| it results in way
|这导致了太多的比例，

1618
00:46:21,635 --> 00:46:23,240
0,150 150,360 360,755 955,1350 1350,1605
too many scales,| especially for
|特别是对于我们今天处理的图像的分辨率类型。

1619
00:46:23,240 --> 00:46:24,440
0,135 135,300 300,465 465,960 960,1200
the types of resolutions of

1620
00:46:24,440 --> 00:46:26,380
0,260 820,1125 1125,1380 1380,1635 1635,1940
images that we deal with

1621
00:46:26,940 --> 00:46:27,800
0,400
today.|
|

1622
00:46:28,770 --> 00:46:30,005
0,380 380,605 605,710 710,920 920,1235
So instead of picking random
所以，不是选择随机的框，

1623
00:46:30,005 --> 00:46:31,400
0,365 535,975 975,1140 1140,1245 1245,1395
boxes,| let's try and use
|让我们尝试并使用一个非常简单的启发式，

1624
00:46:31,400 --> 00:46:33,370
0,195 195,465 465,795 795,1490 1570,1970
a very simple heuristic, right,|
|

1625
00:46:33,810 --> 00:46:36,190
0,400 900,1300 1320,1700 1700,2030 2030,2380
to identify maybe some places
来识别图像中一些具有很大可变性的位置，

1626
00:46:36,360 --> 00:46:38,465
0,400 960,1235 1235,1370 1370,1880 1880,2105
with lots of variability in

1627
00:46:38,465 --> 00:46:39,620
0,105 105,335 355,750 750,1005 1005,1155
the image,| where there is
|其中可能存在对象的可能性很高，

1628
00:46:39,620 --> 00:46:41,855
0,210 210,860 970,1305 1305,1640 1840,2235
high likelihood of having an

1629
00:46:41,855 --> 00:46:43,870
0,390 390,675 675,855 855,1145 1615,2015
objects might be present, right,|
|

1630
00:46:44,040 --> 00:46:46,160
0,335 335,575 575,830 830,1180 1710,2120
these might have meaningful insights
这些可能具有有意义的见解或有意义的对象，

1631
00:46:46,160 --> 00:46:47,690
0,165 165,470 670,1050 1050,1335 1335,1530
or meaningful objects,| that could
|可以在我们的图像中使用，

1632
00:46:47,690 --> 00:46:49,060
0,290 310,705 705,960 960,1095 1095,1370
be available in our image,|
|

1633
00:46:49,350 --> 00:46:50,410
0,260 260,395 395,545 545,740 740,1060
{and,we -} can use those|
我们可以利用这些信息，|

1634
00:46:50,580 --> 00:46:52,010
0,245 245,490 720,1055 1055,1250 1250,1430
to basically just feed in
将那些高关注度的位置输入到我们的卷积神经网络中，

1635
00:46:52,010 --> 00:46:54,935
0,315 315,710 1000,1400 1720,2120 2680,2925
those high attention locations to

1636
00:46:54,935 --> 00:46:56,300
0,135 135,705 705,945 945,1155 1155,1365
our convolutional neural network,| and
|然后我们可以大大加快管道的第一部分，

1637
00:46:56,300 --> 00:46:57,575
0,120 120,240 240,500 520,920 1000,1275
then we can basically speed

1638
00:46:57,575 --> 00:46:58,610
0,150 150,345 345,615 615,840 840,1035
up that first part of

1639
00:46:58,610 --> 00:47:00,155
0,320 640,990 990,1185 1185,1350 1350,1545
the pipeline a lot,| because
|因为现在我们不仅仅是挑选随机的盒子，

1640
00:47:00,155 --> 00:47:00,875
0,120 120,270 270,360 360,510 510,720
now we're not just picking

1641
00:47:00,875 --> 00:47:02,290
0,300 300,665 685,990 990,1155 1155,1415
{random,boxes -},| maybe we use
|也许我们使用一些简单的启发式方法来确定图像的有趣部分可能在哪里，

1642
00:47:02,310 --> 00:47:04,145
0,335 335,605 605,1190 1190,1480 1530,1835
some simple heuristic to identify

1643
00:47:04,145 --> 00:47:05,540
0,305 745,1035 1035,1215 1215,1335 1335,1395
where interesting parts of the

1644
00:47:05,540 --> 00:47:07,220
0,180 180,420 420,710 1090,1410 1410,1680
image might be,| but still
|但这实际上仍然非常慢，在实践中，

1645
00:47:07,220 --> 00:47:08,840
0,210 210,470 670,945 945,1220 1240,1620
this is {actually,very -} slow,

1646
00:47:08,840 --> 00:47:10,310
0,285 285,590 850,1140 1140,1320 1320,1470
in practice,| we have to
|我们必须独立地为模型提供每个区域的数据，

1647
00:47:10,310 --> 00:47:12,350
0,150 150,345 345,630 630,1010 1390,2040
feed in each region independently

1648
00:47:12,350 --> 00:47:13,850
0,135 135,240 240,500 910,1230 1230,1500
to the model,| and plus
|而且它非常脆弱，

1649
00:47:13,850 --> 00:47:15,430
0,330 330,525 525,945 945,1230 1230,1580
it's very brittle,| because ultimately
|因为最终模型中查看潜在对象可能所在位置的部分

1650
00:47:15,810 --> 00:47:16,955
0,320 320,560 560,755 755,890 890,1145
the part of the model

1651
00:47:16,955 --> 00:47:19,180
0,270 270,545 1075,1425 1425,1775 1825,2225
that is looking at where

1652
00:47:19,710 --> 00:47:21,950
0,400 990,1310 1310,1550 1550,1870 1920,2240
potential objects might be| is
|与检测这些对象的部分是分开的，

1653
00:47:21,950 --> 00:47:23,705
0,740 820,1110 1110,1275 1275,1455 1455,1755
detached from the part that's

1654
00:47:23,705 --> 00:47:25,390
0,180 180,435 435,995 1075,1380 1380,1685
doing the {detection,of -} those

1655
00:47:25,410 --> 00:47:26,810
0,320 320,815 815,965 965,1160 1160,1400
objects,| ideally, we want one
|理想情况下，我们希望有一个模型，

1656
00:47:26,810 --> 00:47:28,370
0,320 670,945 945,1080 1080,1320 1320,1560
model,| that is able to
|既能找出要注意的地方，又能进行分类。

1657
00:47:28,370 --> 00:47:29,060
0,260 340,585 585,660
both, you know,

1658
00:47:30,000 --> 00:47:31,145
0,305 305,485 485,680 680,905 905,1145
figure out where to attend

1659
00:47:31,145 --> 00:47:33,040
0,335 565,930 930,1170 1170,1350 1350,1895
to and do that classification

1660
00:47:33,330 --> 00:47:34,780
0,400
afterwards.|
|

1661
00:47:35,570 --> 00:47:36,400
0,245 245,335 335,440 440,605 605,830
So there have been many
所以在这个物体检测领域已经提出了很多变种，

1662
00:47:36,400 --> 00:47:37,915
0,530 580,840 840,960 960,1185 1185,1515
variants that have been proposed

1663
00:47:37,915 --> 00:47:39,490
0,225 225,405 405,725 805,1205 1225,1575
in this field of object

1664
00:47:39,490 --> 00:47:40,360
0,390 390,480 480,570 570,705 705,870
detection,| but I want to,
|但我想，为了今天的课程，

1665
00:47:40,360 --> 00:47:41,320
0,195 195,360 360,480 480,690 690,960
just for the purpose of

1666
00:47:41,320 --> 00:47:43,135
0,390 390,680 1090,1395 1395,1620 1620,1815
today's class,| introduce you to
|向你们介绍最流行的其中之一。

1667
00:47:43,135 --> 00:47:44,040
0,135 135,270 270,405 405,585 585,905
one of the most popular

1668
00:47:44,960 --> 00:47:46,420
0,400
ones.|
|

1669
00:47:46,570 --> 00:47:48,050
0,400 420,695 695,905 905,1160 1160,1480
Now, this is a point
这是一个被称为 R-CNN 或更快的 R-CNN 的模型，

1670
00:47:48,550 --> 00:47:49,455
0,275 275,410 410,530 530,650 650,905
where this is a model

1671
00:47:49,455 --> 00:47:51,105
0,395 475,795 795,1200 1200,1380 1380,1650
called {R-CNN -} or faster

1672
00:47:51,105 --> 00:47:53,535
0,285 285,815 1285,1685 1885,2190 2190,2430
{R-CNN -},| which actually attempts
|它试图不仅学习如何对这些盒子进行分类，

1673
00:47:53,535 --> 00:47:55,860
0,240 240,545 1015,1320 1320,1625 2035,2325
to learn not only how

1674
00:47:55,860 --> 00:47:58,250
0,165 165,675 675,980 1120,1520 1990,2390
to classify these boxes,| but
|而且学习如何提出这些盒子可能在哪里，

1675
00:47:58,690 --> 00:48:00,800
0,455 455,635 635,830 830,1330 1710,2110
learns how to propose those

1676
00:48:00,850 --> 00:48:02,220
0,335 335,560 560,850 930,1235 1235,1370
where those boxes might be

1677
00:48:02,220 --> 00:48:02,895
0,60 60,150 150,330 330,540 540,675
in the first place,| so
|这样你就可以学习如何提供或在哪里提供到下游的神经网络。

1678
00:48:02,895 --> 00:48:04,470
0,180 180,405 405,645 645,995 1255,1575
that you could learn how

1679
00:48:04,470 --> 00:48:05,895
0,210 210,480 480,825 825,1170 1170,1425
to feed or where to

1680
00:48:05,895 --> 00:48:08,175
0,210 210,540 540,935 1435,2025 2025,2280
feed into the downstream neural

1681
00:48:08,175 --> 00:48:09,160
0,275
network.|
|

1682
00:48:09,160 --> 00:48:10,225
0,315 315,540 540,735 735,915 915,1065
Now this means that we
这意味着我们可以将图像输入到所谓的区域提案网络，

1683
00:48:10,225 --> 00:48:11,130
0,180 180,360 360,510 510,645 645,905
can feed in the image

1684
00:48:11,390 --> 00:48:12,480
0,260 260,380 380,530 530,755 755,1090
to what are called these

1685
00:48:12,560 --> 00:48:14,755
0,400 570,970 990,1390 1740,2015 2015,2195
region proposal networks,| {the,goal -}
|这些网络的目标是提出图像中你应该注意的某些区域，

1686
00:48:14,755 --> 00:48:15,865
0,180 180,345 345,635 655,945 945,1110
of these networks is to

1687
00:48:15,865 --> 00:48:17,680
0,425 745,1095 1095,1445 1465,1725 1725,1815
propose certain regions in the

1688
00:48:17,680 --> 00:48:19,015
0,230 280,615 615,855 855,1080 1080,1335
image that you should attend

1689
00:48:19,015 --> 00:48:20,290
0,225 225,375 375,635 685,1020 1020,1275
to,| and then feed just
|然后将这些区域仅提供给下游 CNN ，

1690
00:48:20,290 --> 00:48:22,710
0,255 255,590 730,1130 1600,1860 1860,2420
those regions into the downstream

1691
00:48:22,730 --> 00:48:25,140
0,580 1200,1595 1595,1865 1865,2075 2075,2410
CNNs,| so the goal here
|因此这里的目标是，

1692
00:48:25,160 --> 00:48:27,265
0,380 380,725 725,1090 1620,1925 1925,2105
is| to directly try to
|直接尝试学习或提取所有这些关键区域，

1693
00:48:27,265 --> 00:48:28,870
0,195 195,515 535,935 1105,1410 1410,1605
learn or extract all of

1694
00:48:28,870 --> 00:48:31,530
0,290 610,1010 1060,1460 2020,2340 2340,2660
those key regions| and process
|并通过模型的后面部分对其进行处理，

1695
00:48:31,550 --> 00:48:32,770
0,400 420,695 695,815 815,1010 1010,1220
them through the later part

1696
00:48:32,770 --> 00:48:33,715
0,120 120,225 225,465 465,765 765,945
of the model,| each of
|这些区域中的每一个都使用它们自己的独立特征提取器来处理，

1697
00:48:33,715 --> 00:48:35,425
0,180 180,435 435,690 690,1325 1435,1710
these regions are processed with

1698
00:48:35,425 --> 00:48:37,710
0,165 165,455 595,995 1195,1595 1675,2285
their own independent feature extractors,|
|

1699
00:48:38,410 --> 00:48:39,660
0,260 260,365 365,500 500,1055 1055,1250
and then a classifier can
然后，可以使用分类器将它们全部聚集在一起，

1700
00:48:39,660 --> 00:48:41,055
0,135 135,410 460,765 765,1260 1260,1395
be used to aggregate them

1701
00:48:41,055 --> 00:48:43,425
0,165 165,465 465,845 985,1385 1825,2370
all| and perform feature detection
|并执行特征检测和目标检测。

1702
00:48:43,425 --> 00:48:44,990
0,225 225,420 420,675 675,990 990,1565
as well as object detection.|
|

1703
00:48:45,280 --> 00:48:46,845
0,400 510,890 890,1145 1145,1340 1340,1565
{Now\,,the -} beautiful thing about
好的是，它只需要在网络中通过一次，

1704
00:48:46,845 --> 00:48:48,390
0,305 385,660 660,870 870,1185 1185,1545
this is that this requires

1705
00:48:48,390 --> 00:48:50,145
0,285 285,525 525,840 840,1220 1480,1755
only a single pass through

1706
00:48:50,145 --> 00:48:52,005
0,120 120,365 385,780 780,1230 1230,1860
the network,| so it's extraordinarily
|所以它非常快，

1707
00:48:52,005 --> 00:48:53,480
0,390 390,660 660,825 825,1095 1095,1475
fast,| it can easily run
|可以很容易地实时运行，

1708
00:48:53,710 --> 00:48:55,395
0,305 305,545 545,880 930,1330 1350,1685
in real time,| and is
|并且在许多行业应用中也非常常用，

1709
00:48:55,395 --> 00:48:56,840
0,270 270,600 600,945 945,1170 1170,1445
very commonly used in many

1710
00:48:57,370 --> 00:48:58,920
0,365 365,710 710,980 980,1265 1265,1550
industry applications as well,| even
|它甚至可以在你的智能手机上运行。

1711
00:48:58,920 --> 00:48:59,730
0,120 120,225 225,360 360,555 555,810
if it can even run

1712
00:48:59,730 --> 00:49:01,640
0,195 195,390 390,920
on your smartphone.|
|

1713
00:49:02,220 --> 00:49:04,160
0,335 335,560 560,1090 1470,1745 1745,1940
So in classification,| we just
所以在分类中，|我们只是看到了我们如何预测，

1714
00:49:04,160 --> 00:49:05,230
0,255 255,450 450,570 570,750 750,1070
saw how we could predict,|
|

1715
00:49:05,880 --> 00:49:07,100
0,245 245,455 455,695 695,950 950,1220
you know, not only a
不仅仅是一张图像，每张图像只有一个物体，

1716
00:49:07,100 --> 00:49:10,010
0,290 340,740 2080,2340 2340,2600 2620,2910
single image, a single object

1717
00:49:10,010 --> 00:49:11,270
0,165 165,440 550,840 840,1020 1020,1260
per image,| {we,saw -} an
|我们看到一个物体检测，

1718
00:49:11,270 --> 00:49:14,560
0,285 285,830 1210,1610 2320,2895 2895,3290
object detection| potentially inferring multiple
|可能会在你的图像中用边界框推断多个物体，

1719
00:49:14,850 --> 00:49:17,090
0,400 510,800 800,1205 1205,1480 1980,2240
objects with bounding boxes in

1720
00:49:17,090 --> 00:49:19,205
0,135 135,410 1120,1575 1575,1875 1875,2115
your image,| there's also one
|还有一种类型的任务，

1721
00:49:19,205 --> 00:49:20,330
0,240 240,465 465,660 660,915 915,1125
more type of task,| which
|我想指出，

1722
00:49:20,330 --> 00:49:21,110
0,120 120,255 255,405 405,585 585,780
I want to point out,|
|

1723
00:49:21,110 --> 00:49:23,645
0,150 150,300 300,590 640,1370 1840,2535
which is called segmentation.| Segmentation
它叫做分割。|分割是分类的任务，

1724
00:49:23,645 --> 00:49:25,210
0,255 255,420 420,630 630,915 915,1565
is the task of classification,|
|

1725
00:49:25,680 --> 00:49:26,920
0,305 305,560 560,800 800,965 965,1240
but now done at every
但现在是在每一个单独的像素上做的，

1726
00:49:27,000 --> 00:49:28,565
0,395 395,890 890,1070 1070,1295 1295,1565
single pixel,| this takes the
|这使用了目标检测的思想，把方框的边界限制到了极致，

1727
00:49:28,565 --> 00:49:30,215
0,225 225,495 495,840 840,1365 1365,1650
idea of object detection, which

1728
00:49:30,215 --> 00:49:32,350
0,450 450,725 1285,1545 1545,1770 1770,2135
bounding boxes to {the,extreme -},|
|

1729
00:49:32,790 --> 00:49:34,270
0,400 510,755 755,890 890,1130 1130,1480
now, instead of drawing boxes,|
现在，我们不再画盒子，|

1730
00:49:34,470 --> 00:49:35,495
0,290 290,395 395,575 575,785 785,1025
we're not even going to
我们甚至不会考虑盒子，

1731
00:49:35,495 --> 00:49:36,800
0,255 255,575 685,975 975,1110 1110,1305
consider boxes,| we're going to
|我们将学习如何对这张图像中的每一个像素进行分类，

1732
00:49:36,800 --> 00:49:38,710
0,290 580,870 870,1035 1035,1575 1575,1910
learn how to classify every

1733
00:49:38,820 --> 00:49:40,480
0,395 395,980 980,1205 1205,1385 1385,1660
single pixel in this image

1734
00:49:41,720 --> 00:49:43,255
0,275 275,850 960,1265 1265,1400 1400,1535
in isolation, right,| so it's
|所以我们要做的是大量的分类，

1735
00:49:43,255 --> 00:49:44,820
0,120 120,375 375,645 645,870 870,1565
a huge number of classifications

1736
00:49:44,960 --> 00:49:45,960
0,275 275,440 440,575 575,740 740,1000
that we're going to do|
|

1737
00:49:46,160 --> 00:49:48,220
0,260 260,485 485,665 665,970
and we'll do this.|
我们会这样做。|

1738
00:49:48,290 --> 00:49:49,225
0,320 320,530 530,665 665,785 785,935
Well, first let me show
好的，首先让我展示一下这个例子，

1739
00:49:49,225 --> 00:49:50,560
0,195 195,510 510,900 900,1185 1185,1335
this example,| {so,on -} the
|所以在左手边，这是什么样子，

1740
00:49:50,560 --> 00:49:51,520
0,150 150,345 345,585 585,795 795,960
left hand side what this

1741
00:49:51,520 --> 00:49:52,585
0,195 195,420 420,630 630,855 855,1065
looks like,| as you're feeding
|当你输入原始 RGB 图像时，

1742
00:49:52,585 --> 00:49:54,090
0,120 120,360 360,705 705,1260 1260,1505
in an original RGB image,|
|

1743
00:49:54,230 --> 00:49:55,345
0,290 290,515 515,770 770,950 950,1115
the goal of the right
右手边的目标是学习，

1744
00:49:55,345 --> 00:49:56,845
0,240 240,575 775,1095 1095,1275 1275,1500
hand side is to learn,|
|

1745
00:49:56,845 --> 00:49:58,390
0,240 240,515 535,1140 1140,1395 1395,1545
for every pixel in the
对于左手边的每个像素，

1746
00:49:58,390 --> 00:49:59,995
0,180 180,420 420,740 1120,1410 1410,1605
left hand side,| what was
|那个像素是什么种类的，

1747
00:49:59,995 --> 00:50:01,585
0,195 195,485 595,870 870,1050 1050,1590
the class of that pixel,

1748
00:50:01,585 --> 00:50:02,680
0,390 390,630 630,735 735,915 915,1095
right,| so this is kind
|所以，这与仅仅确定我们的形象上的框框形成了某种对比，

1749
00:50:02,680 --> 00:50:04,170
0,105 105,255 255,560 640,1040 1090,1490
of in contrast to just

1750
00:50:04,250 --> 00:50:05,770
0,530 530,725 725,860 860,1150 1200,1520
determining, you know, boxes over

1751
00:50:05,770 --> 00:50:06,790
0,180 180,440 460,735 735,900 900,1020
{our,image -},| now we're looking
|现在我们孤立地看每一个像素，

1752
00:50:06,790 --> 00:50:08,280
0,180 180,450 450,855 855,945 945,1490
at every pixel in isolation|
|

1753
00:50:08,870 --> 00:50:09,820
0,260 260,365 365,515 515,740 740,950
and you can see for
你可以看到，例如，

1754
00:50:09,820 --> 00:50:12,085
0,290 640,900 900,1125 1125,1490 1660,2265
example,| you know this pixels
|你知道牛的像素与天空或草地的像素明显不同，

1755
00:50:12,085 --> 00:50:14,185
0,395 1045,1335 1335,1560 1560,1815 1815,2100
of the cow are clearly

1756
00:50:14,185 --> 00:50:15,805
0,645 645,870 870,1005 1005,1395 1395,1620
differentiated from the pixels of

1757
00:50:15,805 --> 00:50:16,915
0,195 195,420 420,645 645,795 795,1110
the sky or the pixels

1758
00:50:16,915 --> 00:50:18,850
0,195 195,515 805,1205 1465,1770 1770,1935
of the grass, right,| and
|这是语义分割网络的关键组成部分，

1759
00:50:18,850 --> 00:50:21,480
0,210 210,440 1360,1740 1740,2120 2230,2630
that's a key critical component

1760
00:50:21,770 --> 00:50:24,670
0,290 290,710 710,1270 1410,1810 2520,2900
of {semantic,segmentation -} networks,| the
|这里的输出是通过再次使用这些卷积操作，

1761
00:50:24,670 --> 00:50:26,340
0,270 270,435 435,645 645,980 1270,1670
output here is created by

1762
00:50:27,110 --> 00:50:29,160
0,380 380,725 725,1010 1010,1700 1700,2050
again using these convolutional operations,|
|

1763
00:50:29,840 --> 00:50:32,095
0,380 380,695 695,1180 1320,1720 1890,2255
followed by pooling operations,| which
然后是池化操作来创建的，|这些操作学习了一个编码器，你可以在左侧想到它，

1764
00:50:32,095 --> 00:50:33,865
0,365 475,780 780,1470 1470,1650 1650,1770
learn an encoder which you

1765
00:50:33,865 --> 00:50:34,660
0,135 135,300 300,450 450,600 600,795
can think of on {the,left

1766
00:50:34,660 --> 00:50:36,190
0,255 255,495 495,800 1090,1350 1350,1530
-} hand side,| these are
|这些是从我们的 RGB 图像中学习特征，

1767
00:50:36,190 --> 00:50:37,540
0,300 300,555 555,830 940,1215 1215,1350
learning the features from our

1768
00:50:37,540 --> 00:50:39,925
0,420 420,650 1480,1880 1900,2205 2205,2385
RGB image,| learning how to
|学习如何将它们放到一个空间中，

1769
00:50:39,925 --> 00:50:41,040
0,150 150,300 300,510 510,780 780,1115
put them into a space,|
|

1770
00:50:41,300 --> 00:50:42,930
0,260 260,425 425,620 620,830 830,1630
so that it can reconstruct
以便它可以重建到一个新的语义标签空间中，

1771
00:50:43,070 --> 00:50:44,500
0,335 335,590 590,860 860,1160 1160,1430
into a {new,space -} of

1772
00:50:44,500 --> 00:50:46,150
0,465 465,950 1150,1395 1395,1500 1500,1650
semantic labels,| so you can
|所以你可以想象一种缩小的规模，

1773
00:50:46,150 --> 00:50:47,380
0,270 270,510 510,630 630,870 870,1230
imagine kind of a down

1774
00:50:47,380 --> 00:50:48,620
0,495 495,675 675,870 870,1190
scaling| and then progress
|然后是进步扩展到语义空间，

1775
00:50:48,620 --> 00:50:50,855
0,150 150,920 1090,1470 1470,1725 1725,2235
of upsscaling into the semantic

1776
00:50:50,855 --> 00:50:52,580
0,335 685,1085 1135,1410 1410,1575 1575,1725
space,| {but,when -} you do
|但当你进行扩展时，

1777
00:50:52,580 --> 00:50:54,080
0,135 135,825 825,1080 1080,1290 1290,1500
that upscaling,| it's important, of
|这一点很重要，当然，你不能抹杀那些信息，

1778
00:50:54,080 --> 00:50:55,480
0,290 370,660 660,915 915,1080 1080,1400
course, you can't be pulling

1779
00:50:55,650 --> 00:50:56,945
0,320 320,620 620,935 935,1145 1145,1295
down {that,information -},| you need
|你需要在某种程度上颠倒所有这些操作，

1780
00:50:56,945 --> 00:50:57,935
0,120 120,225 225,375 375,795 795,990
to kind of invert all

1781
00:50:57,935 --> 00:50:59,840
0,135 135,375 375,755 1345,1695 1695,1905
{of,those -} operations,| so instead
|所以现在不是用池化来做卷积，

1782
00:50:59,840 --> 00:51:01,810
0,135 135,360 360,1010 1180,1500 1500,1970
of doing convolutions with pooling,|
|

1783
00:51:02,010 --> 00:51:03,250
0,275 275,410 410,560 560,725 725,1240
you can now do convutions
你现在可以用反向的池化或扩展来做卷积，

1784
00:51:03,540 --> 00:51:05,885
0,400 690,1090 1440,1760 1760,2075 2075,2345
with basically {reverse,pooling -} or

1785
00:51:05,885 --> 00:51:08,585
0,665 1165,1425 1425,1685 1765,2165 2395,2700
expansions,| you can grow your
|你可以在每个标签上扩展您的特征集。

1786
00:51:08,585 --> 00:51:10,060
0,285 285,555 555,720 720,930 930,1475
feature sets at every labels.|
|

1787
00:51:11,010 --> 00:51:12,995
0,400 810,1175 1175,1340 1340,1660 1710,1985
And here's an example on
这是一个代码片段的例子，它定义了这些层，

1788
00:51:12,995 --> 00:51:14,135
0,120 120,315 315,665 685,975 975,1140
the bottom of just a

1789
00:51:14,135 --> 00:51:15,650
0,180 180,435 435,785 865,1125 1125,1515
code piece that actually defines

1790
00:51:15,650 --> 00:51:16,940
0,300 300,720 720,945 945,1110 1110,1290
these layers,| {you,can -} plug
|你可以插入这些层，将它们与卷积层结合，

1791
00:51:16,940 --> 00:51:18,550
0,195 195,600 600,1050 1050,1290 1290,1610
these layers, combine them with

1792
00:51:18,660 --> 00:51:20,420
0,710 710,1180 1260,1520 1520,1625 1625,1760
convolutional layers| and you can
|你就可以建立这些完全卷积的网络来完成这种类型的任务。

1793
00:51:20,420 --> 00:51:22,240
0,210 210,510 510,840 840,1530 1530,1820
build these fully convolutional networks

1794
00:51:22,320 --> 00:51:23,570
0,275 275,455 455,760 780,1070 1070,1250
that can accomplish this type

1795
00:51:23,570 --> 00:51:25,085
0,180 180,470 880,1140 1140,1275 1275,1515
of task.| Now of course,
|当然，这也可以应用于医疗保健中的许多其他应用，

1796
00:51:25,085 --> 00:51:26,045
0,255 255,405 405,570 570,780 780,960
this can be applied in

1797
00:51:26,045 --> 00:51:27,515
0,165 165,455 655,1035 1035,1275 1275,1470
many other applications in health

1798
00:51:27,515 --> 00:51:29,030
0,180 180,300 300,525 525,875 1225,1515
care as well,| especially for
|特别是分割出癌症区域，

1799
00:51:29,030 --> 00:51:30,785
0,525 525,795 795,1125 1125,1290 1290,1755
segmenting out, let's say cancerous

1800
00:51:30,785 --> 00:51:32,945
0,275 805,1065 1065,1325 1405,1935 1935,2160
regions| or even identifying parts
|甚至识别感染疟疾的血液部分。

1801
00:51:32,945 --> 00:51:33,920
0,150 150,285 285,540 540,780 780,975
of the blood which are

1802
00:51:33,920 --> 00:51:35,710
0,420 420,660 660,1245 1245,1485 1485,1790
infected {with,malaria -}, for example.|
|

1803
00:51:36,330 --> 00:51:37,970
0,275 275,440 440,730 750,1150 1290,1640
And one final example here
最后一个例子是自动驾驶汽车，

1804
00:51:37,970 --> 00:51:40,115
0,350 400,705 705,990 990,1370 1810,2145
of self driving cars,| let's
|让我们假设我们想要建立一个用于自动导航的神经网络，

1805
00:51:40,115 --> 00:51:40,850
0,105 105,225 225,360 360,555 555,735
say that we want to

1806
00:51:40,850 --> 00:51:42,020
0,195 195,390 390,600 600,855 855,1170
build a neural network for

1807
00:51:42,020 --> 00:51:44,540
0,555 555,1100 1390,1790 1960,2295 2295,2520
autonomous navigation,| specifically building a
|特别是建立一个模型，

1808
00:51:44,540 --> 00:51:46,055
0,290 670,1005 1005,1155 1155,1350 1350,1515
model,| let's say that can
|假设它可以接受图像作为输入，

1809
00:51:46,055 --> 00:51:47,050
0,150 150,375 375,585 585,720 720,995
take as input an image|
|

1810
00:51:47,520 --> 00:51:48,755
0,320 320,515 515,725 725,1025 1025,1235
as well as let's say
以及它认为它所在位置的一些非常粗略的地图，

1811
00:51:48,755 --> 00:51:51,605
0,365 505,905 1315,1895 2035,2435 2545,2850
some very coarse maps of

1812
00:51:51,605 --> 00:51:52,640
0,285 285,540 540,720 720,870 870,1035
where it {thinks,it -} is,|
|

1813
00:51:52,640 --> 00:51:53,590
0,180 180,315 315,480 480,660 660,950
think of this as basically
把这个想象成一个谷歌地图截图，

1814
00:51:53,970 --> 00:51:55,490
0,320 320,785 785,1090 1140,1385 1385,1520
a screenshot of, you know,

1815
00:51:55,490 --> 00:51:57,605
0,290 490,825 825,1160 1360,1760 1840,2115
the Google maps essentially| to
|给神经网络，

1816
00:51:57,605 --> 00:51:58,835
0,135 135,360 360,635 655,945 945,1230
the neural network, right,| it's
|这是地图的 GPS 位置。

1817
00:51:58,835 --> 00:52:00,740
0,195 195,735 735,1115 1435,1740 1740,1905
the GPS location of the

1818
00:52:00,740 --> 00:52:01,660
0,260
map.|
|

1819
00:52:01,890 --> 00:52:02,920
0,245 245,335 335,485 485,710 710,1030
And it wants to directly
它不想直接推断场景的分类或语义分类，

1820
00:52:02,970 --> 00:52:05,420
0,365 365,650 650,970 1380,2050 2190,2450
infer not a classification or

1821
00:52:05,420 --> 00:52:06,950
0,135 135,600 600,1070 1090,1365 1365,1530
a semantic classification of the

1822
00:52:06,950 --> 00:52:09,040
0,290 550,825 825,1100 1150,1550 1690,2090
scene,| but now directly infer
|而是现在直接推断如何驾驶和驾驶这辆车进入未来，

1823
00:52:09,390 --> 00:52:11,270
0,290 290,880 1020,1370 1370,1610 1610,1880
the actuation how to drive

1824
00:52:11,270 --> 00:52:13,120
0,315 315,645 645,840 840,1160 1450,1850
and steer this car into

1825
00:52:13,770 --> 00:52:15,335
0,320 320,500 500,760 930,1310 1310,1565
into the future, right,| now
|现在，这是整个控制命令空间的全概率分布，

1826
00:52:15,335 --> 00:52:16,900
0,135 135,270 270,510 510,875 925,1565
this is a full probability

1827
00:52:17,040 --> 00:52:18,920
0,400 540,905 905,1205 1205,1535 1535,1880
distribution over the entire space

1828
00:52:18,920 --> 00:52:20,660
0,330 330,675 675,1220 1330,1590 1590,1740
of control commands, right,| it's
|这是一个非常大的连续概率空间，

1829
00:52:20,660 --> 00:52:22,600
0,120 120,390 390,770 970,1350 1350,1940
a very large continuous probability

1830
00:52:22,620 --> 00:52:24,230
0,400 810,1070 1070,1190 1190,1385 1385,1610
space,| and the question is
|问题是，我们如何建立一个神经网络来学习这种功能，

1831
00:52:24,230 --> 00:52:25,010
0,180 180,330 330,465 465,630 630,780
how can we build a

1832
00:52:25,010 --> 00:52:26,150
0,210 210,470 520,780 780,945 945,1140
neural network to learn this

1833
00:52:26,150 --> 00:52:27,815
0,290 730,1005 1005,1215 1215,1455 1455,1665
function,| and the key point
|我在这里强调的，

1834
00:52:27,815 --> 00:52:28,895
0,150 150,375 375,750 750,900 900,1080
that I'm stressing,| with all
|所有这些不同类型的体系结构的关键点是，

1835
00:52:28,895 --> 00:52:29,975
0,165 165,330 330,585 585,870 870,1080
of these different types of

1836
00:52:29,975 --> 00:52:31,625
0,660 660,965 1105,1365 1365,1500 1500,1650
architectures here is,| that all
|所有这些体系结构使用完全相同的编码器，

1837
00:52:31,625 --> 00:52:33,020
0,150 150,300 300,945 945,1140 1140,1395
of these architectures use the

1838
00:52:33,020 --> 00:52:35,285
0,350 370,770 880,1730 1750,2010 2010,2265
exact {same,encoder -},| we haven't
|从分类到检测再到语义分割，再到这里，我们没有做任何改变，

1839
00:52:35,285 --> 00:52:36,680
0,275 355,675 675,900 900,1155 1155,1395
changed anything when going from

1840
00:52:36,680 --> 00:52:39,095
0,530 670,1005 1005,1580 1720,1995 1995,2415
classification to detection {to,semantic -}

1841
00:52:39,095 --> 00:52:40,420
0,540 540,735 735,900 900,1065 1065,1325
segmentation, and now to here,|
|

1842
00:52:40,710 --> 00:52:41,795
0,290 290,455 455,620 620,815 815,1085
all of them are using
它们都在使用相同的卷积、非线性和池化，

1843
00:52:41,795 --> 00:52:43,460
0,255 255,495 495,845 1015,1380 1380,1665
the same underlying building blocks

1844
00:52:43,460 --> 00:52:46,690
0,225 225,770 1270,2300 2410,2730 2730,3230
of {convolutions\,,nonlinearities -} and pooling,|
|

1845
00:52:46,980 --> 00:52:48,185
0,245 245,410 410,695 695,980 980,1205
the only difference is that,|
唯一的区别是，|

1846
00:52:48,185 --> 00:52:50,050
0,285 285,665 1045,1350 1350,1560 1560,1865
after we perform those feature
在我们执行这些特征提取之后，

1847
00:52:50,070 --> 00:52:51,785
0,670 1020,1295 1295,1415 1415,1535 1535,1715
extractions,| how do we take
|我们如何获取这些特征并学习我们的最终任务，

1848
00:52:51,785 --> 00:52:53,585
0,195 195,485 655,1055 1195,1515 1515,1800
those features and learn our

1849
00:52:53,585 --> 00:52:54,920
0,285 285,600 600,885 885,1080 1080,1335
ultimate tasks,| so for example,
|例如，在概率控制命令的情况下，

1850
00:52:54,920 --> 00:52:56,795
0,195 195,300 300,465 465,770 1000,1875
in the case of probabilistic

1851
00:52:56,795 --> 00:52:58,460
0,315 315,875 1105,1365 1365,1500 1500,1665
control commands,| we would want
|我们想要获取这些学习的特征，

1852
00:52:58,460 --> 00:52:59,770
0,195 195,420 420,660 660,945 945,1310
to take those learned features|
|

1853
00:53:00,540 --> 00:53:02,240
0,400 690,1090 1290,1670
and understand how
并了解如何预测全连续概率分布的参数，

1854
00:53:02,240 --> 00:53:03,530
0,135 135,410 700,960 960,1110 1110,1290
to predict, you know, the

1855
00:53:03,530 --> 00:53:05,800
0,590 1000,1320 1320,1545 1545,1850 1870,2270
parameters of a full continuous

1856
00:53:05,820 --> 00:53:07,670
0,580 690,1090 1320,1580 1580,1700 1700,1850
probability distribution,| like you can
|就像你在右手边看到的，

1857
00:53:07,670 --> 00:53:08,555
0,120 120,240 240,420 420,630 630,885
see on the right hand

1858
00:53:08,555 --> 00:53:10,190
0,335 745,1065 1065,1260 1260,1410 1410,1635
side| as well as the
|以及我们期望的目的地的确定性控制，

1859
00:53:10,190 --> 00:53:12,260
0,690 690,1020 1020,1260 1260,1520 1540,2070
deterministic control of our desired

1860
00:53:12,260 --> 00:53:14,225
0,380 910,1215 1215,1520 1540,1815 1815,1965
destination,| {and,again -} like we
|同样，就像我们在这节课一开始谈到的那样，

1861
00:53:14,225 --> 00:53:14,945
0,195 195,345 345,420 420,525 525,720
talked about at the very

1862
00:53:14,945 --> 00:53:16,385
0,225 225,390 390,570 570,875 1105,1440
beginning of this class,| this
|这个模型直接从图像到方向盘角度，

1863
00:53:16,385 --> 00:53:18,275
0,335 655,960 960,1260 1260,1635 1635,1890
model which goes directly from

1864
00:53:18,275 --> 00:53:19,900
0,275 415,750 750,930 930,1175 1225,1625
images all the way to

1865
00:53:20,130 --> 00:53:21,860
0,395 395,515 515,970 1020,1420 1440,1730
steering wheel angles| essentially of
|本质上是汽车的一个模型，

1866
00:53:21,860 --> 00:53:23,300
0,165 165,440 700,1020 1020,1230 1230,1440
the car is a single

1867
00:53:23,300 --> 00:53:25,055
0,240 240,495 495,740 760,1160 1450,1755
model,| it's learned entirely end
|它完全是从头到尾学到的，

1868
00:53:25,055 --> 00:53:26,410
0,150 150,375 375,630 630,905 955,1355
to end,| we never told
|我们从来没有告诉汽车什么是车道标志或者道路规则，

1869
00:53:26,550 --> 00:53:28,055
0,290 290,575 575,875 875,1180 1230,1505
the car, for example, what

1870
00:53:28,055 --> 00:53:29,890
0,135 135,315 315,705 705,965 1435,1835
a lane marker is or

1871
00:53:29,940 --> 00:53:31,025
0,260 260,410 410,605 605,860 860,1085
you know, the rules of

1872
00:53:31,025 --> 00:53:32,375
0,135 135,395 775,1020 1020,1125 1125,1350
{the,road -},| it was able
|它能够观察大量的人类驾驶数据，

1873
00:53:32,375 --> 00:53:33,845
0,365 445,845 895,1170 1170,1320 1320,1470
to observe a lot of

1874
00:53:33,845 --> 00:53:36,065
0,210 210,510 510,875 1555,1920 1920,2220
human driving data,| extract these
|提取这些模式，这些特征，

1875
00:53:36,065 --> 00:53:37,850
0,335 355,690 690,1025 1255,1560 1560,1785
patterns, these features,| from what
|从优秀的人类司机与糟糕的人类司机的不同之处，

1876
00:53:37,850 --> 00:53:39,160
0,300 300,555 555,735 735,975 975,1310
makes a good human driver

1877
00:53:39,420 --> 00:53:40,595
0,380 380,605 605,725 725,920 920,1175
different from a bad human

1878
00:53:40,595 --> 00:53:42,335
0,335 865,1170 1170,1380 1380,1590 1590,1740
driver| and learn how to
|并学习如何模仿发生的相同类型的动作，

1879
00:53:42,335 --> 00:53:44,015
0,480 480,840 840,1170 1170,1470 1470,1680
imitate those same types of

1880
00:53:44,015 --> 00:53:46,250
0,275 565,840 840,1050 1050,1535 1975,2235
actions that are occurring,| so
|这样，在没有任何人类干预或人类规则的情况下，

1881
00:53:46,250 --> 00:53:48,610
0,260 310,705 705,1100 1840,2100 2100,2360
that without any, you know,

1882
00:53:48,660 --> 00:53:50,650
0,335 335,920 920,1220 1220,1540 1590,1990
human intervention or human rules|
|

1883
00:53:50,850 --> 00:53:52,205
0,290 290,455 455,920 920,1175 1175,1355
that we impose on these
我们加入这个系统的，

1884
00:53:52,205 --> 00:53:54,650
0,305 655,930 930,1110 1110,1415 2125,2445
systems,| they can simply watch
|他们可以简单地观察所有这些数据，

1885
00:53:54,650 --> 00:53:55,625
0,165 165,270 270,420 420,675 675,975
all of this data| and
|完全从零开始学习如何驾驶。

1886
00:53:55,625 --> 00:53:56,915
0,255 255,465 465,630 630,900 900,1290
learn how to drive entirely

1887
00:53:56,915 --> 00:53:58,080
0,345 345,695
from scratch.|
|

1888
00:53:58,120 --> 00:53:59,220
0,230 230,320 320,580 600,875 875,1100
So a human, for example,
所以，一个人可以进入汽车，

1889
00:53:59,220 --> 00:54:00,380
0,285 285,495 495,705 705,900 900,1160
can actually enter the car,|
|

1890
00:54:00,760 --> 00:54:03,590
0,275 275,410 410,770 770,1120 2430,2830
input a desired destination,| and
输入想要的目的地，|这个端到端的 CNN 会启动控制命令，

1891
00:54:03,880 --> 00:54:05,130
0,290 290,500 500,650 650,830 830,1250
this end to end CNN

1892
00:54:05,130 --> 00:54:06,930
0,240 240,585 585,1230 1230,1500 1500,1800
will actually actuate the control

1893
00:54:06,930 --> 00:54:08,505
0,450 450,750 750,1035 1035,1335 1335,1575
commands| to bring them to
|将他们带到他们的目的地。

1894
00:54:08,505 --> 00:54:10,820
0,150 150,455
their destination.|
|

1895
00:54:10,860 --> 00:54:13,150
0,440 440,755 755,1085 1085,1330 1890,2290
I'll conclude today's lecture,| with
我将结束今天的演讲，|通过所说的几个 CNN 应用，

1896
00:54:13,230 --> 00:54:15,070
0,320 320,640 690,1010 1010,1330 1440,1840
just saying that the applications

1897
00:54:15,690 --> 00:54:17,525
0,400 420,1030 1170,1520 1520,1685 1685,1835
of CNNs,| we've touched on
|我们今天已经谈到了其中的几个，

1898
00:54:17,525 --> 00:54:18,410
0,135 135,285 285,420 420,600 600,885
a few of them today,|
|

1899
00:54:18,410 --> 00:54:19,940
0,255 255,530 610,960 960,1200 1200,1530
but the applications of CNNs
但是 CNN 的应用是巨大的，

1900
00:54:19,940 --> 00:54:22,370
0,290 520,920 1450,1830 1830,2145 2145,2430
are enormous,| far beyond these
|远远超出了我今天提供的这些例子，

1901
00:54:22,370 --> 00:54:23,890
0,350 370,630 630,840 840,1155 1155,1520
examples that I provided today,|
|

1902
00:54:24,210 --> 00:54:25,685
0,275 275,530 530,830 830,1150 1200,1475
{they,all -} tie back to
它们都与特征提取和检测这一核心概念联系在一起，

1903
00:54:25,685 --> 00:54:27,370
0,240 240,555 555,905 1105,1395 1395,1685
this core concept of feature

1904
00:54:27,480 --> 00:54:30,185
0,700 840,1220 1220,1840 2130,2450 2450,2705
extraction and detection| and after
|在进行特征提取之后，

1905
00:54:30,185 --> 00:54:31,460
0,210 210,345 345,495 495,780 780,1275
you do that feature extraction,|
|

1906
00:54:31,460 --> 00:54:32,620
0,135 135,285 285,540 540,900 900,1160
you can really crop off
你可以切断网络的其余部分，

1907
00:54:32,910 --> 00:54:33,970
0,260 260,440 440,620 620,770 770,1060
the rest of your network|
|

1908
00:54:34,140 --> 00:54:35,225
0,350 350,575 575,725 725,875 875,1085
and apply it to many
并将其应用于许多不同的头部，

1909
00:54:35,225 --> 00:54:36,680
0,300 300,660 660,945 945,1170 1170,1455
different heads| for many different
|以执行你可能关心的许多不同的任务和应用，

1910
00:54:36,680 --> 00:54:38,420
0,350 580,980 1000,1380 1380,1620 1620,1740
tasks and applications that you

1911
00:54:38,420 --> 00:54:39,575
0,180 180,435 435,705 705,990 990,1155
might care about,| we've touched
|我们今天已经谈到了一些，

1912
00:54:39,575 --> 00:54:40,570
0,150 150,255 255,405 405,645 645,995
on a few today,| but
|但在不同的领域确实有很多。

1913
00:54:40,620 --> 00:54:42,005
0,245 245,380 380,670 810,1160 1160,1385
there are really so, so

1914
00:54:42,005 --> 00:54:44,240
0,275 445,735 735,990 990,1505 1975,2235
many in different domains.| And
|有了这个总结，

1915
00:54:44,240 --> 00:54:45,640
0,120 120,270 270,600 600,1035 1035,1400
with that I'll conclude| and
|很快我们就会讨论生成建模，

1916
00:54:46,050 --> 00:54:49,280
0,400 420,820 840,1360 2610,2975 2975,3230
very shortly we'll just be

1917
00:54:49,280 --> 00:54:52,505
0,290 340,740 1900,2445 2445,2955 2955,3225
talking about generative modeling,| which
|这是今天和本周系列讲座的核心，

1918
00:54:52,505 --> 00:54:54,860
0,275 355,645 645,900 900,1265 2005,2355
is a really central, really

1919
00:54:54,860 --> 00:54:56,960
0,345 345,705 705,1020 1020,1640 1810,2100
central part of today's and

1920
00:54:56,960 --> 00:54:59,860
0,195 195,555 555,1100 1450,1850 2500,2900
this week's lectures series,| and
|在那之后，稍后我们将有一个软件实验，

1921
00:55:00,090 --> 00:55:01,820
0,350 350,700 960,1295 1295,1520 1520,1730
after that, later on we'll

1922
00:55:01,820 --> 00:55:03,200
0,210 210,465 465,705 705,1070 1090,1380
have the software lab,| which
|我很高兴你们所有人都能开始参与。

1923
00:55:03,200 --> 00:55:04,070
0,270 270,480 480,645 645,750 750,870
I'm excited for all of

1924
00:55:04,070 --> 00:55:05,840
0,180 180,500 760,1035 1035,1200 1200,1770
you to, to start participating

1925
00:55:05,840 --> 00:55:07,340
0,320 430,825 825,1170 1170,1380 1380,1500
in.| And yeah, we can
|是的，我们可以休息五分钟，

1926
00:55:07,340 --> 00:55:08,315
0,120 120,270 270,480 480,720 720,975
take a short five minute

1927
00:55:08,315 --> 00:55:10,445
0,335 715,1115 1315,1635 1635,1800 1800,2130
break| and continue the lectures
|然后从那里继续讲课。

1928
00:55:10,445 --> 00:55:11,200
0,135 135,395
from there.|
|

1929
00:55:11,200 --> 00:55:12,300
0,120 120,410
Thank you.
谢谢。
