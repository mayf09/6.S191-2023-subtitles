1
00:00:09,220 --> 00:00:10,430
Good afternoon, everyone,

2
00:00:10,600 --> 00:00:11,910
thank you all for joining today,

3
00:00:11,910 --> 00:00:13,610
my name is Alexander Amini

4
00:00:14,200 --> 00:00:17,090
and I'll be one of your course organizers this year

5
00:00:17,140 --> 00:00:18,260
along with Ava

6
00:00:18,640 --> 00:00:21,260
and together we're super excited to

7
00:00:21,310 --> 00:00:24,260
introduce you all to Introduction to Deep Learning,

8
00:00:24,370 --> 00:00:26,925
now MIT Introduction to Deep Learning is

9
00:00:26,925 --> 00:00:31,680
a really, really fun, exciting and fast paced program here at MIT

10
00:00:31,680 --> 00:00:34,460
and let me start by just first of all, giving you a little bit of background

11
00:00:34,900 --> 00:00:38,240
into what we do and what you're going to learn about this year.

12
00:00:38,710 --> 00:00:41,445
So this week of Introduction to Deep Learning,

13
00:00:41,445 --> 00:00:44,420
we're going to cover a ton of material in just one week,

14
00:00:44,620 --> 00:00:48,915
you'll learn the foundations of this really, really fascinating and exciting field

15
00:00:48,915 --> 00:00:50,930
of deep learning and artificial intelligence

16
00:00:51,880 --> 00:00:55,590
and more importantly, you're going to get hands on experience,

17
00:00:55,850 --> 00:00:58,470
actually reinforcing what you learn in the lectures

18
00:00:58,670 --> 00:01:00,660
as part of hands on software labs.

19
00:01:01,570 --> 00:01:02,750
Now, over the past decade,

20
00:01:03,100 --> 00:01:06,225
AI and deep learning have really had a huge resurgence

21
00:01:06,225 --> 00:01:08,210
and had incredible successes

22
00:01:08,410 --> 00:01:11,085
and a lot of problems, that even just a decade ago,

23
00:01:11,085 --> 00:01:14,330
we thought were not really even solvable in the near future,

24
00:01:14,470 --> 00:01:17,540
now, we're solving with deep learning with incredible ease.

25
00:01:18,220 --> 00:01:20,910
Now, this past year in particular of 2022

26
00:01:20,910 --> 00:01:23,960
has been an incredible year for deep learning progress,

27
00:01:24,490 --> 00:01:25,335
and I'd like to say,

28
00:01:25,335 --> 00:01:29,910
that actually this past year in particular has been the year of Generative Deep Learning,

29
00:01:29,930 --> 00:01:33,390
using deep learning to generate brand new types of data,

30
00:01:33,410 --> 00:01:34,710
that I've never been seen before

31
00:01:34,880 --> 00:01:37,530
and never existed in reality.

32
00:01:37,610 --> 00:01:39,565
In fact I want to start this class

33
00:01:39,565 --> 00:01:42,390
by actually showing you how we started this class several years ago,

34
00:01:42,860 --> 00:01:45,690
which was by playing this video, that I'll play in a second,

35
00:01:46,190 --> 00:01:50,280
now, this video actually was an introductory video for the class,

36
00:01:50,300 --> 00:01:54,900
it was and kind of exemplifies this idea that I'm talking about.

37
00:01:55,210 --> 00:01:57,740
So let me just stop there and play this video first of all.

38
00:02:01,050 --> 00:02:06,160
Hi, everybody, and welcome to MIT 6.S191,

39
00:02:06,740 --> 00:02:12,420
the official introductory course on deep learning, taught here at MIT,

40
00:02:13,950 --> 00:02:17,710
deep learning is revolutionizing so many fields,

41
00:02:18,300 --> 00:02:22,550
from robotics to medicine and everything in between,

42
00:02:23,760 --> 00:02:26,710
you'll learn the fundamentals of this field

43
00:02:27,030 --> 00:02:31,780
and how you can build some of these incredible algorithms,

44
00:02:32,850 --> 00:02:38,350
in fact, this entire speech and video are not real,

45
00:02:39,080 --> 00:02:43,600
and were created using deep learning and artificial intelligence,

46
00:02:45,310 --> 00:02:47,840
and in this class, you'll learn how,

47
00:02:48,880 --> 00:02:51,240
it has been an honor to speak with you today,

48
00:02:51,800 --> 00:02:52,560
and I hope you enjoy the course.

49
00:02:56,770 --> 00:02:58,400
So in case you couldn't tell,

50
00:02:58,690 --> 00:03:02,760
this video and its entire audio was actually not real,

51
00:03:02,760 --> 00:03:06,120
it was synthetically generated by a deep learning algorithm,

52
00:03:06,120 --> 00:03:09,090
and when we introduced this class a few years ago,

53
00:03:09,090 --> 00:03:11,060
this video was created several years ago,

54
00:03:11,410 --> 00:03:13,250
but even several years ago,

55
00:03:13,900 --> 00:03:17,505
when we introduced this and put it on Youtube, went somewhat viral, right,

56
00:03:17,505 --> 00:03:18,660
people really loved this video,

57
00:03:18,660 --> 00:03:23,630
they were intrigued by how real the video and audio felt and looked

58
00:03:24,670 --> 00:03:27,470
entirely generated by an algorithm by a computer,

59
00:03:27,970 --> 00:03:32,000
and people were shocked with the power and the realism of these types of approaches,

60
00:03:32,020 --> 00:03:33,860
and this was a few years ago.

61
00:03:34,260 --> 00:03:38,030
Now, fast forward to today and the state of deep learning today,

62
00:03:38,350 --> 00:03:45,290
we have, have seen deep learning accelerating at a rate faster than we've ever seen before,

63
00:03:46,030 --> 00:03:51,080
in fact, we can use deep learning now to generate not just images of faces,

64
00:03:51,460 --> 00:03:53,720
but generate full synthetic environments,

65
00:03:53,830 --> 00:03:56,930
where we can train autonomous vehicles entirely in simulation

66
00:03:57,130 --> 00:04:01,040
and deploy them on full scale vehicles in the real world seamlessly,

67
00:04:01,240 --> 00:04:04,410
the videos here you see are actually from a data driven simulator,

68
00:04:04,410 --> 00:04:09,020
from neural networks generated called VISTA, that we actually built here at MIT

69
00:04:09,550 --> 00:04:11,325
and have open source to the public,

70
00:04:11,325 --> 00:04:15,710
so all of you can actually train and build the future of autonomy and self driving cars.

71
00:04:16,210 --> 00:04:18,405
And of course it goes far beyond this as well,

72
00:04:18,405 --> 00:04:23,175
deep learning can be used to generate content directly from how we speak

73
00:04:23,175 --> 00:04:26,930
and the language that we convey to it, from prompts that we say,

74
00:04:27,500 --> 00:04:31,870
deep learning can reason about the prompts in natural language, in English, for example,

75
00:04:32,070 --> 00:04:37,420
and then guide and control what is generated according to what we specify,

76
00:04:38,090 --> 00:04:40,530
we've seen examples of where we can generate,

77
00:04:40,820 --> 00:04:44,380
for example, things that again have never existed in reality,

78
00:04:44,380 --> 00:04:49,230
we can ask a neural network to generate a photo of an astronaut riding a horse

79
00:04:49,760 --> 00:04:54,010
and it actually can imagine hallucinate what this might look like,

80
00:04:54,010 --> 00:04:55,380
even though of course this photo,

81
00:04:55,910 --> 00:04:58,150
not only this photo has never occurred before,

82
00:04:58,150 --> 00:05:01,780
but I don't think any photo of an astronaut riding a horse has ever occurred before,

83
00:05:01,780 --> 00:05:05,050
so there's not really even training data that you could go off in this case,

84
00:05:05,050 --> 00:05:07,060
and my personal favorite is actually,

85
00:05:07,060 --> 00:05:10,860
how we can not only build software that can generate images and videos,

86
00:05:11,030 --> 00:05:14,640
but build software that can generate software as well,

87
00:05:14,660 --> 00:05:17,910
we can also have algorithms that can take language prompts,

88
00:05:18,080 --> 00:05:19,560
for example a prompt like this,

89
00:05:19,850 --> 00:05:23,460
write code in Tensorflow to generate or to train a neural network,

90
00:05:23,870 --> 00:05:27,925
and not only will it write the code and create that neural network,

91
00:05:27,925 --> 00:05:32,170
but it will have the ability to reason about the code that it's generated

92
00:05:32,170 --> 00:05:35,430
and walk you through step by step explaining the process and procedure

93
00:05:35,930 --> 00:05:37,525
all the way from the ground up to you,

94
00:05:37,525 --> 00:05:40,790
so that you can actually learn how to do this process as well.

95
00:05:41,520 --> 00:05:44,200
Now, I think some of these examples

96
00:05:44,220 --> 00:05:47,920
really just highlight how far deep learning and these methods have come,

97
00:05:48,030 --> 00:05:50,285
in the past six years since we started this course,

98
00:05:50,285 --> 00:05:54,430
and you saw that example just a few years ago from that introductory video,

99
00:05:54,450 --> 00:05:56,570
but now we're seeing such incredible advances,

100
00:05:56,570 --> 00:06:00,290
and the most amazing part of this course, in my opinion, is actually

101
00:06:00,290 --> 00:06:05,110
that within this one week, we're going to take you through from the ground up, starting from today,

102
00:06:05,370 --> 00:06:07,720
all of that foundational building blocks,

103
00:06:07,890 --> 00:06:12,430
that will allow you to understand and make all of this amazing advances possible.

104
00:06:13,730 --> 00:06:18,265
So with that, hopefully now you're all super excited about what this class will teach,

105
00:06:18,265 --> 00:06:21,840
and I want to basically now just start by taking a step back

106
00:06:22,280 --> 00:06:24,790
and introducing some of these terminologies,

107
00:06:24,790 --> 00:06:26,785
that I've kind of been throwing around so far,

108
00:06:26,785 --> 00:06:28,650
with deep learning, artificial intelligence,

109
00:06:28,670 --> 00:06:30,060
what do these things actually mean.

110
00:06:30,800 --> 00:06:34,530
So first of all, I want to maybe just take a second to,

111
00:06:35,430 --> 00:06:36,820
speak a little bit about intelligence

112
00:06:36,840 --> 00:06:39,440
and what intelligence means at its core.

113
00:06:39,440 --> 00:06:43,960
So to me, intelligence is simply the ability to process information,

114
00:06:44,460 --> 00:06:49,120
such that we can use it to inform some future decision or action that we take,

115
00:06:49,810 --> 00:06:55,250
now, the field of artificial intelligence is simply the ability for us to build algorithms,

116
00:06:55,480 --> 00:06:57,710
artificial algorithms that can do exactly this,

117
00:06:57,910 --> 00:07:00,860
process information, to inform some future decision.

118
00:07:01,540 --> 00:07:03,975
Now, machine learning is simply a subset of AI,

119
00:07:03,975 --> 00:07:05,745
which focuses specifically on,

120
00:07:05,745 --> 00:07:10,395
how we can build a machine to or teach a machine,

121
00:07:10,395 --> 00:07:14,180
how to do this from some experiences or data, for example.

122
00:07:14,770 --> 00:07:17,120
Now, deep learning goes one step beyond this

123
00:07:17,200 --> 00:07:18,795
and is a subset of machine learning,

124
00:07:18,795 --> 00:07:21,320
which focuses explicitly on what are called neural networks

125
00:07:21,340 --> 00:07:22,670
and how we can build neural networks,

126
00:07:23,020 --> 00:07:24,945
that can extract features in the data,

127
00:07:24,945 --> 00:07:28,490
these are basically what you can think of as patterns, that occur within the data,

128
00:07:28,570 --> 00:07:31,340
so that it can learn to complete these tasks as well.

129
00:07:32,700 --> 00:07:36,730
Now that's exactly what this class is really all about at its core,

130
00:07:36,810 --> 00:07:37,940
we're going to try and teach you

131
00:07:37,940 --> 00:07:39,340
and give you the foundational understanding

132
00:07:39,570 --> 00:07:43,990
and how we can build and teach computers to learn tasks,

133
00:07:44,340 --> 00:07:45,670
many different types of tasks,

134
00:07:46,050 --> 00:07:47,440
directly from raw data,

135
00:07:47,700 --> 00:07:51,640
and that's really what this class boils down to at its most simple form.

136
00:07:52,140 --> 00:07:54,640
And we'll provide a very solid foundation for you

137
00:07:54,780 --> 00:07:57,430
both on the technical side, through the lectures,

138
00:07:57,810 --> 00:08:00,155
which will happen in two parts throughout the class,

139
00:08:00,155 --> 00:08:01,595
the first lecture and the second lecture,

140
00:08:01,595 --> 00:08:03,040
each one about one hour long,

141
00:08:03,500 --> 00:08:05,005
followed by a software lab,

142
00:08:05,005 --> 00:08:06,750
which will immediately follow the lectures,

143
00:08:06,950 --> 00:08:08,110
which will try to reinforce

144
00:08:08,110 --> 00:08:12,000
a lot of what we cover in the in the technical part of the class,

145
00:08:12,500 --> 00:08:15,930
and you know give you hands on experience implementing those ideas.

146
00:08:16,490 --> 00:08:19,470
So this program is split between these two pieces,

147
00:08:19,580 --> 00:08:21,690
the technical lectures and the software labs,

148
00:08:21,860 --> 00:08:23,910
we have several new updates this year in specific,

149
00:08:24,290 --> 00:08:26,460
especially in many of the later lectures.

150
00:08:27,020 --> 00:08:29,590
The first lecture will cover the foundations of deep learning,

151
00:08:29,590 --> 00:08:31,080
which is going to be right now.

152
00:08:32,090 --> 00:08:34,050
And finally, we'll conclude the course

153
00:08:34,070 --> 00:08:38,380
with some very exciting guest lectures from both academia and industry,

154
00:08:38,380 --> 00:08:42,840
who are really leading and driving forward the state of AI and deep learning,

155
00:08:43,130 --> 00:08:45,670
And of course, we have many awesome prizes,

156
00:08:45,670 --> 00:08:48,625
that go with all of the software labs

157
00:08:48,625 --> 00:08:51,120
and the project competition at the end of the course,

158
00:08:51,230 --> 00:08:53,850
so maybe quickly to go through these each day,

159
00:08:53,900 --> 00:08:57,210
like I said, we'll have dedicated software labs that couple with the lectures,

160
00:08:58,290 --> 00:08:59,770
starting today, with lab one,

161
00:08:59,970 --> 00:09:01,820
you'll actually build a neural network,

162
00:09:01,820 --> 00:09:03,890
keeping with the theme of generative AI,

163
00:09:03,890 --> 00:09:04,835
you'll build a neural network,

164
00:09:04,835 --> 00:09:07,480
that can learn, listen to a lot of music,

165
00:09:07,650 --> 00:09:11,410
and actually learn how to generate brand new songs in that genre of music.

166
00:09:12,710 --> 00:09:15,595
At the end, at the next level of the class, on Friday,

167
00:09:15,595 --> 00:09:17,305
we'll host a project pitch competition,

168
00:09:17,305 --> 00:09:20,640
where either you individually or as part of a group

169
00:09:20,900 --> 00:09:23,520
can participate and present an idea,

170
00:09:23,600 --> 00:09:26,970
a novel deep learning idea to all of us,

171
00:09:27,350 --> 00:09:29,370
it will be roughly three minutes in length

172
00:09:29,780 --> 00:09:32,545
and we will focus not as much

173
00:09:32,545 --> 00:09:34,290
because this is a one week program,

174
00:09:34,460 --> 00:09:37,225
we are not going to focus so much on the results of your pitch,

175
00:09:37,225 --> 00:09:41,460
but rather the innovation and the idea and the novelty of what you're trying to propose.

176
00:09:42,080 --> 00:09:44,425
The prizes here are are quite significant already,

177
00:09:44,425 --> 00:09:46,960
where first prize is going to get an NVIDIA GPU,

178
00:09:46,960 --> 00:09:49,080
which is really a key piece of hardware,

179
00:09:49,250 --> 00:09:50,665
that is instrumental,

180
00:09:50,665 --> 00:09:53,005
if you want to actually build a deep learning project

181
00:09:53,005 --> 00:09:54,355
and train these neural networks,

182
00:09:54,355 --> 00:09:56,520
which can be very large and require a lot of compute,

183
00:09:56,870 --> 00:09:59,070
these prizes will give you the compute to do so.

184
00:09:59,510 --> 00:10:02,160
And finally this year we'll be awarding a grand prize

185
00:10:02,240 --> 00:10:03,810
for labs two and three combined,

186
00:10:04,280 --> 00:10:06,180
which will occur on Tuesday and Wednesday,

187
00:10:06,320 --> 00:10:08,035
focused on, what I believe is

188
00:10:08,035 --> 00:10:12,085
actually solving some of the most exciting problems in this field of deep learning

189
00:10:12,085 --> 00:10:16,710
and how, specifically, how we can build models that can be robust,

190
00:10:16,970 --> 00:10:19,800
not only accurate, but robust and trustworthy and safe,

191
00:10:20,150 --> 00:10:21,610
when they're deployed as well,

192
00:10:21,610 --> 00:10:24,480
and you'll actually get experience developing those types of solutions,

193
00:10:25,040 --> 00:10:27,780
that can actually advance the state-of-the-art in AI.

194
00:10:28,600 --> 00:10:31,820
Now, all of these labs that I mentioned and competitions here

195
00:10:32,080 --> 00:10:36,090
are going to be due on Thursday night, at 11pm,

196
00:10:36,090 --> 00:10:37,610
right before the last day of class.

197
00:10:37,990 --> 00:10:40,005
And we'll be helping you all along the way,

198
00:10:40,005 --> 00:10:45,195
this, this prize or this competition in particular has very significant prizes,

199
00:10:45,195 --> 00:10:47,355
so I encourage all of you to really enter this prize

200
00:10:47,355 --> 00:10:51,000
and try to, try to get a chance to win the prize.

201
00:10:52,290 --> 00:10:55,115
And of course, like I said, we're going to be helping you all along the way,

202
00:10:55,115 --> 00:10:59,380
here many available resources throughout this class to help you achieve this,

203
00:10:59,880 --> 00:11:02,110
please post a Piazza, if you have any questions,

204
00:11:02,340 --> 00:11:05,320
and of course this program has an incredible team,

205
00:11:05,640 --> 00:11:07,330
that you can reach out to at any point,

206
00:11:07,440 --> 00:11:10,240
in case you have any issues or questions on the materials,

207
00:11:10,860 --> 00:11:13,450
myself and Ava will be your two main lectures,

208
00:11:13,650 --> 00:11:14,920
for the first part of the class,

209
00:11:15,360 --> 00:11:16,385
we'll also be hearing,

210
00:11:16,385 --> 00:11:18,215
like I said, in the later part of the class,

211
00:11:18,215 --> 00:11:19,390
from some guest lectures,

212
00:11:19,650 --> 00:11:23,380
who will share some really cutting edge state-of-the-art development in deep learning.

213
00:11:23,820 --> 00:11:25,985
And of course I want to give a huge shout out

214
00:11:25,985 --> 00:11:27,400
and thanks to all of our sponsors,

215
00:11:27,660 --> 00:11:28,655
who without their support,

216
00:11:28,655 --> 00:11:32,645
this program wouldn't have been possible for yet again another year,

217
00:11:32,645 --> 00:11:33,460
so thank you all.

218
00:11:35,010 --> 00:11:36,370
Okay, so now with that,

219
00:11:36,570 --> 00:11:39,520
let's really dive into the really fun stuff of today's lecture,

220
00:11:39,570 --> 00:11:42,440
which is, you know, the, the technical part,

221
00:11:42,440 --> 00:11:44,180
and I think, I want to start this part

222
00:11:44,180 --> 00:11:47,770
by asking all of you and having yourselves ask,

223
00:11:48,060 --> 00:11:50,450
you know, having you ask yourselves this question,

224
00:11:50,450 --> 00:11:53,540
of, you know, why are all of you here, first of all,

225
00:11:53,540 --> 00:11:56,170
why do you care about this topic in the first place.

226
00:11:57,400 --> 00:11:59,265
I think to answer this question,

227
00:11:59,265 --> 00:12:00,435
we have to take a step back

228
00:12:00,435 --> 00:12:03,200
and think about, you know, the history of machine learning

229
00:12:03,340 --> 00:12:04,890
and what machine learning is

230
00:12:04,890 --> 00:12:08,330
and what deep learning brings to the table on top of machine learning.

231
00:12:08,920 --> 00:12:11,690
Now, traditional machine learning algorithms typically define

232
00:12:11,800 --> 00:12:14,700
what are called these set of features in the data,

233
00:12:14,700 --> 00:12:16,880
you can think of these as certain patterns in the data

234
00:12:17,200 --> 00:12:19,365
and then usually these features are hand engineered,

235
00:12:19,365 --> 00:12:21,600
so probably a human will come into the data set

236
00:12:21,600 --> 00:12:23,900
and with a lot of domain knowledge and experience

237
00:12:24,310 --> 00:12:27,110
can try to uncover what these features might be,

238
00:12:27,190 --> 00:12:28,755
now the key idea of deep learning,

239
00:12:28,755 --> 00:12:31,725
and this is really central to this class is that,

240
00:12:31,725 --> 00:12:33,920
instead of having a human define these features,

241
00:12:34,060 --> 00:12:37,480
what if we could have a machine look at all of this data

242
00:12:37,560 --> 00:12:41,770
and actually try to extract and uncover what are the core patterns in the data,

243
00:12:41,850 --> 00:12:45,490
so that it can use those, when it sees new data to make some decisions.

244
00:12:45,870 --> 00:12:48,970
So for example, if we wanted to detect faces in an image,

245
00:12:49,500 --> 00:12:52,490
a deep neural network algorithm might actually learn that,

246
00:12:52,490 --> 00:12:53,650
in order to detect a face,

247
00:12:53,700 --> 00:12:57,820
it first has to detect things like edges in the image lines and edges,

248
00:12:58,020 --> 00:12:59,765
and when you combine those lines and edges,

249
00:12:59,765 --> 00:13:04,210
you can actually create compositions of features like corners and curves,

250
00:13:04,440 --> 00:13:06,605
which when you create, when you combine those,

251
00:13:06,605 --> 00:13:08,200
you can create more high level features,

252
00:13:08,250 --> 00:13:10,240
for example eyes and noses and ears,

253
00:13:10,710 --> 00:13:15,020
and then those are the features that allow you to ultimately detect what you care about,

254
00:13:15,020 --> 00:13:16,070
detecting which is the face,

255
00:13:16,070 --> 00:13:20,050
but all of these come from what are called kind of a hierarchical learning of features

256
00:13:20,430 --> 00:13:22,300
and you can actually see some examples of these,

257
00:13:22,620 --> 00:13:24,710
these are real features learned by a neural network

258
00:13:24,710 --> 00:13:27,910
and how they're combined defines this progression of information.

259
00:13:29,360 --> 00:13:30,910
But in fact, what I just described,

260
00:13:30,910 --> 00:13:37,350
this underlying and fundamental building block of neural networks and deep learning have actually existed for decades,

261
00:13:37,910 --> 00:13:41,610
now. why are we studying all of this now at today in this class

262
00:13:41,660 --> 00:13:44,820
with all this great enthusiasm to learn this, right.

263
00:13:44,900 --> 00:13:49,890
Well, for one, there have been several key advances that have occurred in the past decade.

264
00:13:50,450 --> 00:13:56,460
Number one is that data is so much more pervasive than it has ever been before in our lifetimes,

265
00:13:56,840 --> 00:13:59,800
these models are hungry for more data,

266
00:13:59,800 --> 00:14:02,730
and we're living in the age of big data,

267
00:14:03,260 --> 00:14:05,480
more data is available to these models than ever before

268
00:14:05,480 --> 00:14:07,210
and they thrive off of that.

269
00:14:07,560 --> 00:14:11,585
Secondly, these algorithms are massively parallelizable,

270
00:14:11,585 --> 00:14:12,880
they require a lot of compute

271
00:14:13,020 --> 00:14:16,030
and we're also at a unique time in history,

272
00:14:16,230 --> 00:14:21,370
where we have the ability to train these extremely large scale algorithms and techniques,

273
00:14:21,420 --> 00:14:23,015
that have existed for a very long time,

274
00:14:23,015 --> 00:14:23,990
but we can now train them,

275
00:14:23,990 --> 00:14:26,140
due to the hardware advances that have been made.

276
00:14:26,400 --> 00:14:31,745
And finally, due to open source toolbox and software platforms, like TensorFlow for example,

277
00:14:31,745 --> 00:14:34,600
which all of you will get a lot of experience on in this class,

278
00:14:35,250 --> 00:14:39,130
training and building the code for these neural networks has never been easier,

279
00:14:39,480 --> 00:14:41,470
so that from the software point of view as well,

280
00:14:41,520 --> 00:14:43,630
there have been incredible advances to open source,

281
00:14:43,950 --> 00:14:47,650
you know, the underlying fundamentals of what you're going to learn.

282
00:14:48,530 --> 00:14:51,940
So let me start now with just building up from the ground up,

283
00:14:51,940 --> 00:14:55,375
the fundamental building block of every single neural network,

284
00:14:55,375 --> 00:14:56,905
that you're going to learn in this class,

285
00:14:56,905 --> 00:14:59,400
and that's going to be just a single neuron,

286
00:15:00,060 --> 00:15:01,570
and in neural network language,

287
00:15:01,740 --> 00:15:03,970
a single neuron is called a perceptron.

288
00:15:05,230 --> 00:15:06,600
So what is a perceptron,

289
00:15:06,600 --> 00:15:10,220
a perceptron is, like I said, a single neuron,

290
00:15:10,450 --> 00:15:14,220
and it's actually, I'm going to say it's very, very simple idea,

291
00:15:14,220 --> 00:15:17,780
so I want to make sure that everyone in the audience understands exactly what a perceptron is

292
00:15:17,800 --> 00:15:18,650
and how it works.

293
00:15:19,690 --> 00:15:22,110
So let's start by first defining a perceptron

294
00:15:22,110 --> 00:15:25,215
as taking, as input a set of inputs, right,

295
00:15:25,215 --> 00:15:26,340
so on the left hand side,

296
00:15:26,340 --> 00:15:31,220
you can see this perceptron takes m different inputs, 1 to m,

297
00:15:31,770 --> 00:15:33,010
these are the blue circles,

298
00:15:33,330 --> 00:15:35,170
We're denoting these inputs as xs,

299
00:15:36,520 --> 00:15:42,290
each of these numbers, each of these inputs, is then multiplied by a corresponding weight,

300
00:15:42,340 --> 00:15:43,490
which we can call w,

301
00:15:43,750 --> 00:15:46,220
so x1 will be multiplied by w1,

302
00:15:46,780 --> 00:15:50,390
and we'll add the result of all of these multiplications together,

303
00:15:51,130 --> 00:15:54,050
now, we take that single number after the addition,

304
00:15:54,250 --> 00:15:56,505
and we pass it through this nonlinear,

305
00:15:56,505 --> 00:15:58,490
what we call a nonlinear activation function,

306
00:15:58,810 --> 00:16:01,100
and that produces our final output of the perception,

307
00:16:01,120 --> 00:16:02,270
which we can call it y.

308
00:16:03,390 --> 00:16:09,155
Now, this is actually not entirely accurate of the picture of a perceptron,

309
00:16:09,155 --> 00:16:10,960
there's one step that I forgot to mention here,

310
00:16:11,130 --> 00:16:15,935
so in addition to multiplying all of these inputs with their corresponding weights,

311
00:16:15,935 --> 00:16:18,340
we're also now going to add what's called a bias term,

312
00:16:18,510 --> 00:16:21,100
here denoted as this w0,

313
00:16:21,300 --> 00:16:22,690
which is just a scalar weight,

314
00:16:22,710 --> 00:16:25,570
and you can think of it coming with an input of just 1,

315
00:16:25,860 --> 00:16:30,850
so that's going to allow the network to basically shift its nonlinear activation function,

316
00:16:32,310 --> 00:16:35,980
you know, non linearly as it sees its inputs.

317
00:16:36,780 --> 00:16:37,865
Now on the right hand side,

318
00:16:37,865 --> 00:16:41,750
you can see this diagram mathematically formulated, right,

319
00:16:41,750 --> 00:16:42,785
as a single equation,

320
00:16:42,785 --> 00:16:49,805
we can now rewrite this linear, this equation with linear algebra terms of vectors and dot products, right,

321
00:16:49,805 --> 00:16:57,650
so for example, we can define our entire inputs x1 to xm as large vector X, right,

322
00:16:57,650 --> 00:17:05,590
that large vector X can be multiplied by or take a dot, excuse me, matrix multiplied with our weights W,

323
00:17:05,970 --> 00:17:09,640
again, another vector of our weights w1 to wm,

324
00:17:10,320 --> 00:17:11,380
taking their dot product,

325
00:17:12,270 --> 00:17:16,120
not only multiplies them, but it also adds the resulting terms together,

326
00:17:16,560 --> 00:17:18,730
adding a bias, like we said before,

327
00:17:18,930 --> 00:17:20,590
and applying this non-linearity.

328
00:17:22,710 --> 00:17:23,830
Now you might be wondering,

329
00:17:23,910 --> 00:17:25,370
what is this non-linear function,

330
00:17:25,370 --> 00:17:27,010
I've mentioned it a few times already,

331
00:17:27,420 --> 00:17:29,615
well, I said it is a function, right,

332
00:17:29,615 --> 00:17:33,275
that's that we pass the outputs of the neural network through

333
00:17:33,275 --> 00:17:38,050
before we return it, you know, to the next neuron in the, in the pipeline,

334
00:17:38,280 --> 00:17:40,580
so one common example of a non-linear function,

335
00:17:40,580 --> 00:17:42,635
that's very popular in deep neural networks,

336
00:17:42,635 --> 00:17:43,840
it's called the sigmoid function,

337
00:17:44,190 --> 00:17:47,890
you can think of this as kind of a continuous version of a threshold function, right,

338
00:17:47,940 --> 00:17:49,390
it goes from 0 to 1,

339
00:17:49,950 --> 00:17:54,100
and it's having, it can take us, input any real number on the real number line,

340
00:17:54,780 --> 00:17:57,650
and you can see an example of it illustrated on the bottom right hand,

341
00:17:57,970 --> 00:18:01,460
now, in fact, there are many types of non-linear activation functions,

342
00:18:01,540 --> 00:18:03,390
that are popular in deep neural networks

343
00:18:03,390 --> 00:18:04,700
and here are some common ones,

344
00:18:05,080 --> 00:18:06,020
and throughout this presentation,

345
00:18:06,250 --> 00:18:10,110
you'll actually see some examples of these code snippets on the bottom of the slides,

346
00:18:10,110 --> 00:18:14,120
where we'll try and actually tie in some of what you're learning in the lectures

347
00:18:14,440 --> 00:18:17,030
to actual software and how you can implement these pieces,

348
00:18:17,680 --> 00:18:20,100
which will help you a lot for your software labs explicitly,

349
00:18:20,100 --> 00:18:22,605
so the sigmoid activation on the left is very popular,

350
00:18:22,605 --> 00:18:25,800
since it's a function that outputs, you know, between 0 and 1,

351
00:18:25,800 --> 00:18:29,130
so especially when you want to deal with probabilities distributions, for example,

352
00:18:29,130 --> 00:18:32,810
this is very important because probabilities live between 0 and 1,

353
00:18:33,800 --> 00:18:36,010
in modern deep neural networks though, the ReLU function,

354
00:18:36,150 --> 00:18:37,745
which you can see on the far right hand,

355
00:18:37,745 --> 00:18:39,610
is a very popular activation function,

356
00:18:39,660 --> 00:18:40,820
because it's piece wise, linear,

357
00:18:40,820 --> 00:18:42,490
it's extremely efficient to compute,

358
00:18:42,780 --> 00:18:44,650
especially when computing its derivatives,

359
00:18:44,820 --> 00:18:47,140
its derivatives are constants,

360
00:18:47,220 --> 00:18:50,000
except for one non-linear yet 0,

361
00:18:51,600 --> 00:18:55,205
Now I hope actually all of you are probably asking this question to yourself of

362
00:18:55,205 --> 00:18:57,425
why do we even need this nonlinear activation function,

363
00:18:57,425 --> 00:18:59,740
it seems like it kind of just complicates this whole picture,

364
00:18:59,850 --> 00:19:01,720
when we didn't really need it in the first place,

365
00:19:02,250 --> 00:19:04,955
and I want to just spend a moment on answering this,

366
00:19:04,955 --> 00:19:07,720
because the point of a non-linear activation function is

367
00:19:08,070 --> 00:19:12,560
of course, number one is to introduce non-linearities to our data, right,

368
00:19:12,560 --> 00:19:14,020
if we think about our data,

369
00:19:14,850 --> 00:19:16,445
almost all data that we care about,

370
00:19:16,445 --> 00:19:18,550
all real world data is highly non-linear,

371
00:19:19,080 --> 00:19:20,080
now this is important,

372
00:19:20,130 --> 00:19:23,120
because if we want to be able to deal with those types of datasets,

373
00:19:23,120 --> 00:19:24,950
we need models that are also non-linear,

374
00:19:24,950 --> 00:19:27,040
so they can capture those same types of patterns,

375
00:19:27,240 --> 00:19:28,660
so imagine I told you to separate,

376
00:19:28,680 --> 00:19:31,610
for example, I gave you this dataset, red points and green points,

377
00:19:31,610 --> 00:19:34,720
and I ask you to try and separate those two types of data points,

378
00:19:35,340 --> 00:19:36,920
now you might think that this is easy,

379
00:19:36,920 --> 00:19:41,330
but if I could, only if I told you that you could only use a single line to do so,

380
00:19:41,330 --> 00:19:43,100
well, now it becomes a very complicated problem,

381
00:19:43,100 --> 00:19:46,390
in fact, you can't really solve it effectively with a single line,

382
00:19:47,510 --> 00:19:51,840
and in fact, if you introduce non-linear activation functions to your solution,

383
00:19:52,100 --> 00:19:56,400
that's exactly what allows you to, you know, deal with these types of problems,

384
00:19:56,660 --> 00:20:00,540
non-linear activation functions allow you to deal with non-linear types of data.

385
00:20:01,470 --> 00:20:05,800
Now, and that's what exactly makes neural networks so powerful at their core.

386
00:20:06,690 --> 00:20:09,080
So let's understand this maybe with a very simple example,

387
00:20:09,080 --> 00:20:11,830
walking through this diagram of a perceptron one more time,

388
00:20:12,540 --> 00:20:15,515
imagine I give you this trained neural network with weights,

389
00:20:15,515 --> 00:20:19,570
now not w1 w2, I'm going to actually give you numbers at these locations,

390
00:20:19,770 --> 00:20:23,500
so the trained weights w0 will be 1

391
00:20:23,610 --> 00:20:26,470
and W will be a vector of 3 and -2,

392
00:20:27,210 --> 00:20:29,360
so, this neural network has two inputs,

393
00:20:29,360 --> 00:20:32,470
like we said before, it has input x1, it has input x2,

394
00:20:32,850 --> 00:20:34,360
if we want to get the output of it,

395
00:20:34,800 --> 00:20:36,365
this is also the main thing,

396
00:20:36,365 --> 00:20:38,315
I want all of you to take away from this lecture today

397
00:20:38,315 --> 00:20:40,280
is that to get the output of a perceptron

398
00:20:40,280 --> 00:20:41,710
and there are three steps we need to take,

399
00:20:41,880 --> 00:20:46,750
from this stage, we first compute the multiplication of our inputs with our weights,

400
00:20:48,570 --> 00:20:54,305
sorry, yeah, multiply them together, add their result and compute non-linearity,

401
00:20:54,305 --> 00:21:00,130
it's these three steps that define the forward propagation of information through a perceptron.

402
00:21:00,980 --> 00:21:03,760
So let's take a look at how that exactly works, right,

403
00:21:03,760 --> 00:21:07,140
so if we plug in these numbers to to those equations,

404
00:21:07,910 --> 00:21:11,350
we can see that everything inside of our non-linearity,

405
00:21:11,350 --> 00:21:13,435
here the non-linearity is g, right,

406
00:21:13,435 --> 00:21:15,690
that function g which could be a sigmoid,

407
00:21:15,860 --> 00:21:17,220
we saw in previous slide,

408
00:21:17,930 --> 00:21:23,740
that component inside our non-linearity is in fact just a two dimensional line,

409
00:21:23,740 --> 00:21:24,730
it has two inputs

410
00:21:24,730 --> 00:21:29,760
and if we consider the space of all of the possible inputs that this neural network could see,

411
00:21:30,080 --> 00:21:33,100
we can actually plot this on a decision boundary, right,

412
00:21:33,100 --> 00:21:38,675
we can plot this two dimensional line, as as a decision boundary,

413
00:21:38,675 --> 00:21:41,710
as a [plane] separating these two components of our space,

414
00:21:42,390 --> 00:21:45,050
in fact not only is it a single plane,

415
00:21:45,050 --> 00:21:48,910
there's a directionality component depending on which side of the plane that we live on,

416
00:21:49,110 --> 00:21:52,450
if we see an input, for example here, -1 2,

417
00:21:52,830 --> 00:21:55,810
we actually know that it lives on one side of the plane

418
00:21:56,100 --> 00:21:58,235
and it will have a certain type of output,

419
00:21:58,235 --> 00:22:01,775
in this case, that output is going to be positive, right,

420
00:22:01,775 --> 00:22:05,380
because in this case, when we plug those components into our equation,

421
00:22:05,700 --> 00:22:10,440
we'll get a positive number, that passes through the non-linear component,

422
00:22:10,440 --> 00:22:12,530
and that gets propagated through as well,

423
00:22:12,730 --> 00:22:14,630
of course if you're on the other side of the space,

424
00:22:14,980 --> 00:22:17,565
you're going to have the opposite result, right,

425
00:22:17,565 --> 00:22:22,080
and that thresholding function is going to essentially live at this decision boundary,

426
00:22:22,080 --> 00:22:23,960
so depending on which side of the space you live on,

427
00:22:24,160 --> 00:22:30,770
that thresholding function, that sigmoid function, is going to then control how you move to one side or the other.

428
00:22:32,510 --> 00:22:35,640
Now in this particular example, this is very convenient,

429
00:22:35,990 --> 00:22:37,480
because we can actually visualize,

430
00:22:37,480 --> 00:22:41,125
and I can draw this exact full space for you on this slide,

431
00:22:41,125 --> 00:22:42,415
it's only a two dimensional space,

432
00:22:42,415 --> 00:22:44,155
so it's very easy for us to visualize,

433
00:22:44,155 --> 00:22:47,640
but of course, for almost all problems that we care about,

434
00:22:47,870 --> 00:22:50,670
our data points are not going to be two dimensional,

435
00:22:50,750 --> 00:22:51,810
if you think about an image,

436
00:22:52,460 --> 00:22:56,610
the dimensionality of an image is going to be the number of pixels, that you have in the image,

437
00:22:56,750 --> 00:23:01,050
so these are going to be thousands of dimensions, millions of dimensions or even more,

438
00:23:01,580 --> 00:23:06,240
and then drawing these types of plots like you see here is simply not feasible,

439
00:23:06,560 --> 00:23:07,615
so we can't always do this,

440
00:23:07,615 --> 00:23:09,480
but hopefully this gives you some intuition

441
00:23:09,590 --> 00:23:13,680
to understand kind of as we build up into more complex models.

442
00:23:14,930 --> 00:23:17,005
So now that we have an idea of the perceptron,

443
00:23:17,005 --> 00:23:19,525
let's see how we can actually take this single neuron

444
00:23:19,525 --> 00:23:21,720
and start to build it up into something more complicated,

445
00:23:21,860 --> 00:23:22,780
a full neural network,

446
00:23:22,780 --> 00:23:24,150
and build a model from that.

447
00:23:25,060 --> 00:23:28,460
So let's revisit again this previous diagram of the perceptron,

448
00:23:28,840 --> 00:23:31,430
if, again just to reiterate one more time,

449
00:23:31,510 --> 00:23:33,120
this core piece of information,

450
00:23:33,120 --> 00:23:36,315
that I want all of you to take away from this class is

451
00:23:36,315 --> 00:23:37,725
how a perceptron works and

452
00:23:37,725 --> 00:23:40,550
how it propagates information to its decision,

453
00:23:40,810 --> 00:23:41,730
there are three steps,

454
00:23:41,730 --> 00:23:46,490
first is the dot product, second is the bias, and third is the non-linearity,

455
00:23:46,690 --> 00:23:50,330
and you keep repeating this process for every single perceptron in your neural network.

456
00:23:51,450 --> 00:23:53,230
Let's simplify the diagram a little bit,

457
00:23:53,370 --> 00:23:55,090
I'll get rid of the weights

458
00:23:55,590 --> 00:24:01,000
and you can assume that every line here now basically has an associated weight scalar, that's associated with it,

459
00:24:01,230 --> 00:24:05,030
every line also corresponds to the input that's coming in,

460
00:24:05,030 --> 00:24:09,320
it has a weight that's coming in also at the, on the line itself,

461
00:24:09,430 --> 00:24:12,765
and I've also removed the bias just for sake of simplicity,

462
00:24:12,765 --> 00:24:13,550
but it's still there,

463
00:24:14,320 --> 00:24:17,360
so now the result is that z,

464
00:24:17,440 --> 00:24:22,005
which let's call that the result of our dot product plus the bias is going

465
00:24:22,005 --> 00:24:24,290
and that's what we pass into our non-linear function,

466
00:24:24,880 --> 00:24:29,060
that piece is going to be applied to that activation function,

467
00:24:29,140 --> 00:24:32,930
now the final output here is simply going to be g,

468
00:24:33,550 --> 00:24:36,420
which is our activation function of z,

469
00:24:37,100 --> 00:24:40,090
z is going to be basically what you can think of the state of this neuron,

470
00:24:40,090 --> 00:24:42,840
it's the result of that dot product plus bias.

471
00:24:44,060 --> 00:24:48,450
Now, if we want to define and build up a multi layered output neural network,

472
00:24:49,010 --> 00:24:51,160
if we want two outputs to this function, for example,

473
00:24:51,160 --> 00:24:52,410
it's a very simple procedure,

474
00:24:52,490 --> 00:24:55,090
we just have now two neurons, two perceptons,

475
00:24:55,090 --> 00:24:59,310
each perceptron will control the output for its associated piece, right,

476
00:25:00,110 --> 00:25:01,300
so now we have two outputs,

477
00:25:01,300 --> 00:25:02,830
each one is the normal perceptron,

478
00:25:02,830 --> 00:25:04,285
it takes all of the inputs,

479
00:25:04,285 --> 00:25:05,935
so they both take the same inputs,

480
00:25:05,935 --> 00:25:06,810
but amazingly,

481
00:25:07,070 --> 00:25:08,940
now with this mathematical understanding,

482
00:25:09,410 --> 00:25:13,230
we can start to build our first neural network entirely from scratch.

483
00:25:13,490 --> 00:25:15,025
So what does that look like,

484
00:25:15,025 --> 00:25:18,570
so we can start by firstly initializing these two components,

485
00:25:18,590 --> 00:25:22,920
The first component that we saw was the weight matrix, excuse me, the weight vector,

486
00:25:22,940 --> 00:25:25,080
It's a vector of weights in this case,

487
00:25:26,400 --> 00:25:29,860
and the second component is the, the bias vector,

488
00:25:29,910 --> 00:25:34,595
that we're going to multiply with the dot product of all of our inputs by our weights, right,

489
00:25:34,595 --> 00:25:37,445
so, the only remaining step now

490
00:25:37,445 --> 00:25:41,135
after we've defined these parameters of our layer is to

491
00:25:41,135 --> 00:25:44,960
now define, you know, how this forward propagation of information works,

492
00:25:44,960 --> 00:25:49,205
and that's exactly those three main components, that I've been stressing to you,

493
00:25:49,205 --> 00:25:52,160
so we can create this call function to do exactly that,

494
00:25:52,160 --> 00:25:54,400
to define this forward propagation of information,

495
00:25:55,170 --> 00:25:58,300
and the story here is exactly the same as we've been seeing it, right,

496
00:25:58,320 --> 00:26:01,000
matrix, multiply our inputs with our weights,

497
00:26:02,550 --> 00:26:03,400
add a bias,

498
00:26:04,530 --> 00:26:06,530
and then apply a non-linearity

499
00:26:06,530 --> 00:26:07,600
and return the result

500
00:26:07,980 --> 00:26:10,085
and that literally this code will run,

501
00:26:10,085 --> 00:26:13,420
this will define a full, a full neural network layer,

502
00:26:13,770 --> 00:26:16,150
that you can then take like this,

503
00:26:17,040 --> 00:26:19,150
and of course, actually, luckily for all of you,

504
00:26:19,170 --> 00:26:21,040
all of that code, which wasn't much code,

505
00:26:21,090 --> 00:26:23,990
that's been abstracted away by these libraries, like TensorFlow,

506
00:26:23,990 --> 00:26:26,505
you can simply call functions like this,

507
00:26:26,505 --> 00:26:29,840
which will actually, you know, replicate exactly that piece of code,

508
00:26:30,580 --> 00:26:33,015
so you don't need to necessarily copy all of that code down,

509
00:26:33,015 --> 00:26:35,090
you can just call it.

510
00:26:36,250 --> 00:26:38,270
And with that understanding,

511
00:26:38,500 --> 00:26:41,025
you know, we just saw how you could build a single layer,

512
00:26:41,025 --> 00:26:46,730
but of course, now you can actually start to think about how you can stack these layers as well,

513
00:26:47,080 --> 00:26:53,295
so since we now have this transformation essentially from our inputs to a hidden output,

514
00:26:53,295 --> 00:26:54,530
you can think of this as basically

515
00:26:55,300 --> 00:26:58,735
how we can define some way

516
00:26:58,735 --> 00:27:04,110
of transforming those inputs, right, into some new dimensional space, right,

517
00:27:04,550 --> 00:27:06,990
perhaps closer to the value that we want to predict,

518
00:27:07,040 --> 00:27:10,230
and,that -} transformation is going to be eventually learned

519
00:27:10,370 --> 00:27:13,495
to know how to transform those inputs into our desired outputs,

520
00:27:13,495 --> 00:27:14,490
and we'll get to that later,

521
00:27:14,720 --> 00:27:17,725
but for now, the piece that I want to really focus on is,

522
00:27:17,725 --> 00:27:19,620
if we have these more complex neural networks,

523
00:27:20,030 --> 00:27:21,700
I want to really distill down that,

524
00:27:21,700 --> 00:27:24,025
this is nothing more complex than what we've already seen,

525
00:27:24,025 --> 00:27:27,600
if we focus on just one neuron in this diagram,

526
00:27:28,480 --> 00:27:30,590
take here, for example, z2,

527
00:27:31,090 --> 00:27:33,650
z2 is this neuron that's highlighted in the middle layer,

528
00:27:34,480 --> 00:27:38,360
it's just the same perceptron that we've been seeing so far in this class,

529
00:27:38,470 --> 00:27:39,840
it was, its output is

530
00:27:39,840 --> 00:27:45,380
obtained by taking a dot product, adding a bias, and then applying that non-linearity between all of its inputs,

531
00:27:46,280 --> 00:27:47,710
if we look at a different node,

532
00:27:47,710 --> 00:27:49,830
for example z3, which is the one right below it,

533
00:27:49,940 --> 00:27:51,085
it's the exact same story,

534
00:27:51,085 --> 00:27:52,705
again, it sees all the same inputs,

535
00:27:52,705 --> 00:27:54,690
but it has a different set of weight matrix,

536
00:27:54,830 --> 00:27:56,995
that it's going to apply to those inputs,

537
00:27:56,995 --> 00:27:58,140
so we'll have a different output,

538
00:27:58,370 --> 00:28:00,900
but the mathematical equations are exactly the same.

539
00:28:01,540 --> 00:28:02,235
So from now on,

540
00:28:02,235 --> 00:28:05,535
I'm just going to kind of simplify all of these lines and diagrams

541
00:28:05,535 --> 00:28:07,610
just to show these icons in the middle,

542
00:28:07,660 --> 00:28:09,290
just to demonstrate that these means,

543
00:28:09,550 --> 00:28:11,280
everything is going to be fully connected to everything

544
00:28:11,280 --> 00:28:14,390
and defined by those mathematical equations that we've been covering,

545
00:28:14,920 --> 00:28:17,390
but there's no extra complexity in these models

546
00:28:17,680 --> 00:28:18,830
from what you've already seen.

547
00:28:19,580 --> 00:28:23,935
Now, if you want to stack these types of solutions on top of each other,

548
00:28:23,935 --> 00:28:25,410
these layers on top of each other,

549
00:28:25,820 --> 00:28:27,810
you can not only define one layer very easily,

550
00:28:27,950 --> 00:28:30,630
but you can actually create what are called sequential models,

551
00:28:30,680 --> 00:28:33,840
these sequential models, you can define one layer after another

552
00:28:34,100 --> 00:28:37,270
and they define basically the forward propagation of information,

553
00:28:37,270 --> 00:28:40,660
not just from the neuron level, but now from the layer level,

554
00:28:40,660 --> 00:28:43,680
every layer will be fully connected to the next layer,

555
00:28:43,820 --> 00:28:48,420
and the inputs of the secondary layer will be all of the outputs of the prior layer.

556
00:28:50,030 --> 00:28:52,530
Now, of course, if you want to create a very deep neural network,

557
00:28:52,760 --> 00:28:54,205
all the deep neural network is,

558
00:28:54,205 --> 00:28:56,790
is we just keep stacking these layers on top of each other,

559
00:28:57,050 --> 00:28:58,450
there's nothing else to this story,

560
00:28:58,450 --> 00:28:59,940
that's really as simple as it is,

561
00:29:00,260 --> 00:29:04,150
once, so these layers are basically all they are is just layers,

562
00:29:04,150 --> 00:29:07,040
where the final output is computed, right,

563
00:29:07,360 --> 00:29:11,000
by going deeper and deeper into this progression of different layers,

564
00:29:11,170 --> 00:29:12,465
and you just keep stacking them,

565
00:29:12,465 --> 00:29:15,135
until you get to the last layer, which is your output layer,

566
00:29:15,135 --> 00:29:18,785
it's your final prediction that you want to output, right,

567
00:29:18,785 --> 00:29:20,660
we can create a deep neural network to do all of this

568
00:29:20,660 --> 00:29:24,365
by stacking these layers and creating these more hierarchical models,

569
00:29:24,365 --> 00:29:27,580
like we saw very early in the beginning of today's lecture,

570
00:29:27,600 --> 00:29:30,220
one where the final output is really computed by,

571
00:29:30,240 --> 00:29:32,980
you know, just going deeper and deeper into this system.

572
00:29:34,910 --> 00:29:36,145
Okay, so that's awesome.

573
00:29:36,145 --> 00:29:40,710
So we've now seen how we can go from a single neuron to a layer

574
00:29:41,030 --> 00:29:42,660
to all the way to a deep neural network

575
00:29:42,920 --> 00:29:45,330
building off of these foundational principles.

576
00:29:46,430 --> 00:29:51,810
Let's take a look at how exactly we can use these, you know, principles,

577
00:29:52,070 --> 00:29:53,220
that we've just discussed

578
00:29:53,450 --> 00:29:54,930
to solve a very real problem,

579
00:29:55,190 --> 00:30:01,150
that I think all of you are probably very concerned about this morning when you, when you woke up.

580
00:30:01,150 --> 00:30:02,160
So that problem is,

581
00:30:02,570 --> 00:30:05,470
how we can build a neural network to answer this question,

582
00:30:05,470 --> 00:30:07,795
which is will I, how will I pass this class,

583
00:30:07,795 --> 00:30:09,360
and if I will, will I not.

584
00:30:10,500 --> 00:30:12,170
So to answer this question,

585
00:30:12,220 --> 00:30:14,870
let's see if we can train a neural network to solve this problem.

586
00:30:16,240 --> 00:30:17,035
So to do this,

587
00:30:17,035 --> 00:30:18,930
let's start with a very simple neural network,

588
00:30:19,760 --> 00:30:21,760
we'll train this model with two inputs,

589
00:30:21,760 --> 00:30:22,465
just two inputs,

590
00:30:22,465 --> 00:30:27,030
one input is going to be the number of lectures that you attend, over the course of this one week,

591
00:30:27,350 --> 00:30:32,850
and the second input is going to be how many hours that you spend on your final project or your competition.

592
00:30:34,480 --> 00:30:36,270
Okay, so what we're going to do is,

593
00:30:36,270 --> 00:30:38,220
firstly go out and collect a lot of data

594
00:30:38,220 --> 00:30:40,340
from all of the past years that we've taught this course,

595
00:30:40,600 --> 00:30:42,050
and we can plot all of this data,

596
00:30:42,130 --> 00:30:43,560
because it's only two input space,

597
00:30:43,560 --> 00:30:46,490
we can plot this data on a two dimensional feature space,

598
00:30:46,780 --> 00:30:49,560
we can actually look at all of the students before you,

599
00:30:49,560 --> 00:30:51,900
that have passed the class and failed the class

600
00:30:51,900 --> 00:30:54,590
and see where they lived in this space

601
00:30:54,700 --> 00:30:56,130
for the amount of hours that they've spent,

602
00:30:56,130 --> 00:30:57,525
the number of lectures that they've attended,

603
00:30:57,525 --> 00:30:58,220
and so on,

604
00:30:58,480 --> 00:31:01,460
green point are the people who have passed, red are those who have failed,

605
00:31:02,180 --> 00:31:04,615
now, and here's you, right,

606
00:31:04,615 --> 00:31:05,425
you're right here,

607
00:31:05,425 --> 00:31:07,200
4 5 is your coordinate space,

608
00:31:07,910 --> 00:31:09,175
you fall right there

609
00:31:09,175 --> 00:31:10,495
and you've attended four lectures,

610
00:31:10,495 --> 00:31:12,300
you've spent five hours on your final project,

611
00:31:12,830 --> 00:31:14,935
we want to build a neural network to answer the question

612
00:31:14,935 --> 00:31:17,640
of will you pass the class or will you fail the class.

613
00:31:18,410 --> 00:31:19,420
So let's do it,

614
00:31:19,420 --> 00:31:20,500
we have two inputs,

615
00:31:20,500 --> 00:31:21,900
one is four, one is five,

616
00:31:22,070 --> 00:31:22,870
these are two numbers,

617
00:31:22,870 --> 00:31:24,700
we can feed them through a neural network,

618
00:31:24,700 --> 00:31:26,760
that we've just seen how we can build that,

619
00:31:27,540 --> 00:31:29,690
and we feed that into a single layered neural network,

620
00:31:30,430 --> 00:31:31,860
three hidden units in this example,

621
00:31:31,860 --> 00:31:32,970
but we could make it larger,

622
00:31:32,970 --> 00:31:35,150
if we wanted to be more expressive and more powerful,

623
00:31:36,150 --> 00:31:37,070
and we see here that,

624
00:31:37,070 --> 00:31:39,520
the probability of you passing those classes is 0.1,

625
00:31:40,240 --> 00:31:41,160
pretty abysmal.

626
00:31:41,600 --> 00:31:43,710
So why would this be the case, right,

627
00:31:43,790 --> 00:31:44,760
what did we do wrong,

628
00:31:44,960 --> 00:31:46,740
because I don't think it's correct, right,

629
00:31:46,880 --> 00:31:48,210
when we looked at the space,

630
00:31:48,770 --> 00:31:51,565
it looked like actually you were a good candidate to pass the class,

631
00:31:51,565 --> 00:31:55,980
but why is the neural network saying that there's only a 10% likelihood that you should pass,

632
00:31:56,330 --> 00:31:57,390
does anyone have any ideas?

633
00:32:02,680 --> 00:32:04,305
Exactly, exactly,

634
00:32:04,305 --> 00:32:09,160
so, this neural network is just, like it was just born, right,

635
00:32:09,160 --> 00:32:12,430
it has no information about the world or this class,

636
00:32:12,430 --> 00:32:14,400
it doesn't know what 4 and 5 mean,

637
00:32:14,480 --> 00:32:18,210
or what the notion of passing or failing means, right.

638
00:32:19,750 --> 00:32:20,355
Exactly right,

639
00:32:20,355 --> 00:32:21,885
this neural network has not been trained,

640
00:32:21,885 --> 00:32:23,780
you can think of it kind of as a baby,

641
00:32:23,920 --> 00:32:25,650
it hasn't learned anything yet,

642
00:32:25,650 --> 00:32:28,410
so our job firstly is to train it,

643
00:32:28,410 --> 00:32:29,850
and part of that understanding is

644
00:32:29,850 --> 00:32:32,630
we first need to tell the neural network, when it makes mistakes,

645
00:32:33,550 --> 00:32:36,810
so mathematically we should now think about how we can answer this question,

646
00:32:36,810 --> 00:32:40,070
which is, did my neural network make a mistake,

647
00:32:40,120 --> 00:32:41,330
and if it made a mistake,

648
00:32:41,500 --> 00:32:43,500
how can I tell it how big of a mistake it was,

649
00:32:43,500 --> 00:32:46,130
so that the next time it sees this data point,

650
00:32:46,480 --> 00:32:48,800
can it do better, minimize that mistake.

651
00:32:49,490 --> 00:32:51,240
So in neural network language,

652
00:32:51,710 --> 00:32:53,490
those mistakes are called losses,

653
00:32:54,380 --> 00:32:57,000
and specifically you want to define what's called a loss function,

654
00:32:57,440 --> 00:33:02,800
which is going to take as input your prediction and the true prediction, right,

655
00:33:02,800 --> 00:33:05,970
and how far away your prediction is from the true prediction

656
00:33:06,050 --> 00:33:09,205
tells you how big of a loss there is, right,

657
00:33:09,205 --> 00:33:09,930
so, for example,

658
00:33:11,190 --> 00:33:16,240
let's say we want to build a neural network to do classification of,

659
00:33:17,120 --> 00:33:20,785
or, sorry, actually, even before that, I want to maybe give you some terminology.

660
00:33:20,785 --> 00:33:23,760
So there are multiple different ways of saying the same thing,

661
00:33:24,230 --> 00:33:26,130
in neural networks and deep learning,

662
00:33:26,150 --> 00:33:28,530
so what I just described as a loss function

663
00:33:28,640 --> 00:33:33,300
is also commonly referred to as an objective function, empirical risk, a cost function,

664
00:33:33,320 --> 00:33:35,400
these are all exactly the same thing,

665
00:33:35,480 --> 00:33:37,750
they're all a way for us to train the neural network,

666
00:33:37,750 --> 00:33:40,290
to teach the neural network when it makes mistakes,

667
00:33:40,940 --> 00:33:43,680
and what we really ultimately want to do is

668
00:33:43,760 --> 00:33:45,870
over the course of an entire data set,

669
00:33:46,160 --> 00:33:48,115
not just one data point of mistakes,

670
00:33:48,115 --> 00:33:50,010
we want to say over the entire data set,

671
00:33:50,450 --> 00:33:55,140
we want to minimize all of the mistakes on average that this neural network makes.

672
00:33:56,730 --> 00:33:59,650
So if we look at the problem, like I said, of binary classification,

673
00:33:59,940 --> 00:34:01,790
will I pass this class or will I not,

674
00:34:01,790 --> 00:34:03,260
there's a yes or no answer,

675
00:34:03,260 --> 00:34:04,600
that means binary classification,

676
00:34:05,740 --> 00:34:10,795
now we can use what's called a loss function of the softmax cross entropy loss,

677
00:34:10,795 --> 00:34:12,400
and for those of you who aren't familiar,

678
00:34:12,400 --> 00:34:16,110
this notion of cross entropy is actually developed here at MIT

679
00:34:16,220 --> 00:34:22,380
by Shannon, excuse me, yes, Claude Shannon,

680
00:34:22,880 --> 00:34:24,900
who is visionary,

681
00:34:24,950 --> 00:34:28,015
he did his masters here over fifty years ago,

682
00:34:28,015 --> 00:34:29,935
he introduced this notion of cross entropy,

683
00:34:29,935 --> 00:34:36,130
and that was, you know, pivotal and the ability for us to train these types of neural networks,

684
00:34:36,420 --> 00:34:37,720
even now into the future.

685
00:34:38,560 --> 00:34:40,020
So let's start by,

686
00:34:40,670 --> 00:34:44,125
instead of predicting a binary cross entropy output,

687
00:34:44,125 --> 00:34:49,150
what if we wanted to predict a final grade of your class score, for example,

688
00:34:49,150 --> 00:34:51,300
that's no longer a binary output, yes or no,

689
00:34:51,440 --> 00:34:53,425
it's actually a continuous variable, right,

690
00:34:53,425 --> 00:34:55,410
it's the grade, let's say out of 100 points,

691
00:34:55,790 --> 00:34:58,920
what is the value of your score in the class project,x

692
00:35:00,020 --> 00:35:02,960
for this type of loss, we can use what's called a mean squared error loss,

693
00:35:02,960 --> 00:35:04,220
you can think of this literally as

694
00:35:04,220 --> 00:35:07,480
just subtracting your predicted grade from the true grade

695
00:35:07,830 --> 00:35:10,240
and minimizing that distance apart.

696
00:35:12,450 --> 00:35:16,150
So I think now we're ready to really put all of this information together

697
00:35:16,500 --> 00:35:19,720
and tackle this problem of training a neural network

698
00:35:20,760 --> 00:35:25,315
to not just identify how erroneous it is,

699
00:35:25,315 --> 00:35:26,730
how large its loss is,

700
00:35:26,960 --> 00:35:29,155
but more importantly, minimize that loss,

701
00:35:29,155 --> 00:35:32,280
as a function of seeing all of this training data that it observes.

702
00:35:33,690 --> 00:35:35,980
So we know that we want to find this neural network,

703
00:35:36,150 --> 00:35:37,115
like we mentioned before,

704
00:35:37,115 --> 00:35:44,020
that minimizes this empirical risk or this empirical loss averaged across our entire data set,

705
00:35:44,400 --> 00:35:49,300
now this means that we want to find mathematically these ws, right,

706
00:35:49,410 --> 00:35:51,430
that minimize J(W),

707
00:35:51,450 --> 00:35:54,770
J(W) is loss function averaged over our entire data set,

708
00:35:54,770 --> 00:35:56,165
and W is our weight,

709
00:35:56,165 --> 00:35:57,970
so we want to find the set of weights,

710
00:35:58,440 --> 00:36:04,370
that on average is going to give us the minimum, smallest loss as possible,

711
00:36:05,620 --> 00:36:08,660
now, remember that W here is just a list,

712
00:36:08,740 --> 00:36:11,385
basically, it's just a group of all of the weights in our neural network,

713
00:36:11,385 --> 00:36:15,440
you may have hundreds of weights and a very, very small neural network,

714
00:36:15,460 --> 00:36:19,275
or in today's neural networks, you may have billions or trillions of weights,

715
00:36:19,275 --> 00:36:22,610
And you want to find what is the value of every single one of these weights,

716
00:36:22,780 --> 00:36:25,340
that's going to result in the smallest loss as possible.

717
00:36:26,540 --> 00:36:27,870
Now, how can you do this,

718
00:36:27,980 --> 00:36:33,180
remember that our loss function J(W) is just a function of our weights,

719
00:36:33,290 --> 00:36:35,830
so for any instantiation of our weights,

720
00:36:35,830 --> 00:36:42,390
we can compute a scalar value of you know how how erroneous would our neural network be

721
00:36:42,650 --> 00:36:44,640
for this instantiation of our weights.

722
00:36:45,080 --> 00:36:46,945
So let's try and visualize,

723
00:36:46,945 --> 00:36:50,635
for example, in a very simple example of a two dimensional space,

724
00:36:50,635 --> 00:36:51,930
where we have only two weights,

725
00:36:52,190 --> 00:36:55,440
extremely simple neural network here, very small,

726
00:36:55,490 --> 00:36:56,520
two weight neural network,

727
00:36:56,840 --> 00:37:01,590
and we want to find what are the optimal weights that would train this neural network,

728
00:37:02,080 --> 00:37:04,110
we can plot basically the loss,

729
00:37:05,030 --> 00:37:06,955
how erroneous the neural network is,

730
00:37:06,955 --> 00:37:11,190
for every single instantiation of these two weights, right,

731
00:37:11,210 --> 00:37:13,230
this is a huge space, it's an infinite space,

732
00:37:13,490 --> 00:37:15,210
but still we can try to,

733
00:37:15,350 --> 00:37:16,255
we can have a function,

734
00:37:16,255 --> 00:37:18,630
that evaluates at every point in this space.

735
00:37:19,570 --> 00:37:21,530
Now, what we ultimately want to do is,

736
00:37:21,850 --> 00:37:25,995
again, we want to find which set of ws

737
00:37:25,995 --> 00:37:29,060
will give us the smallest loss possible,

738
00:37:29,290 --> 00:37:33,320
that means basically the lowest point on this landscape that you can see here,

739
00:37:33,910 --> 00:37:37,070
where is the ws that bring us to that lowest point,

740
00:37:39,000 --> 00:37:40,420
the way that we do this is

741
00:37:40,650 --> 00:37:43,385
actually just by firstly starting at a random place,

742
00:37:43,385 --> 00:37:44,630
we have no idea where to start,

743
00:37:44,630 --> 00:37:46,930
so pick a random place to start in this space

744
00:37:47,310 --> 00:37:48,280
and let's start there,

745
00:37:48,480 --> 00:37:51,230
at this location, let's evaluate our neural network,

746
00:37:51,230 --> 00:37:54,340
we can compute the loss at this specific location

747
00:37:54,900 --> 00:37:59,050
and on top of that, we can actually compute how the loss is changing,

748
00:37:59,100 --> 00:38:01,010
we can compute the gradient of the loss,

749
00:38:01,010 --> 00:38:04,600
because our loss function is a continuous function,

750
00:38:04,800 --> 00:38:07,240
so we can actually compute derivatives of our function

751
00:38:07,590 --> 00:38:09,750
across the space of our weights,

752
00:38:09,980 --> 00:38:14,220
and the gradient tells us the direction of the highest point,

753
00:38:14,390 --> 00:38:15,600
so from where we stand,

754
00:38:15,680 --> 00:38:19,380
the gradient tells us where we should go to increase our loss,

755
00:38:20,380 --> 00:38:22,125
now, of course, we don't want to increase our loss,

756
00:38:22,125 --> 00:38:23,235
we want to decrease our loss,

757
00:38:23,235 --> 00:38:25,320
so we negate our gradient

758
00:38:25,320 --> 00:38:28,305
and we take a step in the opposite direction of the gradient,

759
00:38:28,305 --> 00:38:31,790
that brings us one step closer to the bottom of the landscape,

760
00:38:32,230 --> 00:38:35,895
and we just keep repeating this process over and over again,

761
00:38:35,895 --> 00:38:38,240
we evaluate the neural network at this new location,

762
00:38:38,350 --> 00:38:40,970
compute its gradient, and step in that new direction,

763
00:38:40,990 --> 00:38:45,140
we keep traversing this landscape until we converge to the minimum.

764
00:38:47,260 --> 00:38:49,185
We can really summarize this algorithm,

765
00:38:49,185 --> 00:38:52,245
which is known formally as gradient descent, right,

766
00:38:52,245 --> 00:38:55,280
so gradient descent simply can be written like this,

767
00:38:55,300 --> 00:38:56,810
we initialize all of our weights,

768
00:38:57,520 --> 00:38:59,160
this can be two weights,

769
00:38:59,160 --> 00:39:00,240
like you saw in the previous example,

770
00:39:00,240 --> 00:39:01,910
it can be billions of weights,

771
00:39:01,960 --> 00:39:03,620
like in real neural networks,

772
00:39:04,300 --> 00:39:10,800
we compute this gradient of the partial derivative of our loss with respect to the weights

773
00:39:10,800 --> 00:39:14,780
and then we can update our weights in the opposite direction of this gradient,

774
00:39:15,810 --> 00:39:18,340
so essentially we just take this small amount,

775
00:39:18,750 --> 00:39:20,380
small step, you can think of it,

776
00:39:20,490 --> 00:39:22,810
which here is denoted as η,

777
00:39:23,350 --> 00:39:25,790
and we refer to this small step,

778
00:39:26,290 --> 00:39:29,900
this is commonly referred to as what's known as the learning rate,

779
00:39:29,920 --> 00:39:32,580
it's like how much we want to trust that gradient

780
00:39:32,580 --> 00:39:34,485
and step in the direction of that gradient,

781
00:39:34,485 --> 00:39:35,780
we'll talk more about this later.

782
00:39:36,540 --> 00:39:39,170
But just to give you some sense of code,

783
00:39:39,220 --> 00:39:43,020
this algorithm is very well translatable to real code as well,

784
00:39:43,020 --> 00:39:45,230
for every line on the pseudo code you can see on the left,

785
00:39:45,400 --> 00:39:47,835
you can see corresponding real code on the right,

786
00:39:47,835 --> 00:39:48,570
that is runable

787
00:39:48,570 --> 00:39:51,020
and directly implementable by all of you in your labs.

788
00:39:51,860 --> 00:39:54,455
But now let's take a look specifically at this term here,

789
00:39:54,455 --> 00:39:55,445
this is the gradient,

790
00:39:55,445 --> 00:39:58,150
we touch very briefly on this at the visual example,

791
00:39:58,560 --> 00:40:00,035
this explains, like I said,

792
00:40:00,035 --> 00:40:03,370
how the loss is changing as a function of the weights,

793
00:40:03,480 --> 00:40:05,380
so as the weights move around,

794
00:40:05,580 --> 00:40:07,130
will my loss increase or decrease,

795
00:40:07,130 --> 00:40:08,350
and that will tell the neural network,

796
00:40:08,610 --> 00:40:11,590
if it needs to move the weights in a certain direction or not.

797
00:40:12,490 --> 00:40:15,290
But I never actually told you how to compute this, right,

798
00:40:15,430 --> 00:40:17,250
and I think that's an extremely important part,

799
00:40:17,250 --> 00:40:18,210
because if you don't know that,

800
00:40:18,210 --> 00:40:21,290
then you can't, well, you can't train your neural network,

801
00:40:21,490 --> 00:40:24,525
this is a critical part of training neural networks

802
00:40:24,525 --> 00:40:30,020
and that process of computing this line, this gradient line is known as back propagation.

803
00:40:30,160 --> 00:40:34,050
So let's do a very quick intro to back propagation

804
00:40:34,050 --> 00:40:34,880
and how it works.

805
00:40:36,190 --> 00:40:39,380
So again, let's start with the simplest neural network in existence,

806
00:40:39,490 --> 00:40:43,190
this neural network has one input, one output, and only one neuron,

807
00:40:43,510 --> 00:40:44,930
this is as simple as it gets,

808
00:40:45,580 --> 00:40:49,700
we want to compute the gradient of our loss with respect to our weight,

809
00:40:49,720 --> 00:40:53,270
in this case, let's compute it with respect to w2, the second weight,

810
00:40:54,070 --> 00:40:56,520
so this derivative is going to tell us

811
00:40:56,520 --> 00:41:01,050
how much a small change in this weight will affect our loss,

812
00:41:01,050 --> 00:41:04,250
if if a small change, if we change our weight a little bit in one direction,

813
00:41:04,420 --> 00:41:06,410
will increase our loss or decrease our loss.

814
00:41:07,530 --> 00:41:09,860
So to compute that, we can write out this derivative,

815
00:41:09,860 --> 00:41:12,370
we can start with applying the chain rule,

816
00:41:12,810 --> 00:41:16,300
backwards from the loss function through the output,

817
00:41:16,590 --> 00:41:18,035
specifically, what we can do is,

818
00:41:18,035 --> 00:41:22,210
we can actually just decompose this derivative into two components,

819
00:41:22,230 --> 00:41:26,050
the first component is the derivative of our loss with respect to our output

820
00:41:26,250 --> 00:41:30,125
multiplied by the derivative of our output with respect to w2, right,

821
00:41:30,125 --> 00:41:35,775
this is just a standard instantiation of the chain rule

822
00:41:35,775 --> 00:41:38,600
with this original derivative that we had on the left hand side.

823
00:41:39,770 --> 00:41:42,340
Let's suppose we wanted to compute the gradients of the weight,

824
00:41:42,340 --> 00:41:45,540
before that, which in this case are not w1, but w,

825
00:41:45,770 --> 00:41:48,000
excuse me not w2, but w1,

826
00:41:48,830 --> 00:41:51,330
well, all we do is replace w2 with w1

827
00:41:51,500 --> 00:41:53,560
and that chain rule still holds, right,

828
00:41:53,560 --> 00:41:54,840
that same equation holds,

829
00:41:54,860 --> 00:41:57,550
but now you can see on the red component,

830
00:41:57,550 --> 00:41:59,220
that last component of the chain rule,

831
00:41:59,300 --> 00:42:02,650
we have to once again recursively apply one more chain rule,

832
00:42:02,650 --> 00:42:06,310
because that's again another derivative that we can't directly evaluate,

833
00:42:06,310 --> 00:42:09,990
we can expand that once more with another instantiation of the chain rule,

834
00:42:10,190 --> 00:42:11,700
and now all of these components,

835
00:42:12,320 --> 00:42:16,705
we can directly propagate these gradients through the hidden units, right,

836
00:42:16,705 --> 00:42:17,575
in our neural network,

837
00:42:17,575 --> 00:42:21,475
all the way back to the weight that're interested in in this example, right,

838
00:42:21,475 --> 00:42:24,150
so we first computed the derivative with respect to w2,

839
00:42:24,680 --> 00:42:26,770
then we can back propagate that and use that information,

840
00:42:27,180 --> 00:42:28,010
also with w1,

841
00:42:28,010 --> 00:42:29,750
that's why we really call it back propagation,

842
00:42:29,750 --> 00:42:33,190
because this process occurs from the output all the way back to the input.

843
00:42:34,650 --> 00:42:38,200
Now, we repeat this process essentially many, many times

844
00:42:38,610 --> 00:42:43,000
over the course of training by propagating these gradients over and over again through the network,

845
00:42:43,290 --> 00:42:48,310
all the way from the output to the inputs to determine for every single weight, answering this question,

846
00:42:48,810 --> 00:42:53,630
which is how much does a small change in these weights affect our loss function,

847
00:42:53,630 --> 00:42:55,115
if it increases it or decreases,

848
00:42:55,115 --> 00:42:58,160
and how we can use that to improve the loss ultimately,

849
00:42:58,160 --> 00:43:00,480
because that's our final goal in this class.

850
00:43:02,710 --> 00:43:04,650
So that's the back propagation algorithm,

851
00:43:04,650 --> 00:43:07,490
that's, that's the core of training neural networks,

852
00:43:08,050 --> 00:43:09,675
in theory, it's very simple,

853
00:43:09,675 --> 00:43:13,460
it's, it's really just an instantiation of the chain rule,

854
00:43:14,380 --> 00:43:16,530
but let's touch on some insights,

855
00:43:16,530 --> 00:43:20,390
that make training neural networks actually extremely complicated in practice,

856
00:43:20,470 --> 00:43:24,105
even though the algorithm of back propagation is simple

857
00:43:24,105 --> 00:43:26,240
and, you know, many decades old,

858
00:43:26,920 --> 00:43:31,080
in practice though, optimization of neural networks looks something like this,

859
00:43:31,080 --> 00:43:33,350
it looks nothing like that picture that I showed you before,

860
00:43:33,790 --> 00:43:36,890
there are ways that we can visualize very large, deep neural networks,

861
00:43:37,300 --> 00:43:41,565
and you can think of the landscape of these models looking like something like this,

862
00:43:41,565 --> 00:43:44,600
this is an illustration from a paper that came out several years ago,

863
00:43:44,770 --> 00:43:48,440
where they tried to actually visualize the landscape of very, very deep neural networks,

864
00:43:49,090 --> 00:43:50,925
and that's what this landscape actually looks like,

865
00:43:50,925 --> 00:43:53,370
that's what you're trying to deal with and find the minimum in this space,

866
00:43:53,370 --> 00:43:55,970
and you can imagine the challenges that come with that,

867
00:43:57,790 --> 00:43:58,700
to cover the challenges,

868
00:43:58,900 --> 00:44:04,455
let's first think of and recall that update equation defined in gradient descent, right.

869
00:44:04,455 --> 00:44:07,760
So, I didn't talk too much about this parameter η,

870
00:44:07,810 --> 00:44:10,410
but now let's spend a bit of time thinking about this,

871
00:44:10,410 --> 00:44:11,790
this is called the learning rate,

872
00:44:11,790 --> 00:44:12,650
like we saw before,

873
00:44:12,940 --> 00:44:18,315
it determines basically how big of a step we need to take in the direction of our gradient

874
00:44:18,315 --> 00:44:20,330
and every single iteration of back propagation,

875
00:44:21,230 --> 00:44:24,870
in practice, even setting the learning rate can be very challenging,

876
00:44:24,890 --> 00:44:27,175
you as, you as the designer of the neural network,

877
00:44:27,175 --> 00:44:29,520
have to set this value, this learning rate

878
00:44:29,780 --> 00:44:31,315
and how do you pick this value, right,

879
00:44:31,315 --> 00:44:32,830
so that can actually be quite difficult,

880
00:44:32,830 --> 00:44:37,015
it has really large consequences when building a neural network,

881
00:44:37,015 --> 00:44:41,300
So, for example, if we set the learning rate too low,

882
00:44:41,980 --> 00:44:43,990
then we learn very slowly,

883
00:44:43,990 --> 00:44:46,200
so let's assume we start on the right hand side here,

884
00:44:46,280 --> 00:44:47,335
at that initial guess,

885
00:44:47,335 --> 00:44:49,200
if our learning rate is not large enough,

886
00:44:49,490 --> 00:44:51,450
not only do we converge slowly,

887
00:44:51,470 --> 00:44:54,150
we actually don't even converge to the global minimum,

888
00:44:54,440 --> 00:44:56,280
because we kind of get stuck in a local minimum,

889
00:44:57,740 --> 00:45:00,295
now, what if we set our learning rate too high, right,

890
00:45:00,295 --> 00:45:01,240
What can actually happen is,

891
00:45:01,240 --> 00:45:04,920
we overshoot and we can actually start to diverge from the solution,

892
00:45:05,360 --> 00:45:07,080
the gradients can actually explode,

893
00:45:07,340 --> 00:45:08,430
very bad things happen,

894
00:45:08,480 --> 00:45:10,230
and then the neural network doesn't trade,

895
00:45:10,700 --> 00:45:11,765
so that's also not good,

896
00:45:11,765 --> 00:45:16,900
in reality, there's a very happy medium between setting it too small, setting it too large,

897
00:45:17,400 --> 00:45:21,790
where you set it, just large enough to kind of overshoot some of these local minima,

898
00:45:22,470 --> 00:45:24,670
put you into a reasonable part of the search space,

899
00:45:24,930 --> 00:45:28,300
where then you can actually converge on the solutions that you care most about.

900
00:45:29,130 --> 00:45:32,090
But, actually, how do you set these learning rates in practice,

901
00:45:32,200 --> 00:45:34,790
how do you pick what is the ideal learning rate.

902
00:45:35,080 --> 00:45:38,330
One option, and this is actually a very common option in practice,

903
00:45:38,350 --> 00:45:40,800
is to simply try out a bunch of learning rates

904
00:45:40,800 --> 00:45:42,170
and see what works the best,

905
00:45:42,490 --> 00:45:45,290
so try out, let's say, a whole grid of different learning rates

906
00:45:45,670 --> 00:45:47,775
and, you know, train all of these neural networks,

907
00:45:47,775 --> 00:45:48,950
see which one works the best.

908
00:45:49,850 --> 00:45:52,570
But I think we can do something a lot smarter, right,

909
00:45:52,570 --> 00:45:55,000
so what are some more intelligent ways that we could do this,

910
00:45:55,000 --> 00:45:57,990
instead of exhaustively trying out a whole bunch of different learning rates,

911
00:45:58,430 --> 00:46:00,775
can we design a learning rate algorithm

912
00:46:00,775 --> 00:46:04,810
that actually adapts to our neural network and adapts to its landscape,

913
00:46:04,810 --> 00:46:08,010
so that it's a bit more intelligent than that previous idea.

914
00:46:09,450 --> 00:46:11,410
So this really ultimately means that,

915
00:46:12,440 --> 00:46:18,100
the learning rate, the speed at which the algorithm is trusting the gradients that it sees

916
00:46:18,330 --> 00:46:22,300
is going to depend on how large the gradient is in that location

917
00:46:22,890 --> 00:46:24,280
and how fast we're learning,

918
00:46:24,330 --> 00:46:28,330
how many other options, and sorry, and many other options,

919
00:46:29,070 --> 00:46:32,090
that we might have as part of training in neural networks, right,

920
00:46:32,090 --> 00:46:33,850
so it's not only how quickly we're learning,

921
00:46:33,960 --> 00:46:37,420
you may judge in on many different factors in the learning landscape.

922
00:46:39,210 --> 00:46:44,210
In fact, we've all been these different algorithms, that I'm talking about,

923
00:46:44,210 --> 00:46:48,550
these adaptive learning rate algorithms have been very widely studied in practice,

924
00:46:48,660 --> 00:46:52,865
there is a very thriving community, in the deep learning research community,

925
00:46:52,865 --> 00:46:58,540
that focuses on developing and designing new algorithms for learning rate adaptation

926
00:46:58,770 --> 00:47:02,080
and faster optimization of large neural networks like these,

927
00:47:02,760 --> 00:47:03,770
and during your labs,

928
00:47:03,770 --> 00:47:09,320
you'll actually get the opportunity to not only try out a lot of these different adaptive algorithms,

929
00:47:09,320 --> 00:47:10,270
which you can see here,

930
00:47:10,780 --> 00:47:15,100
but also try to uncover what are kind of the patterns and benefits of one versus the other,

931
00:47:15,100 --> 00:47:19,225
and that's going to be something that I think you'll find very insightful

932
00:47:19,225 --> 00:47:20,340
as part of your labs.

933
00:47:21,330 --> 00:47:24,305
So another key component of your labs that you'll see is,

934
00:47:24,305 --> 00:47:30,575
how you can actually put all of this information that we've covered today into a single picture that looks roughly something like this,

935
00:47:30,575 --> 00:47:33,845
which defines your model at the first, at the top here,

936
00:47:33,845 --> 00:47:34,880
that's where you define your model,

937
00:47:34,880 --> 00:47:36,850
where you talked about this in the beginning part of the lecture.

938
00:47:38,100 --> 00:47:39,710
For every piece in your model,

939
00:47:39,850 --> 00:47:42,600
you're now going to need to define this optimizer,

940
00:47:42,600 --> 00:47:43,710
which we've just talked about,

941
00:47:43,710 --> 00:47:47,025
this optimizer is defined together with a learning rate, right,

942
00:47:47,025 --> 00:47:49,940
how quickly you want to optimize your loss landscape

943
00:47:49,990 --> 00:47:54,020
and over many loops, you're going to pass over all of the examples in your data set,

944
00:47:54,720 --> 00:47:58,550
and observe essentially how to improve your network, that's the gradient,

945
00:47:58,630 --> 00:48:00,840
and then actually improves the network in those directions,

946
00:48:00,840 --> 00:48:02,960
and keep doing that over and over and over again,

947
00:48:03,190 --> 00:48:07,010
until eventually your neural network converges to some sort of solution.

948
00:48:09,730 --> 00:48:11,610
So I want to very quickly, briefly,

949
00:48:11,610 --> 00:48:13,040
in the remaining time that we have,

950
00:48:13,390 --> 00:48:17,330
continue to talk about tips for training these neural networks in practice

951
00:48:17,800 --> 00:48:20,360
and focus on this very powerful idea

952
00:48:20,740 --> 00:48:27,590
of batching your data into what are called mini-batches of of smaller pieces of data.

953
00:48:28,420 --> 00:48:31,590
To do this, let's revisit that gradient descent algorithm, right,

954
00:48:31,590 --> 00:48:35,030
so here, this gradient that we talked about before is

955
00:48:35,170 --> 00:48:39,080
actually extraordinarily computationally expensive to compute,

956
00:48:39,160 --> 00:48:44,300
because it's computed as a summation across all of the pieces in your data set,

957
00:48:45,270 --> 00:48:47,860
and in most real life or real world problems,

958
00:48:48,150 --> 00:48:52,670
you know, it's simply not feasible to compute a gradient over your entire data set,

959
00:48:52,670 --> 00:48:54,970
data sets are just too large these days.

960
00:48:55,380 --> 00:48:58,100
So you know there are some alternatives, right,

961
00:48:58,100 --> 00:48:59,050
what are the alternatives,

962
00:48:59,100 --> 00:49:04,270
instead of computing the derivative or the gradients across your entire data set,

963
00:49:04,800 --> 00:49:10,400
what if you instead computed the gradient over just a single example in your dataas set,

964
00:49:10,400 --> 00:49:11,170
just one example,

965
00:49:11,190 --> 00:49:16,085
well, of course this, this estimate of your gradient is going to be exactly that,

966
00:49:16,085 --> 00:49:17,920
it's an estimate, it's going to be very noisy,

967
00:49:18,030 --> 00:49:21,400
it may roughly reflect the trends of your entire data set,

968
00:49:21,540 --> 00:49:25,330
but because it's a very, it's only one example in fact of your entire data set,

969
00:49:25,470 --> 00:49:26,770
it may be very noisy.

970
00:49:29,610 --> 00:49:31,700
Well, the advantage of this though is that,

971
00:49:31,700 --> 00:49:36,520
it's much faster to compute obviously the gradient over a single example,

972
00:49:36,690 --> 00:49:37,700
because it's one example,

973
00:49:37,700 --> 00:49:40,060
so computationally this has huge advantages,

974
00:49:40,620 --> 00:49:42,970
but the downside is that it's extremely stochastic,

975
00:49:43,560 --> 00:49:46,190
that's the reason why this algorithm is not called gradient descent,

976
00:49:46,190 --> 00:49:48,340
it's called stochastic gradient descent now.

977
00:49:49,440 --> 00:49:50,975
Now, what's the middle ground, right,

978
00:49:50,975 --> 00:49:53,740
instead of computing it with respect to one example in your data set,

979
00:49:53,820 --> 00:49:57,070
what if we computed what's called a mini-batch of examples,

980
00:49:57,180 --> 00:50:01,060
a small batch of examples that we can compute the gradients over,

981
00:50:01,140 --> 00:50:03,310
and when we take these gradients,

982
00:50:03,900 --> 00:50:07,030
they're still computationally efficient to compute,

983
00:50:07,140 --> 00:50:09,155
because it's a mini-batch, it's not too large,

984
00:50:09,155 --> 00:50:13,510
maybe we're talking on the order of tens or hundreds of examples in our dataset,

985
00:50:14,040 --> 00:50:16,320
but, more importantly,

986
00:50:16,940 --> 00:50:19,980
because we've expanded from a single example to maybe a hundred examples,

987
00:50:20,540 --> 00:50:22,915
the stochasticity is significantly reduced

988
00:50:22,915 --> 00:50:25,410
and the accuracy of our gradient is much improved.

989
00:50:26,310 --> 00:50:29,855
So normally we're thinking of batch sizes, mini-batch sizes,

990
00:50:29,855 --> 00:50:33,995
roughly on the order of a hundred data points, tens or hundreds of data points,

991
00:50:33,995 --> 00:50:37,220
this is much faster obviously to compute than gradient descent

992
00:50:37,220 --> 00:50:41,105
and much more accurate to compute compared to stochastic gradient descent,

993
00:50:41,105 --> 00:50:43,840
which is that single single point example.

994
00:50:44,830 --> 00:50:48,975
So this increase in gradient accuracy allows us

995
00:50:48,975 --> 00:50:51,720
to essentially converge to our solution much quicker,

996
00:50:51,720 --> 00:50:56,055
than it could have been possible in practice due to gradient descent limitations,

997
00:50:56,055 --> 00:50:59,100
it also means that we can increase our learning rate,

998
00:50:59,100 --> 00:51:03,045
because we can trust each of those gradients much more efficiently, right,

999
00:51:03,045 --> 00:51:04,850
we're now averaging over a batch,

1000
00:51:05,230 --> 00:51:07,755
it's going to be much more accurate than the stochastic version,

1001
00:51:07,755 --> 00:51:09,030
so we can increase that learning rate

1002
00:51:09,030 --> 00:51:11,060
and actually learn faster as well.

1003
00:51:12,240 --> 00:51:17,555
This allows us to also massively parallelize this entire algorithm and computation, right,

1004
00:51:17,555 --> 00:51:19,990
we can split up batches onto separate workers

1005
00:51:20,610 --> 00:51:26,080
and achieve even more significant speedups of this entire problem using GPUs.

1006
00:51:26,160 --> 00:51:31,835
The last topic that I very, very briefly want to cover in today's lecture is

1007
00:51:31,835 --> 00:51:33,920
this topic of overfitting, right,

1008
00:51:33,920 --> 00:51:38,540
when we're optimizing a neural network with stochastic gradient descent,

1009
00:51:39,040 --> 00:51:42,015
we have this challenge of what's called overfitting,

1010
00:51:42,015 --> 00:51:45,195
overfitting looks like this roughly, right,

1011
00:51:45,195 --> 00:51:46,400
so on the left hand side,

1012
00:51:47,440 --> 00:51:49,200
we want to build a neural network,

1013
00:51:49,200 --> 00:51:52,020
or let's say, in general, we want to build a machine learning model,

1014
00:51:52,020 --> 00:51:55,850
that can accurately describe some patterns in our data,

1015
00:51:56,580 --> 00:52:00,530
but remember, ultimately we don't want to describe the patterns in our training data,

1016
00:52:00,700 --> 00:52:03,915
ideally, we want to define the patterns in our test data,

1017
00:52:03,915 --> 00:52:07,155
of course, we don't observe test data, we only observe training data,

1018
00:52:07,155 --> 00:52:10,430
so we have this challenge of extracting patterns from training data

1019
00:52:10,660 --> 00:52:13,340
and hoping that they generalize to our test data.

1020
00:52:13,900 --> 00:52:15,440
So said in one different way,

1021
00:52:15,760 --> 00:52:19,100
we want to build models that can learn representations from our training data,

1022
00:52:19,480 --> 00:52:21,105
that can still generalize,

1023
00:52:21,105 --> 00:52:25,100
even when we show them brand new unseen pieces of test data.

1024
00:52:25,720 --> 00:52:27,380
So assume that you want to build a line,

1025
00:52:27,550 --> 00:52:32,300
that can describe or find the patterns in these points, that you can see on the slide,

1026
00:52:33,130 --> 00:52:35,180
if you have a very simple neural network,

1027
00:52:35,320 --> 00:52:37,640
which is just a single line, straight line.

1028
00:52:38,610 --> 00:52:43,070
You can describe this data suboptimally, right,

1029
00:52:43,070 --> 00:52:44,710
because the data here is non-linear,

1030
00:52:44,820 --> 00:52:49,600
you're not going to accurately capture all of the nuances and subtleties in this data set,

1031
00:52:49,770 --> 00:52:50,870
that's on the left hand side,

1032
00:52:50,870 --> 00:52:52,300
if you move to the right hand side,

1033
00:52:52,650 --> 00:52:54,730
you can see a much more complicated model,

1034
00:52:54,780 --> 00:52:58,120
but here you're actually over expressive, you're too expressive,

1035
00:52:58,200 --> 00:53:03,575
and you're capturing kind of the nuances, the spurious nuances in your training data,

1036
00:53:03,575 --> 00:53:07,000
that are actually not representative of your test data.

1037
00:53:07,790 --> 00:53:09,930
Ideally you want to end up with the model in the middle,

1038
00:53:10,190 --> 00:53:11,980
which is basically the middle ground, right,

1039
00:53:11,980 --> 00:53:14,820
it's not too complex and it's not too simple,

1040
00:53:14,840 --> 00:53:16,950
it still gives you what you want to perform well

1041
00:53:17,360 --> 00:53:19,680
and even when you give it brand new data.

1042
00:53:19,910 --> 00:53:21,090
So, to address this problem,

1043
00:53:21,320 --> 00:53:24,510
let's briefly talk about what's called regularization,

1044
00:53:25,100 --> 00:53:26,490
regularization is a technique,

1045
00:53:26,660 --> 00:53:29,010
that you can introduce to your training pipeline

1046
00:53:29,540 --> 00:53:32,430
to discourage complex models from being learned.

1047
00:53:33,320 --> 00:53:34,830
Now, as we've seen before,

1048
00:53:34,910 --> 00:53:36,030
this is really critical,

1049
00:53:36,380 --> 00:53:38,970
because neural networks are extremely large models,

1050
00:53:39,080 --> 00:53:41,910
they are extremely prone to overfitting,

1051
00:53:42,230 --> 00:53:48,835
so regularization and having techniques for regularization has extreme implications towards the success of neural networks

1052
00:53:48,835 --> 00:53:51,120
and having them generalize beyond training data

1053
00:53:51,410 --> 00:53:52,800
far into our testing domain.

1054
00:53:53,810 --> 00:53:58,200
The most popular technique for regularization in deep learning is called dropout,

1055
00:53:58,340 --> 00:54:00,960
and the idea of dropout is is actually very simple,

1056
00:54:01,640 --> 00:54:04,585
let's revisit it by drawing this picture of deep neural networks,

1057
00:54:04,585 --> 00:54:06,240
that we saw earlier in today's lecture,

1058
00:54:06,890 --> 00:54:08,220
in dropout, during training,

1059
00:54:08,270 --> 00:54:13,080
we essentially randomly select some subset of the neurons in this neural network,

1060
00:54:13,760 --> 00:54:17,990
and we try to tune them out with some random probabilities,

1061
00:54:17,990 --> 00:54:21,425
so, for example, we can select this subset of of neurons,

1062
00:54:21,425 --> 00:54:24,280
we can randomly select them with a probability of 50%

1063
00:54:24,930 --> 00:54:26,530
and with that probability,

1064
00:54:26,610 --> 00:54:31,270
we randomly turn them off or on on different iterations of our training.

1065
00:54:32,170 --> 00:54:36,420
So this is essentially forcing the neural network to learn,

1066
00:54:36,420 --> 00:54:39,050
you can think of an ensemble of different models,

1067
00:54:39,070 --> 00:54:40,100
on every iteration,

1068
00:54:40,360 --> 00:54:44,520
it's going to be exposed to kind of a different model internally

1069
00:54:44,520 --> 00:54:46,125
than the one it had on the last iteration,

1070
00:54:46,125 --> 00:54:51,020
so it has to learn how to build internal pathways to process the same information,

1071
00:54:51,310 --> 00:54:54,680
and it can't rely on information that it learned on previous iterations,

1072
00:54:54,940 --> 00:55:00,020
so it forces it to kind of capture some deeper meaning within the pathways of the neural network,

1073
00:55:00,430 --> 00:55:01,760
and this can be extremely powerful,

1074
00:55:01,840 --> 00:55:05,660
because number one, it lowers the capacity of the neural network significantly,

1075
00:55:06,220 --> 00:55:09,110
you're lowering it by roughly 50% in this example,

1076
00:55:10,310 --> 00:55:12,240
but also because it makes them easier to train,

1077
00:55:12,380 --> 00:55:16,525
because the number of weights that have gradients in this case is also reduced,

1078
00:55:16,525 --> 00:55:19,050
so it's actually much faster to train them as well.

1079
00:55:20,200 --> 00:55:21,170
Now like I mentioned,

1080
00:55:21,700 --> 00:55:26,420
on every iteration we randomly dropout a different set of neurons, right,

1081
00:55:26,530 --> 00:55:28,400
and that helps the data generalize better.

1082
00:55:29,050 --> 00:55:31,250
And the second regularization techniques,

1083
00:55:31,270 --> 00:55:35,180
which is actually a very broad regularization techniques far beyond neural networks

1084
00:55:35,650 --> 00:55:37,550
is simply called early stopping.

1085
00:55:38,290 --> 00:55:43,230
Now we know the definition of overfitting is simply

1086
00:55:43,230 --> 00:55:48,585
when our model starts to represent basically the training data more than the testing data,

1087
00:55:48,585 --> 00:55:50,990
that's really what overfitting comes down to at its core,

1088
00:55:51,700 --> 00:55:55,020
if we set aside some of the training data to use separately,

1089
00:55:55,020 --> 00:55:56,085
that we don't train on it,

1090
00:55:56,085 --> 00:56:01,560
we can use kind of a testing data set, synthetic testing data set,

1091
00:56:01,560 --> 00:56:07,500
in some ways, we can monitor how our network is learning on this unseen portion of data,

1092
00:56:07,500 --> 00:56:09,825
so for example, we can over the course of training,

1093
00:56:09,825 --> 00:56:12,440
we can basically plot the performance of our network,

1094
00:56:13,060 --> 00:56:15,990
on both the training set as well as our held out test set

1095
00:56:16,460 --> 00:56:17,970
and as the network is trained,

1096
00:56:17,990 --> 00:56:20,290
we're going to see that first of all these both decrease,

1097
00:56:20,290 --> 00:56:21,750
but there's going to be a point,

1098
00:56:22,340 --> 00:56:26,130
where the loss plateaus and starts to increase,

1099
00:56:26,180 --> 00:56:28,075
the training loss will actually start to increase,

1100
00:56:28,075 --> 00:56:30,420
this is exactly the point where you start to overfit,

1101
00:56:30,560 --> 00:56:32,520
because now you're starting to have,

1102
00:56:33,410 --> 00:56:34,540
sorry, that was the test loss,

1103
00:56:34,540 --> 00:56:36,265
the test loss actually starts to increase,

1104
00:56:36,265 --> 00:56:38,970
because now you're starting to overfit on your training data,

1105
00:56:39,560 --> 00:56:42,090
this pattern basically continues for the rest of training,

1106
00:56:42,720 --> 00:56:45,470
and this is the point that I want you to focus on,

1107
00:56:45,580 --> 00:56:47,955
this middlepoint is where we need to stop training,

1108
00:56:47,955 --> 00:56:49,010
because after this point,

1109
00:56:49,540 --> 00:56:54,440
assuming that this test set is a valid representation of the true test set,

1110
00:56:54,670 --> 00:56:57,740
this is the place where the accuracy of the model will only get worse,

1111
00:56:58,060 --> 00:57:00,045
so this is where we would want to early stop our model

1112
00:57:00,045 --> 00:57:01,820
and regularize the performance.

1113
00:57:02,970 --> 00:57:03,755
And we can see that,

1114
00:57:03,755 --> 00:57:07,235
stopping anytime before this point is also not good,

1115
00:57:07,235 --> 00:57:08,950
we're going to produce an underfit model,

1116
00:57:09,030 --> 00:57:11,380
where we could have had a better model on the test data,

1117
00:57:11,790 --> 00:57:12,905
but it's this trade off, right,

1118
00:57:12,905 --> 00:57:15,430
you can't stop too late, and you can't stop too early as well.

1119
00:57:17,340 --> 00:57:21,380
So I'll conclude this lecture by just summarizing these three key points,

1120
00:57:21,380 --> 00:57:24,305
that we've covered in today's lecture so far.

1121
00:57:24,305 --> 00:57:27,940
So we first covered these fundamental building blocks of all neural networks,

1122
00:57:27,960 --> 00:57:30,040
which is the single neuron, the perceptron,

1123
00:57:30,240 --> 00:57:33,695
we've built these up into larger neural layers,

1124
00:57:33,695 --> 00:57:36,430
and then from their neural networks and deep neural networks,

1125
00:57:37,050 --> 00:57:38,740
we've learned how we can train these,

1126
00:57:38,790 --> 00:57:40,595
apply them to data sets back,

1127
00:57:40,595 --> 00:57:41,530
propagate through them,

1128
00:57:41,790 --> 00:57:47,140
and we've seen some trips, tips and tricks for optimizing these systems end to end.

1129
00:57:48,010 --> 00:57:48,825
In the next lecture,

1130
00:57:48,825 --> 00:57:52,580
we'll hear from Ava on deep sequence modeling using RNNs,

1131
00:57:53,020 --> 00:57:57,300
and specifically this very exciting new type of model

1132
00:57:57,300 --> 00:58:00,620
called the transformer architecture and attention mechanisms,

1133
00:58:01,120 --> 00:58:03,870
so maybe let's resume the class in about five minutes

1134
00:58:03,870 --> 00:58:05,445
after we have a chance to swap speakers,

1135
00:58:05,445 --> 00:58:07,850
and thank you so much for all of your attention.

