1
00:00:09,410 --> 00:00:11,190
0,320 320,575 575,910 930,1330 1380,1780
This next lecture is my
这一节课是我绝对最喜欢的深度学习入门课程，

2
00:00:11,360 --> 00:00:14,100
0,400 480,880 930,1330 1920,2320 2340,2740
absolute favorite lecture in introduction

3
00:00:14,300 --> 00:00:16,380
0,245 245,395 395,700 1140,1540 1680,2080
to deep learning,| focusing on
|重点讲述当今深度学习方法的局限性，

4
00:00:16,850 --> 00:00:18,775
0,260 260,790 1200,1490 1490,1685 1685,1925
the limitations of deep learning

5
00:00:18,775 --> 00:00:20,190
0,330 330,615 615,840 840,1095 1095,1415
methods as they exist today,|
|

6
00:00:20,630 --> 00:00:22,380
0,335 335,590 590,815 815,1355 1355,1750
and how those limitations and
以及这些局限性和突出挑战如何真正激励

7
00:00:22,430 --> 00:00:25,375
0,400 420,820 1350,1700 1700,2290 2580,2945
outstanding challenges really motivate| new
|深度学习和人工智能前沿和新领域的新研究。

8
00:00:25,375 --> 00:00:27,235
0,365 955,1230 1230,1395 1395,1605 1605,1860
research at the cutting edge

9
00:00:27,235 --> 00:00:28,840
0,195 195,330 330,510 510,1055 1315,1605
and the new frontiers of

10
00:00:28,840 --> 00:00:30,480
0,195 195,450 450,765 765,1130
deep learning and AI.|
|

11
00:00:31,410 --> 00:00:32,675
0,350 350,575 575,755 755,1010 1010,1265
Before we dive in,| we
在我们开始之前，|我们有几件[后勤]方面的事情要讨论，

12
00:00:32,675 --> 00:00:34,610
0,210 210,450 450,735 735,1085 1165,1935
have a couple of logistical

13
00:00:34,610 --> 00:00:37,900
0,320 1000,1400 1960,2360 2650,2970 2970,3290
things to to discuss and

14
00:00:37,980 --> 00:00:40,690
0,335 335,670 960,1355 1355,1750 2310,2710
go through,| starting with perhaps
|首先可能是本课程中最重要的方面之一，

15
00:00:41,010 --> 00:00:42,190
0,260 260,380 380,500 500,760 780,1180
one of the most important

16
00:00:42,420 --> 00:00:43,895
0,305 305,485 485,760 990,1280 1280,1475
aspects of this of this

17
00:00:43,895 --> 00:00:45,190
0,305 385,645 645,780 780,975 975,1295
course,| we have a tradition
|我们有设计和赠送 T恤的传统，

18
00:00:45,210 --> 00:00:48,125
0,400 510,1060 1200,1520 1520,1840 2640,2915
of designing and giving {T-shirts

19
00:00:48,125 --> 00:00:49,070
0,225 225,450 450,630 630,765 765,945
-}| as part of this
|作为课程的一部分，

20
00:00:49,070 --> 00:00:50,660
0,320 580,915 915,1230 1230,1470 1470,1590
course| and hopefully you can
|希望你们都能看到，

21
00:00:50,660 --> 00:00:51,650
0,195 195,405 405,585 585,780 780,990
all see them,| we have
|今天它们就在这里，就在前面，

22
00:00:51,650 --> 00:00:53,225
0,240 240,495 495,830 1120,1410 1410,1575
them here today right at

23
00:00:53,225 --> 00:00:54,665
0,150 150,425 865,1125 1125,1305 1305,1440
the front.| So we're going
|所以我们将在今天的编程结束时分发 T恤，

24
00:00:54,665 --> 00:00:55,970
0,165 165,330 330,585 585,935 1015,1305
to do the distribution of

25
00:00:55,970 --> 00:00:57,305
0,165 165,330 330,650 910,1200 1200,1335
the {T-shirts -} at the

26
00:00:57,305 --> 00:00:59,510
0,245 325,675 675,1295 1345,1745 1915,2205
end of today's program,| and
|所以，请留下来，如果你想要一件 T恤。

27
00:00:59,510 --> 00:01:01,190
0,210 210,495 495,860 1150,1470 1470,1680
so please, please stay if

28
00:01:01,190 --> 00:01:02,630
0,290 340,735 735,1080 1080,1305 1305,1440
you wish to pick up

29
00:01:02,630 --> 00:01:03,540
0,150 150,330 330,620
a {T-shirt -}.|
|

30
00:01:04,610 --> 00:01:06,625
0,260 260,520 930,1330 1530,1835 1835,2015
All right, so where we
好的，所以在我们现在的位置，

31
00:01:06,625 --> 00:01:07,855
0,210 210,465 465,750 750,1020 1020,1230
are right now,| we have
|我们有一节课，

32
00:01:07,855 --> 00:01:10,075
0,210 210,515 1675,1920 1920,2100 2100,2220
this lecture,| that I'm going
|我要讲的是深度学习的局限性和新的领域，

33
00:01:10,075 --> 00:01:11,155
0,120 120,240 240,510 510,855 855,1080
to be giving on deep

34
00:01:11,155 --> 00:01:13,020
0,225 225,845 865,1140 1140,1305 1305,1865
learning limitations and new frontiers,|
|

35
00:01:13,460 --> 00:01:14,610
0,320 320,545 545,680 680,860 860,1150
and we're going to have
在那之后我们还会有三场讲座，

36
00:01:14,750 --> 00:01:16,440
0,290 290,485 485,965 965,1295 1295,1690
three more lectures following that,|
|

37
00:01:16,790 --> 00:01:18,480
0,400 420,770 770,1100 1100,1385 1385,1690
continuing our series of guest
继续我们今年的客座讲座系列。

38
00:01:18,650 --> 00:01:21,220
0,545 545,755 755,935 935,1240
lectures for this year.|
|

39
00:01:21,320 --> 00:01:23,350
0,400 690,1040 1040,1340 1340,1685 1685,2030
Importantly, we still have the
重要的是，我们仍然有围绕软件实验和项目推介提案的竞争，

40
00:01:23,350 --> 00:01:25,920
0,710 1150,1550 1600,1950 1950,2235 2235,2570
competitions surrounding both the software

41
00:01:25,970 --> 00:01:28,290
0,395 395,725 725,1060 1410,1810 1920,2320
lab and the project pitch

42
00:01:28,370 --> 00:01:29,980
0,400
proposal,|
|

43
00:01:29,990 --> 00:01:32,010
0,275 275,545 545,940 1140,1540 1620,2020
for the project pitch proposal,|
对于项目推介提案，|

44
00:01:32,120 --> 00:01:33,340
0,290 290,530 530,755 755,950 950,1220
we ask that you please
我们要求你在今晚之前上传幻灯片，

45
00:01:33,340 --> 00:01:35,100
0,465 465,720 720,1065 1065,1410 1410,1760
upload your slides by tonight,|
|

46
00:01:35,360 --> 00:01:36,840
0,290 290,545 545,845 845,1130 1130,1480
we have all the detailed
我们在课程大纲上有关于这方面的所有详细说明，

47
00:01:36,860 --> 00:01:38,095
0,400 450,725 725,905 905,1085 1085,1235
instructions for that on the

48
00:01:38,095 --> 00:01:40,390
0,225 225,785 1345,1745 1885,2160 2160,2295
course syllabus,| and if you
|如果你有兴趣提交实验竞赛，

49
00:01:40,390 --> 00:01:42,085
0,260 340,645 645,885 885,1340 1360,1695
are interested in submitting to

50
00:01:42,085 --> 00:01:44,650
0,335 415,780 780,1445 1885,2160 2160,2565
the lab competitions,| the deadline
|截止日期已经延长到明天下午 1 点，

51
00:01:44,650 --> 00:01:45,810
0,195 195,345 345,525 525,780 780,1160
for that has been extended

52
00:01:45,860 --> 00:01:48,460
0,365 365,680 680,1030 1890,2270 2270,2600
to tomorrow afternoon at {1pm

53
00:01:48,460 --> 00:01:50,590
0,465 465,840 840,1220 1360,1760 1870,2130
-},| so please submit the
|因此，请同时提交实验。

54
00:01:50,590 --> 00:01:52,090
0,530 760,1020 1020,1140 1140,1380 1380,1500
labs, for the labs as

55
00:01:52,090 --> 00:01:53,080
0,290
well.|
|

56
00:01:53,150 --> 00:01:55,140
0,640 660,1060 1110,1445 1445,1685 1685,1990
Motivation hopefully is not only
希望动机不仅仅是锻炼你在深度学习中的技能和积累知识，

57
00:01:55,190 --> 00:01:57,235
0,365 365,680 680,1030 1140,1540 1680,2045
to is mostly to exercise

58
00:01:57,235 --> 00:01:58,540
0,300 300,615 615,870 870,1050 1050,1305
your skills in deep learning

59
00:01:58,540 --> 00:02:00,115
0,225 225,360 360,540 540,860 1300,1575
and to build knowledge,| but
|而且我们还有这些令人惊叹的比赛和奖品，

60
00:02:00,115 --> 00:02:01,705
0,255 255,510 510,780 780,1175 1195,1590
we also have these amazing

61
00:02:01,705 --> 00:02:03,850
0,660 660,945 945,1565 1705,1980 1980,2145
competitions and prizes| for each
|对于每个实验以及项目推介比赛。

62
00:02:03,850 --> 00:02:05,170
0,165 165,300 300,795 795,1095 1095,1320
of the labs as well

63
00:02:05,170 --> 00:02:07,200
0,210 210,495 495,885 885,1280 1630,2030
as the project pitch proposal

64
00:02:07,460 --> 00:02:09,595
0,400 1050,1340 1340,1535 1535,1790 1790,2135
competition.| So to remind you
|为了提醒你第一个关于设计神经网络以产生音乐的实验，

65
00:02:09,595 --> 00:02:10,920
0,240 240,375 375,615 615,945 945,1325
for the first lab on

66
00:02:10,970 --> 00:02:13,105
0,520 930,1280 1280,1540 1620,1895 1895,2135
designing neural networks for music

67
00:02:13,105 --> 00:02:15,865
0,365 1135,1410 1410,1685 1825,2225 2365,2760
generation,| we have audio related
|我们有与音频相关的奖项悬而未决，

68
00:02:15,865 --> 00:02:18,085
0,615 615,825 825,1085 1165,1565 1945,2220
prizes that are up up

69
00:02:18,085 --> 00:02:19,810
0,135 135,255 255,515 775,1175 1375,1725
in the air,| the competition
|比赛是公开的，

70
00:02:19,810 --> 00:02:21,990
0,270 270,590 640,1040 1630,1905 1905,2180
is wide open,| can be
|可以是任何人的比赛，

71
00:02:22,010 --> 00:02:23,635
0,380 380,605 605,920 920,1265 1265,1625
anyone's game,| so please please,
|所以请提交你们的参赛作品。

72
00:02:23,635 --> 00:02:26,440
0,365 985,1385 1525,1800 1800,2315
please submit your entries.|
|

73
00:02:27,450 --> 00:02:29,105
0,400 510,875 875,1085 1085,1340 1340,1655
Tomorrow we'll have the project
明天我们将举行项目推介大赛，

74
00:02:29,105 --> 00:02:31,520
0,395 415,815 1075,1475 1795,2145 2145,2415
pitch proposal competition,| every year
|每年它都是这个节目的一个亮点，

75
00:02:31,520 --> 00:02:32,585
0,240 240,375 375,645 645,885 885,1065
it's a highlight of this

76
00:02:32,585 --> 00:02:34,630
0,305 745,1050 1050,1350 1350,1695 1695,2045
program,| we hear your amazing
|我们听到了你关于新的深度学习算法应用程序的惊人想法，

77
00:02:34,800 --> 00:02:36,340
0,400 420,755 755,1010 1010,1235 1235,1540
ideas about new deep learning

78
00:02:36,840 --> 00:02:39,610
0,520 780,1180 1830,2150 2150,2420 2420,2770
algorithms applications| in a quick
|一种快速的[鲨鱼缸]风格，

79
00:02:40,290 --> 00:02:41,930
0,395 395,725 725,1090 1110,1400 1400,1640
shark tank style,| Three minute
|三分钟的演讲真的很有趣，

80
00:02:41,930 --> 00:02:43,895
0,330 330,660 660,975 975,1340 1690,1965
pitch is really fun,| not
|不仅能站在这里，有机会向大家展示，

81
00:02:43,895 --> 00:02:44,870
0,240 240,465 465,570 570,720 720,975
only to be up here

82
00:02:44,870 --> 00:02:45,935
0,240 240,420 420,600 600,840 840,1065
and have the chance to

83
00:02:45,935 --> 00:02:47,765
0,275 625,1020 1020,1305 1305,1575 1575,1830
present to everyone,| but also
|还能听到你的同学、同事们令人惊叹的想法，

84
00:02:47,765 --> 00:02:49,595
0,165 165,455 685,975 975,1265 1465,1830
to hear the amazing ideas

85
00:02:49,595 --> 00:02:51,470
0,225 225,485 775,1395 1395,1650 1650,1875
of your coursemates and your

86
00:02:51,470 --> 00:02:53,400
0,315 315,510 510,830
peers and colleagues,|
|

87
00:02:53,730 --> 00:02:55,715
0,400 870,1130 1130,1250 1250,1370 1370,1985
again, some of the logistical
同样，一些关于如何将幻灯片提交到谷歌幻灯片卡片组的后勤信息也包括在教学大纲中。

88
00:02:55,715 --> 00:02:57,440
0,390 390,690 690,975 975,1355 1375,1725
information for how to submit

89
00:02:57,440 --> 00:02:58,810
0,270 270,590 610,930 930,1110 1110,1370
your slides up to the

90
00:02:59,460 --> 00:03:01,085
0,365 365,650 650,890 890,1210 1260,1625
Google slide deck are included

91
00:03:01,085 --> 00:03:02,640
0,270 270,450 450,935
on the syllabus.|
|

92
00:03:03,450 --> 00:03:05,590
0,305 305,610 930,1310 1310,1690 1740,2140
And finally, as we introduced
最后，正如我们在第一节课中介绍的，

93
00:03:05,730 --> 00:03:06,860
0,260 260,395 395,590 590,875 875,1130
in the first lecture,| and
|希望你们已经意识到，

94
00:03:06,860 --> 00:03:08,740
0,255 255,660 660,980 1270,1575 1575,1880
hopefully you've realized,| we have
|我们有一个令人兴奋的大奖竞赛，

95
00:03:09,090 --> 00:03:11,590
0,400 420,820 1110,1475 1475,1840 2100,2500
an exciting grand prize competition|
|

96
00:03:12,030 --> 00:03:15,005
0,400 510,815 815,1120 1140,1540 2310,2975
surrounding the lab on trustworthy,
围绕着实验室的可信性、深度学习、稳健性、不确定性和偏见，

97
00:03:15,005 --> 00:03:17,090
0,180 180,485 625,1325 1345,1745 1795,2085
deep learning, robustness, uncertainty and

98
00:03:17,090 --> 00:03:20,225
0,500 1000,1400 1510,1910 2080,2775 2775,3135
bias,| and again, emphasizing the
|再次强调，竞争现在是完全开放的，

99
00:03:20,225 --> 00:03:21,560
0,330 330,570 570,875 895,1170 1170,1335
competition is wide open right

100
00:03:21,560 --> 00:03:23,320
0,290 460,780 780,1050 1050,1260 1260,1760
now,| please submit your entries,|
|请提交您的参赛作品，|

101
00:03:23,970 --> 00:03:25,265
0,275 275,440 440,650 650,965 965,1295
should be very, very exciting
非常令人兴奋，我们期待着收到你的参赛作品。

102
00:03:25,265 --> 00:03:26,530
0,195 195,330 330,525 525,845 865,1265
and we look forward to

103
00:03:26,640 --> 00:03:28,480
0,395 395,695 695,1120
receiving your submissions.|
|

104
00:03:29,840 --> 00:03:32,605
0,400 840,1240 1950,2270 2270,2555 2555,2765
Okay, so in addition to
好的，除了这门课的那些技术部分，

105
00:03:32,605 --> 00:03:35,170
0,245 685,1085 1165,1565 1765,2165 2275,2565
that, those technical components of

106
00:03:35,170 --> 00:03:38,160
0,195 195,500 1120,1440 1440,1760 2590,2990
the course,| we have three
|我们还有三节客座讲座，

107
00:03:38,330 --> 00:03:40,450
0,400 540,875 875,1480 1650,1910 1910,2120
remaining guest lectures| to round
|来结束我们的系列讲座。

108
00:03:40,450 --> 00:03:42,700
0,315 315,680 790,1190 1270,1670 1960,2250
out our lecture series.| We
|我们昨天听到了来自 Themis AI 的 Sadhana 令人惊叹的演讲，

109
00:03:42,700 --> 00:03:44,310
0,270 270,570 570,890 940,1275 1275,1610
heard an amazing talk yesterday

110
00:03:44,420 --> 00:03:46,540
0,335 335,815 815,1180 1320,1790 1790,2120
by Sadhana from themis AI|
|

111
00:03:46,540 --> 00:03:48,625
0,380 700,1050 1050,1320 1320,1905 1905,2085
on robust and trustworthy deep
关于稳健和值得信赖的深度学习。

112
00:03:48,625 --> 00:03:50,575
0,305 925,1320 1320,1620 1620,1770 1770,1950
learning.| Today we're going to
|今天，我们将听取来自 Vanguard 公司的 Ramin Hasani 的演讲，

113
00:03:50,575 --> 00:03:52,210
0,240 240,510 510,870 870,1410 1410,1635
hear from Ramin Hasani, from

114
00:03:52,210 --> 00:03:53,970
0,560 790,1155 1155,1305 1305,1485 1485,1760
Vanguard,| who's going to talk
|他将谈论统计学的新时代，

115
00:03:53,990 --> 00:03:55,705
0,400 660,935 935,1145 1145,1445 1445,1715
about the new age of

116
00:03:55,705 --> 00:03:57,520
0,305 895,1215 1215,1440 1440,1635 1635,1815
statistics| and what that can
|以及这对深度学习算法意味着什么。

117
00:03:57,520 --> 00:03:59,130
0,240 240,480 480,675 675,980 1120,1610
mean for deep learning algorithms.|
|

118
00:03:59,980 --> 00:04:01,605
0,395 395,710 710,920 920,1270 1350,1625
Tomorrow, we'll have two awesome
明天，我们将有两场精彩的嘉宾演讲，

119
00:04:01,605 --> 00:04:03,810
0,195 195,750 750,1145 1345,1710 1710,2205
guest lectures| from Dilip Krishnan
|来自 Google 的 Dilip Krishnan ，

120
00:04:03,810 --> 00:04:05,985
0,255 255,560 970,1370 1690,1980 1980,2175
from Google| and to round
|以及来自 MIT CSAIL 主管 Daniela Rus ，

121
00:04:05,985 --> 00:04:07,785
0,165 165,425 445,840 840,1425 1425,1800
it out from Daniela Rus,

122
00:04:07,785 --> 00:04:09,660
0,270 270,605 655,1055 1135,1535 1585,1875
the director of MIT {CSAIL

123
00:04:09,660 --> 00:04:13,530
0,290 310,710 970,1370 2950,3350 3580,3870
-} herself,| yes, some, we
|是的，一些，我们知道观众中有很多 Daniela 的粉丝，包括我们。

124
00:04:13,530 --> 00:04:14,475
0,180 180,390 390,570 570,705 705,945
know that there are many

125
00:04:14,475 --> 00:04:16,875
0,365 505,810 810,1385 1885,2160 2160,2400
fans of Daniela in the

126
00:04:16,875 --> 00:04:19,040
0,365 505,905 925,1325 1525,1845 1845,2165
audience, us included.| So please
|所以请出席，这些应该是非常棒的演讲，

127
00:04:19,060 --> 00:04:20,360
0,350 350,605 605,785 785,980 980,1300
attend, these should be really

128
00:04:20,590 --> 00:04:23,160
0,400 1530,1925 1925,2195 2195,2420 2420,2570
awesome talk| and you'll get
|你将听到更多关于深度学习研究的前沿。

129
00:04:23,160 --> 00:04:25,155
0,290 310,630 630,950 970,1370 1720,1995
to hear more about the

130
00:04:25,155 --> 00:04:27,165
0,240 240,570 570,870 870,1205 1705,2010
cutting edge of research in

131
00:04:27,165 --> 00:04:28,260
0,225 225,545
deep learning.|
|

132
00:04:28,590 --> 00:04:30,455
0,380 380,620 620,880 1290,1715 1715,1865
Okay, so that rounds out
好了，这就是我想要宣布的所有后勤和项目公告。

133
00:04:30,455 --> 00:04:31,960
0,240 240,450 450,1050 1050,1200 1200,1505
all the logistical and program

134
00:04:31,980 --> 00:04:33,560
0,575 575,725 725,970 990,1355 1355,1580
announcements that I wanted to

135
00:04:33,560 --> 00:04:35,210
0,260 820,1140 1140,1335 1335,1470 1470,1650
make.| Now we can really
|现在我们可以真正深入到有趣的东西，

136
00:04:35,210 --> 00:04:36,455
0,240 240,510 510,720 720,930 930,1245
dive into the fun stuff,|
|

137
00:04:36,455 --> 00:04:38,135
0,270 270,575 715,1115 1225,1485 1485,1680
the technical content for this
这门课的技术内容。

138
00:04:38,135 --> 00:04:39,320
0,335
class.|
|

139
00:04:39,510 --> 00:04:41,320
0,400 540,860 860,1180 1200,1505 1505,1810
So, so far, in introduction
到目前为止，在深度学习入门中，你已经了解了神经网络算法的基础，也领略了深度学习已经开始对许多不同的研究领域和应用产生影响。

140
00:04:41,670 --> 00:04:43,145
0,260 260,425 425,730 870,1250 1250,1475
to deep learning, you've learned

141
00:04:43,145 --> 00:04:44,870
0,330 330,630 630,1175 1225,1500 1500,1725
about the foundations of neural

142
00:04:44,870 --> 00:04:47,030
0,260 460,980 1270,1670 1750,2010 2010,2160
network algorithms and also got

143
00:04:47,030 --> 00:04:48,845
0,210 210,530 550,950 1180,1545 1545,1815
a taste for how deep

144
00:04:48,845 --> 00:04:50,390
0,270 270,615 615,960 960,1305 1305,1545
learning has already started to

145
00:04:50,390 --> 00:04:52,415
0,165 165,470 490,890 1330,1710 1710,2025
make an impact across many

146
00:04:52,415 --> 00:04:54,580
0,285 285,635 655,1055 1135,1535 1765,2165
different research areas and applications.|
|

147
00:04:55,440 --> 00:04:57,800
0,320 580,1200 1200,1485 1485,2055 2055,2360
From advances in autonomous vehicles
从要思考的自动驾驶汽车的进步，到思考医学和医疗保健应用的进步，到强化学习，这正在改变我们思考收益和游戏的方式，到新的生成性建模，到机器人技术的进步，再到许多其他应用，如自然语言处理、金融、安全等。

148
00:04:58,420 --> 00:04:59,955
0,275 275,500 500,850 1020,1295 1295,1535
to think about, to think

149
00:04:59,955 --> 00:05:01,725
0,365 445,825 825,1095 1095,1385 1465,1770
about applications in medicine and

150
00:05:01,725 --> 00:05:04,635
0,305 1315,1650 1650,2385 2385,2670 2670,2910
healthcare, to reinforcement learning, that

151
00:05:04,635 --> 00:05:05,730
0,180 180,480 480,705 705,900 900,1095
is changing the way that

152
00:05:05,730 --> 00:05:07,095
0,150 150,375 375,660 660,1010 1030,1365
we think about gains and

153
00:05:07,095 --> 00:05:09,810
0,335 1165,1440 1440,1665 1665,2175 2175,2715
play, to new generative modeling,

154
00:05:09,810 --> 00:05:13,110
0,650 1090,1425 1425,1970 2440,2840 2950,3300
advances to robotics, to many,

155
00:05:13,110 --> 00:05:15,495
0,300 300,650 910,1310 1660,2060 2110,2385
many other applications like natural

156
00:05:15,495 --> 00:05:18,300
0,270 270,665 1135,1535 2095,2495 2515,2805
language processing, finance, security and

157
00:05:18,300 --> 00:05:19,040
0,290
more.|
|

158
00:05:19,860 --> 00:05:20,990
0,290 290,455 455,680 680,935 935,1130
What we really hope you
我们真正希望您从这门课程中学到的是对深度神经网络如何工作的具体理解，以及这些基础算法是如何真正实现这些跨众多学科的进步的。

159
00:05:20,990 --> 00:05:22,130
0,225 225,480 480,705 705,900 900,1140
come away with from this

160
00:05:22,130 --> 00:05:24,760
0,350 760,1140 1140,1520 1690,2090 2230,2630
course is a concrete understanding

161
00:05:24,840 --> 00:05:26,320
0,335 335,635 635,920 920,1205 1205,1480
of how deep neural networks

162
00:05:26,700 --> 00:05:30,070
0,400 960,1295 1295,1630 1860,2260 2760,3370
work and how these foundational

163
00:05:30,300 --> 00:05:32,270
0,485 485,755 755,995 995,1685 1685,1970
algorithms are really enabling these

164
00:05:32,270 --> 00:05:34,730
0,630 630,1010 1180,1485 1485,2130 2130,2460
advances across this multitude of

165
00:05:34,730 --> 00:05:36,000
0,680
disciplines.|
|

166
00:05:36,590 --> 00:05:37,930
0,275 275,560 560,850 870,1145 1145,1340
And you've seen in this
你们在这门课和这个节目中已经看到，我们把神经网络作为一种方式，作为一种算法方式来考虑从输入数据中提取信号或测量数据的形式，这些数据可以从我们世界的传感器中获得。

167
00:05:37,930 --> 00:05:38,970
0,255 255,435 435,540 540,720 720,1040
class and in this program,

168
00:05:39,350 --> 00:05:40,945
0,275 275,575 575,875 875,1180 1230,1595
that we've dealt with neural

169
00:05:40,945 --> 00:05:42,220
0,270 270,540 540,690 690,965 1015,1275
networks as a way, as

170
00:05:42,220 --> 00:05:44,170
0,240 240,830 880,1280 1450,1725 1725,1950
an algorithmic way to think

171
00:05:44,170 --> 00:05:45,720
0,285 285,600 600,975 975,1260 1260,1550
about going from input data

172
00:05:46,220 --> 00:05:47,760
0,275 275,425 425,650 650,965 965,1540
in the form of signals

173
00:05:47,780 --> 00:05:49,330
0,350 350,965 965,1190 1190,1370 1370,1550
or measurements that we can

174
00:05:49,330 --> 00:05:50,965
0,380 550,870 870,1335 1335,1470 1470,1635
derive from sensors of our

175
00:05:50,965 --> 00:05:51,940
0,305
world.|
|

176
00:05:52,010 --> 00:05:54,190
0,400 420,820 990,1390 1560,1940 1940,2180
To directly produce some sort
直接产生某种决策，可以是预测，比如类别标签或数值，也可以是动作本身，就像强化学习的情况一样。

177
00:05:54,190 --> 00:05:55,735
0,210 210,560 1030,1305 1305,1440 1440,1545
of decision that could be

178
00:05:55,735 --> 00:05:57,445
0,120 120,485 955,1245 1245,1440 1440,1710
a prediction, like a class

179
00:05:57,445 --> 00:05:59,250
0,365 625,900 900,1110 1110,1530 1530,1805
label or a numerical value,

180
00:06:00,170 --> 00:06:00,985
0,290 290,440 440,575 575,680 680,815
or it could be an

181
00:06:00,985 --> 00:06:02,350
0,305 445,825 825,1095 1095,1230 1230,1365
action itself, like in the

182
00:06:02,350 --> 00:06:04,580
0,255 255,495 495,1185 1185,1460
case of reinforcement learning.|
|

183
00:06:05,230 --> 00:06:07,130
0,515 515,845 845,1175 1175,1430 1430,1900
We've also seen the inverse,
我们也看到了相反的情况，我们现在可以考虑建立神经网络算法，它可以从所需的预测或所需的操作来尝试生成新的数据实例，就像生成性建模一样。

184
00:06:07,570 --> 00:06:08,925
0,365 365,605 605,785 785,1040 1040,1355
where we can think about

185
00:06:08,925 --> 00:06:11,030
0,365 415,795 795,1125 1125,1385 1555,2105
now building neural network algorithms

186
00:06:11,290 --> 00:06:12,975
0,290 290,485 485,790 1080,1445 1445,1685
that can go from a

187
00:06:12,975 --> 00:06:15,090
0,455 745,1265 1315,1605 1605,1770 1770,2115
desired prediction or a desired

188
00:06:15,090 --> 00:06:16,790
0,350 490,825 825,1110 1110,1380 1380,1700
action to try to generate

189
00:06:16,900 --> 00:06:19,125
0,335 335,605 605,1390 1740,2045 2045,2225
new data instances, as is

190
00:06:19,125 --> 00:06:21,320
0,150 150,425 595,995 1105,1665 1665,2195
the case with generative modeling.|
|

191
00:06:22,470 --> 00:06:23,780
0,335 335,605 605,830 830,1025 1025,1310
Now, taking a step back
现在，在这两种情况下退后一步，无论我们是从数据到决策还是从数据到决策。

192
00:06:23,780 --> 00:06:25,370
0,380 520,840 840,1080 1080,1305 1305,1590
right in both these cases,

193
00:06:25,370 --> 00:06:26,740
0,315 315,585 585,795 795,1065 1065,1370
whether we're going from data

194
00:06:26,760 --> 00:06:29,080
0,350 350,700 930,1330 1650,1985 1985,2320
to decision or the reverse.|
|

195
00:06:30,020 --> 00:06:31,720
0,395 395,670 930,1220 1220,1430 1430,1700
Neural networks can be thought
神经网络可以被认为是非常强大的函数逼近器。

196
00:06:31,720 --> 00:06:34,710
0,350 610,1010 1180,1580 1630,2030 2590,2990
of as very powerful function

197
00:06:34,970 --> 00:06:36,480
0,700
approximators.|
|

198
00:06:37,600 --> 00:06:38,670
0,290 290,455 455,680 680,920 920,1070
What that means, and to
这意味着什么，为了更详细地了解这一点，我们必须回到计算机科学和神经网络理论中非常著名的一个定理，那就是1989年提出的这个定理，它在该领域引起了相当大的轰动。

199
00:06:38,670 --> 00:06:40,010
0,150 150,330 330,585 585,945 945,1340
get at this in more

200
00:06:40,120 --> 00:06:41,790
0,400 900,1190 1190,1370 1370,1520 1520,1670
detail, we have to go

201
00:06:41,790 --> 00:06:43,160
0,270 270,525 525,720 720,1005 1005,1370
back to a really famous

202
00:06:43,240 --> 00:06:45,540
0,530 530,860 860,1160 1160,1480 1980,2300
theorem in computer science and

203
00:06:45,540 --> 00:06:46,635
0,180 180,345 345,600 600,840 840,1095
in the theory of neural

204
00:06:46,635 --> 00:06:48,780
0,275 1045,1350 1350,1530 1530,1725 1725,2145
networks, that was this theorem

205
00:06:48,780 --> 00:06:51,020
0,195 195,375 375,680 910,1230 1230,2240
that was presented in 1989

206
00:06:51,550 --> 00:06:53,055
0,275 275,440 440,730 960,1280 1280,1505
and it generated quite the

207
00:06:53,055 --> 00:06:54,460
0,180 180,315 315,450 450,725
stir in the field.|
|

208
00:06:54,800 --> 00:06:56,070
0,290 290,530 530,695 695,950 950,1270
This theorem is called the
这个定理叫做万能逼近定理。

209
00:06:56,090 --> 00:06:58,600
0,400 630,1390 1410,1930
universal approximation theorem.|
|

210
00:06:58,800 --> 00:07:00,515
0,290 290,440 440,635 635,970 1440,1715
And what it states is
它所说的是，如果我们从神经网络开始。

211
00:07:00,515 --> 00:07:02,345
0,275 565,855 855,1095 1095,1445 1495,1830
that if we start with

212
00:07:02,345 --> 00:07:03,940
0,210 210,435 435,695
a neural network.|
|

213
00:07:03,950 --> 00:07:05,635
0,380 380,680 680,935 935,1270 1320,1685
With just a single hidden
只有一个隐藏层。

214
00:07:05,635 --> 00:07:06,880
0,365
layer.|
|

215
00:07:07,030 --> 00:07:09,165
0,400 750,1145 1145,1520 1520,1865 1865,2135
That single layer neural network
这种单层神经网络足以逼近任何函数。

216
00:07:09,165 --> 00:07:11,490
0,390 390,785 835,1200 1200,1950 1950,2325
is sufficient to approximate any

217
00:07:11,490 --> 00:07:12,680
0,380
function.|
|

218
00:07:13,180 --> 00:07:14,820
0,400 510,785 785,980 980,1280 1280,1640
Now, in this class, we've
现在，在这门课上，我们考虑的不是单层神经网络，而是将多个隐藏层堆叠在一起的深层模型。但是这个定理说，哦，你甚至不需要它。

219
00:07:14,820 --> 00:07:16,395
0,180 180,450 450,705 705,1010 1210,1575
been thinking about not single

220
00:07:16,395 --> 00:07:18,470
0,365 775,1125 1125,1385 1465,1770 1770,2075
layer neural networks, but rather

221
00:07:18,490 --> 00:07:20,900
0,320 320,640 1230,1625 1625,2015 2015,2410
deep models that stack multiple

222
00:07:21,070 --> 00:07:22,580
0,275 275,470 470,725 725,1145 1145,1510
of these hidden layers together.

223
00:07:23,140 --> 00:07:24,570
0,290 290,485 485,815 815,1085 1085,1430
But this theorem says, oh,

224
00:07:24,570 --> 00:07:25,700
0,240 240,420 420,585 585,825 825,1130
you don't even need that.|
|

225
00:07:26,200 --> 00:07:28,195
0,180 180,375 375,680 790,1190 1660,1995
All you need is a
您只需要一个层，如果您相信这一点，任何问题都可以简单地归结为通过某个函数将输入映射到输出的想法。

226
00:07:28,195 --> 00:07:30,340
0,300 300,665 1255,1650 1650,1920 1920,2145
single layer, and if you

227
00:07:30,340 --> 00:07:31,795
0,270 270,495 495,780 780,1160 1180,1455
believe that, any problem can

228
00:07:31,795 --> 00:07:33,205
0,195 195,515 625,975 975,1170 1170,1410
be simply reduced to this

229
00:07:33,205 --> 00:07:34,860
0,300 300,540 540,1005 1005,1260 1260,1655
idea of mapping an input

230
00:07:35,120 --> 00:07:36,520
0,245 245,490 570,890 890,1145 1145,1400
to an output through some

231
00:07:36,520 --> 00:07:37,780
0,320
function.|
|

232
00:07:38,160 --> 00:07:40,930
0,400 720,1085 1085,1360 1980,2375 2375,2770
That neural network can exist.|
这种神经网络可以存在。|

233
00:07:41,720 --> 00:07:44,840
0,380 400,800 1720,2390 2410,2835 2835,3120
The universal approximation theorem states
普适逼近定理指出，存在某种神经网络。

234
00:07:44,840 --> 00:07:46,445
0,350 430,690 690,855 855,1160 1240,1605
that there is some neural

235
00:07:46,445 --> 00:07:47,580
0,275
network.|
|

236
00:07:47,590 --> 00:07:48,870
0,275 275,500 500,800 800,1070 1070,1280
With a sufficient number of
有足够数量的神经元可以逼近任何任意函数。

237
00:07:48,870 --> 00:07:51,680
0,410 820,1140 1140,1460 1510,2300 2410,2810
neurons that can approximate any

238
00:07:51,790 --> 00:07:53,880
0,760 930,1330
arbitrary function.|
|

239
00:07:54,930 --> 00:07:56,650
0,400 660,935 935,1070 1070,1325 1325,1720
Now this is an incredibly
这是一个令人难以置信的强大的结果，但如果我们更仔细地观察并深入挖掘一下。

240
00:07:56,730 --> 00:07:59,315
0,400 570,970 1650,2050 2130,2420 2420,2585
powerful result, but if we

241
00:07:59,315 --> 00:08:00,560
0,240 240,555 555,855 855,1095 1095,1245
look more closely at this

242
00:08:00,560 --> 00:08:01,540
0,165 165,330 330,435 435,630 630,980
and dig a little further.|
|

243
00:08:02,340 --> 00:08:03,255
0,150 150,345 345,585 585,765 765,915
You can start to see
您可以开始看到，我们必须牢记一些警告。

244
00:08:03,255 --> 00:08:04,010
0,150 150,270 270,345 345,465 465,755
that there are a few

245
00:08:04,060 --> 00:08:05,295
0,575 575,770 770,905 905,1070 1070,1235
caveats that we have to

246
00:08:05,295 --> 00:08:06,600
0,135 135,270 270,545
keep in mind.|
|

247
00:08:07,220 --> 00:08:08,580
0,290 290,440 440,700
First of all.|
首先，我要说的是。|

248
00:08:08,580 --> 00:08:10,395
0,270 270,600 600,870 870,1170 1170,1815
This theorem makes no guarantees
这个定理不能保证你需要多少个单元，在那一层有多少神经元才能解决这样的问题。

249
00:08:10,395 --> 00:08:12,390
0,365 565,900 900,1185 1185,1535 1705,1995
on how many units, how

250
00:08:12,390 --> 00:08:13,850
0,240 240,735 735,975 975,1155 1155,1460
many neurons in that layer

251
00:08:14,200 --> 00:08:15,540
0,305 305,605 605,890 890,1130 1130,1340
you need to solve such

252
00:08:15,540 --> 00:08:16,680
0,135 135,410
a problem.|
|

253
00:08:17,280 --> 00:08:19,580
0,590 590,785 785,1090 1770,2090 2090,2300
Furthermore, it says, okay, how
此外，它说，好吧，它怎么会没有回答我们如何真正定义那个神经网络的问题。我们如何找到支持该体系结构的方法？它所宣称的就是这样一个神经网络的存在。

254
00:08:19,580 --> 00:08:20,720
0,195 195,375 375,675 675,915 915,1140
do it doesn't answer the

255
00:08:20,720 --> 00:08:21,760
0,195 195,435 435,630 630,780 780,1040
question of how we can

256
00:08:21,810 --> 00:08:24,310
0,380 380,760 1260,1660 1890,2240 2240,2500
actually define that neural network.

257
00:08:24,660 --> 00:08:25,580
0,275 275,395 395,530 530,740 740,920
How do we find the

258
00:08:25,580 --> 00:08:27,040
0,255 255,650 670,990 990,1185 1185,1460
ways to support that architecture?

259
00:08:27,990 --> 00:08:29,320
0,290 290,485 485,770 770,1040 1040,1330
All it claims is that

260
00:08:29,700 --> 00:08:31,390
0,290 290,440 440,635 635,910 1290,1690
such a neural network exists.|
|

261
00:08:32,760 --> 00:08:34,010
0,260 260,395 395,635 635,935 935,1250
But we know that using
但我们知道，使用梯度下降和我们在这节课上学到的一些工具。

262
00:08:34,010 --> 00:08:35,360
0,480 480,840 840,1065 1065,1230 1230,1350
gradient descent and some of

263
00:08:35,360 --> 00:08:36,335
0,165 165,405 405,615 615,765 765,975
the tools that we learned

264
00:08:36,335 --> 00:08:38,120
0,195 195,315 315,510 510,845
about in this class.|
|

265
00:08:38,160 --> 00:08:40,505
0,350 350,700 1140,1540 1590,2120 2120,2345
That finding those weights is
找到这些权重并不总是一个简单的问题。它可以是非常复杂的，非常非线性的，非常非凸的，并且思考我们如何真正实现这一点。

266
00:08:40,505 --> 00:08:42,580
0,305 385,785 805,1125 1125,1755 1755,2075
not always a straightforward problem.

267
00:08:42,960 --> 00:08:44,410
0,275 275,410 410,575 575,880 1050,1450
It can be very complex,

268
00:08:44,970 --> 00:08:47,500
0,320 320,1030 1140,1475 1475,1775 1775,2530
very nonlinear, very non convex

269
00:08:48,090 --> 00:08:50,560
0,400 960,1355 1355,1750 1830,2150 2150,2470
and thinking about how we

270
00:08:50,610 --> 00:08:52,300
0,395 395,710 710,1030
actually achieve that.|
|

271
00:08:52,300 --> 00:08:53,640
0,350 430,720 720,870 870,1080 1080,1340
Training of the neural network
神经网络的训练是一个非常非常困难的问题。

272
00:08:54,020 --> 00:08:55,435
0,380 380,665 665,905 905,1160 1160,1415
is a very, very hard

273
00:08:55,435 --> 00:08:56,520
0,335
problem.|
|

274
00:08:57,500 --> 00:08:59,965
0,700 1290,1580 1580,1790 1790,2090 2090,2465
Secondly, what is really important
其次，这个定理的真正重要之处在于，它不能保证网络对新任务的泛化程度。

275
00:08:59,965 --> 00:09:01,810
0,285 285,480 480,855 855,1145 1585,1845
about this theorem is that

276
00:09:01,810 --> 00:09:02,785
0,120 120,270 270,465 465,690 690,975
it does not make any

277
00:09:02,785 --> 00:09:04,675
0,705 705,1050 1050,1380 1380,1665 1665,1890
guarantees on how well that

278
00:09:04,675 --> 00:09:06,720
0,305 475,795 795,1445 1465,1755 1755,2045
network would generalize to new

279
00:09:06,830 --> 00:09:08,120
0,400
tasks.|
|

280
00:09:08,190 --> 00:09:09,275
0,290 290,485 485,695 695,875 875,1085
All it says is that
它所说的是，给定从输入到输出的所需映射，我们可以找到一些神经网络，它在其他任务或其他设置中的性能不存在保证。

281
00:09:09,275 --> 00:09:12,190
0,335 745,1145 1465,2010 2010,2580 2580,2915
given this desired mapping from

282
00:09:12,390 --> 00:09:14,015
0,290 290,560 560,940 1170,1445 1445,1625
input to output, we can

283
00:09:14,015 --> 00:09:15,380
0,270 270,555 555,810 810,1050 1050,1365
find some neural network that

284
00:09:15,380 --> 00:09:17,495
0,350 940,1245 1245,1725 1725,1860 1860,2115
exists no guarantees on its

285
00:09:17,495 --> 00:09:20,015
0,395 1255,1560 1560,1830 1830,2195 2275,2520
performance in other tasks or

286
00:09:20,015 --> 00:09:21,440
0,120 120,345 345,695
in other settings.|
|

287
00:09:22,500 --> 00:09:23,465
0,245 245,365 365,545 545,740 740,965
And I think that this
我认为这个定理，这个普适近似定理的想法及其潜在的警告，是一个完美的例子，可以被认为是人工智能和深度学习中过度炒作可能产生的影响。

288
00:09:23,465 --> 00:09:25,220
0,545 685,1080 1080,1365 1365,1515 1515,1755
theorem, this idea of the

289
00:09:25,220 --> 00:09:26,945
0,375 375,1095 1095,1455 1455,1560 1560,1725
universal approximation theorem and its

290
00:09:26,945 --> 00:09:29,680
0,305 595,1265 1765,2145 2145,2430 2430,2735
underlying caveats, is a perfect

291
00:09:29,760 --> 00:09:31,295
0,400 570,920 920,1175 1175,1355 1355,1535
example of what can be

292
00:09:31,295 --> 00:09:32,920
0,255 255,605 865,1140 1140,1320 1320,1625
thought of as the possible

293
00:09:33,180 --> 00:09:35,770
0,380 380,680 680,1000 1050,1990 2190,2590
effects of the overhype in

294
00:09:36,060 --> 00:09:37,625
0,395 395,790 960,1220 1220,1370 1370,1565
artificial intelligence and in deep

295
00:09:37,625 --> 00:09:38,520
0,305
learning.|
|

296
00:09:39,000 --> 00:09:41,300
0,275 275,550 810,1210 1470,1985 1985,2300
And now here you've become
现在你已经成为这个社区的一部分，对推动深度学习和人工智能的状态感兴趣。我认为，总的来说，我们需要非常谨慎地考虑这些算法，我们如何营销它们，我们如何宣传它们，我们如何在我们关心的问题中使用它们。

297
00:09:41,300 --> 00:09:42,670
0,225 225,375 375,600 600,950 970,1370
part of this community, right,

298
00:09:42,930 --> 00:09:45,065
0,520 660,965 965,1250 1250,1910 1910,2135
that's interested in advancing the

299
00:09:45,065 --> 00:09:46,420
0,225 225,435 435,630 630,935 955,1355
state of deep learning and

300
00:09:46,440 --> 00:09:48,590
0,400 1020,1250 1250,1355 1355,1595 1595,2150
AI. And I think collectively

301
00:09:48,590 --> 00:09:49,775
0,195 195,420 420,585 585,810 810,1185
we need to be extremely

302
00:09:49,775 --> 00:09:51,875
0,395 955,1275 1275,1530 1530,1815 1815,2100
careful in how we think

303
00:09:51,875 --> 00:09:53,240
0,225 225,515 565,1005 1005,1200 1200,1365
about these algorithms, how we

304
00:09:53,240 --> 00:09:54,860
0,285 285,630 630,855 855,1130 1300,1620
market them, how we advertise

305
00:09:54,860 --> 00:09:56,180
0,320 400,690 690,870 870,1080 1080,1320
them, how we use them

306
00:09:56,180 --> 00:09:57,215
0,180 180,330 330,600 600,870 870,1035
in the problems that we

307
00:09:57,215 --> 00:09:58,160
0,195 195,515
care about.|
|

308
00:09:58,770 --> 00:10:01,175
0,320 320,640 990,1390 1410,2105 2105,2405
And while universal approximation tells
虽然普适近似告诉我们，神经网络可以非常、非常强大，并围绕这个想法产生许多兴奋。

309
00:10:01,175 --> 00:10:03,730
0,305 775,1175 1315,1715 1915,2280 2280,2555
us, and that neural networks

310
00:10:03,780 --> 00:10:05,020
0,275 275,425 425,635 635,905 905,1240
can be very, very powerful

311
00:10:05,040 --> 00:10:06,290
0,320 320,740 740,890 890,1025 1025,1250
and generates a lot of

312
00:10:06,290 --> 00:10:08,320
0,350 460,750 750,1005 1005,1370
excitement around this idea.|
|

313
00:10:08,320 --> 00:10:10,560
0,240 240,405 405,710 850,1250 1840,2240
At a time historically actually
在历史上，神经网络实际上给计算机科学和人工智能社区提供了错误的希望，认为神经网络可以解决任何问题。

314
00:10:10,880 --> 00:10:12,865
0,400 450,850 900,1300 1560,1805 1805,1985
provided false hope to the

315
00:10:12,865 --> 00:10:14,580
0,285 285,630 630,1005 1005,1350 1350,1715
computer science and AI community

316
00:10:15,260 --> 00:10:17,040
0,305 305,590 590,880 1050,1415 1415,1780
that neural networks could solve

317
00:10:17,060 --> 00:10:18,280
0,400
any.|
|

318
00:10:18,600 --> 00:10:20,735
0,350 350,605 605,910 990,1690 1860,2135
Problem of any complexity in
现实世界中任何复杂的问题。

319
00:10:20,735 --> 00:10:22,140
0,150 150,345 345,665
the real world.|
|

320
00:10:22,680 --> 00:10:24,050
0,400 570,845 845,1010 1010,1190 1190,1370
Now, I think it goes
现在，我认为不用说，这种过度炒作可能是极其危险的。

321
00:10:24,050 --> 00:10:25,895
0,270 270,600 600,870 870,1190 1330,1845
without saying that such overh

322
00:10:25,895 --> 00:10:27,610
0,240 240,435 435,725 955,1335 1335,1715
hype can be extremely dangerous.|
|

323
00:10:28,600 --> 00:10:29,860
0,210 210,375 375,680 730,1110 1110,1260
And in fact, it's not
事实上，这不仅在研究过程中产生了影响，而且可能会在整个社会产生影响。

324
00:10:29,860 --> 00:10:32,770
0,290 1570,1875 1875,2085 2085,2390 2620,2910
only the effect is not

325
00:10:32,770 --> 00:10:33,805
0,210 210,390 390,555 555,795 795,1035
only in the course of

326
00:10:33,805 --> 00:10:36,180
0,305 415,815 835,1235 1495,1895 1975,2375
research, but also potentially at

327
00:10:36,380 --> 00:10:38,140
0,380 380,665 665,970
society at large.|
|

328
00:10:39,610 --> 00:10:42,090
0,400 930,1220 1220,1415 1415,1720 2100,2480
So this is why that
这就是为什么今天，在这节课剩下的时间里，我真的想重点关注深度学习算法的一些局限性，你们在这节课上已经学到了。

329
00:10:42,090 --> 00:10:43,320
0,380 460,720 720,840 840,1035 1035,1230
today, for the rest of

330
00:10:43,320 --> 00:10:44,610
0,150 150,440 460,765 765,1020 1020,1290
this lecture, I really want

331
00:10:44,610 --> 00:10:45,900
0,195 195,450 450,765 765,1050 1050,1290
to focus on some of

332
00:10:45,900 --> 00:10:47,870
0,135 135,650 1120,1440 1440,1665 1665,1970
the limitations of deep learning

333
00:10:48,040 --> 00:10:50,180
0,490 750,1150 1230,1580 1580,1790 1790,2140
algorithms that you've learned about

334
00:10:50,200 --> 00:10:51,360
0,395 395,695 695,875 875,1025 1025,1160
this over the course of

335
00:10:51,360 --> 00:10:52,600
0,180 180,500
this class.|
|

336
00:10:52,850 --> 00:10:54,070
0,365 365,635 635,860 860,1055 1055,1220
And I want to not
我不仅想止步于此，我还想超越这一点，看看这些限制如何激发新的研究机会，旨在解决其中的一些问题并克服它们。

337
00:10:54,070 --> 00:10:55,405
0,255 255,525 525,830 940,1200 1200,1335
only stop there, I want

338
00:10:55,405 --> 00:10:56,620
0,210 210,525 525,810 810,1005 1005,1215
to extend beyond that to

339
00:10:56,620 --> 00:10:59,170
0,320 460,780 780,990 990,1550 1900,2550
see how these limitations motivate

340
00:10:59,170 --> 00:11:01,800
0,350 610,1010 1780,2100 2100,2325 2325,2630
new opportunities for new research

341
00:11:01,880 --> 00:11:03,360
0,290 290,455 455,680 680,1175 1175,1480
that can be aimed at

342
00:11:03,740 --> 00:11:05,100
0,545 545,710 710,845 845,1040 1040,1360
addressing some of those problems

343
00:11:05,450 --> 00:11:07,120
0,290 290,830 830,1180
and overcoming them.|
|

344
00:11:08,200 --> 00:11:09,915
0,365 365,620 620,910 1290,1565 1565,1715
So to start, one of
因此，首先，我最喜欢的深层神经网络潜在危险的例子之一来自这篇论文，这篇论文说，理解深层神经网络需要重新思考概括。

345
00:11:09,915 --> 00:11:11,850
0,225 225,575 715,1115 1315,1650 1650,1935
my favorite examples of a

346
00:11:11,850 --> 00:11:13,560
0,345 345,740 940,1245 1245,1455 1455,1710
potential danger of deep neural

347
00:11:13,560 --> 00:11:15,440
0,260 850,1170 1170,1365 1365,1560 1560,1880
networks comes from this paper

348
00:11:15,760 --> 00:11:16,950
0,320 320,515 515,680 680,920 920,1190
comes from this paper that

349
00:11:16,950 --> 00:11:19,610
0,320 1360,1760 1810,2145 2145,2400 2400,2660
says understanding deep neural networks

350
00:11:19,960 --> 00:11:22,960
0,400 630,1475 1475,2290
requires rethinking generalization.|
|

351
00:11:23,260 --> 00:11:25,010
0,335 335,670 750,1205 1205,1415 1415,1750
This paper proposes a really
本文提出了一个非常优雅和简单的实验，突出了这种泛化概念及其在深度学习模型中的局限性。

352
00:11:25,270 --> 00:11:28,140
0,620 620,905 905,1240 1710,2110 2550,2870
elegant and simple experiment that

353
00:11:28,140 --> 00:11:30,740
0,320 790,1125 1125,1460 1540,1845 1845,2600
highlights this notion of generalization

354
00:11:31,150 --> 00:11:32,670
0,275 275,455 455,1000 1020,1325 1325,1520
and its limitations with deep

355
00:11:32,670 --> 00:11:34,040
0,240 240,590
learning models.|
|

356
00:11:34,300 --> 00:11:35,160
0,260 260,395 395,575 575,740 740,860
So what they do in
因此，他们在这篇论文中所做的是，他们从一个名为ImageNet的大型数据集中获取图像。

357
00:11:35,160 --> 00:11:36,930
0,165 165,470 1030,1365 1365,1590 1590,1770
this paper is that they

358
00:11:36,930 --> 00:11:39,440
0,225 225,560 820,1170 1170,1520 2110,2510
took images from this large

359
00:11:39,460 --> 00:11:41,540
0,620 620,875 875,1540
dataset called imagenet.|
|

360
00:11:41,540 --> 00:11:42,760
0,330 330,585 585,720 720,900 900,1220
And each of these images
这些图像中的每一张都与一个特定的标签狗相关联，香蕉狗树，就像你在这里看到的。

361
00:11:42,930 --> 00:11:44,830
0,400 480,880 1050,1355 1355,1580 1580,1900
is associated with a particular

362
00:11:44,850 --> 00:11:47,770
0,400 870,1270 1560,1960 2070,2470 2520,2920
label dog, banana dog tree,

363
00:11:48,150 --> 00:11:49,240
0,305 305,470 470,620 620,800 800,1090
as you can see here.|
|

364
00:11:50,720 --> 00:11:52,735
0,400 600,1000 1170,1445 1445,1625 1625,2015
Then what, what the authors
然后，这篇论文的作者所做的是，他们考虑了这些形象的每一个例子。

365
00:11:52,735 --> 00:11:54,325
0,135 135,300 300,570 570,935 1315,1590
of this paper did was

366
00:11:54,325 --> 00:11:55,480
0,150 150,405 405,750 750,1005 1005,1155
that they considered each of

367
00:11:55,480 --> 00:11:57,480
0,180 180,500 580,980
these image examples.|
|

368
00:11:57,480 --> 00:11:58,725
0,210 210,420 420,720 720,1035 1035,1245
And for every image in
对于数据中的每一幅图像，他们取了Ak边骰子，其中k是该数据集中可能的类别标签的数量。

369
00:11:58,725 --> 00:12:01,440
0,135 135,395 1015,1365 1365,1715 2400,2715
the data they took AK

370
00:12:01,440 --> 00:12:04,215
0,380 430,830 1450,1850 2110,2505 2505,2775
sided die, where k is

371
00:12:04,215 --> 00:12:06,110
0,165 165,450 450,765 765,1085 1495,1895
the number of possible class

372
00:12:06,130 --> 00:12:07,700
0,545 545,770 770,965 965,1220 1220,1570
labels in this data set.|
|

373
00:12:08,490 --> 00:12:10,400
0,400 570,935 935,1220 1220,1540 1590,1910
And use that die to
并使用该骰子将新标签随机分配给这些实例中的每一个。

374
00:12:10,400 --> 00:12:12,680
0,705 705,1070 1270,1575 1575,2055 2055,2280
randomly assign new labels to

375
00:12:12,680 --> 00:12:14,480
0,150 150,330 330,525 525,1220
each of these instances.|
|

376
00:12:15,990 --> 00:12:17,900
0,400 900,1205 1205,1385 1385,1535 1535,1910
Now, all of the labels
现在，所有的标签都被完全扰乱了，对吗？他们正在进行随机抽样，为这些图像分配这些全新的标签。这意味着与图像相关联的标签，如您所见，相对于该图像中的实际内容是完全随机的。

377
00:12:17,900 --> 00:12:19,700
0,150 150,390 390,750 750,1400 1450,1800
have been completely scrambled, right?

378
00:12:19,700 --> 00:12:22,000
0,500 760,1095 1095,1335 1335,1635 1635,2300
They're doing this random sampling,

379
00:12:22,320 --> 00:12:24,155
0,605 605,920 920,1175 1175,1400 1400,1835
assigning these brand new labels

380
00:12:24,155 --> 00:12:26,210
0,180 180,315 315,605 1615,1890 1890,2055
to these images. And what

381
00:12:26,210 --> 00:12:27,620
0,210 210,510 510,795 795,1100 1120,1410
this means is that the

382
00:12:27,620 --> 00:12:29,765
0,500 550,900 900,1230 1230,1610 1870,2145
labels that are associated with

383
00:12:29,765 --> 00:12:30,940
0,135 135,395 415,705 705,885 885,1175
an image, as you see,

384
00:12:31,410 --> 00:12:33,575
0,400 780,1115 1115,1450 1470,1865 1865,2165
are completely random with respect

385
00:12:33,575 --> 00:12:34,955
0,180 180,375 375,695 865,1185 1185,1380
to what is actually in

386
00:12:34,955 --> 00:12:36,120
0,180 180,485
that image.|
|

387
00:12:37,280 --> 00:12:39,090
0,305 305,485 485,635 635,910 1410,1810
Then what they did, and
然后他们所做的，如果你知道这里，你可以拥有实例，因为你有多个实例的重叠对应于同一个类。但是现在，当您引入随机性时，您可以拥有同一类的两个实例，它们现在以完全不同的标签结束。

388
00:12:39,110 --> 00:12:40,920
0,275 275,425 425,635 635,970 1410,1810
if you know here, is

389
00:12:41,060 --> 00:12:42,745
0,260 260,410 410,635 635,1400 1400,1685
you can have instances because

390
00:12:42,745 --> 00:12:44,970
0,180 180,390 390,1025 1405,1805 1825,2225
you have overlaps of multiple

391
00:12:45,050 --> 00:12:46,945
0,740 740,1415 1415,1535 1535,1655 1655,1895
instances corresponding to the same

392
00:12:46,945 --> 00:12:48,840
0,365 925,1215 1215,1425 1425,1620 1620,1895
class. But now when you

393
00:12:48,860 --> 00:12:50,880
0,305 305,1000 1320,1580 1580,1730 1730,2020
introduce randomness, you can have

394
00:12:50,930 --> 00:12:52,315
0,305 305,860 860,995 995,1160 1160,1385
two instances of the same

395
00:12:52,315 --> 00:12:53,950
0,335 805,1080 1080,1260 1260,1455 1455,1635
class that now end up

396
00:12:53,950 --> 00:12:56,020
0,285 285,645 645,975 975,1520
with completely different labels.|
|

397
00:12:56,090 --> 00:12:58,015
0,395 395,710 710,1030 1320,1640 1640,1925
Dog here, maps to banana
狗在这里，在一个案例中映射到香蕉，在另一个案例中映射到两棵树。

398
00:12:58,015 --> 00:12:59,640
0,240 240,435 435,755 985,1305 1305,1625
in one case and two

399
00:12:59,660 --> 00:13:01,720
0,400 480,800 800,1070 1070,1420
tree in another case.|
|

400
00:13:01,790 --> 00:13:04,195
0,275 275,550 690,1180 1440,1840 2010,2405
So literally, they completely, completely
因此，从字面上讲，他们完全、完全地将标签随机化。

401
00:13:04,195 --> 00:13:06,760
0,690 690,945 945,1365 1365,1745
randomizing the labels entirely.|
|

402
00:13:08,740 --> 00:13:09,855
0,275 275,455 455,665 665,905 905,1115
With that in hand, what
有了这一点，他们所做的就是尝试将深度神经网络模型与来自数据集ImageNet的采样数据相匹配。

403
00:13:09,855 --> 00:13:11,355
0,165 165,435 435,810 810,1155 1155,1500
they did was they tried

404
00:13:11,355 --> 00:13:12,675
0,330 330,585 585,795 795,1020 1020,1320
to fit a deep neural

405
00:13:12,675 --> 00:13:14,955
0,275 325,725 1165,1500 1500,1725 1725,2280
network model to the sampled

406
00:13:14,955 --> 00:13:16,940
0,395 445,735 735,900 900,1335 1335,1985
data from this dataset, imagenet.|
|

407
00:13:17,910 --> 00:13:19,670
0,400 480,800 800,1115 1115,1475 1475,1760
And they vary the degree
它们的随机性程度从。

408
00:13:19,670 --> 00:13:22,300
0,270 270,975 975,1590 1590,1910
of randomness ranging from.|
|

409
00:13:22,340 --> 00:13:25,260
0,290 290,730 990,1390 1440,2050 2520,2920
The preserved original labels to
将保留下来的原始标签完全随机。

410
00:13:25,400 --> 00:13:27,020
0,380 380,760
completely random.|
|

411
00:13:27,300 --> 00:13:28,310
0,275 275,440 440,620 620,800 800,1010
And then they took the
然后他们得到了最终的模型。

412
00:13:28,310 --> 00:13:29,760
0,300 300,680
resulting model.|
|

413
00:13:29,770 --> 00:13:31,380
0,290 290,455 455,730 750,1150 1230,1610
Looked at its performance on
看看它在测试集上的表现，这是一个独立的数据集，现在我们得到了新的图像，网络的任务是预测相关的标签。

414
00:13:31,380 --> 00:13:33,090
0,285 285,525 525,840 840,1220 1330,1710
the test set, an independent

415
00:13:33,090 --> 00:13:35,520
0,315 315,650 970,1370 1540,1940 2080,2430
data set where now we're

416
00:13:35,520 --> 00:13:37,065
0,225 225,495 495,800 1030,1335 1335,1545
given new images and the

417
00:13:37,065 --> 00:13:38,685
0,255 255,465 465,630 630,935 1315,1620
task of the network is

418
00:13:38,685 --> 00:13:40,370
0,165 165,420 420,765 765,1115 1285,1685
to predict the associated label.|
|

419
00:13:41,610 --> 00:13:43,030
0,380 380,650 650,815 815,1055 1055,1420
And as you can expect.|
正如你所预料的那样。|

420
00:13:43,840 --> 00:13:45,090
0,260 280,735 735,855 855,990 990,1250
The accuracy of the model
当您在标签过程中引入越来越多的随机性时，此独立测试集上的模型的准确性会降低。

421
00:13:45,200 --> 00:13:47,070
0,320 320,640 810,1205 1205,1535 1535,1870
on this independent test set

422
00:13:47,630 --> 00:13:49,960
0,610 900,1265 1265,1630 1680,2045 2045,2330
decreases as you introduce more

423
00:13:49,960 --> 00:13:51,970
0,210 210,405 405,1065 1065,1430 1750,2010
and more randomness into the

424
00:13:51,970 --> 00:13:53,520
0,260 280,680
label process.|
|

425
00:13:54,540 --> 00:13:56,410
0,320 320,575 575,910 1140,1505 1505,1870
What was really interesting, however,
然而，真正有趣的是，当他们现在不看测试集时看到的是什么。

426
00:13:56,640 --> 00:13:57,980
0,320 320,530 530,740 740,1055 1055,1340
was what they saw when

427
00:13:57,980 --> 00:13:59,375
0,180 180,435 435,800 910,1215 1215,1395
they now looked not at

428
00:13:59,375 --> 00:14:00,660
0,150 150,375 375,725
the test set.|
|

429
00:14:00,660 --> 00:14:02,210
0,260 310,585 585,780 780,1100 1150,1550
But at the training set.|
但在训练场。|

430
00:14:02,990 --> 00:14:03,880
0,275 275,425 425,575 575,725 725,890
And this is what they
这就是他们的发现。

431
00:14:03,880 --> 00:14:05,000
0,290
found.|
|

432
00:14:05,000 --> 00:14:06,350
0,285 285,510 510,825 825,1125 1125,1350
That no matter how much
无论他们对这些标签进行了多大程度的随机化，神经网络模型在得到的数据上进行训练时，都能够在训练集上获得100%的准确率。

433
00:14:06,350 --> 00:14:08,675
0,240 240,825 825,1110 1110,1640 2050,2325
they randomized these labels, the

434
00:14:08,675 --> 00:14:10,310
0,255 255,540 540,930 930,1290 1290,1635
neural network model, when trained

435
00:14:10,310 --> 00:14:12,770
0,330 330,675 675,1065 1065,1460 2140,2460
on that resulting data, was

436
00:14:12,770 --> 00:14:15,670
0,320 370,645 645,920 1060,1970 2230,2900
able to get 100% accuracy

437
00:14:15,810 --> 00:14:17,760
0,400 420,710 710,995 995,1390
on the training set.|
|

438
00:14:19,060 --> 00:14:21,110
0,290 290,500 500,820 1410,1730 1730,2050
What this means is that
这意味着这些神经网络模型真的可以进行这种完美的拟合，这种非常强大的函数逼近。

439
00:14:21,670 --> 00:14:24,210
0,400 660,1010 1010,1265 1265,1660 2220,2540
these neural network models can

440
00:14:24,210 --> 00:14:26,570
0,320 520,810 810,1100 1360,1760 1840,2360
really do this perfect fitting,

441
00:14:26,710 --> 00:14:29,330
0,400 420,800 800,1180 1320,1720 1920,2620
this very powerful function approximation.|
|

442
00:14:30,090 --> 00:14:30,965
0,245 245,380 380,560 560,725 725,875
And I think that this
我认为这是一个非常有力的例子，因为它再次表明，就像万能逼近定理一样，深度神经网络可以完美地适用于任何函数。

443
00:14:30,965 --> 00:14:32,380
0,135 135,285 285,525 525,875 1015,1415
is a very powerful example,

444
00:14:32,520 --> 00:14:34,510
0,260 260,440 440,760 1230,1610 1610,1990
because it shows once again,

445
00:14:34,920 --> 00:14:37,240
0,365 365,695 695,1060 1080,1780 1800,2320
like the universal approximation theorem,

446
00:14:37,530 --> 00:14:38,900
0,320 320,560 560,845 845,1085 1085,1370
that deep neural nets can

447
00:14:38,900 --> 00:14:40,870
0,350 490,890 1060,1350 1350,1605 1605,1970
perfectly fit to any function.|
|

448
00:14:41,640 --> 00:14:43,730
0,330 330,555 555,750 750,1070 1690,2090
Even if that function is
即使该函数是由完全随机的标签定义的。

449
00:14:43,960 --> 00:14:46,910
0,400 630,1030 1200,1600 1800,2200 2340,2950
defined by entirely random labels.|
|

450
00:14:48,390 --> 00:14:50,180
0,400 810,1160 1160,1355 1355,1580 1580,1790
Now you'll note that this
现在您将注意到，训练集性能和测试集性能之间的差异捕捉到了这种泛化思想。神经网络在函数拟合方面的极限是什么，以及它在未知数据上的实际表现是什么？

451
00:14:50,180 --> 00:14:52,430
0,320 670,1035 1035,1400 1420,1820 1870,2250
difference between the training set

452
00:14:52,430 --> 00:14:53,795
0,375 375,660 660,855 855,1065 1065,1365
performance and the test set

453
00:14:53,795 --> 00:14:56,230
0,395 835,1380 1380,1655 1675,2055 2055,2435
performance captures this idea of

454
00:14:56,280 --> 00:14:58,690
0,820 1470,1760 1760,1940 1940,2120 2120,2410
generalization. What is the limit

455
00:14:58,770 --> 00:15:00,455
0,290 290,455 455,695 695,970 1290,1685
of the neural network with

456
00:15:00,455 --> 00:15:02,180
0,300 300,495 495,785 805,1295 1405,1725
respect to function fitting and

457
00:15:02,180 --> 00:15:04,330
0,225 225,530 580,975 975,1580 1750,2150
how it actually performs on

458
00:15:04,650 --> 00:15:06,460
0,700 780,1180
unseen data?|
|

459
00:15:07,760 --> 00:15:08,935
0,305 305,485 485,680 680,920 920,1175
And to drive this point
为了进一步阐明这一点，我们可以简单地将神经网络理解为函数逼近器。

460
00:15:08,935 --> 00:15:11,935
0,330 330,705 705,1085 2035,2435 2725,3000
home even further, again, we

461
00:15:11,935 --> 00:15:13,470
0,180 180,485 685,1020 1020,1275 1275,1535
can really understand neural networks

462
00:15:13,820 --> 00:15:16,640
0,400 450,830 830,1430 1430,2050
simply as functional approximators.|
|

463
00:15:16,680 --> 00:15:18,940
0,305 305,560 560,910 1170,1570 1590,2260
And what the universal approximation
而普适近似定理告诉我们的是，神经网络非常非常擅长做这项工作。

464
00:15:18,960 --> 00:15:20,645
0,395 395,560 560,830 830,1180 1410,1685
theorem is telling us is

465
00:15:20,645 --> 00:15:21,830
0,165 165,405 405,660 660,930 930,1185
that neural networks are just

466
00:15:21,830 --> 00:15:23,570
0,380 430,795 795,1125 1125,1455 1455,1740
really, really, really good at

467
00:15:23,570 --> 00:15:25,080
0,240 240,480 480,800
doing this job.|
|

468
00:15:25,350 --> 00:15:26,915
0,350 350,575 575,850 960,1280 1280,1565
So if we consider this
所以，如果我们考虑这个例子，我已经在这个二维空间中展示了这些数据点。

469
00:15:26,915 --> 00:15:28,480
0,365 475,810 810,1005 1005,1275 1275,1565
example here, where I've shown

470
00:15:28,860 --> 00:15:30,965
0,400 780,1130 1130,1480 1560,1865 1865,2105
these data points in this

471
00:15:30,965 --> 00:15:32,420
0,195 195,360 360,665
two d space.|
|

472
00:15:33,310 --> 00:15:34,815
0,260 260,515 515,910 990,1310 1310,1505
We can always train a
我们总是可以训练神经网络来学习数据在这个空间中最可能的位置，在它以前见过的这个领域中。

473
00:15:34,815 --> 00:15:36,930
0,210 210,485 685,1085 1285,1685 1855,2115
neural network to learn what

474
00:15:36,930 --> 00:15:39,000
0,150 150,360 360,680 700,1100 1690,2070
is the most likely position

475
00:15:39,000 --> 00:15:40,425
0,255 255,420 420,690 690,1070 1090,1425
of the data within this

476
00:15:40,425 --> 00:15:42,540
0,335 685,1085 1105,1425 1425,1845 1845,2115
space, within this realm that

477
00:15:42,540 --> 00:15:44,300
0,300 300,555 555,890 940,1340 1360,1760
it has seen examples before.|
|

478
00:15:45,380 --> 00:15:46,740
0,335 335,665 665,950 950,1100 1100,1360
Such that if we get,
这样，如果我们得到，如果我们给模型一个新的紫色数据点。

479
00:15:47,150 --> 00:15:48,115
0,290 290,455 455,605 605,740 740,965
if we give the model

480
00:15:48,115 --> 00:15:49,975
0,270 270,575 625,975 975,1325 1555,1860
a new data point here

481
00:15:49,975 --> 00:15:51,220
0,210 210,515
in purple.|
|

482
00:15:51,260 --> 00:15:52,630
0,275 275,530 530,905 905,1190 1190,1370
We can expect that the
我们可以预期，神经网络将生成对该数据点的估计的合理预测。

483
00:15:52,630 --> 00:15:53,755
0,240 240,465 465,720 720,915 915,1125
neural network is going to

484
00:15:53,755 --> 00:15:56,490
0,305 445,810 810,1175 1255,1775 2335,2735
generate a reasonable prediction of

485
00:15:56,780 --> 00:15:58,300
0,400 480,830 830,1070 1070,1265 1265,1520
the estimate for that data

486
00:15:58,300 --> 00:15:59,260
0,350
point.|
|

487
00:15:59,600 --> 00:16:01,450
0,400 540,860 860,1180 1260,1625 1625,1850
Now, the challenge becomes, what
现在，挑战变成了，如果你现在开始超越这一领域，超越你拥有信息的地区，你拥有数据例子，会发生什么？

488
00:16:01,450 --> 00:16:02,515
0,150 150,330 330,570 570,795 795,1065
if you now start to

489
00:16:02,515 --> 00:16:05,215
0,395 415,810 810,1205 1675,2075 2365,2700
extend beyond this landscape, beyond

490
00:16:05,215 --> 00:16:06,420
0,195 195,435 435,705 705,900 900,1205
the region that you have

491
00:16:06,830 --> 00:16:08,515
0,400 630,905 905,1070 1070,1325 1325,1685
information, that you have data

492
00:16:08,515 --> 00:16:09,740
0,395
examples.|
|

493
00:16:10,120 --> 00:16:12,315
0,400 690,980 980,1190 1190,1490 1490,2195
Well, we have no guarantees
嗯，我们不能保证这些地区的训练数据是什么样子的，这实际上是现代深度神经网络存在的巨大限制之一。

494
00:16:12,315 --> 00:16:13,860
0,365 445,765 765,975 975,1230 1230,1545
on what the training data

495
00:16:13,860 --> 00:16:15,410
0,270 270,590 730,1020 1020,1230 1230,1550
looks like in these regions,

496
00:16:16,090 --> 00:16:17,040
0,275 275,425 425,560 560,710 710,950
and this is in fact

497
00:16:17,040 --> 00:16:18,950
0,225 225,375 375,650 790,1190 1270,1910
one of the large limitations

498
00:16:19,000 --> 00:16:20,655
0,365 365,680 680,920 920,1210 1350,1655
that exist in modern deep

499
00:16:20,655 --> 00:16:22,040
0,255 255,515
neural networks.|
|

500
00:16:23,560 --> 00:16:25,400
0,400 420,830 830,1070 1070,1420 1440,1840
That they're very, very effective
当我们有训练数据时，它们在区域的函数逼近方面非常非常有效。

501
00:16:25,450 --> 00:16:27,590
0,350 350,935 935,1535 1535,1835 1835,2140
at functional approximation in regions

502
00:16:27,670 --> 00:16:29,180
0,320 320,545 545,815 815,1145 1145,1510
when we have training data.|
|

503
00:16:29,930 --> 00:16:31,615
0,275 275,485 710,905 905,1115 1115,1685
But we can't make guarantees
但我们不能保证他们在这些地区的表现。

504
00:16:31,615 --> 00:16:33,490
0,180 180,435 435,815 1195,1590 1590,1875
on their performance out of

505
00:16:33,490 --> 00:16:35,180
0,290 370,770
those regions.|
|

506
00:16:35,760 --> 00:16:37,505
0,290 290,575 575,970 1080,1565 1565,1745
And so this raises this
这就提出了这个问题，在昨天的课上，我们如何才能得出方法，告诉我们网络何时不知道它需要更多信息，需要更多例子。

507
00:16:37,505 --> 00:16:39,365
0,305 385,785 985,1305 1305,1590 1590,1860
question, which again, ties back

508
00:16:39,365 --> 00:16:40,990
0,210 210,375 375,660 660,935 1225,1625
to so in lecture yesterday,

509
00:16:41,910 --> 00:16:43,265
0,335 335,605 605,815 815,995 995,1355
of how can we derive

510
00:16:43,265 --> 00:16:45,830
0,365 475,810 810,1125 1125,1505 2245,2565
methods that tell us when

511
00:16:45,830 --> 00:16:47,645
0,195 195,470 640,1080 1080,1370 1540,1815
the network doesn't know when

512
00:16:47,645 --> 00:16:49,460
0,180 180,450 450,815 895,1295 1465,1815
it needs more information, needs

513
00:16:49,460 --> 00:16:50,960
0,330 330,710
more examples.|
|

514
00:16:52,530 --> 00:16:54,050
0,365 365,620 620,890 890,1265 1265,1520
Building off this idea a
在这个想法的基础上，我认为经常有这样一个共同的概念，确实可以被媒体夸大，深度学习基本上就是这个神奇的解决方案。

515
00:16:54,050 --> 00:16:56,000
0,210 210,560 1120,1425 1425,1620 1620,1950
little further, I think there's

516
00:16:56,000 --> 00:16:58,450
0,315 315,660 660,975 975,1790 2050,2450
often this common conception, which

517
00:16:58,710 --> 00:17:00,485
0,400 630,935 935,1100 1100,1550 1550,1775
can indeed be inflated by

518
00:17:00,485 --> 00:17:02,290
0,135 135,395 895,1215 1215,1470 1470,1805
the media, that deep learning

519
00:17:02,310 --> 00:17:04,180
0,305 305,610 750,1070 1070,1390 1470,1870
is basically this magic solution.|
|

520
00:17:05,200 --> 00:17:06,915
0,490 1020,1295 1295,1430 1430,1565 1565,1715
Alchemy. It can be the
炼金术。它可以是任何问题的最终解决方案。

521
00:17:06,915 --> 00:17:08,300
0,165 165,435 435,720 720,1005 1005,1385
be all end, all solution

522
00:17:08,320 --> 00:17:09,960
0,260 260,485 485,850
to any problem.|
|

523
00:17:10,750 --> 00:17:12,555
0,400 660,965 965,1325 1325,1520 1520,1805
But this spawns the belief
但这让人相信，如果我们拿一些数据例子，并对其应用一些训练架构，训练产生的模型，转向深度学习算法，它将产生一些漂亮的、优秀的结果，解决我们的问题。

524
00:17:12,555 --> 00:17:14,235
0,365 595,900 900,1125 1125,1380 1380,1680
that if we take some

525
00:17:14,235 --> 00:17:16,610
0,360 360,755 895,1295 1585,1980 1980,2375
data examples and apply some

526
00:17:16,690 --> 00:17:19,530
0,400 1080,1480 1860,2135 2135,2410 2520,2840
training architecture to it, train

527
00:17:19,530 --> 00:17:21,270
0,240 240,540 540,920 1240,1545 1545,1740
the resulting model, turn the

528
00:17:21,270 --> 00:17:22,310
0,240 240,435 435,570 570,735 735,1040
crank on the deep learning

529
00:17:23,170 --> 00:17:25,305
0,490 690,1090 1410,1730 1730,1940 1940,2135
algorithm that will spit out

530
00:17:25,305 --> 00:17:27,915
0,335 655,1055 1075,1440 1440,1805 2125,2610
some beautiful, excellent results, solving

531
00:17:27,915 --> 00:17:29,140
0,180 180,485
our problem.|
|

532
00:17:29,310 --> 00:17:31,480
0,400 840,1115 1115,1340 1340,1690 1770,2170
But this is simply not
但这根本不是深度学习的运作方式。

533
00:17:31,500 --> 00:17:33,560
0,350 350,605 605,875 875,1240
how deep learning works.|
|

534
00:17:33,560 --> 00:17:35,440
0,330 330,555 555,920 1060,1460 1480,1880
There's this common idea of
如果你的数据很杂乱，如果你没有足够的数据，如果你试图建立一个非常大的神经网络模型来操作数据，你就不能从一开始就保证性能结果，这是一种常见的想法。

535
00:17:35,700 --> 00:17:37,940
0,500 500,790 960,1430 1430,1690 1950,2240
garbage in garbage out if

536
00:17:37,940 --> 00:17:40,235
0,210 210,530 580,980 1510,2115 2115,2295
your data is noisy, if

537
00:17:40,235 --> 00:17:41,945
0,150 150,330 330,975 975,1265 1435,1710
you have insufficient data, if

538
00:17:41,945 --> 00:17:43,070
0,210 210,480 480,675 675,840 840,1125
you try to build this

539
00:17:43,070 --> 00:17:44,800
0,330 330,680 760,1080 1080,1335 1335,1730
very large neural network model

540
00:17:44,820 --> 00:17:46,450
0,400 660,980 980,1190 1190,1355 1355,1630
to operate on the data,

541
00:17:46,980 --> 00:17:48,550
0,350 350,610 720,1070 1070,1295 1295,1570
you're not going to guarantee

542
00:17:49,260 --> 00:17:51,250
0,400 540,940 1050,1340 1340,1475 1475,1990
performance results at the outset.|
|

543
00:17:52,960 --> 00:17:54,410
0,275 275,470 470,965 965,1175 1175,1450
And this motivates one of
这激发了现代深层神经网络最恰当的故障模式之一，它突显了它们对训练所用的基础数据的依赖程度。

544
00:17:54,550 --> 00:17:56,715
0,290 290,575 575,1265 1265,1660 1710,2165
the most pertinent failure modes

545
00:17:56,715 --> 00:17:58,040
0,180 180,480 480,780 780,1035 1035,1325
of modern deep neural nets,

546
00:17:58,480 --> 00:17:59,880
0,245 245,395 395,700 810,1145 1145,1400
and it highlights just how

547
00:17:59,880 --> 00:18:01,725
0,320 430,795 795,1160 1420,1710 1710,1845
much they depend on the

548
00:18:01,725 --> 00:18:03,285
0,245 505,825 825,1020 1020,1320 1320,1560
underlying data that they're trained

549
00:18:03,285 --> 00:18:04,240
0,335
with.|
|

550
00:18:04,400 --> 00:18:05,515
0,275 275,560 560,725 725,920 920,1115
So let's say we have
让我们假设我们有一只狗的图像，我们将把它传递到CNN的架构中，我们的任务是尝试将这张狗的黑白图像着色，并产生彩色输出。

551
00:18:05,515 --> 00:18:06,450
0,195 195,390 390,555 555,675 675,935
this image of a dog

552
00:18:06,920 --> 00:18:07,870
0,350 350,530 530,620 620,770 770,950
and we are going to

553
00:18:07,870 --> 00:18:09,715
0,210 210,405 405,680 790,1190 1330,1845
pass it into a CNN

554
00:18:09,715 --> 00:18:12,370
0,305 1315,1715 1825,2130 2130,2400 2400,2655
architecture, and our task is

555
00:18:12,370 --> 00:18:13,930
0,225 225,465 465,705 705,990 990,1560
now to try to colorize

556
00:18:13,930 --> 00:18:15,610
0,285 285,590 1060,1365 1365,1530 1530,1680
this image black and white

557
00:18:15,610 --> 00:18:17,190
0,195 195,360 360,465 465,710 1180,1580
image of a dog and

558
00:18:17,420 --> 00:18:19,460
0,365 365,635 635,940 1260,1660
produce a colored output.|
|

559
00:18:21,520 --> 00:18:22,760
0,350 350,575 575,725 725,920 920,1240
This can be the result.|
这可能就是结果。|

560
00:18:23,580 --> 00:18:25,400
0,350 350,700 840,1145 1145,1445 1445,1820
Look closely at this resulting
仔细观察这张生成的图像。

561
00:18:25,400 --> 00:18:26,600
0,380
image.|
|

562
00:18:27,470 --> 00:18:29,550
0,275 275,550 660,1040 1040,1420 1680,2080
Anyone notice anything unusual about
有人注意到这只狗有什么不寻常的地方吗？

563
00:18:29,600 --> 00:18:30,760
0,260 260,520
the dog?|
|

564
00:18:31,660 --> 00:18:32,880
0,400
Yes.|
是。|

565
00:18:34,120 --> 00:18:35,880
0,275 275,425 425,605 605,910 1410,1760
The ear is green. Awesome
耳朵是绿色的。很棒的观察。还要别的吗？

566
00:18:35,880 --> 00:18:38,200
0,350 1000,1260 1260,1520
observation. Anything else?|
|

567
00:18:40,710 --> 00:18:42,180
0,400
Yes.|
是。|

568
00:18:42,520 --> 00:18:43,920
0,320 320,545 545,725 725,970 1080,1400
Yeah, the chin is pink
是的，下巴是粉红色或紫色的。所以两个不同的人指出了两个不同的例子，这些东西并不是真正一致的。

569
00:18:43,920 --> 00:18:46,050
0,210 210,500 850,1140 1140,1430 1840,2130
or purple. And so two

570
00:18:46,050 --> 00:18:47,655
0,240 240,915 915,1170 1170,1410 1410,1605
different instances pointed out by

571
00:18:47,655 --> 00:18:48,915
0,180 180,420 420,750 750,1020 1020,1260
two different people of things

572
00:18:48,915 --> 00:18:51,360
0,335 445,825 825,1085 1255,1655
that don't really align.|
|

573
00:18:51,400 --> 00:18:52,395
0,335 335,515 515,665 665,845 845,995
Why could this be the
为什么会是这样呢？

574
00:18:52,395 --> 00:18:53,520
0,275
case?|
|

575
00:18:53,520 --> 00:18:55,280
0,330 330,680
In particular.|
尤其是。|

576
00:18:55,280 --> 00:18:56,770
0,225 225,405 405,675 675,1040 1090,1490
If we look at the
如果我们看一下这个模型训练时使用的数据。

577
00:18:56,820 --> 00:18:59,705
0,400 660,1060 2070,2360 2360,2600 2600,2885
data that this model was

578
00:18:59,705 --> 00:19:00,940
0,270 270,605
trained with.|
|

579
00:19:01,380 --> 00:19:02,570
0,350 350,560 560,815 815,1070 1070,1190
Amongst the images of the
在狗的图像中，许多图像可能是狗伸出舌头或背景中有一些草。因此，CNN的模型可能已经将下巴周围的区域映射为粉色，或将耳朵周围的区域映射为绿色。这个例子在思考训练数据看起来像什么和预测输出可能是什么之间的对比时真正突出了什么。

580
00:19:02,570 --> 00:19:04,480
0,260 940,1245 1245,1425 1425,1605 1605,1910
dogs, many of those images

581
00:19:04,710 --> 00:19:05,980
0,305 305,515 515,680 680,905 905,1270
are going to be probably

582
00:19:06,060 --> 00:19:07,475
0,335 335,545 545,820 840,1235 1235,1415
of the dogs sticking their

583
00:19:07,475 --> 00:19:09,275
0,300 300,545 745,1065 1065,1385 1405,1800
tongues out or having some

584
00:19:09,275 --> 00:19:11,375
0,395 415,690 690,825 825,1085 1825,2100
grass in the background. And

585
00:19:11,375 --> 00:19:13,265
0,165 165,375 375,695 1165,1470 1470,1890
as a result, the CNN

586
00:19:13,265 --> 00:19:15,005
0,305 505,810 810,1065 1065,1515 1515,1740
model may have mapped that

587
00:19:15,005 --> 00:19:17,690
0,335 1015,1365 1365,1620 1620,1955 2365,2685
region around the chin to

588
00:19:17,690 --> 00:19:19,010
0,255 255,540 540,780 780,1005 1005,1320
be pink in color or

589
00:19:19,010 --> 00:19:20,330
0,315 315,540 540,830 850,1110 1110,1320
around the ears to be

590
00:19:20,330 --> 00:19:22,325
0,300 300,555 555,860 1510,1800 1800,1995
green in color. And what

591
00:19:22,325 --> 00:19:24,185
0,255 255,605 625,1005 1005,1385 1555,1860
this example really highlights in

592
00:19:24,185 --> 00:19:26,440
0,300 300,660 660,1025 1165,1565 1855,2255
thinking about this contrast between

593
00:19:26,640 --> 00:19:27,965
0,305 305,500 500,740 740,1040 1040,1325
what the training data looks

594
00:19:27,965 --> 00:19:31,060
0,335 925,1325 1825,2145 2145,2445 2445,3095
like and what the predictive

595
00:19:31,140 --> 00:19:32,580
0,350 350,545 545,850
outputs can be.|
|

596
00:19:32,580 --> 00:19:33,770
0,120 120,285 285,525 525,825 825,1190
Is that deep learning models?
这就是深度学习模式吗？他们所做的只是基于他们在培训过程中看到的数据来建立这种表征。

597
00:19:34,060 --> 00:19:35,175
0,320 320,575 575,770 770,980 980,1115
All they're doing is that

598
00:19:35,175 --> 00:19:37,160
0,240 240,515 535,900 900,1170 1170,1985
they're building up this representation

599
00:19:37,600 --> 00:19:39,450
0,365 365,650 650,970 1290,1640 1640,1850
based on the data that

600
00:19:39,450 --> 00:19:41,010
0,150 150,375 375,710 940,1305 1305,1560
they have seen over the

601
00:19:41,010 --> 00:19:42,380
0,195 195,405 405,710
course of training.|
|

602
00:19:43,590 --> 00:19:44,885
0,275 275,455 455,830 830,1010 1010,1295
And that raises this question
这就提出了一个问题，即神经网络如何有效地处理它们可能没有看到足够信息的情况？他们可能非常不确定。

603
00:19:44,885 --> 00:19:47,200
0,395 775,1175 1315,1695 1695,2040 2040,2315
of how do neural networks

604
00:19:47,430 --> 00:19:50,135
0,400 660,1055 1055,1385 1385,2200 2370,2705
effectively handle those instances where

605
00:19:50,135 --> 00:19:51,370
0,225 225,435 435,675 675,915 915,1235
they may not have seen

606
00:19:51,720 --> 00:19:53,705
0,400 510,910 1350,1640 1640,1805 1805,1985
enough information? They may be

607
00:19:53,705 --> 00:19:55,280
0,305 475,875
highly uncertain.|
|

608
00:19:55,280 --> 00:19:57,275
0,255 255,620 640,945 945,1380 1380,1995
And exactly as sana motivated
而正是萨娜在昨天的S演讲中受到激励，布局精美。

609
00:19:57,275 --> 00:19:58,940
0,395 445,845 955,1215 1215,1425 1425,1665
in yesterday s lecture and

610
00:19:58,940 --> 00:20:00,640
0,165 165,375 375,980
laid out beautifully.|
|

611
00:20:00,640 --> 00:20:02,395
0,210 210,560 610,990 990,1370 1450,1755
This is highly relevant to
这与现实世界的安全关键场景高度相关，例如，在自动驾驶的情况下，处于自动驾驶模式的汽车可能会或处于自动驾驶模式，最终可能会发生碰撞，结果往往是致命的或具有非常重大的后果。

612
00:20:02,395 --> 00:20:05,160
0,255 255,605 655,1035 1035,1415 1975,2765
real world safety critical scenarios,

613
00:20:05,420 --> 00:20:06,625
0,290 290,575 575,845 845,995 995,1205
for example, in the case

614
00:20:06,625 --> 00:20:09,420
0,195 195,720 720,1025 1705,2105 2395,2795
of autonomous driving, where cars

615
00:20:09,470 --> 00:20:11,860
0,350 350,620 620,890 890,1750 2010,2390
that are on autopilot can

616
00:20:11,860 --> 00:20:13,660
0,285 285,480 480,1080 1080,1400 1510,1800
or in autonomous mode can

617
00:20:13,660 --> 00:20:15,520
0,210 210,530 640,1310 1390,1665 1665,1860
end up crashing, with the

618
00:20:15,520 --> 00:20:17,620
0,300 300,675 675,1070 1270,1800 1800,2100
results often being fatal or

619
00:20:17,620 --> 00:20:20,400
0,285 285,600 600,950 1720,2120 2380,2780
having very significant significant consequences.|
|

620
00:20:21,870 --> 00:20:23,630
0,275 275,530 530,910 1140,1505 1505,1760
In this particular instance to
在这个特殊的情况下，真正从几年前进一步突出这一点。

621
00:20:23,630 --> 00:20:25,010
0,240 240,570 570,870 870,1155 1155,1380
really highlight this further from

622
00:20:25,010 --> 00:20:26,600
0,150 150,345 345,600 600,950
a few years ago.|
|

623
00:20:26,880 --> 00:20:29,360
0,275 275,485 485,820 1740,2135 2135,2480
There was the case of
有一起自动驾驶汽车的案例，导致了一起致命的事故。

624
00:20:29,360 --> 00:20:31,730
0,350 520,1185 1185,1490 1630,2010 2010,2370
an autonomous vehicle that resulted

625
00:20:31,730 --> 00:20:33,910
0,225 225,360 360,800 820,1220 1780,2180
in a fatal accident where.|
|

626
00:20:34,760 --> 00:20:37,295
0,350 1000,1305 1305,1740 1740,2060 2170,2535
It it crashed into a
它撞上了一个塔架，一个出现在路上的建筑塔架。事实证明，当他们回顾用于训练汽车的数据时，产生的神经网络用于控制汽车。

627
00:20:37,295 --> 00:20:39,515
0,510 510,810 810,1205 1525,2040 2040,2220
pylon, a construction pylon that

628
00:20:39,515 --> 00:20:40,690
0,195 195,510 510,780 780,915 915,1175
was present on the road.

629
00:20:41,400 --> 00:20:42,610
0,260 260,425 425,650 650,890 890,1210
And it turned out that

630
00:20:42,960 --> 00:20:44,090
0,275 275,425 425,620 620,905 905,1130
when they looked back at

631
00:20:44,090 --> 00:20:45,250
0,165 165,435 435,675 675,855 855,1160
the data that was used

632
00:20:45,270 --> 00:20:48,080
0,400 420,820 1050,1385 1385,1720 2460,2810
to train the resulting neural

633
00:20:48,080 --> 00:20:49,160
0,240 240,480 480,630 630,810 810,1080
network that was used to

634
00:20:49,160 --> 00:20:50,480
0,285 285,465 465,740
control the car.|
|

635
00:20:50,480 --> 00:20:52,060
0,255 255,450 450,705 705,1070 1180,1580
That in those training examples,
在这些培训例子中，谷歌街景图像中该特定区域的道路没有那些最终导致汽车相撞的建筑障碍物和建筑塔架。

636
00:20:52,740 --> 00:20:54,130
0,290 290,575 575,875 875,1085 1085,1390
the Google street view images

637
00:20:54,450 --> 00:20:56,015
0,320 320,620 620,995 995,1340 1340,1565
of that particular region of

638
00:20:56,015 --> 00:20:57,770
0,150 150,425 835,1125 1125,1410 1410,1755
the road did not have

639
00:20:57,770 --> 00:20:59,945
0,350 550,945 945,1515 1515,1800 1800,2175
those construction barriers and construction

640
00:20:59,945 --> 00:21:02,450
0,635 955,1355 1525,1925 2065,2340 2340,2505
pylons that resulted in the

641
00:21:02,450 --> 00:21:04,030
0,255 255,860 910,1185 1185,1320 1320,1580
car crashing in the end.|
|

642
00:21:05,430 --> 00:21:07,205
0,305 305,610 690,1040 1040,1390 1440,1775
And so, again, this idea
因此，同样，这种关于训练数据的差异和问题如何导致这些下游后果的想法，对于神经网络系统来说是一个非常突出的故障模式。

643
00:21:07,205 --> 00:21:09,280
0,335 385,720 720,1485 1485,1755 1755,2075
of how disparities and issues

644
00:21:09,780 --> 00:21:11,255
0,320 320,545 545,815 815,1160 1160,1475
with the training data can

645
00:21:11,255 --> 00:21:13,120
0,315 315,555 555,815 835,1530 1530,1865
lead to these downstream consequences

646
00:21:13,890 --> 00:21:15,725
0,260 260,410 410,680 680,1360 1440,1835
is a really prominent failure

647
00:21:15,725 --> 00:21:18,100
0,395 1075,1425 1425,1725 1725,1980 1980,2375
mode with neural network systems.|
|

648
00:21:18,970 --> 00:21:20,415
0,275 275,575 575,910 930,1235 1235,1445
And it's exactly these types
正是这些类型的故障模式促使人们需要了解神经网络系统中的不确定性。这就是你们在昨天的课程中学到的，希望你们已经通过软件实验室获得了深入的经验。

649
00:21:20,415 --> 00:21:22,020
0,180 180,435 435,840 840,1035 1035,1605
of failure modes that motivated

650
00:21:22,020 --> 00:21:24,500
0,240 240,530 760,1160 1360,1760 2080,2480
the need for understanding uncertainty

651
00:21:24,670 --> 00:21:26,790
0,305 305,575 575,830 830,1210 1830,2120
in neural network systems. And

652
00:21:26,790 --> 00:21:29,130
0,210 210,530 1000,1305 1305,1610 1990,2340
this was what you learned

653
00:21:29,130 --> 00:21:31,050
0,270 270,480 480,1050 1050,1310 1630,1920
about in yesterday's lecture and

654
00:21:31,050 --> 00:21:32,930
0,290 340,630 630,920 1270,1575 1575,1880
hopefully have gotten in depth

655
00:21:33,010 --> 00:21:34,460
0,380 380,740 740,1010 1010,1175 1175,1450
experience with through the software

656
00:21:34,480 --> 00:21:35,520
0,400
lab.|
|

657
00:21:36,010 --> 00:21:38,780
0,790 840,1235 1235,1630 1740,2140 2370,2770
Highlighting the importance of developing
强调了开发强大的方法来了解这些不确定性和风险度量的重要性，以及它们对于从自主到医学再到面部识别等安全关键应用程序是多么重要，就像你正在探索的那样。

658
00:21:39,160 --> 00:21:40,965
0,365 365,730 750,1150 1230,1565 1565,1805
robust methods to understand these

659
00:21:40,965 --> 00:21:43,130
0,305 355,750 750,1145 1525,1845 1845,2165
metrics of uncertainty and risk,

660
00:21:43,660 --> 00:21:44,810
0,320 320,545 545,725 725,875 875,1150
and how they can be

661
00:21:44,950 --> 00:21:46,940
0,395 395,790 990,1280 1280,1570 1590,1990
really important for safety critical

662
00:21:47,170 --> 00:21:49,820
0,400 870,1145 1145,1870 2130,2390 2390,2650
applications from autonomy to medicine

663
00:21:50,290 --> 00:21:52,220
0,290 290,680 680,940 1170,1505 1505,1930
to facial recognition, as you're

664
00:21:52,420 --> 00:21:53,780
0,430
exploring.|
|

665
00:21:54,350 --> 00:21:56,280
0,335 335,590 590,910 960,1610 1610,1930
And how these downstream consequences
以及这些下游后果如何从根本上与数据不平衡、功能不平衡和噪音问题联系在一起。

666
00:21:56,780 --> 00:21:59,100
0,320 320,605 605,1570 1770,2045 2045,2320
are linked fundamentally to issues

667
00:21:59,240 --> 00:22:01,740
0,320 320,575 575,1300 1470,1805 1805,2500
of data imbalance, feature imbalance

668
00:22:01,760 --> 00:22:03,200
0,305 305,610
and noise.|
|

669
00:22:04,310 --> 00:22:05,500
0,400
So.|
所以。|

670
00:22:05,930 --> 00:22:07,830
0,400 630,920 920,1130 1130,1450 1500,1900
Overall, I think that what
总体而言，我认为这些情况和这些考虑向我们指出的是神经网络对。

671
00:22:07,880 --> 00:22:10,440
0,400 870,1610 1610,1775 1775,2030 2030,2560
these instances and these considerations

672
00:22:10,490 --> 00:22:12,370
0,290 290,515 515,850 1140,1540 1590,1880
point us to is the

673
00:22:12,370 --> 00:22:15,210
0,770 940,1230 1230,1485 1485,1760 2440,2840
susceptibilities of neural networks to.|
|

674
00:22:16,320 --> 00:22:18,250
0,550 600,860 860,1040 1040,1355 1355,1930
Succumb to these failure modes.|
屈服于这些失败模式。|

675
00:22:19,560 --> 00:22:21,035
0,290 290,575 575,950 950,1265 1265,1475
The final failure mode that
我想要考虑和强调的最后一个失败模式是这种对抗性例子的想法。

676
00:22:21,035 --> 00:22:22,055
0,225 225,345 345,585 585,825 825,1020
I'd like to consider and

677
00:22:22,055 --> 00:22:24,220
0,305 835,1140 1140,1440 1440,1800 1800,2165
highlight is this idea of

678
00:22:24,240 --> 00:22:26,540
0,980 980,1360
adversarial examples.|
|

679
00:22:27,160 --> 00:22:28,760
0,320 320,905 905,1130 1130,1295 1295,1600
The intuition, and the key
直觉，这里的关键思想是我们可以拿一个数据实例，例如一个图像。

680
00:22:28,780 --> 00:22:30,030
0,335 335,635 635,890 890,1070 1070,1250
idea here is that we

681
00:22:30,030 --> 00:22:32,330
0,210 210,530 760,1160 1360,1760 1900,2300
can take a data instance,

682
00:22:32,770 --> 00:22:35,060
0,320 320,635 635,905 905,1180 1890,2290
for example an image which.|
|

683
00:22:35,820 --> 00:22:37,575
0,285 285,735 735,1040 1150,1530 1530,1755
If deployed and inputted into
如果部署并输入到标准中，CNN将以97%的概率预测包含一座寺庙。

684
00:22:37,575 --> 00:22:39,810
0,255 255,575 925,1535 1765,2055 2055,2235
a standard, CNN is going

685
00:22:39,810 --> 00:22:42,105
0,150 150,330 330,870 870,1190 2020,2295
to be predicted to contain

686
00:22:42,105 --> 00:22:44,930
0,165 165,455 835,1185 1185,2160 2160,2825
a temple with 97% probability.|
|

687
00:22:46,430 --> 00:22:47,635
0,305 305,485 485,650 650,920 920,1205
What we can do is
我们现在所能做的就是以随机噪声选择随机噪声的形式对原始图像施加一些微小的扰动，然后产生一个扰动的图像，对我们人类来说，这在视觉上看起来大体相同。

688
00:22:47,635 --> 00:22:50,010
0,305 415,810 810,1205 1255,1655 1765,2375
now apply some tiny perturbation

689
00:22:50,390 --> 00:22:52,410
0,290 290,485 485,770 770,1150 1620,2020
in the form of random

690
00:22:52,880 --> 00:22:55,525
0,400 810,1190 1190,1565 1565,1960 2370,2645
noise choice random noise to

691
00:22:55,525 --> 00:22:57,660
0,225 225,555 555,935 1585,1860 1860,2135
that original image that then

692
00:22:57,710 --> 00:22:59,820
0,400 750,995 995,1115 1115,1775 1775,2110
results in a perturbed image,

693
00:23:00,410 --> 00:23:02,430
0,335 335,560 560,850 930,1330 1620,2020
which to us humans appears

694
00:23:02,690 --> 00:23:04,940
0,560 560,940 1110,1400 1400,1690
visually largely the same.|
|

695
00:23:05,190 --> 00:23:06,935
0,400 630,920 920,1130 1130,1400 1400,1745
But if you pass that
但如果你把得到的图像返回给同一个神经网络，它产生的预测与图像中的实际情况完全不一致。

696
00:23:06,935 --> 00:23:08,795
0,395 415,815 1105,1470 1470,1680 1680,1860
resulting image back to that

697
00:23:08,795 --> 00:23:11,050
0,270 270,570 570,845 1495,1800 1800,2255
same neural network, it produces

698
00:23:11,190 --> 00:23:13,730
0,400 900,1390 1500,1895 1895,2270 2270,2540
a prediction that completely does

699
00:23:13,730 --> 00:23:15,460
0,270 270,650 970,1290 1290,1470 1470,1730
not align with what is

700
00:23:15,540 --> 00:23:17,540
0,400 540,845 845,1010 1010,1270
actually in the image.|
|

701
00:23:17,540 --> 00:23:19,990
0,465 465,1100 1330,1650 1650,1860 1860,2450
Predicting ostrich label of ostrich
预测鸵鸟标签的概率为98%。

702
00:23:20,220 --> 00:23:23,420
0,380 380,1510 1770,2410
with 98% probability.|
|

703
00:23:23,520 --> 00:23:24,670
0,290 290,470 470,635 635,830 830,1150
And this is this notion
这就是对抗性攻击或对抗性扰动的概念。

704
00:23:24,720 --> 00:23:26,930
0,320 320,560 560,1430 1430,1810 1950,2210
of an adversarial attack or

705
00:23:26,930 --> 00:23:29,160
0,150 150,945 945,1460
an adversarial perturbation.|
|

706
00:23:29,410 --> 00:23:31,335
0,400 480,880 900,1205 1205,1415 1415,1925
What exactly is this perturbation
这种微扰究竟在做什么？

707
00:23:31,335 --> 00:23:32,460
0,395
doing?|
|

708
00:23:33,970 --> 00:23:35,265
0,350 350,575 575,740 740,980 980,1295
Remember that when we train
记住，当我们使用梯度下降来训练神经网络时，任务是优化或最小化某些损失、函数和目标。

709
00:23:35,265 --> 00:23:37,610
0,330 330,605 955,1355 1375,1905 1905,2345
neural networks using gradient descent,

710
00:23:38,080 --> 00:23:40,710
0,305 305,610 660,1060 1110,1510 2100,2630
the task is to optimize

711
00:23:40,710 --> 00:23:43,100
0,300 300,860 1090,1490 1540,1940 1990,2390
or minimize some loss, function

712
00:23:43,570 --> 00:23:45,060
0,365 365,730
and objective.|
|

713
00:23:45,450 --> 00:23:46,930
0,305 305,485 485,755 755,1115 1115,1480
And the goal in standard
标准梯度下降的目标是，好的，我如何改变神经网络的权重来减少损失，从而优化这个目标。

714
00:23:46,950 --> 00:23:48,940
0,515 515,940 1230,1535 1535,1715 1715,1990
gradient descent is to say,

715
00:23:49,050 --> 00:23:50,770
0,400 660,980 980,1145 1145,1355 1355,1720
okay, how can I change

716
00:23:50,820 --> 00:23:52,010
0,290 290,665 665,830 830,965 965,1190
the weights of the neural

717
00:23:52,010 --> 00:23:53,990
0,260 700,1100 1180,1530 1530,1755 1755,1980
network to decrease the loss

718
00:23:53,990 --> 00:23:56,140
0,345 345,750 750,1050 1050,1430
to optimize that objective.|
|

719
00:23:57,080 --> 00:23:58,890
0,245 245,410 410,730 1110,1460 1460,1810
We are specifically concerned with
我们特别关注以某种方式改变这些权重w，以将我们的损失降至最低。

720
00:23:59,030 --> 00:24:01,180
0,400 420,740 740,1100 1100,1390 1890,2150
changing those weights w in

721
00:24:01,180 --> 00:24:02,635
0,135 135,405 405,705 705,1185 1185,1455
a way to minimize our

722
00:24:02,635 --> 00:24:03,900
0,305
loss.|
|

723
00:24:04,740 --> 00:24:06,100
0,275 275,425 425,620 620,940 960,1360
If you look closer here.|
如果你仔细看这里。|

724
00:24:07,100 --> 00:24:09,605
0,240 240,500 970,1370 1840,2220 2220,2505
We are considering only the
我们只考虑权重相对于输入数据以及对应的标签X和Y的变化。

725
00:24:09,605 --> 00:24:11,170
0,305 325,600 600,750 750,1185 1185,1565
changes of the weights with

726
00:24:11,220 --> 00:24:13,120
0,400 630,980 980,1325 1325,1610 1610,1900
respect to the input data

727
00:24:13,440 --> 00:24:15,610
0,335 335,560 560,1220 1220,1510 1770,2170
and the corresponding label X

728
00:24:15,900 --> 00:24:17,180
0,320 320,640
and y.|
|

729
00:24:17,990 --> 00:24:20,095
0,365 365,730 1080,1415 1415,1730 1730,2105
In contrast, in thinking about
相比之下，在思考对抗性攻击时，我们现在提出了一个不同的问题。

730
00:24:20,095 --> 00:24:23,250
0,965 1105,1505 2275,2580 2580,2820 2820,3155
adversarial attacks, we now ask

731
00:24:23,480 --> 00:24:25,120
0,305 305,575 575,940
a different question.|
|

732
00:24:25,220 --> 00:24:26,850
0,335 335,545 545,725 725,1310 1310,1630
How can we modify that
我们如何修改输入数据，例如图像，以增加误差？

733
00:24:26,930 --> 00:24:28,810
0,290 290,580 1050,1355 1355,1625 1625,1880
input data, for example that

734
00:24:28,810 --> 00:24:31,225
0,290 760,1080 1080,1400 1450,1850 2080,2415
image, in order to increase

735
00:24:31,225 --> 00:24:32,580
0,225 225,515
the error?|
|

736
00:24:33,470 --> 00:24:35,680
0,320 320,515 515,790 930,1420
In the network prediction.|
在网络预测中。|

737
00:24:36,080 --> 00:24:38,065
0,610 930,1235 1235,1415 1415,1640 1640,1985
Concretely, how does a small
具体地说，输入数据X中微小的微扰是如何变化的？

738
00:24:38,065 --> 00:24:40,900
0,395 805,1170 1170,1530 1530,2135 2515,2835
change of tiny perturbation in

739
00:24:40,900 --> 00:24:43,000
0,270 270,510 510,765 765,1130
the input data X?|
|

740
00:24:43,250 --> 00:24:45,235
0,365 365,620 620,845 845,1600 1650,1985
Result in a maximal increase
导致损失最大幅度的增加。

741
00:24:45,235 --> 00:24:46,860
0,210 210,360 360,635
in the loss.|
|

742
00:24:46,860 --> 00:24:47,880
0,210 210,390 390,645 645,870 870,1020
What that means is that
这意味着我们可以固定权重，保持网络不变，并研究如何干扰、更改和操纵输入数据，以尝试增加损失函数。

743
00:24:47,880 --> 00:24:49,130
0,150 150,330 330,600 600,810 810,1250
we can fix the weights,

744
00:24:49,480 --> 00:24:50,690
0,305 305,470 470,695 695,935 935,1210
keep the network the same,

745
00:24:51,160 --> 00:24:52,260
0,305 305,485 485,680 680,920 920,1100
and look at how we

746
00:24:52,260 --> 00:24:53,700
0,150 150,675 675,900 900,1200 1200,1440
can perturb and change and

747
00:24:53,700 --> 00:24:55,875
0,660 660,960 960,1230 1230,1520 1870,2175
manipulate that input data to

748
00:24:55,875 --> 00:24:57,390
0,255 255,605 685,1020 1020,1260 1260,1515
try to increase that loss

749
00:24:57,390 --> 00:24:58,320
0,350
function.|
|

750
00:24:59,910 --> 00:25:01,340
0,380 380,710 710,935 935,1175 1175,1430
An extension of this idea
这个想法的扩展是由麻省理工学院的一群学生开发和提出的，他们利用对抗性微扰的想法，创造了一个算法，不仅可以在二维内合成对抗性实现，而且可以使用一组不同的变换，比如旋转，颜色变化，其他类型的微扰，在三维内综合实现。

751
00:25:01,340 --> 00:25:03,185
0,255 255,620 730,1050 1050,1370 1480,1845
was developed and presented by

752
00:25:03,185 --> 00:25:04,370
0,255 255,435 435,630 630,900 900,1185
a group of students right

753
00:25:04,370 --> 00:25:05,720
0,180 180,375 375,710 880,1155 1155,1350
here at MIT, where they

754
00:25:05,720 --> 00:25:07,880
0,210 210,480 480,825 825,1190 1330,2160
took this idea of adversarial

755
00:25:07,880 --> 00:25:10,870
0,500 1120,1520 1690,2040 2040,2390 2470,2990
perturbation and created an algorithm

756
00:25:10,890 --> 00:25:12,940
0,320 320,640 690,1415 1415,1730 1730,2050
that could synthesize not only

757
00:25:13,080 --> 00:25:15,730
0,1010 1010,1690 1770,2120 2120,2360 2360,2650
adversarial realizations in two d,

758
00:25:16,110 --> 00:25:17,830
0,400 480,800 800,1100 1100,1400 1400,1720
but actually in three d

759
00:25:18,660 --> 00:25:20,120
0,395 395,695 695,905 905,1145 1145,1460
using a set of different

760
00:25:20,120 --> 00:25:23,410
0,740 820,1155 1155,1700 2530,2910 2910,3290
transformations like rotations, color changes,

761
00:25:23,610 --> 00:25:25,640
0,365 365,635 635,830 830,1330
other types of perturbations.|
|

762
00:25:26,020 --> 00:25:28,580
0,400 510,910 960,1360 1440,1840 1950,2560
Then, using those learned perturbations,
然后，利用这些习得的扰动，他们获取了这些信息，并使用三维打印实际合成了对象，这些打印被设计为神经网络的对抗性例子。

763
00:25:29,020 --> 00:25:31,580
0,320 320,545 545,850 1350,1750 2160,2560
they took those information and

764
00:25:31,630 --> 00:25:34,610
0,350 350,700 930,1690 2250,2615 2615,2980
actually physically synthesized objects using

765
00:25:34,630 --> 00:25:36,650
0,290 290,485 485,1000 1440,1730 1730,2020
three d printing that were

766
00:25:36,820 --> 00:25:39,590
0,400 720,1070 1070,1370 1370,2390 2390,2770
designed to be adversarial examples

767
00:25:39,730 --> 00:25:41,260
0,260 260,395 395,605 605,880
for a neural network.|
|

768
00:25:41,490 --> 00:25:43,415
0,305 305,575 575,860 860,1390 1650,1925
And so they printed three
所以他们打印了三个d，打印了一堆这些乌龟的例子，三个物理物体，这样那些图像就会完全愚弄神经网络，它看着图像，试图分类正确的标签。

769
00:25:43,415 --> 00:25:44,495
0,165 165,480 480,660 660,885 885,1080
d, printed a bunch of

770
00:25:44,495 --> 00:25:46,670
0,255 255,635 685,1085 1135,1715 1885,2175
these examples of turtles, three

771
00:25:46,670 --> 00:25:49,210
0,210 210,530 790,1190 1870,2205 2205,2540
d physical objects, such that

772
00:25:49,650 --> 00:25:51,460
0,400 420,725 725,1030 1230,1520 1520,1810
images of those would be

773
00:25:51,570 --> 00:25:54,680
0,400 720,1300 2100,2465 2465,2740 2790,3110
completely fooling neural network that

774
00:25:54,680 --> 00:25:55,820
0,210 210,375 375,540 540,825 825,1140
looked at that image and

775
00:25:55,820 --> 00:25:57,395
0,270 270,525 525,1095 1095,1320 1320,1575
tried to classify the correct

776
00:25:57,395 --> 00:25:58,280
0,335
label.|
|

777
00:25:59,050 --> 00:26:00,825
0,290 290,580 840,1190 1190,1520 1520,1775
And so this shows that
这表明，这种对抗性例子和扰动的概念可以扩展到现实世界中的不同领域，现在我们可以考虑构建这些合成例子，旨在探索神经网络的弱点并揭示其脆弱性。

778
00:26:00,825 --> 00:26:02,805
0,180 180,485 535,840 840,1650 1650,1980
this notion of adversarial examples

779
00:26:02,805 --> 00:26:05,270
0,300 300,845 1225,1625 1645,2045 2065,2465
and perturbation can extend to

780
00:26:05,350 --> 00:26:06,750
0,380 380,875 875,1100 1100,1220 1220,1400
different domains in the real

781
00:26:06,750 --> 00:26:08,235
0,320 640,945 945,1155 1155,1335 1335,1485
world, where now we can

782
00:26:08,235 --> 00:26:10,910
0,195 195,515 1375,1950 1950,2175 2175,2675
think about constructing these synthetic

783
00:26:10,990 --> 00:26:13,245
0,400 810,1070 1070,1295 1295,1660 1920,2255
examples that are designed to

784
00:26:13,245 --> 00:26:14,910
0,420 420,645 645,840 840,1440 1440,1665
probe at the weaknesses of

785
00:26:14,910 --> 00:26:17,055
0,255 255,530 820,1095 1095,1640 1840,2145
neural networks and expose their

786
00:26:17,055 --> 00:26:18,460
0,635
vulnerabilities.|
|

787
00:26:20,010 --> 00:26:22,100
0,400 630,950 950,1390 1500,1865 1865,2090
Finally, and we've discussed a
最后，我们昨天也讨论了很多关于这个想法的问题，这种算法偏差的概念是否意味着，随着人工智能系统在社会上得到更广泛的部署，它们很容易受到我之前强调的许多问题造成的重大偏差的影响？

788
00:26:22,100 --> 00:26:24,820
0,195 195,405 405,675 675,1070 2320,2720
lot about this idea yesterday

789
00:26:24,870 --> 00:26:26,740
0,305 305,610 840,1130 1130,1420 1470,1870
as well, is this notion

790
00:26:26,790 --> 00:26:29,060
0,400 420,920 920,1420 1770,2045 2045,2270
of algorithmic bias the fact

791
00:26:29,060 --> 00:26:30,740
0,255 255,560 580,900 900,1220 1360,1680
that as AI systems become

792
00:26:30,740 --> 00:26:33,010
0,320 520,1160 1180,1635 1635,1890 1890,2270
more broadly deployed in society,

793
00:26:33,330 --> 00:26:35,470
0,275 275,515 515,1300 1560,1850 1850,2140
that they're susceptible to significant

794
00:26:35,700 --> 00:26:37,925
0,760 810,1210 1260,1660 1740,2045 2045,2225
biases resulting from many of

795
00:26:37,925 --> 00:26:39,380
0,210 210,540 540,780 780,930 930,1455
those issues that I highlighted

796
00:26:39,380 --> 00:26:40,320
0,290
earlier?|
|

797
00:26:40,390 --> 00:26:41,805
0,320 320,575 575,800 800,1070 1070,1415
And these can lead to
而这些可能会导致非常真实的有害后果，正如我们在这门课程的实验和课程中一直在探索的那样。

798
00:26:41,805 --> 00:26:44,820
0,345 345,725 835,1575 1575,1865 2665,3015
very real detrimental consequences, as

799
00:26:44,820 --> 00:26:46,530
0,270 270,480 480,890 1090,1440 1440,1710
we've been exploring throughout this

800
00:26:46,530 --> 00:26:48,405
0,320 490,795 795,1080 1080,1635 1635,1875
course in both labs and

801
00:26:48,405 --> 00:26:49,480
0,515
lectures.|
|

802
00:26:50,990 --> 00:26:52,915
0,400 870,1220 1220,1550 1550,1775 1775,1925
So the examples that I
因此，我到目前为止所举的例子肯定不是神经网络所有局限性的详尽清单。

803
00:26:52,915 --> 00:26:54,150
0,255 255,495 495,690 690,915 915,1235
covered so far are certainly

804
00:26:54,320 --> 00:26:56,425
0,400 450,850 900,1535 1535,1820 1820,2105
not an exhaustive list of

805
00:26:56,425 --> 00:26:57,925
0,225 225,405 405,935 985,1260 1260,1500
all the limitations of neural

806
00:26:57,925 --> 00:26:58,980
0,275
networks.|
|

807
00:26:59,650 --> 00:27:00,600
0,275 275,440 440,635 635,815 815,950
But I'd like to think
但我不想把这些严格地看作是限制。

808
00:27:00,600 --> 00:27:02,430
0,120 120,380 400,800 970,1575 1575,1830
of these not strictly as

809
00:27:02,430 --> 00:27:04,120
0,560
limitations.|
|

810
00:27:04,120 --> 00:27:05,935
0,165 165,470 580,885 885,1550 1570,1815
But rather an invitation and
而是对新创新的邀请和动力，这些创新和创造性解决方案旨在试图解决其中一些问题，这些问题确实是当今人工智能和深度学习研究中的悬而未决的问题。

811
00:27:05,935 --> 00:27:07,770
0,105 105,635 715,1035 1035,1355 1435,1835
a motivation for new innovation,

812
00:27:08,570 --> 00:27:10,180
0,400 420,820 900,1160 1160,1295 1295,1610
creative solutions that are aimed

813
00:27:10,180 --> 00:27:11,440
0,180 180,435 435,675 675,1110 1110,1260
at trying to tackle some

814
00:27:11,440 --> 00:27:13,165
0,150 150,375 375,710 1210,1485 1485,1725
of these questions, which are

815
00:27:13,165 --> 00:27:15,370
0,365 805,1110 1110,1415 1555,1920 1920,2205
indeed open problems in AI

816
00:27:15,370 --> 00:27:16,860
0,210 210,405 405,675 675,1040 1090,1490
and deep learning research today.|
|

817
00:27:17,980 --> 00:27:20,100
0,400 630,1040 1040,1490 1490,1790 1790,2120
Specifically, we've tackled and already
具体地说，我们已经解决并已经考虑了解决神经网络中的偏见和不确定性问题的方法。

818
00:27:20,100 --> 00:27:22,560
0,285 285,555 555,890 1600,2000 2110,2460
thought about ways to address

819
00:27:22,560 --> 00:27:24,680
0,350 400,720 720,1250 1330,1725 1725,2120
issues of bias and uncertainty

820
00:27:24,760 --> 00:27:26,480
0,290 290,530 530,820
in neural networks.|
|

821
00:27:26,520 --> 00:27:27,875
0,290 290,560 560,800 800,1010 1010,1355
And now for the remainder
在接下来的演讲中，我想把重点放在深度学习的一些非常令人兴奋的新领域上，解决引入的一些额外限制。

822
00:27:27,875 --> 00:27:29,675
0,270 270,465 465,785 1285,1590 1590,1800
of this talk, I want

823
00:27:29,675 --> 00:27:31,115
0,180 180,455 655,1050 1050,1305 1305,1440
to focus on some of

824
00:27:31,115 --> 00:27:33,365
0,225 225,575 715,1115 1375,1710 1710,2250
the really exciting new frontiers

825
00:27:33,365 --> 00:27:35,120
0,255 255,450 450,755 1045,1575 1575,1755
of deep learning, tackling some

826
00:27:35,120 --> 00:27:37,235
0,120 120,380 400,800 1120,1790 1840,2115
of these additional limitations that

827
00:27:37,235 --> 00:27:38,640
0,275 295,695
are introduced.|
|

828
00:27:39,700 --> 00:27:41,360
0,275 275,515 515,845 845,1210 1260,1660
The first being this idea.|
第一个是这个想法。|

829
00:27:42,410 --> 00:27:43,795
0,320 320,635 635,785 785,1040 1040,1385
That stems from this idea
这源于对神经网络的规模和规模的看法。

830
00:27:43,795 --> 00:27:46,015
0,240 240,435 435,755 1315,1715 1825,2220
of the size and scale

831
00:27:46,015 --> 00:27:47,820
0,300 300,555 555,815
of neural networks.|
|

832
00:27:47,820 --> 00:27:49,250
0,210 210,540 540,885 885,1140 1140,1430
They rely heavily on data,
它们严重依赖数据，它们是海量的，因此可能很难理解神经网络正在挑选的底层结构是什么，如果有的话。

833
00:27:49,510 --> 00:27:51,510
0,350 350,610 1380,1655 1655,1805 1805,2000
they're massive and as a

834
00:27:51,510 --> 00:27:53,115
0,320 520,795 795,930 930,1190 1270,1605
result it can be difficult

835
00:27:53,115 --> 00:27:55,050
0,335 985,1335 1335,1605 1605,1800 1800,1935
to understand what is the

836
00:27:55,050 --> 00:27:57,705
0,260 820,1220 1510,1815 1815,2120 2380,2655
underlying structure, if any, that

837
00:27:57,705 --> 00:27:58,815
0,135 135,360 360,600 600,870 870,1110
the neural network is picking

838
00:27:58,815 --> 00:27:59,840
0,225 225,515
up on.|
|

839
00:28:00,060 --> 00:28:01,265
0,290 290,455 455,680 680,875 875,1205
And so there's a very,
因此，现在有一项非常非常令人兴奋的工作是将结构和更多来自先前人类知识的信息引入神经网络结构，使它们变得更小、更紧凑、更有表现力、更高效。

840
00:28:01,265 --> 00:28:02,780
0,365 385,780 780,1095 1095,1290 1290,1515
very exciting line of work

841
00:28:02,780 --> 00:28:04,940
0,350 790,1170 1170,1485 1485,1820 1840,2160
now that is focused on

842
00:28:04,940 --> 00:28:07,900
0,710 1150,1550 1720,2040 2040,2360 2560,2960
introducing structure and more information

843
00:28:07,950 --> 00:28:10,510
0,365 365,730 810,1175 1175,1540 2160,2560
from prior human knowledge into

844
00:28:10,530 --> 00:28:13,340
0,335 335,610 630,1570 2070,2450 2450,2810
neural network architectures to enable

845
00:28:13,340 --> 00:28:15,040
0,300 300,540 540,860 940,1320 1320,1700
them to become much smaller,

846
00:28:15,600 --> 00:28:18,500
0,350 350,970 1200,1600 1740,2410 2520,2900
more compact, more expressive, more

847
00:28:18,500 --> 00:28:19,640
0,380
efficient.|
|

848
00:28:19,640 --> 00:28:20,555
0,270 270,450 450,570 570,750 750,915
And we're going to see
我们将在今天的演讲中看到这一点，也会在我们接下来的拉米的客座演讲中看到这一点，他将在那里谈到结构编码的想法，以及它如何产生高度表达和高效的神经网络结构。

849
00:28:20,555 --> 00:28:21,635
0,150 150,315 315,480 480,735 735,1080
a bit of this in

850
00:28:21,635 --> 00:28:23,495
0,465 465,725 1075,1470 1470,1740 1740,1860
today's lecture, but also in

851
00:28:23,495 --> 00:28:25,160
0,150 150,455 685,1005 1005,1325 1345,1665
our following guest lecture by

852
00:28:25,160 --> 00:28:26,750
0,530 760,1065 1065,1230 1230,1365 1365,1590
rami, where he will talk

853
00:28:26,750 --> 00:28:29,000
0,350 490,890 910,1230 1230,1550 1720,2250
about this idea of structural

854
00:28:29,000 --> 00:28:30,430
0,525 525,720 720,960 960,1155 1155,1430
encoding and how that can

855
00:28:31,410 --> 00:28:33,545
0,365 365,730 870,1270 1410,1910 1910,2135
result in highly expressive and

856
00:28:33,545 --> 00:28:36,080
0,335 625,975 975,1215 1215,2075
efficient neural network architectures.|
|

857
00:28:37,230 --> 00:28:38,570
0,290 290,530 530,875 875,1130 1130,1340
The second issue that I'm
我要谈的第二个问题与此密切相关。

858
00:28:38,570 --> 00:28:39,905
0,180 180,500 550,885 885,1095 1095,1335
going to talk about and

859
00:28:39,905 --> 00:28:41,990
0,365 655,960 960,1265 1285,1685 1705,2085
discuss and is very related

860
00:28:41,990 --> 00:28:43,680
0,225 225,470
to this.|
|

861
00:28:43,680 --> 00:28:45,320
0,375 375,720 720,1035 1035,1320 1320,1640
Problem of scale and structure
规模和结构的问题是，我们现在如何鼓励神经网络在数据之外更有效地进行推断。

862
00:28:46,030 --> 00:28:47,775
0,400 450,770 770,980 980,1270 1350,1745
is how we can encourage

863
00:28:47,775 --> 00:28:50,025
0,360 360,635 655,930 930,1205 1255,2250
neural networks to now extrapolate

864
00:28:50,025 --> 00:28:52,740
0,330 330,725 1285,1635 1635,1985
more effectively beyond data.|
|

865
00:28:52,740 --> 00:28:54,150
0,315 315,570 570,825 825,1110 1110,1410
And this will tie very
这将非常巧妙地与我们一直在讨论的关于生成性人工智能的一些想法联系在一起，这将是我将在今天剩余的演讲中讨论的最后一个话题。

866
00:28:54,150 --> 00:28:55,695
0,480 480,770 940,1200 1200,1320 1320,1545
neatly into some of the

867
00:28:55,695 --> 00:28:57,375
0,315 315,585 585,795 795,1175 1315,1680
ideas we've been discussing around

868
00:28:57,375 --> 00:28:59,055
0,600 600,965 1165,1440 1440,1560 1560,1680
generative AI, and will be

869
00:28:59,055 --> 00:29:00,750
0,150 150,425 835,1235 1255,1515 1515,1695
the last topic that I'm

870
00:29:00,750 --> 00:29:02,160
0,150 150,390 390,735 735,1110 1110,1410
going to discuss in the

871
00:29:02,160 --> 00:29:04,100
0,320 400,690 690,1050 1050,1310
remainder of today's lecture.|
|

872
00:29:05,340 --> 00:29:06,935
0,400 660,920 920,1160 1160,1370 1370,1595
Okay, so let's focus on
好的，让我们先来关注一下这个。

873
00:29:06,935 --> 00:29:08,320
0,180 180,485
this first.|
|

874
00:29:08,320 --> 00:29:10,150
0,300 300,650 1120,1440 1440,1665 1665,1830
First concept of how we
第一个概念是我们如何利用更多的人类领域知识来将更大的结构编码到深度学习算法中。

875
00:29:10,150 --> 00:29:11,680
0,165 165,470 730,1050 1050,1275 1275,1530
can leverage more of our

876
00:29:11,680 --> 00:29:14,155
0,315 315,680 700,1100 1540,1830 1830,2475
human domain knowledge to encode

877
00:29:14,155 --> 00:29:16,110
0,365 415,815 1045,1395 1395,1650 1650,1955
greater structure into deep learning

878
00:29:16,220 --> 00:29:17,440
0,520
algorithms.|
|

879
00:29:18,110 --> 00:29:19,530
0,275 275,440 440,785 785,1070 1070,1420
Turns out we've already seen
到目前为止，我们已经在课程中看到了这方面的例子，特别是CNN中强调的用于空间处理和视觉数据的例子。

880
00:29:19,550 --> 00:29:21,115
0,400 420,695 695,935 935,1295 1295,1565
examples of this in the

881
00:29:21,115 --> 00:29:24,625
0,195 195,420 420,725 2185,2585 2815,3510
course so far, particularly highlighted

882
00:29:24,625 --> 00:29:27,130
0,300 300,905 1075,1475 1525,1875 1875,2505
in CNN, used for spatial

883
00:29:27,130 --> 00:29:29,440
0,380 550,855 855,1140 1140,1520
processing and visual data.|
|

884
00:29:29,510 --> 00:29:31,140
0,320 320,530 530,815 815,1310 1310,1630
As we saw, cnns were
正如我们所看到的，CNN被引入作为一种高效的架构来捕获数据中的空间依赖关系。

885
00:29:31,190 --> 00:29:32,370
0,335 335,530 530,665 665,860 860,1180
introduced to be a highly

886
00:29:32,450 --> 00:29:35,410
0,350 350,700 1380,1775 1775,2435 2435,2960
efficient architecture for capturing spatial

887
00:29:35,410 --> 00:29:37,300
0,660 660,855 855,1130
dependencies in data.|
|

888
00:29:37,370 --> 00:29:39,190
0,275 275,455 455,760 870,1270 1440,1820
And the core idea behind
CNN背后的核心思想是卷积的概念，它如何通过这种有效的卷积运算将输入图像的空间相似和相邻部分相互关联。

889
00:29:39,190 --> 00:29:40,975
0,540 540,810 810,1110 1110,1485 1485,1785
CNN was this notion of

890
00:29:40,975 --> 00:29:43,500
0,635 1315,1665 1665,1890 1890,2145 2145,2525
convolution, how it would relate

891
00:29:43,760 --> 00:29:47,620
0,790 1170,1570 1710,2110 2550,3350 3350,3860
spatially similar and adjacent portions

892
00:29:47,620 --> 00:29:50,140
0,350 520,900 900,1260 1260,1640 2260,2520
of an input image to

893
00:29:50,140 --> 00:29:51,490
0,150 150,435 435,720 720,990 990,1350
each other through this efficient

894
00:29:51,490 --> 00:29:53,460
0,650 880,1280
convolution operation.|
|

895
00:29:53,760 --> 00:29:55,430
0,290 290,485 485,755 755,1040 1040,1670
And we saw that convolution
我们看到卷积能够有效地从视觉数据中提取局部特征。

896
00:29:55,430 --> 00:29:57,490
0,285 285,590 730,1130 1360,1710 1710,2060
was able to be effective

897
00:29:57,510 --> 00:29:59,795
0,400 510,1130 1130,1450 1470,1870 1980,2285
at extracting local features from

898
00:29:59,795 --> 00:30:01,260
0,300 300,695
visual data.|
|

899
00:30:01,260 --> 00:30:02,835
0,225 225,510 510,890 940,1260 1260,1575
And then using this local
然后使用该局部特征提取来实现诸如分类、目标检测等下游预测任务。

900
00:30:02,835 --> 00:30:05,870
0,395 475,1175 1315,1665 1665,2015 2635,3035
feature extraction to then enable

901
00:30:05,950 --> 00:30:08,750
0,710 710,1205 1205,1510 1860,2180 2180,2800
downstream predictive tasks like classification,

902
00:30:09,280 --> 00:30:10,880
0,365 365,860 860,1115 1115,1310 1310,1600
object detection and so forth.|
|

903
00:30:13,010 --> 00:30:14,695
0,335 335,635 635,1000 1290,1550 1550,1685
What about now, instead of
现在，如果我们考虑更不规则的数据结构，而不是图像，情况会怎样？

904
00:30:14,695 --> 00:30:16,290
0,275 355,630 630,870 870,1215 1215,1595
images, if we consider more

905
00:30:16,400 --> 00:30:18,980
0,760 930,1310 1310,1690
irregular data structures?|
|

906
00:30:19,440 --> 00:30:20,900
0,275 275,425 425,635 635,1280 1280,1460
That can be encoded in
它可以通过2个深度像素网格以外的不同方式进行编码。

907
00:30:20,900 --> 00:30:23,075
0,270 270,650 820,1220 1780,2025 2025,2175
different ways beyond 2 deep

908
00:30:23,075 --> 00:30:24,840
0,270 270,495 495,995
grids of pixels.|
|

909
00:30:25,150 --> 00:30:27,170
0,560 560,725 725,980 980,1360 1620,2020
Graphs are a particularly powerful
图是一种编码结构信息的特别强大的方法，在许多情况下，图或网络的概念对于我们可能正在考虑的问题非常重要。

910
00:30:27,250 --> 00:30:29,840
0,365 365,730 750,1510 1530,2170 2190,2590
way of encoding structural information,

911
00:30:30,340 --> 00:30:31,395
0,305 305,500 500,680 680,875 875,1055
and it can be in

912
00:30:31,395 --> 00:30:33,150
0,240 240,600 600,995 1225,1500 1500,1755
many cases that the notion

913
00:30:33,150 --> 00:30:34,095
0,240 240,375 375,615 615,825 825,945
of a graph or a

914
00:30:34,095 --> 00:30:36,495
0,275 925,1325 1435,1830 1830,2190 2190,2400
network is very important to

915
00:30:36,495 --> 00:30:37,575
0,180 180,510 510,780 780,930 930,1080
the problem that we may

916
00:30:37,575 --> 00:30:38,780
0,240 240,605
be considering.|
|

917
00:30:40,690 --> 00:30:42,600
0,640 660,950 950,1145 1145,1450 1530,1910
Graphs as a structure and
作为一种结构和表示数据的示例，图确实存在于我们周围，跨越许多不同的应用程序。

918
00:30:42,600 --> 00:30:44,865
0,360 360,735 735,1130 1210,1500 1500,2265
as an example for representing

919
00:30:44,865 --> 00:30:46,545
0,335 385,780 780,1125 1125,1410 1410,1680
data are really all around

920
00:30:46,545 --> 00:30:48,590
0,335 475,840 840,1140 1140,1475 1645,2045
us, across many different applications.|
|

921
00:30:49,460 --> 00:30:52,205
0,480 480,830 850,1230 1230,1610 2200,2745
Ranging from social networks, defining
范围从社交网络，定义了我们彼此之间的联系方式。

922
00:30:52,205 --> 00:30:53,735
0,240 240,420 420,555 555,845 1135,1530
how we are all connected

923
00:30:53,735 --> 00:30:55,140
0,240 240,360 360,635
to each other.|
|

924
00:30:55,140 --> 00:30:57,600
0,380 520,915 915,1290 1290,1670 2080,2460
To state machines that describe
到描述一个过程如何从一个状态转换到另一个状态的状态机，到人类流动性、城市交通到化学分子和生物网络的度量和模型。

925
00:30:57,600 --> 00:30:59,090
0,240 240,405 405,710 760,1125 1125,1490
how a process can transition

926
00:30:59,560 --> 00:31:00,830
0,290 290,530 530,785 785,980 980,1270
from one state to another

927
00:31:01,540 --> 00:31:03,945
0,290 290,580 720,1120 1740,2120 2120,2405
to metrics and models of

928
00:31:03,945 --> 00:31:07,035
0,240 240,755 1015,1410 1410,1805 2695,3090
human mobility, urban transport to

929
00:31:07,035 --> 00:31:09,800
0,390 390,1295 1465,1800 1800,2135 2365,2765
chemical molecules and biological networks.|
|

930
00:31:10,890 --> 00:31:12,620
0,275 275,410 410,910 1080,1445 1445,1730
And the motivation across all
所有这些例子的动机是，如此多的真实世界数据的实例自然地落入这种网络或图形结构的概念中，但这种结构不容易被标准的数据编码或几何图形结合或捕获。

931
00:31:12,620 --> 00:31:14,540
0,285 285,650 790,1080 1080,1370 1600,1920
these examples is that so

932
00:31:14,540 --> 00:31:16,325
0,270 270,960 960,1215 1215,1485 1485,1785
many instances of real world

933
00:31:16,325 --> 00:31:19,450
0,365 1165,1530 1530,1895 2455,2790 2790,3125
data fall into naturally this

934
00:31:19,590 --> 00:31:20,825
0,305 305,455 455,590 590,880 900,1235
idea of a network or

935
00:31:20,825 --> 00:31:23,465
0,300 300,665 1375,1775 2095,2400 2400,2640
graph structure, but that this

936
00:31:23,465 --> 00:31:27,275
0,335 415,815 865,1265 1465,2165 3205,3810
structure cannot be readily incorporated

937
00:31:27,275 --> 00:31:29,270
0,285 285,605 625,1025 1075,1475 1675,1995
or captured by standard data

938
00:31:29,270 --> 00:31:31,580
0,630 630,975 975,1760
encodings or geometries.|
|

939
00:31:31,590 --> 00:31:32,480
0,275 275,425 425,590 590,710 710,890
And so we're going to
因此，我们将稍微讨论一下将图形作为一种结构，以及如何尝试将编码在图形中的信息表示出来，以构建能够处理这些数据类型的神经网络。

940
00:31:32,480 --> 00:31:33,500
0,180 180,330 330,480 480,720 720,1020
talk a little bit about

941
00:31:33,500 --> 00:31:35,120
0,420 420,540 540,705 705,1010 1300,1620
graphs as a structure and

942
00:31:35,120 --> 00:31:36,275
0,255 255,465 465,675 675,945 945,1155
how we can try to

943
00:31:36,275 --> 00:31:38,810
0,275 445,845 925,1325 1735,2415 2415,2535
represent this information encoded in

944
00:31:38,810 --> 00:31:40,550
0,135 135,410 670,930 930,1190 1390,1740
a graph to build neural

945
00:31:40,550 --> 00:31:42,965
0,260 310,710 1090,1395 1395,1700 2020,2415
networks that are capable of

946
00:31:42,965 --> 00:31:45,240
0,695 715,1050 1050,1335 1335,1685
handling these data types.|
|

947
00:31:46,560 --> 00:31:47,525
0,290 290,485 485,680 680,830 830,965
To see how we can
为了看看我们如何做到这一点，并处理这个问题，让我们花一点时间回到CNN。

948
00:31:47,525 --> 00:31:49,625
0,165 165,390 390,725 1165,1565 1765,2100
do this and, and approach

949
00:31:49,625 --> 00:31:51,470
0,240 240,545 895,1365 1365,1605 1605,1845
this problem, let's go back

950
00:31:51,470 --> 00:31:52,970
0,180 180,270 270,530 760,1160 1210,1500
for a moment to the

951
00:31:52,970 --> 00:31:54,060
0,500
CNN.|
|

952
00:31:54,260 --> 00:31:55,915
0,335 335,845 845,1115 1115,1385 1385,1655
In CNN we saw that
在美国有线电视新闻网中，我们看到了这个矩形的二维内核，它有效地在图像中滑动，关注并拾取存在于这个二维网格中的特征。

953
00:31:55,915 --> 00:31:58,195
0,225 225,510 510,875 1045,1955 2005,2280
we have this rectangular two

954
00:31:58,195 --> 00:32:00,385
0,210 210,695 1225,1625 1735,1995 1995,2190
d kernel, and what it

955
00:32:00,385 --> 00:32:02,070
0,335 415,810 810,1080 1080,1320 1320,1685
effectively does is it slides

956
00:32:02,240 --> 00:32:04,500
0,380 380,605 605,850 1500,1880 1880,2260
across the image, paying attention

957
00:32:04,520 --> 00:32:06,240
0,350 350,665 665,1025 1025,1370 1370,1720
and picking up to features

958
00:32:06,620 --> 00:32:08,785
0,380 380,760 990,1390 1620,1955 1955,2165
that exist across this two

959
00:32:08,785 --> 00:32:09,800
0,180 180,515
d grid.|
|

960
00:32:10,060 --> 00:32:12,660
0,335 335,590 590,910 1980,2345 2345,2600
And all this function is
这个函数所做的就是定义卷积运算的元素乘法。

961
00:32:12,660 --> 00:32:14,180
0,255 255,510 510,720 720,1040 1120,1520
doing is that element wise

962
00:32:14,380 --> 00:32:17,060
0,830 830,1100 1100,1570 1830,2105 2105,2680
multiplication that defines the convolution

963
00:32:17,260 --> 00:32:18,380
0,400
operation.|
|

964
00:32:18,450 --> 00:32:20,015
0,275 275,980 980,1130 1130,1310 1310,1565
And intuitively we can think
直观地说，我们可以把卷积看作是在图像上一块一块迭代地滑动这个核心的可视化，继续尝试提取在这两个D中捕捉到的信息特征。

965
00:32:20,015 --> 00:32:22,510
0,255 255,845 1195,1530 1530,1770 1770,2495
about convolution as this visualization

966
00:32:22,650 --> 00:32:25,270
0,350 350,730 750,1085 1085,1570 1740,2620
of sliding this kernel iteratively

967
00:32:25,350 --> 00:32:27,530
0,365 365,665 665,1000 1590,1955 1955,2180
patch by patch across the

968
00:32:27,530 --> 00:32:30,260
0,260 1000,1395 1395,1790 2080,2430 2430,2730
image, continuing on to try

969
00:32:30,260 --> 00:32:32,105
0,240 240,435 435,690 690,1040 1270,1845
to pick up on informative

970
00:32:32,105 --> 00:32:33,770
0,305 565,825 825,990 990,1295 1375,1665
features that are captured in

971
00:32:33,770 --> 00:32:34,980
0,240 240,465 465,740
this two d.|
|

972
00:32:35,200 --> 00:32:36,860
0,560 560,940
Pixel space.|
像素空间。|

973
00:32:37,680 --> 00:32:39,395
0,350 350,695 695,980 980,1190 1190,1715
This core idea of convolution
卷积在滑动中的核心思想这个特征提取过滤器是现在我们如何将这个思想扩展到基于图形的核心。

974
00:32:39,395 --> 00:32:41,710
0,330 330,725 775,1175 1285,1685 1915,2315
in sliding this this feature

975
00:32:41,790 --> 00:32:43,790
0,545 545,850 1410,1685 1685,1835 1835,2000
extraction filter is at the

976
00:32:43,790 --> 00:32:45,305
0,225 225,495 495,830 1060,1350 1350,1515
heart of now how we

977
00:32:45,305 --> 00:32:47,140
0,240 240,510 510,815 835,1235 1435,1835
can extend this idea to

978
00:32:47,280 --> 00:32:48,960
0,400 540,940
graph based.|
|

979
00:32:49,000 --> 00:32:50,160
0,400
Data.|
数据。|

980
00:32:50,800 --> 00:32:52,880
0,335 335,905 905,1270 1380,1730 1730,2080
This motivates a very recent
这激发了一种称为图卷积网络的最新类型的神经网络结构，其思想与标准的CNN非常相似。

981
00:32:53,050 --> 00:32:54,350
0,275 275,440 440,695 695,935 935,1300
type of neural network architecture

982
00:32:54,700 --> 00:32:57,465
0,365 365,710 710,1475 1475,1750 2490,2765
called graph convolutional networks, where

983
00:32:57,465 --> 00:32:58,880
0,225 225,480 480,720 720,1035 1035,1415
the idea is very similar

984
00:32:59,020 --> 00:33:00,940
0,290 290,580 630,1210
to standard cnns.|
|

985
00:33:00,940 --> 00:33:02,395
0,270 270,560 700,1035 1035,1245 1245,1455
We have some type of
我们有某种类型的重量核。

986
00:33:02,395 --> 00:33:04,120
0,335 475,1055
weight kernel.|
|

987
00:33:04,120 --> 00:33:05,365
0,315 315,555 555,720 720,915 915,1245
And all it is is
它只是一个像以前和现在一样由神经网络权重定义的矩阵，而不是在二维像素网格上滑动内核。

988
00:33:05,365 --> 00:33:07,840
0,300 300,1025 1075,1455 1455,1835 2095,2475
a matrix defined by neural

989
00:33:07,840 --> 00:33:10,315
0,255 255,780 780,1050 1050,1370 2170,2475
network weights as before, and

990
00:33:10,315 --> 00:33:12,505
0,305 685,1085 1105,1455 1455,1835 1855,2190
now, rather than sliding that

991
00:33:12,505 --> 00:33:14,250
0,480 480,840 840,1170 1170,1440 1440,1745
kernel across a two d

992
00:33:14,420 --> 00:33:16,160
0,335 335,575 575,1120
grid of pixels.|
|

993
00:33:16,160 --> 00:33:18,320
0,270 270,660 660,980 1060,1460 1870,2160
The kernel goes around to
内核转到图形的不同部分。

994
00:33:18,320 --> 00:33:19,810
0,290 520,855 855,1050 1050,1200 1200,1490
different parts of the graph.|
|

995
00:33:20,940 --> 00:33:23,225
0,365 365,710 710,1090 1110,1750 2010,2285
Defined by particular nodes, and
由特定节点定义，它查看该节点的邻居是什么，它是如何连接到相邻节点的。

996
00:33:23,225 --> 00:33:24,905
0,165 165,420 420,785 1165,1485 1485,1680
it looks at what the

997
00:33:24,905 --> 00:33:26,345
0,275 385,675 675,945 945,1215 1215,1440
neighbors of the of that

998
00:33:26,345 --> 00:33:28,025
0,345 345,605 745,1035 1035,1350 1350,1680
node is, how it's connected

999
00:33:28,025 --> 00:33:29,740
0,240 240,765 765,1205
to adjacent nodes.|
|

1000
00:33:29,740 --> 00:33:30,910
0,195 195,375 375,705 705,900 900,1170
And the kernel is used
而核被用来从图中的局部邻域中提取特征，然后捕获结构中的相关信息。

1001
00:33:30,910 --> 00:33:33,070
0,350 370,770 790,1190 1600,1920 1920,2160
to extract features from that

1002
00:33:33,070 --> 00:33:35,160
0,320 340,740 1210,1560 1560,1800 1800,2090
local neighborhood within the graph

1003
00:33:35,540 --> 00:33:37,260
0,275 275,550 750,1150 1200,1460 1460,1720
to then capture the relevant

1004
00:33:37,640 --> 00:33:40,060
0,400 780,1145 1145,1430 1430,1750
information within the structure.|
|

1005
00:33:40,820 --> 00:33:42,210
0,290 290,440 440,590 590,1070 1070,1390
So we can visualize this
所以我们可以在这里具体地看到这一点，现在不是看到二维内核查看图像的特定块，我要强调的是内核是如何关注节点及其本地邻居的，以及有效的图形卷积操作。它所做的是拾取特定节点的局部连通性，并学习与这些边相关联的权重，定义该局部连通性。

1006
00:33:42,290 --> 00:33:44,395
0,455 455,725 725,1090 1110,1510 1860,2105
concretely here, where now instead

1007
00:33:44,395 --> 00:33:46,615
0,165 165,485 775,1175 1705,1995 1995,2220
of seeing the two d

1008
00:33:46,615 --> 00:33:48,030
0,405 405,660 660,855 855,1065 1065,1415
kernel looking at a particular

1009
00:33:48,080 --> 00:33:49,795
0,320 320,500 500,620 620,880 1470,1715
patch of an image, what

1010
00:33:49,795 --> 00:33:51,520
0,210 210,780 780,1035 1035,1385 1435,1725
I'm highlighting is how the

1011
00:33:51,520 --> 00:33:53,710
0,465 465,780 780,1080 1080,1460 1870,2190
kernel is paying attention to

1012
00:33:53,710 --> 00:33:55,140
0,360 360,525 525,720 720,1035 1035,1430
node and its local neighbors

1013
00:33:55,970 --> 00:33:57,960
0,335 335,670 780,1070 1070,1340 1340,1990
and effectively the graph convolution

1014
00:33:58,250 --> 00:34:00,730
0,400 870,1130 1130,1370 1370,1630 2100,2480
operation. What it's doing is

1015
00:34:00,730 --> 00:34:02,010
0,330 330,615 615,825 825,990 990,1280
picking up on the local

1016
00:34:02,240 --> 00:34:05,010
0,580 1110,1475 1475,1820 1820,2195 2195,2770
connectivity of a particular node

1017
00:34:05,570 --> 00:34:07,360
0,400 450,850 930,1400 1400,1550 1550,1790
and learning weights that are

1018
00:34:07,360 --> 00:34:09,640
0,380 610,930 930,1200 1200,1730 1780,2280
associated with those edges, defining

1019
00:34:09,640 --> 00:34:11,280
0,195 195,500 580,1160
that local connectivity.|
|

1020
00:34:11,920 --> 00:34:13,250
0,305 305,635 635,815 815,1025 1025,1330
The kernel is just then
然后，将内核迭代应用于图中的每个节点，试图提取有关本地连接性的信息。

1021
00:34:13,390 --> 00:34:15,555
0,395 395,1240 1320,1610 1610,1865 1865,2165
applied iteratively to each node

1022
00:34:15,555 --> 00:34:17,295
0,105 105,240 240,515 1045,1410 1410,1740
in the graph, trying to

1023
00:34:17,295 --> 00:34:19,070
0,365 535,935 1015,1335 1335,1515 1515,1775
extract information about the local

1024
00:34:19,150 --> 00:34:20,520
0,580
connectivity.|
|

1025
00:34:20,560 --> 00:34:21,890
0,350 350,665 665,890 890,1040 1040,1330
Such that we can go
这样我们就可以四处走动，最后开始把所有这些信息聚集在一起，然后根据当地的观测结果对权重进行编码和更新。

1026
00:34:22,000 --> 00:34:23,925
0,320 320,545 545,850 1380,1700 1700,1925
around and around and at

1027
00:34:23,925 --> 00:34:26,040
0,180 180,455 1105,1470 1470,1800 1800,2115
the end start to bring

1028
00:34:26,040 --> 00:34:27,870
0,255 255,560 640,1040 1120,1515 1515,1830
all this information together in

1029
00:34:27,870 --> 00:34:30,830
0,680 1240,1500 1500,1695 1695,2510 2560,2960
aggregation to then encode and

1030
00:34:30,910 --> 00:34:32,750
0,305 305,485 485,910 960,1360 1440,1840
update the weights according to

1031
00:34:32,830 --> 00:34:34,730
0,320 320,515 515,790 930,1480 1500,1900
what the local observations were.|
|

1032
00:34:36,270 --> 00:34:37,955
0,290 290,580 600,995 995,1355 1355,1685
The key idea is very,
它的关键思想非常非常类似于标准的卷积运算，这种方法是核心，它所做的是一个特征提取过程，现在由这个图中的节点和边来定义，而不是图像中的二维像素网格。

1033
00:34:37,955 --> 00:34:40,120
0,330 330,695 835,1140 1140,1445 1765,2165
very similar to the standard

1034
00:34:40,200 --> 00:34:42,845
0,640 810,1210 1860,2165 2165,2390 2390,2645
convolution operation, where this way

1035
00:34:42,845 --> 00:34:44,420
0,515 595,885 885,1140 1140,1365 1365,1575
kernel, all it's doing is

1036
00:34:44,420 --> 00:34:46,850
0,150 150,440 610,1185 1185,1490 1990,2430
a feature extraction procedure that's

1037
00:34:46,850 --> 00:34:48,845
0,290 310,675 675,1040 1060,1695 1695,1995
now defined by nodes and

1038
00:34:48,845 --> 00:34:50,885
0,510 510,780 780,975 975,1295 1645,2040
edges in this graph rather

1039
00:34:50,885 --> 00:34:52,325
0,390 390,720 720,945 945,1170 1170,1440
than a two d grid

1040
00:34:52,325 --> 00:34:53,920
0,195 195,725 865,1140 1140,1305 1305,1595
of pixels in an image.|
|

1041
00:34:55,080 --> 00:34:57,335
0,395 395,755 755,1120 1230,1595 1595,2255
This idea of graph encoding
图形编码和图形卷积的想法非常令人兴奋，就我们现在如何将其扩展到现实世界的设置而言，非常强大。

1042
00:34:57,335 --> 00:34:59,740
0,270 270,555 555,1145 1525,1925 2005,2405
and graph convolution is very

1043
00:34:59,790 --> 00:35:01,415
0,335 335,575 575,875 875,1270 1320,1625
exciting and very powerful in

1044
00:35:01,415 --> 00:35:02,960
0,210 210,435 435,755 1075,1380 1380,1545
terms of now how we

1045
00:35:02,960 --> 00:35:04,505
0,240 240,555 555,890 1060,1335 1335,1545
can extend this to real

1046
00:35:04,505 --> 00:35:07,060
0,285 285,635 1225,1560 1560,1895
world settings that are.|
|

1047
00:35:07,160 --> 00:35:09,090
0,400 420,820 900,1265 1265,1580 1580,1930
Defined by data sets that
由数据集定义，这些数据集自然地适合于这个图，比如结构。

1048
00:35:09,350 --> 00:35:11,050
0,335 335,670 720,1120 1260,1520 1520,1700
naturally lend themselves to this

1049
00:35:11,050 --> 00:35:12,660
0,255 255,540 540,890
graph, like structure.|
|

1050
00:35:12,830 --> 00:35:13,960
0,275 275,425 425,700 750,1010 1010,1130
So to highlight some of
为了强调其中一些在不同领域的应用，我想要引起注意的第一个例子是在化学和生物科学中，现在如果我们考虑一个分子结构，对，它是由原子定义的，这些原子根据分子键相互连接。

1051
00:35:13,960 --> 00:35:15,820
0,260 550,950 1120,1425 1425,1605 1605,1860
those applications across a variety

1052
00:35:15,820 --> 00:35:18,010
0,255 255,680 1300,1575 1575,1845 1845,2190
of domains, the first example

1053
00:35:18,010 --> 00:35:19,290
0,270 270,405 405,600 600,885 885,1280
I'd like to draw attention

1054
00:35:19,310 --> 00:35:20,910
0,400 570,920 920,1145 1145,1310 1310,1600
to is in the chemical

1055
00:35:20,990 --> 00:35:23,545
0,335 335,670 990,1690 2010,2315 2315,2555
and biological sciences, where now

1056
00:35:23,545 --> 00:35:24,760
0,210 210,390 390,615 615,915 915,1215
if we think about the

1057
00:35:24,760 --> 00:35:26,490
0,285 285,510 510,645 645,1280 1330,1730
structure of a molecule, right,

1058
00:35:26,930 --> 00:35:29,440
0,440 440,725 725,1090 1200,1810 2220,2510
it's defined by atoms, and

1059
00:35:29,440 --> 00:35:31,450
0,240 240,720 720,1040 1090,1635 1635,2010
those atoms are individually connected

1060
00:35:31,450 --> 00:35:33,190
0,225 225,375 375,680 940,1340 1390,1740
to each other according to

1061
00:35:33,190 --> 00:35:34,860
0,585 585,1190
molecular bonds.|
|

1062
00:35:34,900 --> 00:35:35,970
0,245 245,410 410,665 665,905 905,1070
And it turns out that
事实证明，我们可以设计图形神经网络，它可以有效地构建分子的表示。

1063
00:35:35,970 --> 00:35:37,695
0,135 135,375 375,740 1030,1395 1395,1725
we can design graph neural

1064
00:35:37,695 --> 00:35:40,065
0,275 655,945 945,1200 1200,1565 2005,2370
networks that can effectively build

1065
00:35:40,065 --> 00:35:43,160
0,845 1225,1545 1545,2345
representations of molecules.|
|

1066
00:35:43,470 --> 00:35:45,290
0,275 275,470 470,790 990,1390 1500,1820
In the same operation of
在这张图的相同操作中，卷积建立了化学结构的信息表示。

1067
00:35:45,290 --> 00:35:47,645
0,320 370,765 765,1430 1900,2175 2175,2355
this graph, convolution to build

1068
00:35:47,645 --> 00:35:49,990
0,305 325,705 705,1200 1200,1925 1945,2345
up an informative representation of

1069
00:35:50,280 --> 00:35:52,020
0,400 450,850
chemical structures.|
|

1070
00:35:52,490 --> 00:35:54,400
0,335 335,650 650,1030 1050,1450 1590,1910
This very same idea of
图形卷积的这个想法最近也被应用到了药物发现的问题上，事实上，在麻省理工学院的工作中也是如此。事实证明，我们可以定义图形神经网络结构，它可以查看小分子药物的数据，然后查看新的数据集，试图预测和发现具有强大活性的新治疗化合物，如杀菌和用作抗生素。

1071
00:35:54,400 --> 00:35:57,240
0,270 270,920 1420,1785 1785,2150 2440,2840
graph convolution was recently applied

1072
00:35:57,320 --> 00:35:59,190
0,380 380,665 665,970 990,1390 1470,1870
to the problem of drug

1073
00:35:59,210 --> 00:36:01,330
0,400 960,1235 1235,1400 1400,1690 1860,2120
discovery, and in fact, out

1074
00:36:01,330 --> 00:36:02,700
0,150 150,435 435,750 750,1020 1020,1370
of work here at MIT.

1075
00:36:03,710 --> 00:36:04,840
0,305 305,500 500,710 710,950 950,1130
It turned out that we

1076
00:36:04,840 --> 00:36:06,850
0,240 240,620 1120,1470 1470,1770 1770,2010
can define graph neural network

1077
00:36:06,850 --> 00:36:08,635
0,870 870,1065 1065,1230 1230,1485 1485,1785
architectures that can look at

1078
00:36:08,635 --> 00:36:11,010
0,335 625,1025 1135,1470 1470,2040 2040,2375
data of small molecule drugs

1079
00:36:11,570 --> 00:36:13,495
0,305 305,610 840,1100 1100,1360 1620,1925
and then look at new

1080
00:36:13,495 --> 00:36:14,755
0,255 255,525 525,765 765,1020 1020,1260
data sets to try to

1081
00:36:14,755 --> 00:36:17,380
0,305 535,930 930,1325 1615,1950 1950,2625
predict and discover new therapeutic

1082
00:36:17,380 --> 00:36:20,040
0,590 820,1110 1110,1365 1365,1940 2260,2660
compounds that have potent activity,

1083
00:36:20,570 --> 00:36:23,305
0,290 290,580 930,1330 2130,2480 2480,2735
such as killing bacteria and

1084
00:36:23,305 --> 00:36:25,420
0,585 585,840 840,1745
functioning as antibiotics.|
|

1085
00:36:26,640 --> 00:36:28,990
0,305 305,610 630,1030 1650,2000 2000,2350
Beyond this example, another recent
除了这个例子之外，图神经网络最近的另一个应用是在交通预测中，其中的目标是将街道和十字路口视为节点和边定义的Bioggraph，并将图神经网络应用于该结构和移动模式，以学习预测所产生的交通模式。

1086
00:36:29,190 --> 00:36:30,640
0,365 365,650 650,920 920,1190 1190,1450
application of graph neural networks

1087
00:36:30,720 --> 00:36:32,530
0,320 320,590 590,920 920,1300 1320,1810
has been in traffic prediction,

1088
00:36:33,180 --> 00:36:34,630
0,275 275,425 425,700 720,1085 1085,1450
where the goal is to

1089
00:36:34,800 --> 00:36:37,480
0,350 350,700 840,1240 1710,2015 2015,2680
look at streets and intersections

1090
00:36:37,920 --> 00:36:40,640
0,400 510,1145 1145,1370 1370,1870 2340,2720
as nodes and edges defined

1091
00:36:40,640 --> 00:36:43,100
0,680 1120,1500 1500,1860 1860,2190 2190,2460
biograph and apply graph neural

1092
00:36:43,100 --> 00:36:46,505
0,260 520,825 825,1130 2320,2720 3100,3405
network to that structure and

1093
00:36:46,505 --> 00:36:48,290
0,210 210,515 535,810 810,1235 1495,1785
to patterns of mobility to

1094
00:36:48,290 --> 00:36:50,435
0,240 240,465 465,740 1240,1640 1750,2145
learn to predict what resulting

1095
00:36:50,435 --> 00:36:52,200
0,375 375,755 775,1050 1050,1325
traffic patterns could be.|
|

1096
00:36:52,460 --> 00:36:54,150
0,365 365,730 750,1040 1040,1310 1310,1690
And indeed, in work from
事实上，在谷歌和DeepMind的工作中，同样的建模方法能够显著改善谷歌地图中ETA估计到达时间的预测。

1097
00:36:54,170 --> 00:36:56,160
0,335 335,560 560,1030 1320,1655 1655,1990
Google and deepmind, this same

1098
00:36:56,210 --> 00:36:58,240
0,575 575,940 1140,1445 1445,1730 1730,2030
modeling approach was able to

1099
00:36:58,240 --> 00:37:00,895
0,300 300,680 700,1100 1450,1850 2380,2655
result in significant improvements in

1100
00:37:00,895 --> 00:37:03,805
0,150 150,575 655,1055 1195,1925 2455,2910
the prediction of eta estimated

1101
00:37:03,805 --> 00:37:05,620
0,360 360,690 690,1215 1215,1485 1485,1815
time of arrival in Google

1102
00:37:05,620 --> 00:37:06,620
0,380
maps.|
|

1103
00:37:06,620 --> 00:37:08,140
0,320 430,735 735,975 975,1170 1170,1520
So when you're looking at
因此，当你在手机上查看谷歌地图并预测你何时将到达某个位置时，它在后端所做的是应用这种图形神经网络体系结构来查看交通数据，并就这些模式如何随着时间的变化做出更有信息的预测。

1104
00:37:08,520 --> 00:37:09,695
0,335 335,590 590,785 785,950 950,1175
Google maps on your phone

1105
00:37:09,695 --> 00:37:11,165
0,225 225,815 835,1125 1125,1335 1335,1470
and predicting when you're going

1106
00:37:11,165 --> 00:37:12,460
0,195 195,405 405,630 630,915 915,1295
to arrive at some location,

1107
00:37:13,200 --> 00:37:14,135
0,260 260,470 470,635 635,800 800,935
what it's doing on the

1108
00:37:14,135 --> 00:37:15,830
0,195 195,515 625,1025 1045,1410 1410,1695
back end is applying this

1109
00:37:15,830 --> 00:37:18,005
0,255 255,510 510,770 820,1220 1900,2175
graph neural network architecture to

1110
00:37:18,005 --> 00:37:19,600
0,195 195,405 405,695 925,1260 1260,1595
look at data of traffic

1111
00:37:19,620 --> 00:37:21,850
0,395 395,725 725,1060 1080,1595 1595,2230
and make more informative predictions

1112
00:37:22,080 --> 00:37:23,525
0,335 335,590 590,845 845,1160 1160,1445
about how those patterns are

1113
00:37:23,525 --> 00:37:25,020
0,300 300,645 645,995
changing over time.|
|

1114
00:37:26,180 --> 00:37:28,530
0,290 290,580 690,1090 1410,1810 1950,2350
A final and another recent
最后一个也是最近的另一个例子是在预测Covid19的传播中，同样，基于同样的活动模式和接触图的想法，神经网络被用来不仅研究Covid传播的空间分布，还结合了一个时间维度，以便研究人员可以有效地预测哪些地区最有可能受到Covid19的影响，以及这些影响可能发生在什么时间尺度上。

1115
00:37:28,640 --> 00:37:31,195
0,400 420,785 785,1150 1710,2315 2315,2555
example was in forecasting the

1116
00:37:31,195 --> 00:37:33,340
0,285 285,540 540,855 855,1115 1825,2145
spread of covid nineteen where,

1117
00:37:33,340 --> 00:37:35,275
0,320 790,1170 1170,1455 1455,1650 1650,1935
again, building off of this

1118
00:37:35,275 --> 00:37:37,800
0,395 445,825 825,1205 1495,2075 2125,2525
same idea of mobility patterns

1119
00:37:37,940 --> 00:37:40,440
0,400 450,1330 1620,1970 1970,2240 2240,2500
and contacts graph, neural networks

1120
00:37:40,520 --> 00:37:42,390
0,380 380,760 1110,1370 1370,1550 1550,1870
were employed to look at

1121
00:37:42,560 --> 00:37:44,830
0,290 290,580 600,1270 1350,1870 1980,2270
not only spatial distributions of

1122
00:37:44,830 --> 00:37:47,035
0,360 360,680 940,1320 1320,1695 1695,2205
covid spread, but also incorporate

1123
00:37:47,035 --> 00:37:49,440
0,195 195,690 690,1205 1825,2115 2115,2405
a temporal dimension so that

1124
00:37:49,460 --> 00:37:52,090
0,400 570,920 920,1270 2010,2360 2360,2630
researchers could effectively forecast what

1125
00:37:52,090 --> 00:37:53,760
0,320 430,690 690,900 900,1250 1270,1670
were the most likely areas

1126
00:37:53,930 --> 00:37:55,405
0,260 260,515 515,890 890,1175 1175,1475
to be affected by covid

1127
00:37:55,405 --> 00:37:58,230
0,275 805,1205 1435,1835 2065,2430 2430,2825
nineteen, and what time scales

1128
00:37:58,460 --> 00:38:00,355
0,400 600,965 965,1235 1235,1540 1590,1895
those effects were likely to

1129
00:38:00,355 --> 00:38:01,200
0,225 225,545
occur on.|
|

1130
00:38:02,670 --> 00:38:05,230
0,395 395,790 840,1240 1410,1810 2160,2560
So hopefully this this highlights
因此，希望这突出了这样一个想法，即关于数据结构的简单想法可以转化为新的神经网络体系结构，这些体系结构是专门为相关应用领域的特定问题设计的。

1131
00:38:05,820 --> 00:38:07,540
0,380 380,635 635,845 845,1180 1320,1720
this idea of how simple

1132
00:38:07,710 --> 00:38:10,040
0,400 450,785 785,1100 1100,1480 2040,2330
ideas about data structure can

1133
00:38:10,040 --> 00:38:12,260
0,225 225,495 495,1080 1080,1460 1900,2220
then be translated into new

1134
00:38:12,260 --> 00:38:14,435
0,315 315,590 610,1550 1720,1965 1965,2175
neural network architectures that are

1135
00:38:14,435 --> 00:38:17,530
0,365 715,1115 1375,1775 2215,2615 2695,3095
specifically designed for particular problems

1136
00:38:17,670 --> 00:38:21,280
0,400 570,970 2160,2525 2525,2890
in relevant application areas.|
|

1137
00:38:21,580 --> 00:38:23,720
0,290 290,580 1050,1430 1430,1850 1850,2140
And this also lends very
这也自然而然地决定了我们如何才能不扩展。

1138
00:38:23,890 --> 00:38:25,635
0,365 365,730 1050,1355 1355,1520 1520,1745
naturally to how we can

1139
00:38:25,635 --> 00:38:26,960
0,300 300,635
extend not.|
|

1140
00:38:27,150 --> 00:38:29,960
0,400 780,1180 1650,1925 1925,2465 2465,2810
Beyond data to graphs to
从数据到图表，再到现在的三维数据。

1141
00:38:29,960 --> 00:38:32,280
0,350 370,690 690,1320 1320,1610
now three dimensional data.|
|

1142
00:38:32,740 --> 00:38:34,230
0,275 275,440 440,730 900,1250 1250,1490
Which is often referred to
这通常被称为三维点云。这些都是无序的数据集。你可以把它们想象成散布在三维空间中。事实证明，你可以非常自然地扩展图形神经网络，以处理三个三维数据集，生命点云。

1143
00:38:34,230 --> 00:38:35,870
0,290 490,795 795,1020 1020,1290 1290,1640
as three d point clouds.

1144
00:38:36,460 --> 00:38:38,385
0,290 290,545 545,905 905,1655 1655,1925
And these are unordered sets

1145
00:38:38,385 --> 00:38:39,510
0,210 210,480 480,780 780,960 960,1125
of data. And you can

1146
00:38:39,510 --> 00:38:40,760
0,210 210,420 420,615 615,885 885,1250
think about them as being

1147
00:38:41,470 --> 00:38:42,890
0,395 395,710 710,905 905,1100 1100,1420
scattered in three d space.

1148
00:38:43,540 --> 00:38:44,760
0,320 320,545 545,755 755,995 995,1220
And it turns out that

1149
00:38:44,760 --> 00:38:47,025
0,165 165,440 760,1155 1155,1550 1900,2265
you can extend graph neural

1150
00:38:47,025 --> 00:38:49,440
0,275 355,755 955,1355 1615,2015 2065,2415
networks very naturally to operate

1151
00:38:49,440 --> 00:38:52,005
0,350 580,945 945,1310 1510,2220 2220,2565
on three d datasets, life

1152
00:38:52,005 --> 00:38:53,160
0,285 285,605
point clouds.|
|

1153
00:38:53,200 --> 00:38:54,945
0,380 380,680 680,1000 1050,1450 1470,1745
Where the core idea is
其中的核心思想是，你可以使用图形卷积和图形神经网络的相同思想来动态计算这个三维空间中存在的网格。

1154
00:38:54,945 --> 00:38:57,380
0,165 165,315 315,495 495,1445 1765,2435
that you can dynamically compute

1155
00:38:57,730 --> 00:39:00,285
0,670 900,1300 1470,1775 1775,2080 2250,2555
meshes present in this three

1156
00:39:00,285 --> 00:39:02,660
0,645 645,995 1405,1785 1785,2070 2070,2375
dimensional space using the same

1157
00:39:02,770 --> 00:39:04,635
0,290 290,485 485,755 755,1360 1560,1865
idea of graph convolution and

1158
00:39:04,635 --> 00:39:06,260
0,240 240,525 525,785
graph neural networks.|
|

1159
00:39:08,070 --> 00:39:10,660
0,400 900,1300 1650,2045 2045,2315 2315,2590
Okay, so hopefully with that
好的，希望通过对图形结构、图形和神经网络的了解，您已经开始了解如何使用数据基础结构的检查和我们的先验知识来为非常适合特定任务的新的神经网络体系结构提供信息。

1160
00:39:10,770 --> 00:39:12,490
0,335 335,590 590,910 990,1355 1355,1720
run through of graph structure,

1161
00:39:12,630 --> 00:39:14,600
0,335 335,590 590,850 1350,1730 1730,1970
graph, neural networks, you've started

1162
00:39:14,600 --> 00:39:15,350
0,195 195,300 300,420 420,585 585,750
to get a little bit

1163
00:39:15,350 --> 00:39:17,945
0,255 255,650 670,1070 1240,1640 2020,2595
of idea about how inspection

1164
00:39:17,945 --> 00:39:19,805
0,270 270,570 570,905 1195,1590 1590,1860
of the underlying structure of

1165
00:39:19,805 --> 00:39:21,950
0,275 625,1025 1105,1710 1710,1950 1950,2145
data and incorporation of our

1166
00:39:21,950 --> 00:39:23,825
0,330 330,710 1150,1440 1440,1605 1605,1875
prior knowledge can be used

1167
00:39:23,825 --> 00:39:25,355
0,360 360,705 705,990 990,1275 1275,1530
to inform new neural network

1168
00:39:25,355 --> 00:39:27,365
0,905 1075,1335 1335,1485 1485,1710 1710,2010
architectures that are very well

1169
00:39:27,365 --> 00:39:29,620
0,510 510,855 855,1205 1225,1625
suited for particular tasks.|
|

1170
00:39:32,510 --> 00:39:33,810
0,290 290,515 515,755 755,980 980,1300
On that, I want to
关于这一点，我想把这堂课剩下的时间花在我们的第二个新领域，重点是产生式人工智能。

1171
00:39:34,070 --> 00:39:35,550
0,305 305,545 545,860 860,1160 1160,1480
spend the remaining of time

1172
00:39:35,600 --> 00:39:37,735
0,400 930,1250 1250,1570 1590,1895 1895,2135
of this lecture on our

1173
00:39:37,735 --> 00:39:40,315
0,335 445,735 735,1295 1645,2045 2245,2580
second new frontier, focusing on

1174
00:39:40,315 --> 00:39:41,900
0,585 585,965
generative AI.|
|

1175
00:39:43,450 --> 00:39:45,525
0,400 750,1055 1055,1250 1250,1540 1680,2075
So as we first introduced
因此，正如我们在这门课程的第一节课中所介绍的那样，我认为今天我们真的处于人工智能的这个转折点。

1176
00:39:45,525 --> 00:39:46,845
0,390 390,660 660,870 870,1125 1125,1320
in the first lecture of

1177
00:39:46,845 --> 00:39:49,410
0,195 195,515 535,935 1105,1505 2275,2565
this course and throughout, I

1178
00:39:49,410 --> 00:39:50,985
0,255 255,555 555,890 940,1320 1320,1575
think that today we're really

1179
00:39:50,985 --> 00:39:52,965
0,300 300,540 540,1115 1135,1535 1615,1980
at this inflection point in

1180
00:39:52,965 --> 00:39:53,980
0,365
AI.|
|

1181
00:39:53,980 --> 00:39:55,915
0,285 285,585 585,890 910,1310 1600,1935
Where we're seeing tremendous new
我们看到了巨大的新功能，特别是产生式模型，使各种领域的新进展成为可能。

1182
00:39:55,915 --> 00:39:58,150
0,335 835,1155 1155,1635 1635,1905 1905,2235
capabilities, with generative models in

1183
00:39:58,150 --> 00:40:01,530
0,350 1150,2030 2110,2475 2475,3060 3060,3380
particular, enabling new advances in

1184
00:40:01,640 --> 00:40:03,540
0,275 275,545 545,845 845,1150
a variety of fields.|
|

1185
00:40:03,610 --> 00:40:04,995
0,245 245,380 380,670 780,1160 1160,1385
And I think we're just
我认为我们正处在这个转折点上，在未来的几年里，我们将看到产生式人工智能从根本上改变我们世界的面貌，我们社会的面貌。

1186
00:40:04,995 --> 00:40:06,405
0,225 225,480 480,750 750,960 960,1410
at the, at the casa

1187
00:40:06,405 --> 00:40:07,755
0,135 135,315 315,780 780,1125 1125,1350
of this inflection point in

1188
00:40:07,755 --> 00:40:09,270
0,275 445,750 750,930 930,1170 1170,1515
that in the coming years,

1189
00:40:09,270 --> 00:40:10,755
0,315 315,450 450,645 645,900 900,1485
we're going to see generative

1190
00:40:10,755 --> 00:40:13,680
0,365 565,1295 1405,1805 2155,2555 2635,2925
AI radically transform the landscape

1191
00:40:13,680 --> 00:40:14,940
0,135 135,300 300,555 555,890 940,1260
of our world, the landscape

1192
00:40:14,940 --> 00:40:16,540
0,180 180,405 405,770
of our society.|
|

1193
00:40:16,640 --> 00:40:18,205
0,365 365,730 780,1040 1040,1235 1235,1565
So today, in the remaining
因此，今天，在新前沿课程的剩余部分，我们将专门关注一类新的生成性模型，扩散模型正在推动生成性人工智能的一些最新进展。

1194
00:40:18,205 --> 00:40:19,675
0,395 415,675 675,810 810,975 975,1470
portion of the new frontiers

1195
00:40:19,675 --> 00:40:21,480
0,365 895,1200 1200,1335 1335,1515 1515,1805
lecture, we're going to focus

1196
00:40:21,500 --> 00:40:23,490
0,400 720,1120 1140,1430 1430,1655 1655,1990
specifically on a new class

1197
00:40:23,540 --> 00:40:25,830
0,305 305,755 755,1030 1380,1925 1925,2290
of generative models, diffusion models

1198
00:40:26,180 --> 00:40:27,385
0,245 245,425 425,890 890,1070 1070,1205
that are powering some of

1199
00:40:27,385 --> 00:40:30,205
0,275 865,1265 1555,2115 2115,2295 2295,2820
these latest advances in generative

1200
00:40:30,205 --> 00:40:31,100
0,365
AI.|
|

1201
00:40:32,130 --> 00:40:33,935
0,245 245,490 840,1145 1145,1430 1430,1805
All right, to start, let's
好的，首先，让我们回顾一下关于生成性建模的课程，我们主要关注两类生成性模型，VAS和Gans。

1202
00:40:33,935 --> 00:40:35,320
0,195 195,515 595,915 915,1110 1110,1385
think back to the lecture

1203
00:40:35,370 --> 00:40:37,925
0,305 305,755 755,1240 1950,2300 2300,2555
on generative modeling, where we

1204
00:40:37,925 --> 00:40:39,910
0,305 325,725 925,1320 1320,1650 1650,1985
focus primarily on two classes

1205
00:40:40,110 --> 00:40:43,205
0,320 320,785 785,1060 1710,2200 2790,3095
of generative models, vas and

1206
00:40:43,205 --> 00:40:44,300
0,335
gans.|
|

1207
00:40:45,540 --> 00:40:46,805
0,320 320,515 515,785 785,995 995,1265
What we didn't have time
我们没有时间深入探讨的是，这些罐装VA型号的潜在局限性是什么。

1208
00:40:46,805 --> 00:40:48,160
0,195 195,360 360,660 660,1005 1005,1355
to go into into depth

1209
00:40:48,360 --> 00:40:49,685
0,365 365,680 680,980 980,1190 1190,1325
was what are some of

1210
00:40:49,685 --> 00:40:52,070
0,135 135,395 985,1655 1915,2190 2190,2385
the underlying limitations of these

1211
00:40:52,070 --> 00:40:54,360
0,320 610,1035 1035,1260 1260,1760
models va in cans.|
|

1212
00:40:55,470 --> 00:40:56,765
0,320 320,590 590,875 875,1070 1070,1295
Turns out that there are
事实证明，这有三个主要限制。

1213
00:40:56,765 --> 00:40:59,600
0,365 415,815 895,1565
three primary limitations.|
|

1214
00:40:59,600 --> 00:41:00,770
0,255 255,480 480,735 735,930 930,1170
The first is what we
第一个是我们所说的模式崩溃，这意味着在生成过程中，VAS和Gans可以向下崩溃到这个模式，在这个阶段，他们会产生很多预测，很多彼此非常相似的新样本。

1215
00:41:00,770 --> 00:41:03,500
0,350 460,860 880,1340 2050,2400 2400,2730
call mode collapse, meaning that

1216
00:41:03,500 --> 00:41:05,420
0,270 270,450 450,930 930,1220 1450,1920
in the generative process, vas

1217
00:41:05,420 --> 00:41:06,875
0,225 225,560 730,1065 1065,1275 1275,1455
and gans can kind of

1218
00:41:06,875 --> 00:41:09,550
0,405 405,750 750,1005 1005,1295 2275,2675
collapse down to this mode,

1219
00:41:09,900 --> 00:41:11,930
0,395 395,785 785,1070 1070,1460 1460,2030
this phase where they're generating

1220
00:41:11,930 --> 00:41:14,150
0,255 255,495 495,830 1000,1760 1960,2220
a lot of predictions, a

1221
00:41:14,150 --> 00:41:15,215
0,135 135,300 300,510 510,930 930,1065
lot of new samples that

1222
00:41:15,215 --> 00:41:16,445
0,165 165,420 420,690 690,1005 1005,1230
are very, very similar to

1223
00:41:16,445 --> 00:41:17,340
0,135 135,425
each other.|
|

1224
00:41:17,410 --> 00:41:18,855
0,400 480,875 875,1130 1130,1265 1265,1445
We often kind of think
我们经常认为这是对平均值或最常见值的回归。

1225
00:41:18,855 --> 00:41:20,175
0,210 210,405 405,600 600,1080 1080,1320
about this as regression to

1226
00:41:20,175 --> 00:41:21,840
0,135 135,425 505,905 1225,1515 1515,1665
the average value or the

1227
00:41:21,840 --> 00:41:23,540
0,240 240,600 600,980
most common value.|
|

1228
00:41:24,140 --> 00:41:25,765
0,290 290,560 560,830 830,1370 1370,1625
The second key limitation, as
第二个关键限制，就像我在我们的生成性建模课程中提到的那样，是这些模型真的很难生成全新的实例。

1229
00:41:25,765 --> 00:41:26,935
0,180 180,360 360,465 465,900 900,1170
I kind of alluded to

1230
00:41:26,935 --> 00:41:28,950
0,285 285,605 775,1290 1290,1725 1725,2015
in our generative modeling lecture,

1231
00:41:29,600 --> 00:41:31,020
0,290 290,470 470,680 680,1000 1020,1420
was that these models really

1232
00:41:31,040 --> 00:41:33,600
0,400 480,785 785,1090 1560,2240 2240,2560
struggle to generate radically new

1233
00:41:33,740 --> 00:41:35,840
0,770 770,1090 1140,1540
instances that are.|
|

1234
00:41:36,210 --> 00:41:38,075
0,400 720,1120 1230,1490 1490,1625 1625,1865
Not similar to the training
与训练数据不同的是更加多样化。

1235
00:41:38,075 --> 00:41:39,310
0,300 300,495 495,660 660,900 900,1235
data that are more diverse.|
|

1236
00:41:40,660 --> 00:41:42,690
0,400 870,1175 1175,1385 1385,1670 1670,2030
Finally, it turns out that
最后，事实证明，尤其是甘斯，以及VAS在实践中可能很难训练。它们不稳定，效率低下，这导致在实践中考虑如何扩展这些模型时会出现很多问题。

1237
00:41:42,690 --> 00:41:45,015
0,410 790,1170 1170,1545 1545,1905 1905,2325
gans in particular and vas

1238
00:41:45,015 --> 00:41:46,785
0,270 270,575 835,1140 1140,1445 1495,1770
as well in practice can

1239
00:41:46,785 --> 00:41:48,260
0,225 225,575 715,1005 1005,1185 1185,1475
be very difficult to train.

1240
00:41:48,640 --> 00:41:51,330
0,335 335,940 1200,2020 2160,2465 2465,2690
They're unstable, inefficient, and this

1241
00:41:51,330 --> 00:41:52,490
0,270 270,495 495,645 645,840 840,1160
leads to a lot of

1242
00:41:52,750 --> 00:41:54,675
0,395 395,695 695,1000 1350,1640 1640,1925
problems in practice when thinking

1243
00:41:54,675 --> 00:41:56,685
0,360 360,660 660,995 1165,1565 1705,2010
about how to scale these

1244
00:41:56,685 --> 00:41:58,180
0,305
models.|
|

1245
00:41:58,850 --> 00:42:01,020
0,305 305,880 930,1250 1250,1820 1820,2170
These limitations then motivate a
然后，这些限制激发了一组具体的挑战或标准，当我们考虑生成性模型时，这些挑战或标准是我们真正想要满足的。

1246
00:42:01,130 --> 00:42:02,890
0,320 320,545 545,845 845,1240 1440,1760
concrete set of challenges or

1247
00:42:02,890 --> 00:42:04,870
0,770 1000,1290 1290,1470 1470,1710 1710,1980
criteria that we really want

1248
00:42:04,870 --> 00:42:06,265
0,195 195,405 405,675 675,1010 1060,1395
to meet when thinking about

1249
00:42:06,265 --> 00:42:07,780
0,465 465,725
generative models.|
|

1250
00:42:07,900 --> 00:42:09,180
0,275 275,455 455,650 650,1070 1070,1280
We want our generative model
我们希望我们的生成模型稳定、高效地进行训练。

1251
00:42:09,180 --> 00:42:10,995
0,195 195,360 360,680 1210,1560 1560,1815
to be stable, efficient to

1252
00:42:10,995 --> 00:42:12,080
0,305
train.|
|

1253
00:42:12,180 --> 00:42:13,745
0,275 275,530 530,785 785,1060 1200,1565
We wanted to generate high
我们希望生成高质量的样本，这些样本是人工合成的，而且是新颖的多样化，不同于该模型以前在其训练示例中看到的。

1254
00:42:13,745 --> 00:42:16,360
0,365 415,1055 1705,1965 1965,2130 2130,2615
quality samples that are synthetic

1255
00:42:16,680 --> 00:42:19,325
0,400 480,880 1080,1480 1860,2260 2280,2645
and novel diverse, different from

1256
00:42:19,325 --> 00:42:21,035
0,365 745,990 990,1200 1200,1485 1485,1710
what the model has seen

1257
00:42:21,035 --> 00:42:23,050
0,305 655,960 960,1200 1200,1535 1615,2015
before in its training examples.|
|

1258
00:42:24,950 --> 00:42:26,560
0,380 380,725 725,1090 1110,1445 1445,1610
Today and tomorrow we're going
今天和明天，我们将关注两个非常、非常令人兴奋的新的生成性模型，它们将正面应对这些挑战。

1259
00:42:26,560 --> 00:42:28,660
0,320 700,1080 1080,1425 1425,1770 1770,2100
to focus on two very,

1260
00:42:28,660 --> 00:42:30,820
0,345 345,740 1180,1530 1530,1875 1875,2160
very exciting new classes of

1261
00:42:30,820 --> 00:42:33,340
0,450 450,740 1180,1580 1780,2295 2295,2520
generative models that tackle these

1262
00:42:33,340 --> 00:42:34,980
0,350 520,795 795,1070
challenges head on.|
|

1263
00:42:35,080 --> 00:42:36,350
0,335 335,620 620,755 755,950 950,1270
Today I'm going to specifically
今天，我将特别关注扩散模型，提供它们工作原理背后的直觉，你以前可能在哪里见过它们，以及它们带来了哪些进步。

1264
00:42:36,520 --> 00:42:39,075
0,395 395,710 710,1145 1145,1510 2220,2555
focus on diffusion models, provide

1265
00:42:39,075 --> 00:42:40,950
0,195 195,725 775,1175 1375,1680 1680,1875
the intuition behind how they

1266
00:42:40,950 --> 00:42:42,435
0,290 640,960 960,1155 1155,1305 1305,1485
work, where you may have

1267
00:42:42,435 --> 00:42:43,605
0,195 195,345 345,585 585,885 885,1170
seen them before and what

1268
00:42:43,605 --> 00:42:44,820
0,270 270,450 450,585 585,765 765,1215
are some of the advances

1269
00:42:44,820 --> 00:42:46,400
0,135 135,330 330,920
that they're enabling.|
|

1270
00:42:46,500 --> 00:42:48,065
0,365 365,730 900,1190 1190,1355 1355,1565
And tomorrow in the guest
明天，在谷歌的客座演讲中，我们将听到Dillip关于另一种生成性建模方法的演讲，该方法专门专注于从文本到图像生成的任务。

1271
00:42:48,065 --> 00:42:49,730
0,315 315,600 600,905 1195,1515 1515,1665
lecture from Google, we're going

1272
00:42:49,730 --> 00:42:51,410
0,195 195,420 420,690 690,1130 1300,1680
to hear from dillip on

1273
00:42:51,410 --> 00:42:53,810
0,345 345,915 915,1410 1410,1760 1990,2400
another generative modeling approach that's

1274
00:42:53,810 --> 00:42:55,720
0,290 490,890 1060,1350 1350,1575 1575,1910
focused specifically on this task

1275
00:42:56,040 --> 00:42:58,030
0,350 350,700 870,1130 1130,1390 1590,1990
of text to image generation.|
|

1276
00:42:59,630 --> 00:43:00,680
0,400
Okay.|
好吧。|

1277
00:43:01,070 --> 00:43:01,930
0,105 105,210 210,345 345,540 540,860
So let's get into it.|
所以让我们开始吧。|

1278
00:43:03,670 --> 00:43:05,145
0,335 335,740 740,1070 1070,1325 1325,1475
For diffusion models, I think
对于扩散模型，我认为它首先帮助我们再次与我们看到的模型、我们看到和了解的体系结构进行比较。

1279
00:43:05,145 --> 00:43:06,345
0,180 180,435 435,690 690,930 930,1200
it first helps us to

1280
00:43:06,345 --> 00:43:08,445
0,270 270,600 600,995 1195,1595 1825,2100
compare back again to the

1281
00:43:08,445 --> 00:43:10,215
0,270 270,645 645,870 870,1110 1110,1770
models we've seen, the architectures

1282
00:43:10,215 --> 00:43:12,020
0,270 270,525 525,905 1135,1470 1470,1805
we've seen and know about.|
|

1283
00:43:12,800 --> 00:43:14,245
0,335 335,710 710,935 935,1205 1205,1445
With vas and gans that
对于我们在第四讲中讨论的VAS和Gans，我们的任务是生成示例，合成示例，比方说一张照片中的图像，通过获取一些压缩或噪声的潜在空间，然后尝试解码或生成，在我们的原始数据空间中产生一个新的实例。

1284
00:43:14,245 --> 00:43:15,480
0,210 210,495 495,750 750,945 945,1235
we talked about in lecture

1285
00:43:15,500 --> 00:43:18,085
0,400 1230,1550 1550,1870 1920,2285 2285,2585
four, the task is to

1286
00:43:18,085 --> 00:43:20,995
0,335 805,1205 1315,1890 1890,2285 2545,2910
generate examples, synthetic examples, let's

1287
00:43:20,995 --> 00:43:22,900
0,105 105,240 240,515 1465,1740 1740,1905
say an image in a

1288
00:43:22,900 --> 00:43:24,685
0,270 270,650 760,1110 1110,1460 1480,1785
single shot, by taking some

1289
00:43:24,685 --> 00:43:27,030
0,575 775,1110 1110,1605 1605,2010 2010,2345
compressed or noisy latent space

1290
00:43:27,560 --> 00:43:29,335
0,275 275,500 500,845 845,1175 1175,1775
and then trying to decode

1291
00:43:29,335 --> 00:43:31,045
0,195 195,515 595,900 900,1205 1345,1710
or generate from that to

1292
00:43:31,045 --> 00:43:32,850
0,360 360,705 705,945 945,1235 1405,1805
produce now a new instance

1293
00:43:32,930 --> 00:43:34,860
0,275 275,530 530,910 1170,1550 1550,1930
in our original data space.|
|

1294
00:43:36,220 --> 00:43:38,600
0,500 500,875 875,1220 1220,2075 2075,2380
Diffusion models work fundamentally differently
扩散模型的工作原理与这种方法截然不同。

1295
00:43:38,800 --> 00:43:40,460
0,290 290,530 530,880
from this approach.|
|

1296
00:43:40,590 --> 00:43:42,320
0,400 480,800 800,1085 1085,1400 1400,1730
Rather than doing this one
扩散模型所做的是迭代地产生新的样本，而不是进行这种尖锐的预测。

1297
00:43:42,320 --> 00:43:44,860
0,315 315,770 1480,1785 1785,2175 2175,2540
sharp prediction, what diffusion models

1298
00:43:44,880 --> 00:43:46,630
0,400 750,1025 1025,1190 1190,1415 1415,1750
do is that they generate

1299
00:43:46,770 --> 00:43:49,100
0,350 350,950 950,1810
new samples iteratively.|
|

1300
00:43:49,110 --> 00:43:51,280
0,400 660,1060 1110,1460 1460,1880 1880,2170
By starting from purely random
从纯粹的随机噪声开始，学习一个过程，这个过程可以迭代地从完全随机状态中删除小增量的噪声，直到能够在我们开始并关心的原始数据环境中生成合成示例。

1301
00:43:51,300 --> 00:43:53,650
0,400 1110,1460 1460,1775 1775,2045 2045,2350
noise and learning a process

1302
00:43:53,760 --> 00:43:56,080
0,275 275,470 470,1205 1205,1570 1920,2320
that can iteratively remove small

1303
00:43:56,280 --> 00:43:58,205
0,440 440,665 665,970 1320,1625 1625,1925
increments of noise from that

1304
00:43:58,205 --> 00:44:00,335
0,345 345,695 745,1145 1645,1965 1965,2130
complete random state all the

1305
00:44:00,335 --> 00:44:01,970
0,165 165,485 685,1085 1165,1425 1425,1635
way up back to being

1306
00:44:01,970 --> 00:44:03,700
0,330 330,585 585,860 910,1215 1215,1730
able to generate a synthetic

1307
00:44:03,810 --> 00:44:06,220
0,400 840,1145 1145,1355 1355,1660 2010,2410
example in the original data

1308
00:44:06,420 --> 00:44:07,595
0,320 320,515 515,695 695,920 920,1175
landscape that we started off

1309
00:44:07,595 --> 00:44:09,560
0,335 445,750 750,1110 1110,1415
with and caring about.|
|

1310
00:44:10,320 --> 00:44:12,035
0,260 260,800 800,1085 1085,1355 1355,1715
The intuition here is really
这里的直觉真的很聪明，我认为，在这方面真的很强大。

1311
00:44:12,035 --> 00:44:14,330
0,395 655,1050 1050,1445 1765,2055 2055,2295
clever and really, I think,

1312
00:44:14,330 --> 00:44:16,740
0,350 970,1275 1275,1580
powerful in that.|
|

1313
00:44:17,040 --> 00:44:18,680
0,320 320,725 725,1090 1110,1415 1415,1640
What diffusion models allow us
扩散模型使我们能够从一个完全随机的状态开始，捕捉最大的可变性，最大的信息量。

1314
00:44:18,680 --> 00:44:19,960
0,180 180,405 405,690 690,945 945,1280
to do is to capture

1315
00:44:20,700 --> 00:44:23,590
0,400 630,1330 1470,1870 2280,2585 2585,2890
maximum variability, maximum amount of

1316
00:44:23,670 --> 00:44:26,315
0,400 870,1270 1500,1900 2010,2330 2330,2645
information, by starting from a

1317
00:44:26,315 --> 00:44:28,360
0,395 505,905 985,1385
completely random state.|
|

1318
00:44:28,990 --> 00:44:30,480
0,485 485,800 800,1040 1040,1205 1205,1490
Diffusion models can be broken
扩散模型可以分为两个关键方面。

1319
00:44:30,480 --> 00:44:33,170
0,330 330,680 730,1130 1360,1760 2290,2690
down into two key aspects.|
|

1320
00:44:33,960 --> 00:44:35,390
0,305 305,610 660,995 995,1220 1220,1430
The first is what we
第一个是我们所说的前向噪声过程。

1321
00:44:35,390 --> 00:44:37,450
0,315 315,645 645,980 1180,1740 1740,2060
call the forward noising process.|
|

1322
00:44:38,830 --> 00:44:40,155
0,400 480,785 785,995 995,1175 1175,1325
And at the core of
这个想法的核心是我们如何建立数据，让扩散模型查看，然后学习如何去噪和生成数据。

1323
00:44:40,155 --> 00:44:41,685
0,225 225,450 450,725 805,1185 1185,1530
this is this idea of

1324
00:44:41,685 --> 00:44:43,520
0,270 270,495 495,780 780,1145 1435,1835
how we build up data

1325
00:44:44,050 --> 00:44:45,480
0,290 290,455 455,830 830,1175 1175,1430
for the diffusion model to

1326
00:44:45,480 --> 00:44:46,950
0,150 150,440 640,930 930,1155 1155,1470
look at and then learn

1327
00:44:46,950 --> 00:44:48,510
0,330 330,680 700,975 975,1245 1245,1560
how to de noise and

1328
00:44:48,510 --> 00:44:49,820
0,320 340,740
generate from.|
|

1329
00:44:50,870 --> 00:44:52,510
0,290 290,500 500,740 740,1060 1350,1640
The key step here is
这里的关键一步是我们从训练数据开始，比如说图像的例子。

1330
00:44:52,510 --> 00:44:54,055
0,165 165,375 375,710 850,1215 1215,1545
that we start with training

1331
00:44:54,055 --> 00:44:56,305
0,365 955,1320 1320,1565 1615,1995 1995,2250
data, let's say examples of

1332
00:44:56,305 --> 00:44:57,420
0,275
images.|
|

1333
00:44:57,460 --> 00:44:58,710
0,320 320,590 590,830 830,1055 1055,1250
And over the course of
在这个前向噪声扩散过程中。

1334
00:44:58,710 --> 00:45:00,740
0,210 210,560 670,1230 1230,1650 1650,2030
this forward noising diffusion process.|
|

1335
00:45:01,810 --> 00:45:03,135
0,290 290,470 470,760 810,1115 1115,1325
What we do is we
我们所做的是逐渐增加噪音，这样我们就会慢慢地抹去图像中的细节，破坏信息，摧毁信息，直到我们达到纯粹的噪音状态。

1336
00:45:03,135 --> 00:45:06,240
0,825 825,1205 1765,2165 2455,2895 2895,3105
progressively add increasing increments of

1337
00:45:06,240 --> 00:45:08,280
0,290 1030,1380 1380,1650 1650,1815 1815,2040
noise such that we are

1338
00:45:08,280 --> 00:45:10,160
0,380 460,990 990,1185 1185,1500 1500,1880
slowly wiping out the details

1339
00:45:10,360 --> 00:45:12,465
0,290 290,440 440,700 1170,1835 1835,2105
in the image, corrupting the

1340
00:45:12,465 --> 00:45:15,315
0,395 625,1050 1050,1325 1345,1745 2545,2850
information, destroying that information until

1341
00:45:15,315 --> 00:45:16,700
0,305 355,720 720,930 930,1080 1080,1385
we arrive at a state

1342
00:45:16,780 --> 00:45:18,660
0,260 260,470 470,770 770,1120
that is pure noise.|
|

1343
00:45:19,070 --> 00:45:20,360
0,400
Then.|
然后。|

1344
00:45:20,710 --> 00:45:22,095
0,290 290,580 600,920 920,1160 1160,1385
What we actually build our
我们实际上构建神经网络的目的是学习从完全噪声状态返回到原始数据空间的映射，我们称之为反向过程。

1345
00:45:22,095 --> 00:45:23,790
0,240 240,510 510,780 780,1055 1375,1695
neural network to do is

1346
00:45:23,790 --> 00:45:25,875
0,225 225,530 790,1080 1080,1670 1750,2085
to learn a mapping that

1347
00:45:25,875 --> 00:45:27,810
0,315 315,585 585,875 1135,1535 1555,1935
goes from that completely noise

1348
00:45:27,810 --> 00:45:29,820
0,380 700,1080 1080,1460 1570,1830 1830,2010
state back up to the

1349
00:45:29,820 --> 00:45:32,160
0,320 610,1005 1005,1400 1870,2145 2145,2340
original data space, what we

1350
00:45:32,160 --> 00:45:34,140
0,225 225,495 495,825 825,1190
call the reverse process.|
|

1351
00:45:34,330 --> 00:45:36,075
0,380 380,620 620,1120 1170,1505 1505,1745
Learning and mapping that d
学习和映射从噪声返回到数据的d个噪声。这里的核心思想是，去噪是从完全随机的状态迭代地恢复越来越多的信息。

1352
00:45:36,075 --> 00:45:38,475
0,575 805,1155 1155,1410 1410,1715 2035,2400
noises going from noise back

1353
00:45:38,475 --> 00:45:40,875
0,365 565,825 825,1085 1945,2220 2220,2400
up to data. And the

1354
00:45:40,875 --> 00:45:42,285
0,300 300,645 645,930 930,1155 1155,1410
core idea here is that

1355
00:45:42,285 --> 00:45:44,880
0,255 255,780 780,1145 1165,2045 2065,2595
de noising is iteratively recovering

1356
00:45:44,880 --> 00:45:46,790
0,320 550,855 855,1050 1050,1340 1510,1910
back more and more information

1357
00:45:47,410 --> 00:45:49,550
0,335 335,665 665,1060 1140,1540 1740,2140
from a completely random state.|
|

1358
00:45:51,510 --> 00:45:52,600
0,290 290,440 440,590 590,785 785,1090
These are the two core
这是两个核心部分，正向去噪过程和反向去噪过程，我们将看看每个过程是如何工作的，归结为核心直觉。

1359
00:45:52,920 --> 00:45:55,030
0,400 510,800 800,1090 1290,1805 1805,2110
components, the forward noising process,

1360
00:45:55,380 --> 00:45:57,160
0,335 335,670 840,1115 1115,1490 1490,1780
the reverse de noising process,

1361
00:45:57,720 --> 00:45:58,670
0,275 275,455 455,590 590,755 755,950
and we're going to look

1362
00:45:58,670 --> 00:46:00,400
0,320 730,1035 1035,1230 1230,1425 1425,1730
at how each one works,

1363
00:46:00,570 --> 00:46:01,985
0,575 575,815 815,1025 1025,1175 1175,1415
distilled down to the core

1364
00:46:01,985 --> 00:46:03,160
0,605
intuition.|
|

1365
00:46:03,830 --> 00:46:05,455
0,400 540,935 935,1205 1205,1370 1370,1625
Underlying this is the forward
这背后是前进的不，第一步。

1366
00:46:05,455 --> 00:46:07,800
0,365 805,1095 1095,1335 1335,1685
no, the first step.|
|

1367
00:46:07,800 --> 00:46:09,260
0,380 490,840 840,990 990,1170 1170,1460
Where we're given an image
我们得到一幅图像，我们想要得到一个随机的噪声样本。

1368
00:46:09,400 --> 00:46:10,920
0,290 290,485 485,785 785,1160 1160,1520
and we want to arrive

1369
00:46:10,920 --> 00:46:12,435
0,240 240,390 390,680 790,1190 1240,1515
at a random sample of

1370
00:46:12,435 --> 00:46:13,480
0,275
noise.|
|

1371
00:46:14,890 --> 00:46:17,210
0,400 600,950 950,1300 1650,1985 1985,2320
Importantly, this process does not
重要的是，这个过程不需要任何神经网络学习任何训练。我们能够做的是定义一种固定的方式，一种从输入图像到噪声的确定方式。

1372
00:46:17,560 --> 00:46:19,550
0,400 660,1010 1010,1295 1295,1570 1590,1990
require any neural network learning

1373
00:46:19,870 --> 00:46:22,230
0,380 380,760 1500,1820 1820,2090 2090,2360
any training. What we are

1374
00:46:22,230 --> 00:46:23,670
0,300 300,570 570,780 780,1095 1095,1440
able to do is define

1375
00:46:23,670 --> 00:46:25,700
0,350 640,1035 1035,1350 1350,1650 1650,2030
a fixed way, a determined

1376
00:46:25,720 --> 00:46:27,380
0,350 350,590 590,845 845,1210 1260,1660
way to go from that

1377
00:46:27,400 --> 00:46:29,030
0,290 290,580 900,1190 1190,1355 1355,1630
input image to the noise.|
|

1378
00:46:29,970 --> 00:46:31,625
0,320 320,640 720,1070 1070,1355 1355,1655
The core idea is really
其核心理念非常简单。我们所做的就是我们有一些噪音功能。

1379
00:46:31,625 --> 00:46:33,590
0,365 1015,1305 1305,1485 1485,1725 1725,1965
simple. All we do is

1380
00:46:33,590 --> 00:46:35,140
0,165 165,375 375,710 730,1260 1260,1550
we have some noising function.|
|

1381
00:46:35,980 --> 00:46:37,680
0,330 330,680 910,1215 1215,1410 1410,1700
And starting at the first
从第一个时间步长开始，我们的初始时间步长t为零，其中我们有100%的图像，没有添加任何噪声。

1382
00:46:37,850 --> 00:46:39,880
0,365 365,730 1020,1385 1385,1730 1730,2030
time step, our initial time

1383
00:46:39,880 --> 00:46:41,965
0,255 255,495 495,800 1660,1935 1935,2085
step t zero, where we

1384
00:46:41,965 --> 00:46:44,185
0,195 195,1005 1005,1355 1675,1980 1980,2220
have 100% image, no noise

1385
00:46:44,185 --> 00:46:45,220
0,335
added.|
|

1386
00:46:45,320 --> 00:46:47,455
0,290 290,470 470,760 1050,1385 1385,2135
All we do is progressively
我们所做的就是以迭代的方式逐渐增加越来越多的噪音。

1387
00:46:47,455 --> 00:46:49,110
0,305 445,840 840,1125 1125,1335 1335,1655
add more and more noise

1388
00:46:49,640 --> 00:46:51,340
0,260 260,410 410,830 830,1090
in an iterative way.|
|

1389
00:46:51,670 --> 00:46:52,910
0,350 350,590 590,785 785,965 965,1240
Over the course of these
在这些单独的时间步的过程中。

1390
00:46:52,990 --> 00:46:54,900
0,380 380,710 710,1060
individual time steps.|
|

1391
00:46:54,900 --> 00:46:56,510
0,315 315,585 585,870 870,1215 1215,1610
Such that the result is
以至于结果变得越来越嘈杂。

1392
00:46:56,590 --> 00:46:58,660
0,400 660,950 950,1480
increasingly more noisy.|
|

1393
00:46:59,020 --> 00:47:00,480
0,350 350,575 575,785 785,1120 1170,1460
First, we go from all
首先，我们从所有的形象开始。更少的图像，更少的图像，更多的噪音，更多的噪音，所有的噪音。在这个明确的前进过程中，没有训练，就没有学习。我们所拥有的只是一个噪音功能。

1394
00:47:00,480 --> 00:47:02,750
0,290 910,1215 1215,1520 1660,1965 1965,2270
image. Less image, less image,

1395
00:47:02,770 --> 00:47:04,485
0,320 320,640 750,1055 1055,1360 1410,1715
more noise, more noise, all

1396
00:47:04,485 --> 00:47:06,770
0,305 535,825 825,1115 1225,1625 1885,2285
noise. In this defined forward

1397
00:47:06,790 --> 00:47:09,260
0,400 840,1205 1205,1570 1860,2165 2165,2470
process, no training, no learning.

1398
00:47:09,670 --> 00:47:10,665
0,290 290,440 440,665 665,875 875,995
All we have is a

1399
00:47:10,665 --> 00:47:11,960
0,375 375,665
noising function.|
|

1400
00:47:12,530 --> 00:47:14,335
0,400 900,1205 1205,1385 1385,1595 1595,1805
Now, this gives us a
现在，这给了我们一堆例子，对吗？如果我们在数据集中有一堆实例，我们会将这种噪声应用于每个实例。所以我们在每个不同的噪声时间步长上都有这些切片。

1401
00:47:14,335 --> 00:47:15,865
0,195 195,465 465,815 955,1305 1305,1530
bunch of examples, right? If

1402
00:47:15,865 --> 00:47:16,645
0,150 150,270 270,405 405,600 600,780
we have a bunch of

1403
00:47:16,645 --> 00:47:19,140
0,635 745,1530 1530,1770 1770,1905 1905,2495
instances instances in the dataset,

1404
00:47:19,340 --> 00:47:20,755
0,365 365,665 665,905 905,1280 1280,1415
we apply this noising to

1405
00:47:20,755 --> 00:47:22,075
0,225 225,480 480,615 615,875 1015,1320
every one of them. And

1406
00:47:22,075 --> 00:47:24,060
0,240 240,495 495,815 1105,1505 1555,1985
so we have those slices

1407
00:47:24,230 --> 00:47:25,500
0,320 320,545 545,725 725,935 935,1270
at each of these different

1408
00:47:25,670 --> 00:47:27,520
0,530 530,800 800,1150
noising time steps.|
|

1409
00:47:27,950 --> 00:47:29,725
0,335 335,605 605,920 920,1300 1380,1775
Our goal now in learning
我们现在的目标是学习神经网络。

1410
00:47:29,725 --> 00:47:31,400
0,270 270,495 495,755
the neural network.|
|

1411
00:47:31,400 --> 00:47:33,560
0,285 285,480 480,770 1060,1460 1840,2160
Is to then learn how
然后学习如何在这个相反的过程中做，从。

1412
00:47:33,560 --> 00:47:35,150
0,195 195,620 700,975 975,1245 1245,1590
to do in this reverse

1413
00:47:35,150 --> 00:47:38,060
0,350 1300,1700 1720,2120
process, starting from.|
|

1414
00:47:38,060 --> 00:47:39,340
0,195 195,405 405,660 660,960 960,1280
Data in the input space
输入空间中的数据噪声过程引导我们得到的是这些迭代的、更有噪声的例子的时间序列。

1415
00:47:39,600 --> 00:47:41,540
0,290 290,470 470,860 860,1150 1650,1940
what the noising process LED

1416
00:47:41,540 --> 00:47:43,565
0,255 255,620 1000,1350 1350,1665 1665,2025
us to is this time

1417
00:47:43,565 --> 00:47:45,755
0,395 775,1065 1065,1275 1275,1935 1935,2190
series of these iteratively, more

1418
00:47:45,755 --> 00:47:47,580
0,510 510,905
noised examples.|
|

1419
00:47:48,170 --> 00:47:51,010
0,400 780,1115 1115,1450 1620,2020 2520,2840
Now our task is, given
现在我们的任务是，给出一幅图像，我们能不能在这些时间步长上，给出一个给定的切片，让我们说，我们所要问的是，我们能不能知道下一个，下一个最多的。

1420
00:47:51,010 --> 00:47:53,545
0,180 180,440 1090,1410 1410,1730 2230,2535
an image, can we given

1421
00:47:53,545 --> 00:47:54,805
0,305 325,690 690,930 930,1065 1065,1260
a given one of these

1422
00:47:54,805 --> 00:47:56,410
0,365 475,780 780,1020 1020,1305 1305,1605
slices at these time steps,

1423
00:47:56,410 --> 00:47:58,660
0,345 345,555 555,920 1780,2055 2055,2250
let's say t all we're

1424
00:47:58,660 --> 00:47:59,940
0,225 225,570 570,810 810,990 990,1280
asking is can we learn

1425
00:48:00,440 --> 00:48:04,200
0,320 320,640 2130,2530 2610,3010
the next, next most.|
|

1426
00:48:04,390 --> 00:48:06,135
0,485 485,620 620,815 815,1150 1440,1745
Do is the example from
DO就是那个时间设定的例子。

1427
00:48:06,135 --> 00:48:07,520
0,225 225,480 480,815
that time set.|
|

1428
00:48:07,530 --> 00:48:09,590
0,400 420,725 725,1030 1050,1450 1710,2060
Making this more concrete, walk
让它变得更具体，穿过它。

1429
00:48:09,590 --> 00:48:10,620
0,195 195,440
through it.|
|

1430
00:48:11,200 --> 00:48:13,140
0,365 365,710 710,1090 1500,1775 1775,1940
A particular image at a
一张切片上的特定图像，让我们称之为t，我们要求神经网络学习的是什么。

1431
00:48:13,140 --> 00:48:14,750
0,320 550,990 990,1140 1140,1305 1305,1610
slice, let's call it t,

1432
00:48:15,820 --> 00:48:17,010
0,290 290,470 470,710 710,950 950,1190
what we ask the neural

1433
00:48:17,010 --> 00:48:19,170
0,255 255,510 510,770 1360,1760 1870,2160
network to learn is what

1434
00:48:19,170 --> 00:48:20,180
0,290
is.|
|

1435
00:48:20,850 --> 00:48:22,870
0,455 455,820 1050,1340 1340,1625 1625,2020
Estimated image at that prior
在前一步估计图像，t减1。

1436
00:48:23,130 --> 00:48:25,600
0,400 660,1055 1055,1445 1445,1840
step, t minus one.|
|

1437
00:48:26,020 --> 00:48:28,350
0,365 365,620 620,910 1020,1420 1920,2330
Making this very concrete, let's
让这个非常具体，让我们假设我们在时间3有这张有噪音的图像。

1438
00:48:28,350 --> 00:48:30,750
0,165 165,360 360,650 1240,1640 1990,2400
say we have this noised

1439
00:48:30,750 --> 00:48:33,000
0,260 640,975 975,1275 1275,1640
image at time three.|
|

1440
00:48:33,000 --> 00:48:34,410
0,240 240,525 525,800 820,1125 1125,1410
The neural network is then
然后，神经网络试图预测之前一个步骤的去噪结果。

1441
00:48:34,410 --> 00:48:36,410
0,330 330,555 555,830 1360,1680 1680,2000
trying to predict what was

1442
00:48:36,430 --> 00:48:37,980
0,290 290,470 470,875 875,1235 1235,1550
the de noise result at

1443
00:48:37,980 --> 00:48:40,080
0,315 315,705 705,990 990,1280
just one step before.|
|

1444
00:48:41,800 --> 00:48:43,230
0,545 545,815 815,1040 1040,1310 1310,1430
Comparing these two, let's take
将这两者进行比较，让我们后退一步。

1445
00:48:43,230 --> 00:48:44,580
0,150 150,345 345,650
a step back.|
|

1446
00:48:44,890 --> 00:48:46,410
0,335 335,605 605,830 830,1120 1170,1520
All these two images differ
所有这两个图像的不同之处在于噪声函数。

1447
00:48:46,410 --> 00:48:48,500
0,350 610,945 945,1280 1360,1725 1725,2090
by is that noise function.|
|

1448
00:48:49,690 --> 00:48:51,810
0,320 320,640 1050,1385 1385,1720 1740,2120
And so the question that
因此，向神经网络提出的问题。

1449
00:48:51,810 --> 00:48:53,090
0,450 450,615 615,750 750,990 990,1280
posed to the neural network.|
|

1450
00:48:53,960 --> 00:48:55,090
0,240 240,450 450,630 630,825 825,1130
In the course of training.|
在训练过程中。|

1451
00:48:56,070 --> 00:48:57,520
0,400
Is.|
是。|

1452
00:48:57,560 --> 00:48:59,290
0,305 305,485 485,760 1260,1565 1565,1730
How can we? How can
我们怎么能做到呢？我们如何才能了解这种区别呢？

1453
00:48:59,290 --> 00:49:01,160
0,165 165,390 390,630 630,950
we learn this difference?|
|

1454
00:49:01,770 --> 00:49:02,960
0,275 275,470 470,740 740,995 995,1190
If our goal is to
如果我们的目标是学习这个迭代去噪过程，我们有所有这些连续的步骤。

1455
00:49:02,960 --> 00:49:04,870
0,240 240,510 510,1020 1020,1605 1605,1910
learn this iterative denoising process,

1456
00:49:05,280 --> 00:49:06,515
0,290 290,470 470,695 695,935 935,1235
and we have all these

1457
00:49:06,515 --> 00:49:08,360
0,840 840,1175
consecutive steps.|
|

1458
00:49:08,890 --> 00:49:10,260
0,350 350,590 590,800 800,1115 1115,1370
Going from more noise to
从更多的噪音到更少的噪音，我向大家提出一个问题。你怎么能想到呢？

1459
00:49:10,260 --> 00:49:12,090
0,210 210,560 1120,1410 1410,1650 1650,1830
less noise, I pose a

1460
00:49:12,090 --> 00:49:13,620
0,255 255,495 495,600 600,860 1210,1530
question to you all. How

1461
00:49:13,620 --> 00:49:15,160
0,180 180,315 315,525 525,860
could you think about?|
|

1462
00:49:15,260 --> 00:49:16,975
0,500 500,635 635,910 1080,1550 1550,1715
Defining a loss defining a
定义损失定义神经网络要学习的训练目标。

1463
00:49:16,975 --> 00:49:18,760
0,305 385,785 1075,1380 1380,1560 1560,1785
training objective for the neural

1464
00:49:18,760 --> 00:49:20,080
0,260 280,540 540,800
network to learn.|
|

1465
00:49:22,810 --> 00:49:24,340
0,380 380,760
Any ideas?|
有什么主意吗？|

1466
00:49:25,650 --> 00:49:26,940
0,400
Yes.|
是。|

1467
00:49:29,820 --> 00:49:30,970
0,260 260,455 455,710 710,890 890,1150
Can you expand a little
你能在这一点上做进一步的阐述吗？

1468
00:49:31,020 --> 00:49:32,420
0,305 305,485 485,760
further on that?|
|

1469
00:49:47,310 --> 00:49:48,890
0,275 275,550 810,1115 1115,1355 1355,1580
It gets. So the idea
它会变得。所以我们的想法是，我们能不能看看同样的概念，试图将图像中的信息编码到一个可能减少的潜在空间，并试图了解它正在得到的，它得到的是与网络实际正在做的事情非常相关的东西，但更简单的东西。

1470
00:49:48,890 --> 00:49:50,225
0,290 340,645 645,930 930,1185 1185,1335
was, can we look at

1471
00:49:50,225 --> 00:49:51,815
0,180 180,420 420,755 805,1205 1225,1590
the same concept of trying

1472
00:49:51,815 --> 00:49:54,905
0,365 505,1355 1675,2075 2695,2970 2970,3090
to encode information from an

1473
00:49:54,905 --> 00:49:56,630
0,245 325,615 615,905 1105,1455 1455,1725
image to maybe a reduced

1474
00:49:56,630 --> 00:49:57,830
0,375 375,600 600,825 825,1035 1035,1200
latent space and try to

1475
00:49:57,830 --> 00:50:00,140
0,165 165,470 1240,1620 1620,1880 1960,2310
learn that it's getting, it's

1476
00:50:00,140 --> 00:50:02,690
0,260 610,1190 1600,1980 1980,2325 2325,2550
getting that's very related to

1477
00:50:02,690 --> 00:50:04,010
0,195 195,480 480,780 780,1080 1080,1320
what the network is actually

1478
00:50:04,010 --> 00:50:06,790
0,290 640,1040 1450,1830 1830,2190 2190,2780
doing, but something even simpler.|
|

1479
00:50:07,280 --> 00:50:09,200
0,315 315,680 1360,1665 1665,1815 1815,1920
Thinking about how can we
思考我们如何才能只比较这两个图像？

1480
00:50:09,200 --> 00:50:10,420
0,240 240,540 540,780 780,960 960,1220
just compare these two images?|
|

1481
00:50:11,630 --> 00:50:13,120
0,400
Yes.|
是。|

1482
00:50:18,680 --> 00:50:20,520
0,400 660,950 950,1175 1175,1475 1475,1840
Simple. Just think, really. Simply,
很简单。想一想，真的。简单地说，是的。

1483
00:50:21,020 --> 00:50:22,420
0,400
yes.|
|

1484
00:50:23,450 --> 00:50:25,510
0,400 660,1010 1010,1265 1265,1570 1770,2060
Exactly. The idea was how
一点儿没错。我们的想法是，有多少像素是相同的。事实证明，我们所需要做的就是看看这两个步骤之间的像素差异，而扩散模型背后的智慧就是定义这一点。

1485
00:50:25,510 --> 00:50:26,500
0,165 165,300 300,435 435,810 810,990
many of the pixels are

1486
00:50:26,500 --> 00:50:28,090
0,165 165,440 850,1200 1200,1425 1425,1590
the same. All we need

1487
00:50:28,090 --> 00:50:29,100
0,165 165,315 315,480 480,690 690,1010
to do, it turns out,

1488
00:50:29,450 --> 00:50:30,790
0,305 305,515 515,710 710,920 920,1340
is look at the pixel

1489
00:50:30,790 --> 00:50:33,025
0,290 310,710 1360,1695 1695,1965 1965,2235
wise difference between these two

1490
00:50:33,025 --> 00:50:35,905
0,335 895,1230 1230,1515 1515,2165 2545,2880
steps and the cleverness behind

1491
00:50:35,905 --> 00:50:38,160
0,480 480,845 1165,1530 1530,1980 1980,2255
diffusion models is defining that

1492
00:50:38,210 --> 00:50:39,320
0,400
exact.|
|

1493
00:50:40,920 --> 00:50:43,300
0,650 650,970 1290,1640 1640,1910 1910,2380
Residual noise as what defines
定义损耗的是残余噪声。

1494
00:50:43,380 --> 00:50:44,740
0,275 275,550
the loss.|
|

1495
00:50:44,740 --> 00:50:46,645
0,225 225,530 640,1040 1120,1520 1630,1905
The training objective for such
对于这样一个模型的训练目标，你所要做的就是比较这些迭代、迭代噪声的连续时间步长，然后问，均方误差是多少？这两者之间的像素差异是什么？

1496
00:50:46,645 --> 00:50:48,340
0,150 150,425 955,1260 1260,1440 1440,1695
a model, all you do

1497
00:50:48,340 --> 00:50:50,350
0,380 430,765 765,1020 1020,1740 1740,2010
is compare these consecutive time

1498
00:50:50,350 --> 00:50:53,550
0,350 610,915 915,1520 2110,2655 2655,3200
steps of iterative, iterative noying

1499
00:50:53,810 --> 00:50:55,570
0,290 290,580 1050,1325 1325,1490 1490,1760
and ask, what is that

1500
00:50:55,570 --> 00:50:56,890
0,330 330,615 615,900 900,1155 1155,1320
mean squared error? What is

1501
00:50:56,890 --> 00:50:58,765
0,255 255,735 735,1020 1020,1400 1570,1875
that pixel wise difference between

1502
00:50:58,765 --> 00:50:59,920
0,255 255,605
those two?|
|

1503
00:50:59,930 --> 00:51:01,270
0,275 275,440 440,635 635,940 1050,1340
And it turns out that
事实证明，这在实践中非常有效，我们可以训练神经网络，通过这种非常直观的损失概念来进行迭代去噪。

1504
00:51:01,270 --> 00:51:03,085
0,210 210,530 640,1040 1090,1490 1510,1815
this works very effectively in

1505
00:51:03,085 --> 00:51:04,945
0,305 985,1260 1260,1395 1395,1605 1605,1860
practice, that we can train

1506
00:51:04,945 --> 00:51:06,490
0,195 195,450 450,725 1075,1365 1365,1545
a neural network to do

1507
00:51:06,490 --> 00:51:08,530
0,290 370,930 930,1110 1110,1640 1690,2040
this iterative de noising by

1508
00:51:08,530 --> 00:51:11,785
0,255 255,560 1510,2300 2740,3060 3060,3255
this very intuitive idea of

1509
00:51:11,785 --> 00:51:12,940
0,150 150,425
the loss.|
|

1510
00:51:13,990 --> 00:51:15,260
0,260 260,520
All right.|
好的。|

1511
00:51:15,330 --> 00:51:16,960
0,365 365,730 750,1055 1055,1295 1295,1630
So hopefully this gives you
因此，希望这能给你提供空间，说明扩散模型如何建立对去噪过程的理解，并能够学习迭代地去噪例子。

1512
00:51:16,980 --> 00:51:18,740
0,305 305,605 605,1000 1260,1580 1580,1760
the space of how the

1513
00:51:18,740 --> 00:51:21,190
0,360 360,710 1420,1905 1905,2130 2130,2450
diffusion model builds up this

1514
00:51:21,330 --> 00:51:23,230
0,400 420,725 725,950 950,1610 1610,1900
understanding of the denoising process

1515
00:51:23,610 --> 00:51:24,850
0,275 275,455 455,740 740,980 980,1240
and is able to go

1516
00:51:25,740 --> 00:51:28,150
0,320 320,640 1080,1355 1355,1955 1955,2410
in learning to iteratively deno

1517
00:51:28,890 --> 00:51:30,100
0,400
examples.|
|

1518
00:51:30,310 --> 00:51:33,225
0,400 780,1115 1115,1450 1470,1870 2610,2915
Now the task is how
现在的任务是我们如何才能真正品尝到全新的东西？我们如何生成一个新实例？

1519
00:51:33,225 --> 00:51:34,850
0,165 165,425 445,765 765,1085 1225,1625
can we actually sample something

1520
00:51:34,930 --> 00:51:36,225
0,365 365,710 710,995 995,1145 1145,1295
brand new? How can we

1521
00:51:36,225 --> 00:51:38,360
0,300 300,585 585,875 1045,1445
generate a new instance?|
|

1522
00:51:38,360 --> 00:51:40,070
0,345 345,690 690,980 1030,1430 1450,1710
Well, it's very related to
嗯，这与我们刚刚经历的那个完全相反的过程非常相关，去噪过程。

1523
00:51:40,070 --> 00:51:42,320
0,255 255,650 850,1250 1270,1670 1960,2250
that exact reverse process, the

1524
00:51:42,320 --> 00:51:43,565
0,165 165,540 540,825 825,1095 1095,1245
de noising process that we

1525
00:51:43,565 --> 00:51:44,740
0,180 180,420 420,755
just walked through.|
|

1526
00:51:45,040 --> 00:51:46,185
0,275 275,425 425,665 665,920 920,1145
What we do is we
我们要做的就是拿出一个全新的完全随机噪音的例子。

1527
00:51:46,185 --> 00:51:48,390
0,330 330,725 805,1140 1140,1475 1825,2205
take a brand new instance

1528
00:51:48,390 --> 00:51:50,680
0,380 430,780 780,1125 1125,1520
of completely random noise.|
|

1529
00:51:51,150 --> 00:51:54,440
0,400 1410,1715 1715,1970 1970,2320 2790,3290
And take our trained diffusion
然后用我们训练过的扩散模型来问它，好的，只需预测剩余噪声差，就可以让我们得到噪音稍微小一些的东西。这就是在这些迭代的时间步长上重复做的所有事情，这样我们就可以走了。

1530
00:51:54,440 --> 00:51:56,650
0,350 970,1290 1290,1530 1530,1830 1830,2210
model and ask it, okay,

1531
00:51:56,730 --> 00:51:58,550
0,335 335,670 690,980 980,1550 1550,1820
just predict that residual noise

1532
00:51:58,550 --> 00:52:00,460
0,350 850,1155 1155,1395 1395,1620 1620,1910
difference that will get us

1533
00:52:00,480 --> 00:52:02,080
0,290 290,575 575,830 830,1090 1200,1600
to something that is slightly

1534
00:52:02,160 --> 00:52:04,540
0,400 480,1120 1590,1895 1895,2090 2090,2380
less noisy. And this is

1535
00:52:04,620 --> 00:52:06,635
0,305 305,470 470,620 620,910 1350,2015
all that is done repeatedly

1536
00:52:06,635 --> 00:52:08,230
0,270 270,510 510,990 990,1245 1245,1595
at these iterative time steps

1537
00:52:08,700 --> 00:52:09,880
0,320 320,545 545,725 725,890 890,1180
such that we can go.|
|

1538
00:52:11,270 --> 00:52:13,495
0,400 870,1175 1175,1480 1680,1970 1970,2225
From pure noise to something
从纯粹的噪音到不那么嘈杂的东西，反复发生，随着这个过程的发生，希望你能看到反映图像相关内容的东西的出现。

1539
00:52:13,495 --> 00:52:16,735
0,300 300,905 1315,2045 2605,2955 2955,3240
less noisy, repeatedly, and as

1540
00:52:16,735 --> 00:52:18,775
0,240 240,545 655,1055 1375,1770 1770,2040
this process occurs, hopefully you

1541
00:52:18,775 --> 00:52:20,640
0,275 325,645 645,915 915,1500 1500,1865
can see the emergence of

1542
00:52:20,840 --> 00:52:23,610
0,400 480,880 1140,1540 1770,2170 2370,2770
something that reflects image related

1543
00:52:23,660 --> 00:52:24,820
0,400
content.|
|

1544
00:52:24,890 --> 00:52:26,310
0,350 350,605 605,815 815,1070 1070,1420
Time step by time step
时间一步一步地迭代地做这个采样程序。

1545
00:52:26,450 --> 00:52:28,800
0,785 785,1100 1100,1370 1370,1955 1955,2350
iteratively doing this sampling procedure.|
|

1546
00:52:29,970 --> 00:52:31,925
0,365 365,680 680,1030 1410,1700 1700,1955
Over many generations and many
跨越了许多世代和许多时间步调，这样我们就可以回到这只通过我们窥视的狗的角度。

1547
00:52:31,925 --> 00:52:33,830
0,330 330,695 1165,1500 1500,1740 1740,1905
time steps such that we

1548
00:52:33,830 --> 00:52:35,105
0,150 150,360 360,680 910,1155 1155,1275
can get back to the

1549
00:52:35,105 --> 00:52:36,710
0,255 255,525 525,735 735,1055 1195,1605
point of this dog that's

1550
00:52:36,710 --> 00:52:38,360
0,300 300,510 510,720 720,980
peeking out through us.|
|

1551
00:52:38,730 --> 00:52:39,635
0,260 260,410 410,590 590,755 755,905
What I want you to
我想让你们明白的是，扩散模型实现的是。

1552
00:52:39,635 --> 00:52:40,960
0,210 210,450 450,735 735,1020 1020,1325
take away with is that

1553
00:52:41,370 --> 00:52:42,970
0,290 290,455 455,830 830,1115 1115,1600
what the diffusion model enables

1554
00:52:43,080 --> 00:52:44,520
0,290 290,580
is that.|
|

1555
00:52:44,520 --> 00:52:46,310
0,240 240,480 480,740 760,1160 1390,1790
Going from a completely random
从一个完全随机的噪声状态，我们有最大的可变性概念。

1556
00:52:46,630 --> 00:52:48,800
0,395 395,790 1320,1625 1625,1850 1850,2170
noise state where we have

1557
00:52:48,820 --> 00:52:50,990
0,365 365,730 960,1325 1325,1580 1580,2170
this maximum notion of variability.|
|

1558
00:52:52,150 --> 00:52:53,865
0,400 630,1115 1115,1250 1250,1430 1430,1715
By defining it and breaking
通过定义它并将其分解为这个迭代过程，在每个步骤中都有一个做出的预测。有一个世代，所以最大的变异性转化为生成的样本中的最大多样性。

1559
00:52:53,865 --> 00:52:55,245
0,255 255,510 510,720 720,885 885,1380
it down to this iterative

1560
00:52:55,245 --> 00:52:57,050
0,305 775,1110 1110,1350 1350,1530 1530,1805
process, at each of those

1561
00:52:57,310 --> 00:52:58,920
0,400 720,965 965,1070 1070,1220 1220,1610
steps there is a prediction

1562
00:52:58,920 --> 00:52:59,835
0,360 360,525 525,675 675,780 780,915
that's made. There is a

1563
00:52:59,835 --> 00:53:01,850
0,275 445,840 840,1115 1375,1695 1695,2015
generation that's made, and so

1564
00:53:02,020 --> 00:53:04,935
0,400 450,850 1020,1720 1950,2285 2285,2915
that maximum variability is translated

1565
00:53:04,935 --> 00:53:07,470
0,395 595,995 1135,1535 2065,2355 2355,2535
into maximum diversity in the

1566
00:53:07,470 --> 00:53:09,140
0,290 430,1070
generated samples.|
|

1567
00:53:09,550 --> 00:53:11,265
0,335 335,670 1050,1355 1355,1535 1535,1715
Such that when we get
这样，当我们到达最终状态时，我们就得到了一幅图像，一个完全没有噪音的数据空间中的合成样本。

1568
00:53:11,265 --> 00:53:12,600
0,165 165,300 300,525 525,875 1075,1335
to the end state, we

1569
00:53:12,600 --> 00:53:13,940
0,255 255,650 670,945 945,1080 1080,1340
have arrived at an image,

1570
00:53:14,050 --> 00:53:16,100
0,305 305,725 725,1090 1350,1775 1775,2050
a synthetic sample that's back

1571
00:53:16,150 --> 00:53:18,260
0,260 260,520 540,940 1230,1630 1710,2110
in our completely noise free

1572
00:53:18,400 --> 00:53:20,300
0,400 540,940
data space.|
|

1573
00:53:21,180 --> 00:53:22,955
0,400 480,755 755,965 965,1300 1470,1775
So at its core, this
所以在它的核心，这是一个扩散模型如何工作的总结，以及它们如何能够从完全随机的噪音变成非常忠实的、多样化的、新产生的样本。

1574
00:53:22,955 --> 00:53:24,740
0,305 535,825 825,1115 1225,1560 1560,1785
is the summary of how

1575
00:53:24,740 --> 00:53:26,690
0,165 165,510 510,840 840,1220 1600,1950
a diffusion model works and

1576
00:53:26,690 --> 00:53:28,480
0,350 550,960 960,1220 1270,1530 1530,1790
how they're able to go

1577
00:53:28,620 --> 00:53:30,880
0,400 420,785 785,1130 1130,1510 1860,2260
from completely random noise to

1578
00:53:31,110 --> 00:53:34,330
0,400 750,1450 1530,1930 2460,2930 2930,3220
very faithful, diverse, newly generated

1579
00:53:34,500 --> 00:53:35,720
0,640
samples.|
|

1580
00:53:36,840 --> 00:53:37,880
0,290 290,530 530,635 635,815 815,1040
As I've kind of been
正如我已经提到的，我认为这种方法的强大之处在于，我们可以从完全随机开始。

1581
00:53:37,880 --> 00:53:39,065
0,465 465,675 675,825 825,975 975,1185
alluding to, what I think

1582
00:53:39,065 --> 00:53:40,720
0,270 270,585 585,935 1075,1365 1365,1655
is so powerful about this

1583
00:53:40,740 --> 00:53:43,175
0,400 600,875 875,1145 1145,1540 2130,2435
approach and this idea is

1584
00:53:43,175 --> 00:53:44,570
0,210 210,515 685,990 990,1170 1170,1395
the fact that we can

1585
00:53:44,570 --> 00:53:47,220
0,345 345,740 820,1215 1215,1970
start from complete randomness.|
|

1586
00:53:47,570 --> 00:53:50,950
0,275 275,1115 1115,1480 1710,2380 3060,3380
It encapsulates maximum variability such
它封装了最大的可变性，因此现在扩散模型能够产生非常、非常多样化的样本。

1587
00:53:50,950 --> 00:53:52,645
0,255 255,590 610,1110 1110,1425 1425,1695
that now diffusion models are

1588
00:53:52,645 --> 00:53:54,840
0,300 300,645 645,960 960,1325 1525,2195
able to produce generated samples

1589
00:53:55,160 --> 00:53:56,550
0,275 275,485 485,770 770,1055 1055,1390
that are very, very diverse.|
|

1590
00:53:57,580 --> 00:53:59,350
0,380 520,870 870,1350 1350,1530 1530,1770
From noise samples that are
来自彼此根本不同的噪声样本。

1591
00:53:59,350 --> 00:54:01,110
0,855 855,1125 1125,1335 1335,1470 1470,1760
fundamentally different to each other.|
|

1592
00:54:02,220 --> 00:54:03,670
0,290 290,485 485,790 810,1130 1130,1450
What this means is now
这意味着现在在这个例子中，我们从一些噪声样本转到这张图像。

1593
00:54:04,050 --> 00:54:05,360
0,275 275,455 455,760 780,1100 1100,1310
in this one instance, we

1594
00:54:05,360 --> 00:54:07,145
0,180 180,405 405,740 1090,1440 1440,1785
go from some noise sample

1595
00:54:07,145 --> 00:54:08,660
0,255 255,420 420,725
to this image.|
|

1596
00:54:08,770 --> 00:54:11,475
0,400 1110,1960 2100,2390 2390,2540 2540,2705
But equivalently and what is
但同样令人震惊的是，如果我们现在考虑一个完全不同的随机噪声实例。

1597
00:54:11,475 --> 00:54:13,230
0,465 465,675 675,855 855,1145 1435,1755
striking is if we consider

1598
00:54:13,230 --> 00:54:15,350
0,210 210,500 580,980 1000,1400 1720,2120
now a completely different instance

1599
00:54:15,520 --> 00:54:17,300
0,290 290,575 575,970
of random noise.|
|

1600
00:54:17,550 --> 00:54:19,220
0,410 410,730 750,1150 1230,1505 1505,1670
They're very variable from each
他们彼此之间的差异很大，对吗？随机噪声、随机噪声、内部最大变异性和相互比较。

1601
00:54:19,220 --> 00:54:21,455
0,285 285,680 1060,1440 1440,1820 1870,2235
other, right? Random noise, random

1602
00:54:21,455 --> 00:54:24,290
0,365 595,995 1135,1835 2035,2580 2580,2835
noise, maximum variability internally and

1603
00:54:24,290 --> 00:54:25,630
0,315 315,680 700,945 945,1065 1065,1340
in comparison to each other.|
|

1604
00:54:26,530 --> 00:54:28,710
0,365 365,730 1380,1655 1655,1880 1880,2180
The result as a result
去噪的结果将是一个不同的采样图像，它高度多样化，与我们以前看到的非常不同。

1605
00:54:28,710 --> 00:54:30,050
0,225 225,420 420,630 630,1035 1035,1340
of that de noising generation

1606
00:54:30,670 --> 00:54:31,695
0,320 320,545 545,695 695,830 830,1025
is going to be a

1607
00:54:31,695 --> 00:54:33,960
0,305 325,900 900,1265 1555,1980 1980,2265
different sampled image that's highly

1608
00:54:33,960 --> 00:54:36,090
0,380 760,1125 1125,1490 1690,1965 1965,2130
diverse, highly different to what

1609
00:54:36,090 --> 00:54:37,480
0,180 180,390 390,710
we saw before.|
|

1610
00:54:37,580 --> 00:54:38,965
0,290 290,650 650,875 875,1100 1100,1385
And that's really the power
这就是扩散模型在思考我们如何产生非常不同和多样化的新样本时的力量。问个小问题，是的。

1611
00:54:38,965 --> 00:54:40,770
0,255 255,615 615,965 1195,1500 1500,1805
of diffusion models in thinking

1612
00:54:40,790 --> 00:54:42,360
0,400 600,920 920,1100 1100,1265 1265,1570
about how we can generate

1613
00:54:42,770 --> 00:54:44,995
0,320 320,880 1110,1370 1370,1630 1890,2225
new samples that are very

1614
00:54:44,995 --> 00:54:47,070
0,330 330,615 615,905 1435,1755 1755,2075
different and diverse. Quick question,

1615
00:54:47,120 --> 00:54:48,220
0,400
yes.|
|

1616
00:54:52,120 --> 00:54:53,460
0,400
Yep.|
是啊。|

1617
00:54:54,150 --> 00:54:55,670
0,440 440,560 560,820 900,1280 1280,1520
That's a fantastic question. So
这是个很棒的问题。因此，问题是，模型如何知道何时停止？当您构建这些模型时，您定义了一组时间步长，这是一个在构建和训练扩散模型时可以使用的参数。而且有不同的研究来观察什么有效，什么不有效。

1618
00:54:55,670 --> 00:54:56,855
0,135 135,375 375,675 675,945 945,1185
the question was, how does

1619
00:54:56,855 --> 00:54:58,040
0,195 195,480 480,795 795,1005 1005,1185
the model know when to

1620
00:54:58,040 --> 00:54:59,690
0,290 790,1065 1065,1215 1215,1410 1410,1650
stop? When you build these

1621
00:54:59,690 --> 00:55:01,445
0,320 370,750 750,1130 1180,1500 1500,1755
models, you define a set

1622
00:55:01,445 --> 00:55:03,550
0,300 300,585 585,870 870,1235 1705,2105
number of time steps, and

1623
00:55:03,630 --> 00:55:05,770
0,290 290,580 660,1060 1230,1805 1805,2140
this is a parameter that

1624
00:55:06,000 --> 00:55:07,235
0,275 275,455 455,695 695,965 965,1235
you can play around with

1625
00:55:07,235 --> 00:55:08,750
0,255 255,570 570,870 870,1175 1255,1515
when building and training the

1626
00:55:08,750 --> 00:55:11,030
0,345 345,710 1390,1790 1900,2145 2145,2280
diffusion model. And there are

1627
00:55:11,030 --> 00:55:12,725
0,255 255,570 570,825 825,1130 1390,1695
different studies looking at what

1628
00:55:12,725 --> 00:55:14,290
0,305 535,825 825,1005 1005,1305 1305,1565
works and what doesn't work.|
|

1629
00:55:15,690 --> 00:55:16,970
0,365 365,680 680,890 890,1055 1055,1280
The core idea is, by
核心思想是，通过更多的时间步长，你将产生更好的世代分辨率。但就模型的稳定性以及培训和学习的效率而言，这也是一种权衡。

1630
00:55:16,970 --> 00:55:18,425
0,270 270,570 570,915 915,1260 1260,1455
more time steps, you're going

1631
00:55:18,425 --> 00:55:20,680
0,335 415,780 780,1145 1555,1905 1905,2255
to result in better resolution

1632
00:55:21,000 --> 00:55:23,240
0,400 570,860 860,1150 1800,2060 2060,2240
effectively of generations. But it's

1633
00:55:23,240 --> 00:55:24,250
0,120 120,300 300,510 510,720 720,1010
a trade off as well

1634
00:55:24,420 --> 00:55:25,625
0,290 290,515 515,740 740,920 920,1205
in terms of the stability

1635
00:55:25,625 --> 00:55:26,770
0,270 270,405 405,600 600,840 840,1145
of the model and how

1636
00:55:27,120 --> 00:55:28,700
0,605 605,785 785,965 965,1265 1265,1580
efficiently it can train and

1637
00:55:28,700 --> 00:55:29,500
0,320
learn.|
|

1638
00:55:56,850 --> 00:55:59,300
0,400 1470,1745 1745,1925 1925,2195 2195,2450
So, so the question was
所以，所以问题是如果。

1639
00:55:59,300 --> 00:56:00,320
0,290
if.|
|

1640
00:56:00,420 --> 00:56:01,640
0,395 395,680 680,890 890,1040 1040,1220
Explaining why there could be
解释为什么扩散模型的结果中可能会有一些遗漏的细节或不正确的细节，特别是汉斯。

1641
00:56:01,640 --> 00:56:03,460
0,240 240,560 700,1035 1035,1245 1245,1820
some missing details or incorrect

1642
00:56:03,480 --> 00:56:06,130
0,400 1200,1600 1710,2045 2045,2315 2315,2650
details in the results of

1643
00:56:06,240 --> 00:56:07,970
0,260 260,590 590,875 875,1240 1380,1730
the diffusion models and the

1644
00:56:07,970 --> 00:56:09,965
0,350 400,800 820,1220 1330,1695 1695,1995
particular example was Hans in

1645
00:56:09,965 --> 00:56:11,540
0,365
particular.|
|

1646
00:56:11,540 --> 00:56:12,440
0,180 180,390 390,510 510,720 720,900
I don't know if there
我不知道为什么手似乎会引起问题，是否有特定的原因。我个人还没有看到讨论这一问题的报告、例子或文献，但我认为。

1647
00:56:12,440 --> 00:56:14,690
0,120 120,285 285,590 1450,1850 1900,2250
is a specific reason for

1648
00:56:14,690 --> 00:56:17,855
0,350 550,950 1750,2100 2100,2450 2830,3165
why hands seem to cause

1649
00:56:17,855 --> 00:56:20,410
0,335 775,1080 1080,1380 1380,1925 2155,2555
issues. I personally haven't seen

1650
00:56:20,580 --> 00:56:22,630
0,320 320,590 590,940 990,1390 1650,2050
reports or examples or literature

1651
00:56:23,460 --> 00:56:25,330
0,365 365,730 1080,1385 1385,1580 1580,1870
discussing that, but I think.|
|

1652
00:56:27,330 --> 00:56:28,960
0,320 320,515 515,790 960,1295 1295,1630
The, the general point is,
这个，大体上是，是的，会有，会有不完美的地方，对吧？不会是100%。

1653
00:56:29,130 --> 00:56:30,260
0,335 335,545 545,740 740,935 935,1130
yes, there is, there are

1654
00:56:30,260 --> 00:56:31,805
0,210 210,345 345,590 730,1380 1380,1545
going to be imperfections, right?

1655
00:56:31,805 --> 00:56:32,615
0,225 225,390 390,570 570,690 690,810
It's not going to be

1656
00:56:32,615 --> 00:56:34,640
0,845
100%.|
|

1657
00:56:34,840 --> 00:56:36,525
0,400 480,880 900,1235 1235,1475 1475,1685
Perfect or accurate in terms
完美的，准确的在对真实的榜样的忠诚方面完美或准确的我认为现在的许多进展都像是思考对底层体系结构进行哪些修改，可以尝试缓解其中的一些细节或问题。但为了讲课，我也可以在后面进一步讨论这个特定的例子。

1658
00:56:36,525 --> 00:56:39,375
0,305 1315,2105 2125,2385 2385,2625 2625,2850
of faithfulness to the, to

1659
00:56:39,375 --> 00:56:41,760
0,135 135,425 745,1145 1975,2235 2235,2385
the true example. And I

1660
00:56:41,760 --> 00:56:44,100
0,290 610,900 900,1080 1080,1370 1840,2340
think a lot of advances

1661
00:56:44,100 --> 00:56:45,465
0,150 150,440 460,780 780,1065 1065,1365
are now like thinking about

1662
00:56:45,465 --> 00:56:47,250
0,210 210,485 595,1385 1405,1665 1665,1785
what are modifications to the

1663
00:56:47,250 --> 00:56:49,245
0,260 400,800 1030,1335 1335,1640 1690,1995
underlying architecture that could try

1664
00:56:49,245 --> 00:56:50,450
0,150 150,630 630,810 810,930 930,1205
to alleviate some of those

1665
00:56:50,470 --> 00:56:53,150
0,400 720,1040 1040,1360 1800,2200 2280,2680
details or those issues. But

1666
00:56:53,590 --> 00:56:54,825
0,260 260,380 380,545 545,850 870,1235
for the sake of lecture,

1667
00:56:54,825 --> 00:56:56,625
0,360 360,720 720,1085 1225,1575 1575,1800
I can discuss further with

1668
00:56:56,625 --> 00:56:58,040
0,195 195,405 405,645 645,995 1015,1415
you about that particular example

1669
00:56:58,420 --> 00:57:00,220
0,400 510,785 785,1060
afterwards as well.|
|

1670
00:57:01,180 --> 00:57:02,360
0,400
Okay.|
好吧。|

1671
00:57:02,620 --> 00:57:03,960
0,400
So.|
所以。|

1672
00:57:04,160 --> 00:57:06,580
0,400 450,710 710,970 1380,1780 2130,2420
So, so, yeah, indeed, I
所以，是的，确实，我认为这种力量和。

1673
00:57:06,580 --> 00:57:09,500
0,270 270,540 540,830 1120,1520
think the power and.|
|

1674
00:57:09,760 --> 00:57:10,880
0,275 275,455 455,635 635,815 815,1120
The power of these models
这些模型的力量是非常非常重要的，回到我们在第一堂课上展示的例子，对，这种能力现在可以从一张宇航员骑马的语言提示照片中生成一个合成图像。

1675
00:57:10,900 --> 00:57:13,310
0,400 480,880 1410,1775 1775,2075 2075,2410
is, is very, very significant

1676
00:57:14,230 --> 00:57:16,605
0,400 540,890 890,1240 1950,2195 2195,2375
and going back to the

1677
00:57:16,605 --> 00:57:18,620
0,315 315,585 585,780 780,1085 1615,2015
example that we showed at

1678
00:57:18,910 --> 00:57:20,840
0,380 380,760 960,1295 1295,1580 1580,1930
lecture one, right, this ability

1679
00:57:20,890 --> 00:57:22,440
0,290 290,580 630,935 935,1130 1130,1550
to generate now a synthetic

1680
00:57:22,440 --> 00:57:24,290
0,350 370,720 720,990 990,1310 1450,1850
image from a language prompt

1681
00:57:24,910 --> 00:57:26,250
0,320 320,485 485,605 605,1085 1085,1340
photo of an astronaut riding

1682
00:57:26,250 --> 00:57:27,320
0,225 225,500
a horse.|
|

1683
00:57:27,360 --> 00:57:28,505
0,305 305,590 590,815 815,965 965,1145
In fact, it is a
事实上，这种方法的基础是一个扩散模型。而扩散模型所做的是，它能够在文本和语言之间进行转换，然后在两者之间运行扩散过程，这样图像生成就可以由特定的语言提示来指导。

1684
00:57:28,505 --> 00:57:30,010
0,390 390,750 750,1005 1005,1185 1185,1505
diffusion model that is underlying

1685
00:57:30,660 --> 00:57:33,935
0,400 1170,1520 1520,1870 2490,2890 2970,3275
this this approach. And what

1686
00:57:33,935 --> 00:57:35,410
0,240 240,675 675,945 945,1170 1170,1475
the diffusion model is doing

1687
00:57:35,460 --> 00:57:37,610
0,320 320,575 575,850 930,1330 1770,2150
is it's able to take

1688
00:57:37,610 --> 00:57:40,330
0,380 760,1290 1290,1605 1605,2270 2320,2720
an embedding that translates between

1689
00:57:40,590 --> 00:57:43,030
0,400 750,1070 1070,1390 1860,2150 2150,2440
text and language and then

1690
00:57:43,260 --> 00:57:45,335
0,305 305,485 485,860 860,1240 1770,2075
run a diffusion process between

1691
00:57:45,335 --> 00:57:47,435
0,195 195,485 775,1125 1125,1475 1705,2100
the two, such that image

1692
00:57:47,435 --> 00:57:49,835
0,395 655,975 975,1290 1290,1955 2065,2400
generation can be guided by

1693
00:57:49,835 --> 00:57:51,960
0,285 285,635 715,1115 1225,1625
the particular language prompt.|
|

1694
00:57:52,560 --> 00:57:54,845
0,400 420,820 1200,1565 1565,1930 1980,2285
More examples of this idea
我认为，更多这种从文本到图像生成的想法已经在互联网上掀起了一场风暴。但我认为这个想法的强大之处在于，我们可以根据我们通过语言指定的约束来指导生成过程，这是非常非常强大的。

1695
00:57:54,845 --> 00:57:56,860
0,210 210,495 495,875 1195,1595 1615,2015
of text to image generation

1696
00:57:57,060 --> 00:57:58,900
0,320 320,515 515,790 930,1330 1440,1840
have, I think, really taken

1697
00:57:59,040 --> 00:58:00,500
0,380 380,760 840,1115 1115,1250 1250,1460
the Internet kind of by

1698
00:58:00,500 --> 00:58:02,270
0,350 970,1245 1245,1410 1410,1605 1605,1770
storm. But I think what

1699
00:58:02,270 --> 00:58:03,650
0,180 180,420 420,740 820,1110 1110,1380
is so powerful about this

1700
00:58:03,650 --> 00:58:05,680
0,380 850,1140 1140,1430 1480,1755 1755,2030
idea is that we can

1701
00:58:05,700 --> 00:58:08,200
0,400 480,785 785,1310 1310,1630 2100,2500
guide the generative process according

1702
00:58:08,250 --> 00:58:11,560
0,400 600,1450 1530,1835 1835,2140 2520,3310
to constraints that we specify

1703
00:58:12,000 --> 00:58:13,385
0,290 290,580 690,950 950,1130 1130,1385
through language, which is very,

1704
00:58:13,385 --> 00:58:14,740
0,285 285,635
very powerful.|
|

1705
00:58:14,990 --> 00:58:16,390
0,290 290,485 485,755 755,1130 1130,1400
Everything from something that's highly
所有的东西都是高度照片逼真的，或者是用特定的艺术、艺术风格产生的，就像我在这里展示的例子一样。

1706
00:58:16,390 --> 00:58:19,045
0,315 315,1040 1360,1740 1740,2120 2380,2655
photo realistic or generated with

1707
00:58:19,045 --> 00:58:22,140
0,180 180,485 985,1385 1765,2745 2745,3095
a specific art, artistic style,

1708
00:58:22,940 --> 00:58:24,565
0,305 305,485 485,680 680,1000 1380,1625
as in the examples that

1709
00:58:24,565 --> 00:58:25,880
0,210 210,420 420,755
I'm showing here.|
|

1710
00:58:27,030 --> 00:58:29,410
0,400 480,800 800,1120 1770,2075 2075,2380
So, so far, in both
因此，到目前为止，在今天关于扩散模型的部分和我们对生成模型的更广泛的讨论中，我们主要集中在图像中的例子。

1711
00:58:29,640 --> 00:58:32,440
0,545 545,820 870,1270 1950,2450 2450,2800
today's portion on diffusion models

1712
00:58:32,820 --> 00:58:34,370
0,275 275,515 515,845 845,1100 1100,1550
and our discussion of generative

1713
00:58:34,370 --> 00:58:37,210
0,290 730,1035 1035,1640 2140,2535 2535,2840
models more broadly, we largely

1714
00:58:37,350 --> 00:58:39,580
0,400 450,850 960,1360 1560,1895 1895,2230
focused on examples in images.|
|

1715
00:58:40,790 --> 00:58:42,190
0,350 350,590 590,815 815,1115 1115,1400
But what about other data
但是，其他数据形态、其他应用程序呢？

1716
00:58:42,190 --> 00:58:45,080
0,800 910,1310 1510,1910
modalities, other applications?|
|

1717
00:58:45,080 --> 00:58:46,805
0,210 210,435 435,770 940,1260 1260,1725
Can we design new generative
我们能设计出新的可再生模式吗？

1718
00:58:46,805 --> 00:58:47,960
0,275
models?|
|

1719
00:58:47,970 --> 00:58:49,750
0,575 575,830 830,1160 1160,1445 1445,1780
Leveraging these ideas to design
利用这些想法在其他现实世界应用领域中设计新的合成实例。

1720
00:58:50,490 --> 00:58:53,470
0,400 570,1120 1710,2435 2435,2675 2675,2980
new synthetic instances in other

1721
00:58:53,550 --> 00:58:56,100
0,335 335,670 930,1295 1295,1660
real world application areas.|
|

1722
00:58:56,840 --> 00:58:58,030
0,260 260,440 440,665 665,965 965,1190
To me, and I'm definitely
对我来说，我这样说肯定是有偏见的，但思考这个想法最令人兴奋的方面之一是在分子设计的背景下，以及它如何与化学、生命科学和环境科学相关联。

1723
00:58:58,030 --> 00:58:59,980
0,480 480,675 675,915 915,1250 1600,1950
biased in saying this, but

1724
00:58:59,980 --> 00:59:01,320
0,225 225,375 375,525 525,800 940,1340
one of the most exciting

1725
00:59:01,910 --> 00:59:03,730
0,400 600,935 935,1265 1265,1550 1550,1820
aspects about thinking about this

1726
00:59:03,730 --> 00:59:05,130
0,380 550,825 825,960 960,1110 1110,1400
idea is in the context

1727
00:59:05,330 --> 00:59:07,690
0,400 480,1145 1145,1540 1800,2135 2135,2360
of molecular design and how

1728
00:59:07,690 --> 00:59:09,480
0,150 150,390 390,750 750,1065 1065,1790
it can relate to chemistry,

1729
00:59:09,620 --> 00:59:11,880
0,260 260,470 470,1180 1380,1780 1860,2260
the life sciences and environmental

1730
00:59:12,050 --> 00:59:13,460
0,350 350,605 605,910
science as well.|
|

1731
00:59:13,730 --> 00:59:15,655
0,350 350,620 620,940 1080,1480 1530,1925
So for example, in the,
例如，在化学和小分子领域，我们现在可以考虑三维空间中的原子和分子，而不是图像和像素。最近在建立扩散模型方面已经做了很多工作，可以观察定义分子的原子的三维坐标，从三维空间中的完全随机状态到再次执行同样的迭代去噪过程，现在生成定义良好的分子结构，可以用于药物发现或治疗设计。

1732
00:59:15,655 --> 00:59:17,370
0,270 270,545 595,870 870,1050 1050,1715
in the landscape of chemistry

1733
00:59:17,690 --> 00:59:20,440
0,380 380,665 665,1390 1920,2320 2490,2750
and small molecules, now instead

1734
00:59:20,440 --> 00:59:22,165
0,150 150,440 460,810 810,1160 1390,1725
of thinking about images and

1735
00:59:22,165 --> 00:59:24,070
0,545 955,1230 1230,1395 1395,1605 1605,1905
pixels, we can think about

1736
00:59:24,070 --> 00:59:26,230
0,620 730,1020 1020,1680 1680,1935 1935,2160
atoms and molecules in three

1737
00:59:26,230 --> 00:59:28,555
0,585 585,920 1570,1875 1875,2175 2175,2325
dimensional space. And there's been

1738
00:59:28,555 --> 00:59:29,800
0,165 165,390 390,615 615,900 900,1245
a lot of recent work

1739
00:59:29,800 --> 00:59:31,860
0,255 255,560 730,1130 1180,1695 1695,2060
in building now diffusion models

1740
00:59:32,210 --> 00:59:33,550
0,275 275,455 455,710 710,1010 1010,1340
that can look at the

1741
00:59:33,550 --> 00:59:36,180
0,285 285,540 540,1190 1540,1940 2020,2630
three d coordinates of atoms

1742
00:59:36,230 --> 00:59:38,275
0,470 470,590 590,1210 1500,1805 1805,2045
defining a molecule and go

1743
00:59:38,275 --> 00:59:40,080
0,195 195,450 450,845 865,1265 1405,1805
from a completely random state

1744
00:59:40,220 --> 00:59:41,520
0,305 305,545 545,755 755,965 965,1300
in that three d space

1745
00:59:41,960 --> 00:59:43,675
0,320 320,640 780,1115 1115,1400 1400,1715
to again perform this same

1746
00:59:43,675 --> 00:59:46,060
0,540 540,705 705,1155 1155,1475 2125,2385
iterative de noising procedure to

1747
00:59:46,060 --> 00:59:48,610
0,225 225,590 790,1530 1530,1910 2290,2550
now generate molecular structures that

1748
00:59:48,610 --> 00:59:50,320
0,150 150,440 460,860 1150,1500 1500,1710
are well defined and could

1749
00:59:50,320 --> 00:59:52,375
0,165 165,470 490,890 1000,1400 1750,2055
be used for applications in

1750
00:59:52,375 --> 00:59:54,360
0,305 325,725 745,1050 1050,1650 1650,1985
drug discovery or therapeutic design.|
|

1751
00:59:55,880 --> 00:59:57,295
0,305 305,545 545,880 900,1190 1190,1415
Beyond that, and in my
除此之外，特别是在我的研究中，我们正在建立新的扩散模型，可以产生生物分子或生物序列，比如蛋白质，它们是人类生活中所有生物功能的执行者和执行者，跨越生命的所有领域。

1752
00:59:57,295 --> 00:59:59,520
0,335 385,785 1585,1830 1830,1950 1950,2225
research specifically, we are building

1753
00:59:59,600 --> 01:00:01,405
0,320 320,770 770,1150 1230,1550 1550,1805
new diffusion models that can

1754
01:00:01,405 --> 01:00:03,660
0,335 385,785 1135,1815 1815,1980 1980,2255
generate biological molecules or biological

1755
01:00:03,980 --> 01:00:06,660
0,760 1200,1520 1520,1820 1820,2135 2135,2680
sequences like those of proteins,

1756
01:00:07,160 --> 01:00:09,240
0,380 380,740 740,1010 1010,1685 1685,2080
which are the actuators and

1757
01:00:09,440 --> 01:00:11,160
0,550 660,965 965,1220 1220,1445 1445,1720
executors of all of biological

1758
01:00:11,390 --> 01:00:13,660
0,400 900,1300 1320,1655 1655,1955 1955,2270
function in human life and

1759
01:00:13,660 --> 01:00:14,845
0,285 285,525 525,690 690,1005 1005,1185
across all the domains of

1760
01:00:14,845 --> 01:00:15,680
0,275
life.|
|

1761
01:00:15,910 --> 01:00:17,625
0,320 320,640 960,1265 1265,1475 1475,1715
And specifically in my work
具体地说，在我的工作和我的研究团队中，我们开发了一种新的扩散模型，它能够产生全新的蛋白质结构。我将分享一点关于这个模式的工作原理和我们的核心理念。

1762
01:00:17,625 --> 01:00:18,890
0,210 210,345 345,495 495,785 865,1265
and in my research team,

1763
01:00:18,970 --> 01:00:20,775
0,410 410,710 710,1070 1070,1370 1370,1805
we've developed a new diffusion

1764
01:00:20,775 --> 01:00:22,635
0,365 685,945 945,1125 1125,1445 1555,1860
model that is capable of

1765
01:00:22,635 --> 01:00:24,860
0,545 595,990 990,1380 1380,1775 1825,2225
generating brand new protein structures.

1766
01:00:25,600 --> 01:00:26,625
0,365 365,545 545,740 740,890 890,1025
I'm going to share a

1767
01:00:26,625 --> 01:00:28,010
0,195 195,465 465,780 780,1065 1065,1385
little bit about how that

1768
01:00:28,510 --> 01:00:29,990
0,380 380,725 725,950 950,1145 1145,1480
model works and our core

1769
01:00:30,040 --> 01:00:31,100
0,400
idea.|
|

1770
01:00:31,460 --> 01:00:33,040
0,320 320,640 900,1220 1220,1400 1400,1580
Behind it to kind of
在它的后面，有点像是关闭了这一部分。

1771
01:00:33,040 --> 01:00:34,960
0,225 225,450 450,660 660,950
close out this section.|
|

1772
01:00:35,250 --> 01:00:36,875
0,260 260,785 785,1100 1100,1340 1340,1625
The motivation that is really
真正推动整个领域这项工作的动机是这个目标，即试图设计新的生物实体，比如可以具有治疗功能的蛋白质，或者扩大生物学的功能空间。

1773
01:00:36,875 --> 01:00:38,435
0,360 360,675 675,995 1045,1365 1365,1560
driving this work across the

1774
01:00:38,435 --> 01:00:40,295
0,165 165,360 360,665 1315,1620 1620,1860
field at large is this

1775
01:00:40,295 --> 01:00:41,950
0,300 300,665 715,1020 1020,1290 1290,1655
goal of trying to design

1776
01:00:42,180 --> 01:00:45,070
0,400 480,880 1140,1930 1950,2285 2285,2890
new biological entities like proteins

1777
01:00:45,450 --> 01:00:47,530
0,290 290,500 500,820 1080,1775 1775,2080
that could have therapeutic functions

1778
01:00:47,970 --> 01:00:50,480
0,400 450,815 815,1180 1620,2270 2270,2510
or expand the functional space

1779
01:00:50,480 --> 01:00:52,420
0,210 210,500 700,1005 1005,1310
of biology at large.|
|

1780
01:00:52,550 --> 01:00:53,695
0,290 290,425 425,575 575,845 845,1145
And there are many, many
在产生式人工智能领域，现在有很多很多努力都在针对这个问题，因为它可能会产生巨大的潜在影响。

1781
01:00:53,695 --> 01:00:55,795
0,335 625,990 990,1560 1560,1890 1890,2100
efforts across generative AI that

1782
01:00:55,795 --> 01:00:57,400
0,150 150,455 715,1230 1230,1395 1395,1605
are now aiming at this

1783
01:00:57,400 --> 01:00:58,950
0,320 610,915 915,1095 1095,1260 1260,1550
problem because of the tremendous

1784
01:00:59,510 --> 01:01:00,910
0,400 540,905 905,1130 1130,1250 1250,1400
potential impact that it can

1785
01:01:00,910 --> 01:01:01,920
0,290
have.|
|

1786
01:01:02,670 --> 01:01:03,785
0,275 275,395 395,575 575,860 860,1115
So in our work in
因此，在我们的具体工作中，我们通过回到生物学并从中获得灵感来考虑蛋白质设计的这个问题。

1787
01:01:03,785 --> 01:01:06,020
0,305 835,1235 1345,1680 1680,1935 1935,2235
specific, we considered this problem

1788
01:01:06,020 --> 01:01:08,360
0,380 460,860 910,1310 1720,2055 2055,2340
of protein design by going

1789
01:01:08,360 --> 01:01:10,025
0,350 430,720 720,870 870,1130 1330,1665
back to the biology and

1790
01:01:10,025 --> 01:01:12,220
0,330 330,975 975,1230 1230,1505
drawing inspiration from it.|
|

1791
01:01:12,320 --> 01:01:14,190
0,400 600,875 875,1115 1115,1475 1475,1870
And if you consider how
如果你考虑蛋白质功能是如何决定和编码的呢？

1792
01:01:14,330 --> 01:01:17,080
0,400 510,910 1650,2030 2030,2410 2490,2750
protein function is determined and

1793
01:01:17,080 --> 01:01:18,280
0,650
encoded?|
|

1794
01:01:18,530 --> 01:01:19,885
0,305 305,455 455,605 605,1115 1115,1355
All it is distilled down
它被浓缩成蛋白质在三维空间中的结构，以及这种结构是如何告知和编码特定的生物功能的。

1795
01:01:19,885 --> 01:01:21,460
0,270 270,575 625,990 990,1335 1335,1575
to is the structure of

1796
01:01:21,460 --> 01:01:23,185
0,165 165,470 520,855 855,1080 1080,1725
that protein in three dimensional

1797
01:01:23,185 --> 01:01:25,200
0,335 835,1155 1155,1395 1395,1665 1665,2015
space, and how that structure

1798
01:01:26,030 --> 01:01:28,080
0,560 560,770 770,1445 1445,1685 1685,2050
informs and encodes a particular

1799
01:01:28,100 --> 01:01:29,860
0,400 660,1060
biological function.|
|

1800
01:01:30,350 --> 01:01:32,650
0,350 350,700 1140,1640 1640,2030 2030,2300
In turn, proteins don't always
反过来，蛋白质并不总是一开始就采用特定的三维结构。它们经历了我们所称的蛋白质折叠的过程，该过程定义了蛋白质中的原子链如何在三维空间中摆动，以采用最终定义的三维结构，该结构高度特定，并与特定的生物功能高度相关。

1801
01:01:32,650 --> 01:01:34,795
0,195 195,470 1120,1650 1650,1830 1830,2145
start out adopting a particular

1802
01:01:34,795 --> 01:01:36,445
0,270 270,450 450,755 1135,1425 1425,1650
three d structure. They go

1803
01:01:36,445 --> 01:01:38,050
0,210 210,420 420,755 895,1295 1345,1605
through this process of what

1804
01:01:38,050 --> 01:01:39,985
0,150 150,375 375,705 705,1340 1630,1935
we call protein folding that

1805
01:01:39,985 --> 01:01:42,370
0,485 865,1265 1525,1845 1845,2130 2130,2385
defines how the chain of

1806
01:01:42,370 --> 01:01:45,280
0,450 450,660 660,795 795,1070 2380,2910
atoms in the protein wiggle

1807
01:01:45,280 --> 01:01:47,070
0,330 330,630 630,825 825,1455 1455,1790
around in three dimensional space

1808
01:01:47,300 --> 01:01:49,140
0,350 350,785 785,965 965,1270 1440,1840
to adopt a final defined

1809
01:01:49,220 --> 01:01:50,970
0,275 275,470 470,790 1230,1490 1490,1750
three d structure that is

1810
01:01:50,990 --> 01:01:53,340
0,400 420,820 1260,1595 1595,1930 1950,2350
highly specific and highly linked

1811
01:01:53,360 --> 01:01:55,320
0,230 230,425 425,790 900,1300 1560,1960
to a particular biological function.|
|

1812
01:01:56,440 --> 01:01:57,765
0,275 275,485 485,820 870,1160 1160,1325
So we asked, and we
所以我们问，我们研究了蛋白质折叠的问题，以及蛋白质折叠是如何导致结构的，以启发一种新的扩散模型方法来解决这个蛋白质结构设计问题。

1813
01:01:57,765 --> 01:01:59,055
0,165 165,315 315,495 495,815 925,1290
looked at this question of

1814
01:01:59,055 --> 01:02:00,990
0,360 360,930 930,1265 1345,1650 1650,1935
protein folding and how protein

1815
01:02:00,990 --> 01:02:03,495
0,450 450,675 675,915 915,1220 2170,2505
folding leads to structure to

1816
01:02:03,495 --> 01:02:05,420
0,635 685,975 975,1155 1155,1575 1575,1925
inspire a new diffusion model

1817
01:02:05,530 --> 01:02:07,785
0,400 750,1055 1055,1325 1325,1690 1710,2255
approach for this protein structured

1818
01:02:07,785 --> 01:02:09,160
0,315 315,695
design problem.|
|

1819
01:02:09,260 --> 01:02:11,305
0,290 290,470 470,760 930,1330 1770,2045
Where we said, okay, if
我们说，好的，如果蛋白质处于完全未折叠的状态，你可以认为这等同于图像中的随机噪声状态。它的配置完全是软盘。它没有明确的结构，它是展开的。

1820
01:02:11,305 --> 01:02:12,390
0,180 180,465 465,705 705,825 825,1085
a protein is in a

1821
01:02:12,470 --> 01:02:14,890
0,380 380,1160 1160,1480 1920,2210 2210,2420
completely unfolded state, you can

1822
01:02:14,890 --> 01:02:16,440
0,255 255,510 510,765 765,1100 1150,1550
think about that as equivalent

1823
01:02:16,700 --> 01:02:18,760
0,400 480,755 755,1030 1170,1570 1680,2060
to the random noise state

1824
01:02:18,760 --> 01:02:20,545
0,240 240,375 375,650 1360,1710 1710,1785
in an image. It's in

1825
01:02:20,545 --> 01:02:23,335
0,195 195,545 595,1235 1525,2165 2485,2790
a completely floppy configuration. It

1826
01:02:23,335 --> 01:02:24,780
0,360 360,555 555,765 765,1065 1065,1445
doesn't have a defined structure,

1827
01:02:25,070 --> 01:02:26,660
0,365 365,1120
it's unfolded.|
|

1828
01:02:26,760 --> 01:02:27,875
0,290 290,455 455,620 620,860 860,1115
And what we did is
我们所做的是把蛋白质结构被展开的概念作为扩散模型的噪声状态，我们设计了一个扩散模型，我们训练它从蛋白质的展开状态出发，然后产生一个关于新结构的预测，它将定义一个特定的三维结构。

1829
01:02:27,875 --> 01:02:29,440
0,240 240,465 465,675 675,995 1165,1565
we took that notion of

1830
01:02:29,850 --> 01:02:32,620
0,400 930,1330 1350,1715 1715,2015 2015,2770
protein protein structure being unfolded

1831
01:02:32,790 --> 01:02:34,970
0,400 600,890 890,1385 1385,1720 1920,2180
as the noisy state for

1832
01:02:34,970 --> 01:02:36,935
0,120 120,495 495,860 1420,1710 1710,1965
a diffusion model, and we

1833
01:02:36,935 --> 01:02:38,810
0,300 300,495 495,885 885,1265 1615,1875
designed a diffusion model that

1834
01:02:38,810 --> 01:02:40,325
0,195 195,530 760,1050 1050,1275 1275,1515
we trained to go from

1835
01:02:40,325 --> 01:02:42,395
0,305 385,1325 1465,1755 1755,1920 1920,2070
that unfolded state of the

1836
01:02:42,395 --> 01:02:44,860
0,275 955,1245 1245,1535 1705,2085 2085,2465
protein to then produce a

1837
01:02:45,330 --> 01:02:48,880
0,400 570,1090 1620,2020 2520,2920 3150,3550
generated prediction about a new

1838
01:02:49,050 --> 01:02:50,945
0,400 720,995 995,1220 1220,1565 1565,1895
structure that would define a

1839
01:02:50,945 --> 01:02:53,020
0,335 715,1005 1005,1215 1215,1535
specific three d structure.|
|

1840
01:02:53,620 --> 01:02:55,035
0,275 275,410 410,575 575,1130 1130,1415
And we can visualize how
我们可以直观地看到这个算法是如何工作的，在这段视频中我们称之为折叠扩散，我将在这里展示，在这个初始帧的开头，我们有一个随机的噪音蛋白质链。

1841
01:02:55,035 --> 01:02:57,330
0,300 300,845 1195,1595 1825,2115 2115,2295
this algorithm works and we

1842
01:02:57,330 --> 01:02:59,580
0,165 165,360 360,750 750,1220 1930,2250
call it folding diffusion in

1843
01:02:59,580 --> 01:03:00,885
0,320 340,720 720,960 960,1155 1155,1305
this video that I'm going

1844
01:03:00,885 --> 01:03:02,955
0,195 195,485 685,1085 1585,1890 1890,2070
to show here, where at

1845
01:03:02,955 --> 01:03:04,310
0,180 180,480 480,750 750,990 990,1355
the start in this initial

1846
01:03:04,540 --> 01:03:06,620
0,400 750,1040 1040,1265 1265,1600 1680,2080
frame, we have a random

1847
01:03:06,640 --> 01:03:09,120
0,400 810,1210 1470,1870
noise protein chain.|
|

1848
01:03:09,280 --> 01:03:13,430
0,350 350,700 810,1420 2880,3730 3750,4150
Completely random configuration unfolded and
完全随机的配置在3D空间中展开和展开。

1849
01:03:14,680 --> 01:03:16,900
0,695 695,905 905,1265 1265,1570
unfolded in 3D space.|
|

1850
01:03:17,100 --> 01:03:18,770
0,400 750,1040 1040,1310 1310,1475 1475,1670
Now, what it's going to
现在，这段视频将展示的是从无随机状态到最终生成的结构的迭代去噪步骤。

1851
01:03:18,770 --> 01:03:19,940
0,210 210,435 435,720 720,1005 1005,1170
show this video is the

1852
01:03:19,940 --> 01:03:22,025
0,495 495,690 690,1155 1155,1490 1780,2085
iterative de noising steps that

1853
01:03:22,025 --> 01:03:23,740
0,255 255,510 510,815 895,1295 1315,1715
go from that no random

1854
01:03:23,850 --> 01:03:26,590
0,400 840,1175 1175,1415 1415,1720 2340,2740
state to the final generated

1855
01:03:26,820 --> 01:03:27,920
0,400
structure.|
|

1856
01:03:28,640 --> 01:03:29,760
0,290 290,485 485,650 650,815 815,1120
And as you can see,
正如你所看到的，这个过程非常类似于我用图像介绍的概念，我们试图从嘈杂的东西变成有结构的东西，现在到达最终生成的蛋白质结构。

1857
01:03:29,810 --> 01:03:31,750
0,410 410,635 635,970 1230,1595 1595,1940
it's this process very similar

1858
01:03:31,750 --> 01:03:33,330
0,240 240,390 390,680 1060,1320 1320,1580
to the concept that I

1859
01:03:33,350 --> 01:03:35,305
0,305 305,500 500,790 1410,1700 1700,1955
introduce with images, where we're

1860
01:03:35,305 --> 01:03:37,045
0,210 210,405 405,615 615,965 1345,1740
trying to go from something

1861
01:03:37,045 --> 01:03:39,850
0,675 675,945 945,1235 1285,1925 2245,2805
noisy to something structured, arriving

1862
01:03:39,850 --> 01:03:41,190
0,240 240,450 450,600 600,890 940,1340
now at a final generated

1863
01:03:41,330 --> 01:03:42,760
0,400 420,820
protein structure.|
|

1864
01:03:43,810 --> 01:03:45,105
0,305 305,545 545,815 815,1040 1040,1295
So our work was really
因此，我们的工作实际上集中在通过扩散模型为蛋白质结构设计问题引入一种新的基础性算法。

1865
01:03:45,105 --> 01:03:47,925
0,365 415,815 955,1685 1825,2225 2425,2820
focused on introducing a new

1866
01:03:47,925 --> 01:03:50,880
0,300 300,845 1165,1715 1945,2345 2635,2955
and foundational algorithm for this

1867
01:03:50,880 --> 01:03:53,205
0,315 315,855 855,1155 1155,1520 1960,2325
protein structured design problem VIA

1868
01:03:53,205 --> 01:03:55,640
0,365 505,855 855,1335 1335,1685
this VIA diffusion models.|
|

1869
01:03:55,710 --> 01:03:56,885
0,260 260,395 395,590 590,845 845,1175
But it turns out that
但事实证明，在我们和其他人引入这些算法后的很短时间内，在缩放和扩展这些扩散模型以进行非常、非常具体的可编程蛋白质设计时，确实出现了沉淀。现在我们看到了大规模的努力，使用这些扩散模型算法来产生蛋白质设计，并在物理世界中实际实验实现它们。

1870
01:03:56,885 --> 01:03:59,615
0,300 300,605 715,1115 2095,2445 2445,2730
in just a very short

1871
01:03:59,615 --> 01:04:00,950
0,300 300,665 685,990 990,1170 1170,1335
time from when we and

1872
01:04:00,950 --> 01:04:03,350
0,290 550,855 855,1160 1210,1700 2020,2400
others introduced these algorithms, that

1873
01:04:03,350 --> 01:04:06,040
0,285 285,1070 1090,1490 1570,1970 2290,2690
it precipitated really arise in

1874
01:04:06,210 --> 01:04:08,285
0,590 590,940 960,1460 1460,1655 1655,2075
scaling and extending these diffusion

1875
01:04:08,285 --> 01:04:10,390
0,365 775,1125 1125,1440 1440,1755 1755,2105
models for very, very specific

1876
01:04:10,770 --> 01:04:14,060
0,400 750,1655 1655,1990 2040,2440 2970,3290
controlled programmable protein design, where

1877
01:04:14,060 --> 01:04:15,280
0,270 270,465 465,630 630,885 885,1220
now we are seeing large

1878
01:04:15,300 --> 01:04:17,225
0,305 305,610 1110,1430 1430,1685 1685,1925
scale efforts that use these

1879
01:04:17,225 --> 01:04:19,690
0,420 420,785 955,1475 1735,2100 2100,2465
diffusion model algorithms to generate

1880
01:04:19,770 --> 01:04:22,520
0,400 450,850 1260,1660 1830,2230 2400,2750
protein designs and actually realize

1881
01:04:22,520 --> 01:04:24,370
0,350 400,1245 1245,1425 1425,1575 1575,1850
them experimentally in the physical

1882
01:04:24,420 --> 01:04:25,360
0,400
world.|
|

1883
01:04:25,360 --> 01:04:26,470
0,255 255,405 405,570 570,795 795,1110
So in this image I'm
所以在这张我在左边展示的图像中，彩色图像是扩散模型生成的输出。黑白图像是如果你得到这个结果，在生物实验室里合成它，然后看着并拍下产生的蛋白质的照片，然后评估它的结构。我们看到有很强的一致性，这意味着我们有能力利用这些强大的扩散模型来产生新的蛋白质，这些蛋白质可以在物理世界中实现。

1884
01:04:26,470 --> 01:04:27,910
0,180 180,360 360,495 495,770 1120,1440
showing on the left, the

1885
01:04:27,910 --> 01:04:30,480
0,285 285,650 970,1305 1305,1640 2170,2570
colored image is the generated

1886
01:04:30,920 --> 01:04:32,400
0,320 320,560 560,740 740,1115 1115,1480
output by the diffusion model.

1887
01:04:32,810 --> 01:04:33,745
0,260 260,410 410,590 590,755 755,935
And the black and white

1888
01:04:33,745 --> 01:04:35,635
0,305 625,960 960,1295 1405,1680 1680,1890
image is what if you

1889
01:04:35,635 --> 01:04:38,700
0,255 255,575 625,1025 2065,2730 2730,3065
take that result, synthesize it

1890
01:04:38,810 --> 01:04:41,380
0,320 320,640 810,1210 1530,1930 2220,2570
in the biological lab and

1891
01:04:41,380 --> 01:04:42,550
0,330 330,630 630,810 810,945 945,1170
look and take a picture

1892
01:04:42,550 --> 01:04:44,410
0,225 225,420 420,735 735,1130 1480,1860
of the resulting protein and

1893
01:04:44,410 --> 01:04:46,330
0,285 285,525 525,860 1420,1725 1725,1920
assess its structure. We see

1894
01:04:46,330 --> 01:04:47,230
0,165 165,300 300,405 405,585 585,900
that there is a strong

1895
01:04:47,230 --> 01:04:49,390
0,860 1120,1470 1470,1710 1710,1965 1965,2160
correspondence, meaning that we're at

1896
01:04:49,390 --> 01:04:51,925
0,285 285,650 1150,1550 1720,2120 2200,2535
the ability to leverage these

1897
01:04:51,925 --> 01:04:53,905
0,335 355,855 855,1205 1495,1755 1755,1980
powerful diffusion models to now

1898
01:04:53,905 --> 01:04:55,660
0,365 625,930 930,1410 1410,1620 1620,1755
generate new proteins that can

1899
01:04:55,660 --> 01:04:56,910
0,255 255,630 630,870 870,990 990,1250
be realized in the physical

1900
01:04:56,930 --> 01:04:57,640
0,400
world.|
|

1901
01:04:58,090 --> 01:05:00,075
0,275 275,635 635,875 875,1210 1710,1985
It doesn't stop there. We
这并不止步于此。我们现在可以考虑为非常具体的治疗应用设计蛋白质。例如，在这个可视化中，这被设计成一部小说。

1902
01:05:00,075 --> 01:05:02,180
0,150 150,375 375,690 690,1055 1525,2105
can now think about designing

1903
01:05:02,230 --> 01:05:04,760
0,515 515,770 770,1090 1140,1540 1740,2530
proteins for very specific therapeutic

1904
01:05:04,840 --> 01:05:07,065
0,400 1050,1385 1385,1625 1625,1925 1925,2225
applications. So for example, in

1905
01:05:07,065 --> 01:05:09,800
0,305 535,1355 1765,2070 2070,2355 2355,2735
this visualization, this is designed

1906
01:05:09,970 --> 01:05:11,620
0,275 275,455 455,760
as a novel.|
|

1907
01:05:11,630 --> 01:05:14,065
0,400 450,1150 1230,1700 1700,2045 2045,2435
Protein binder that's designed to
一种蛋白质结合剂，旨在阻止、结合和阻止Covid Spike受体的活动。所以你再一次看到的是，在最终的设计中，扩散过程的可视化，这个最终的设计结合并阻止了Covid Spike受体的顶部。

1908
01:05:14,065 --> 01:05:15,700
0,395 685,945 945,1170 1170,1410 1410,1635
block, to bind to and

1909
01:05:15,700 --> 01:05:17,755
0,350 490,840 840,1190 1510,1830 1830,2055
block the activity of the

1910
01:05:17,755 --> 01:05:20,710
0,480 480,885 885,1655 2215,2615 2665,2955
covid spike receptor. So what

1911
01:05:20,710 --> 01:05:22,140
0,270 270,555 555,885 885,1125 1125,1430
you're seeing, again, is that

1912
01:05:22,310 --> 01:05:24,330
0,755 755,995 995,1160 1160,1600 1620,2020
visualization of the diffusion process

1913
01:05:24,740 --> 01:05:26,220
0,350 350,875 875,1040 1040,1190 1190,1480
in arriving at a final

1914
01:05:26,930 --> 01:05:29,110
0,400 450,850 1140,1670 1670,1895 1895,2180
design that binds to and

1915
01:05:29,110 --> 01:05:30,490
0,375 375,660 660,915 915,1170 1170,1380
blocks the top of the

1916
01:05:30,490 --> 01:05:32,140
0,360 360,585 585,1250
covid spike receptor.|
|

1917
01:05:33,410 --> 01:05:36,175
0,400 1140,1540 2130,2375 2375,2525 2525,2765
So hopefully, you know, this
因此，希望，你知道，这种方式描绘了我们今天所处的风景，有了生成性的人工智能。

1918
01:05:36,175 --> 01:05:38,140
0,210 210,420 420,905 1495,1770 1770,1965
kind of paints a bit

1919
01:05:38,140 --> 01:05:39,520
0,210 210,500 610,915 915,1140 1140,1380
of the landscape of where

1920
01:05:39,520 --> 01:05:41,335
0,180 180,345 345,650 940,1260 1260,1815
we are today with generative

1921
01:05:41,335 --> 01:05:42,380
0,365
AI.|
|

1922
01:05:42,510 --> 01:05:43,790
0,245 245,395 395,605 605,905 905,1280
And I think that this
我认为在蛋白质科学和更广泛的生物学中的这个例子确实突出了这样一个事实，即生成性人工智能现在能够实现真正有影响力的应用程序。

1923
01:05:43,790 --> 01:05:46,385
0,380 670,1070 1630,1935 1935,2235 2235,2595
example in in protein science

1924
01:05:46,385 --> 01:05:48,610
0,225 225,360 360,635 1315,1635 1635,2225
and in biology more broadly

1925
01:05:48,900 --> 01:05:50,570
0,400 450,850 870,1145 1145,1370 1370,1670
really highlights the fact that

1926
01:05:50,570 --> 01:05:52,595
0,615 615,900 900,1140 1140,1460 1720,2025
generative AI is now at

1927
01:05:52,595 --> 01:05:54,520
0,210 210,515 685,1035 1035,1385 1525,1925
the ability to enable truly

1928
01:05:54,660 --> 01:05:56,640
0,670 780,1180
impactful applications.|
|

1929
01:05:56,960 --> 01:05:58,495
0,335 335,620 620,920 920,1265 1265,1535
Not just for creating cool
不仅仅是为了创造很酷的图像，或者我们可以认为是人工智能合成艺术。

1930
01:05:58,495 --> 01:06:00,340
0,275 865,1215 1215,1455 1455,1650 1650,1845
images or what we can

1931
01:06:00,340 --> 01:06:02,005
0,165 165,315 315,590 700,1100 1150,1665
think of as AI synthetic

1932
01:06:02,005 --> 01:06:03,540
0,365
art.|
|

1933
01:06:03,540 --> 01:06:05,355
0,525 525,810 810,1020 1020,1200 1200,1815
Generative AI is now enabling
生成性人工智能现在使我们能够思考现实世界问题的设计解决方案。

1934
01:06:05,355 --> 01:06:07,400
0,300 300,570 570,825 825,1175 1645,2045
us to think about design

1935
01:06:07,480 --> 01:06:09,440
0,400 600,920 920,1205 1205,1565 1565,1960
solutions for real world problems.|
|

1936
01:06:10,360 --> 01:06:11,250
0,245 245,380 380,560 560,695 695,890
And I think there are
我认为，在理解生成性人工智能方法的能力、局限性、应用以及它们如何增强我们的社会能力方面，还有很多悬而未决的问题。

1937
01:06:11,250 --> 01:06:13,100
0,315 315,680 730,1035 1035,1340 1450,1850
so many open questions in

1938
01:06:13,210 --> 01:06:15,890
0,400 450,740 740,1030 1860,2120 2120,2680
understanding the capabilities, the limitations,

1939
01:06:16,360 --> 01:06:18,495
0,400 480,880 990,1280 1280,1805 1805,2135
the applications of generative AI

1940
01:06:18,495 --> 01:06:20,130
0,365 775,1095 1095,1320 1320,1500 1500,1635
approaches and how they can

1941
01:06:20,130 --> 01:06:21,960
0,435 435,795 795,1190
empower our society.|
|

1942
01:06:23,640 --> 01:06:25,175
0,400 480,845 845,1145 1145,1310 1310,1535
So to conclude, I want
因此，作为总结，我想以一个更高层次的思考来结束。

1943
01:06:25,175 --> 01:06:27,140
0,270 270,635 685,1035 1035,1385 1615,1965
to end on a higher

1944
01:06:27,140 --> 01:06:28,340
0,315 315,680
level thought.|
|

1945
01:06:29,650 --> 01:06:31,230
0,275 275,760 840,1115 1115,1340 1340,1580
And that's the fact that
这是一个事实，这种生成性设计的想法提升了这个更高层次的概念，即理解它意味着什么。

1946
01:06:31,230 --> 01:06:32,870
0,290 340,615 615,780 780,1305 1305,1640
this idea of generative design

1947
01:06:33,130 --> 01:06:34,730
0,275 275,545 545,1055 1055,1265 1265,1600
kind of raises this higher

1948
01:06:35,020 --> 01:06:37,065
0,395 395,790 1440,1745 1745,1910 1910,2045
level concept of what it

1949
01:06:37,065 --> 01:06:38,920
0,240 240,605 685,1085
means to understand.|
|

1950
01:06:39,190 --> 01:06:40,220
0,245 245,365 365,515 515,755 755,1030
And I think it's captured
我认为物理学家理查德·费曼的这句话完美地捕捉到了这一点，他说了我无法创造的东西，我无法理解。

1951
01:06:40,270 --> 01:06:41,835
0,400 510,785 785,995 995,1280 1280,1565
perfectly in this quote by

1952
01:06:41,835 --> 01:06:44,280
0,210 210,780 780,1140 1140,1715 2155,2445
the physicist Richard feynman, who

1953
01:06:44,280 --> 01:06:46,130
0,290 700,1005 1005,1200 1200,1470 1470,1850
said what I cannot create,

1954
01:06:46,240 --> 01:06:48,220
0,320 320,640 900,1300
I cannot understand.|
|

1955
01:06:49,030 --> 01:06:50,720
0,305 305,610 750,1055 1055,1325 1325,1690
In order to create something,
为了创造一些东西，作为人类，我们真的必须了解它是如何工作的。

1956
01:06:51,370 --> 01:06:52,800
0,275 275,470 470,790 840,1190 1190,1430
we as humans really have

1957
01:06:52,800 --> 01:06:54,110
0,290 340,675 675,885 885,1035 1035,1310
to understand how it works.|
|

1958
01:06:54,940 --> 01:06:56,720
0,305 305,995 995,1190 1190,1430 1430,1780
And conversely, in order to
相反，为了完全理解一些东西，我认为我们必须创造出工程设计。

1959
01:06:56,740 --> 01:06:59,070
0,335 335,665 665,1060 1560,2075 2075,2330
understand something fully, I'd argue

1960
01:06:59,070 --> 01:07:00,030
0,165 165,390 390,525 525,720 720,960
that we'd have to create

1961
01:07:00,030 --> 01:07:02,180
0,320 490,890 1060,1460
it engineer design.|
|

1962
01:07:02,800 --> 01:07:05,120
0,710 710,1040 1040,1370 1370,1955 1955,2320
Generative AI and generative intelligence
生成性人工智能和生成性智能是我所认为的概念学习的核心。

1963
01:07:05,260 --> 01:07:06,225
0,260 260,395 395,590 590,800 800,965
is at the core of

1964
01:07:06,225 --> 01:07:07,665
0,270 270,585 585,905 1075,1320 1320,1440
this idea of what I

1965
01:07:07,665 --> 01:07:09,350
0,195 195,510 510,870 870,1235 1285,1685
think about as concept learning.|
|

1966
01:07:10,130 --> 01:07:12,595
0,320 320,605 605,970 1680,2240 2240,2465
Being able to distill a
能够提炼出一个非常复杂的问题的核心根源。

1967
01:07:12,595 --> 01:07:14,785
0,275 445,795 795,1145 1585,1965 1965,2190
very complex problem down to

1968
01:07:14,785 --> 01:07:16,380
0,195 195,480 480,995
its core roots.|
|

1969
01:07:16,380 --> 01:07:17,400
0,180 180,390 390,600 600,810 810,1020
And then build up from
然后在这个基础上发展到设计和创造。

1970
01:07:17,400 --> 01:07:19,455
0,195 195,500 910,1245 1245,1580 1750,2055
that foundation to design and

1971
01:07:19,455 --> 01:07:20,700
0,195 195,485
to create.|
|

1972
01:07:21,860 --> 01:07:23,430
0,305 305,545 545,880 900,1235 1235,1570
When this course began on
周一，当这门课程开始时，亚历山大介绍了什么是聪明的概念，通俗地说，是指能够获取信息，并将其用于未来决策的能力。

1973
01:07:23,570 --> 01:07:26,560
0,400 1380,1780 2010,2345 2345,2660 2660,2990
monday, Alexander introduced the idea

1974
01:07:26,560 --> 01:07:27,670
0,270 270,435 435,570 570,855 855,1110
of what it means to

1975
01:07:27,670 --> 01:07:30,450
0,260 280,680 1270,1830 1830,2150 2380,2780
be intelligent, loosely speaking, right,

1976
01:07:31,010 --> 01:07:32,550
0,275 275,550 570,830 830,1090 1140,1540
the ability to take information,

1977
01:07:32,930 --> 01:07:34,105
0,305 305,470 470,680 680,965 965,1175
use it to inform a

1978
01:07:34,105 --> 01:07:35,500
0,275 325,725
future decision.|
|

1979
01:07:36,100 --> 01:07:38,030
0,350 350,700 750,1070 1070,1390 1530,1930
Human learning is not only
人类的学习不仅限于解决具体、不同任务的能力。

1980
01:07:38,230 --> 01:07:39,885
0,560 560,770 770,935 935,1240 1320,1655
restricted to the ability to

1981
01:07:39,885 --> 01:07:43,140
0,335 505,905 1615,2015 2035,2435
solve specific, distinct tasks.|
|

1982
01:07:43,490 --> 01:07:45,850
0,485 485,1090 1380,1715 1715,2050 2070,2360
It's foundational and founded by
它是基础的，建立在能够理解概念的想法上，并使用这些概念来创造、想象、启发和梦想。

1983
01:07:45,850 --> 01:07:47,650
0,255 255,620 940,1260 1260,1515 1515,1800
the idea of being able

1984
01:07:47,650 --> 01:07:50,215
0,345 345,720 720,1520 1870,2220 2220,2565
to understand concepts and using

1985
01:07:50,215 --> 01:07:51,990
0,330 330,975 975,1185 1185,1425 1425,1775
those concepts to be creative,

1986
01:07:52,430 --> 01:07:54,520
0,335 335,670 870,1145 1145,1720 1800,2090
to imagine, to inspire, to

1987
01:07:54,520 --> 01:07:55,500
0,290
dream.|
|

1988
01:07:56,710 --> 01:07:57,855
0,305 305,545 545,815 815,1040 1040,1145
I think now we're at
我认为，现在我们必须真正思考智力这个概念的真正含义。

1989
01:07:57,855 --> 01:07:59,580
0,165 165,420 420,785 1195,1500 1500,1725
the point where we have

1990
01:07:59,580 --> 01:08:01,530
0,180 180,405 405,705 705,1040 1600,1950
to really think about what

1991
01:08:01,530 --> 01:08:03,195
0,285 285,570 570,885 885,1250 1330,1665
this notion of intelligence really

1992
01:08:03,195 --> 01:08:04,400
0,335
means.|
|

1993
01:08:04,400 --> 01:08:06,035
0,380 430,735 735,915 915,1340 1360,1635
And how it relates to
以及它与人工智能和我们作为人类能够做和创造的东西之间的联系和区别。

1994
01:08:06,035 --> 01:08:09,070
0,210 210,545 1165,1470 1470,2195 2635,3035
the connections and distinctions between

1995
01:08:09,330 --> 01:08:11,375
0,400 420,820 1260,1565 1565,1775 1775,2045
artificial intelligence and what we

1996
01:08:11,375 --> 01:08:13,490
0,300 300,635 865,1170 1170,1475 1765,2115
as humans can do and

1997
01:08:13,490 --> 01:08:14,560
0,350
create.|
|

1998
01:08:15,550 --> 01:08:16,560
0,260 260,395 395,605 605,875 875,1010
So with that, I'm going
说到这里，我就把这个留给你们。我希望这能启发你们进一步思考我们在课程中谈到的内容，以及更广泛的智能、人工智能、深度学习。

1999
01:08:16,560 --> 01:08:17,510
0,180 180,375 375,540 540,675 675,950
to leave you with that.

2000
01:08:17,740 --> 01:08:19,220
0,275 275,455 455,680 680,890 890,1480
And I hope that inspires

2001
01:08:19,240 --> 01:08:21,050
0,335 335,620 620,905 905,1240 1410,1810
you to think further on

2002
01:08:21,400 --> 01:08:22,500
0,290 290,560 560,755 755,965 965,1100
what we've talked about in

2003
01:08:22,500 --> 01:08:25,550
0,150 150,440 1090,1490 1900,2300 2650,3050
the course and intelligence, AI,

2004
01:08:25,750 --> 01:08:28,540
0,400 1020,1400 1400,1685 1685,2260
deep learning more broadly.|
|

2005
01:08:29,010 --> 01:08:30,640
0,400 690,995 995,1160 1160,1325 1325,1630
So thank you very much
因此，非常感谢您的关注。我们要稍作停顿，因为我们，我知道我们有点晚了，然后由Rami NeXT开始我们精彩的演讲。非常感谢。

2006
01:08:30,690 --> 01:08:32,900
0,400 630,950 950,1270 1770,2075 2075,2210
for your attention. We're going

2007
01:08:32,900 --> 01:08:34,715
0,165 165,345 345,650 1090,1485 1485,1815
to take a very brief

2008
01:08:34,715 --> 01:08:35,840
0,285 285,540 540,750 750,915 915,1125
pause since we're, I know

2009
01:08:35,840 --> 01:08:36,725
0,195 195,360 360,540 540,690 690,885
we're running a little bit

2010
01:08:36,725 --> 01:08:38,060
0,305 535,825 825,1005 1005,1170 1170,1335
late and then have our

2011
01:08:38,060 --> 01:08:40,480
0,290 730,1130 1240,1640 1660,2130 2130,2420
fantastic lecture from rami next.

2012
01:08:40,860 --> 01:08:46,557
0,290 290,455 455,590 590,850
Thank you so much.|
|
