1
00:00:09,660 --> 00:00:11,240
0,320 320,515 515,880 900,1300 1320,1580
I'm really excited especially for
我真的很兴奋，特别是这次演讲，

2
00:00:11,240 --> 00:00:12,275
0,180 180,500 520,780 780,915 915,1035
this lecture,| which is a
|这是一个非常特别的演讲，

3
00:00:12,275 --> 00:00:13,985
0,245 385,735 735,1065 1065,1395 1395,1710
very special lecture| on robust
|关于健壮和值得信赖的深度学习，

4
00:00:13,985 --> 00:00:15,755
0,255 255,810 810,990 990,1295 1435,1770
and trustworthy deep learning| by
|由我们这门令人惊叹的课程的赞助商之一 Themis AI 提供。

5
00:00:15,755 --> 00:00:17,405
0,195 195,315 315,575 925,1455 1455,1650
one of our sponsors of

6
00:00:17,405 --> 00:00:19,150
0,195 195,495 495,870 870,1380 1380,1745
this amazing course, Themis AI.|
|

7
00:00:19,800 --> 00:00:21,070
0,365 365,620 620,845 845,980 980,1270
{And,as -} you'll see today,|
正如你们今天将看到的，|

8
00:00:21,240 --> 00:00:24,310
0,515 515,880 990,1390 1890,2290 2460,3070
Themis AI is a startup
Temeis AI 是一家总部位于 Cambridge 的初创公司，

9
00:00:24,510 --> 00:00:26,015
0,275 275,710 710,995 995,1265 1265,1505
actually locally based here in

10
00:00:26,015 --> 00:00:27,890
0,575 865,1155 1155,1380 1380,1620 1620,1875
Cambridge,| our mission is to
|我们的使命是设计、推进和部署人工智能和值得信赖的人工智能，

11
00:00:27,890 --> 00:00:30,500
0,350 670,1070 1180,1560 1560,1940 2320,2610
design, advance and deploy the

12
00:00:30,500 --> 00:00:32,870
0,290 340,740 940,1340 1390,1725 1725,2370
future of AI {and,trustworthy -}

13
00:00:32,870 --> 00:00:35,320
0,240 240,560 1180,1680 1680,2055 2055,2450
AI specifically,| I'm especially excited
|我尤其对今天的演讲感到兴奋，

14
00:00:35,340 --> 00:00:37,865
0,350 350,970 1050,1450 1800,2200 2220,2525
about today's lecture,| because I
|因为我是 Themis 的联合创始人，就在 MIT ，

15
00:00:37,865 --> 00:00:39,575
0,195 195,485 565,1145 1165,1470 1470,1710
{co-founded -} Themis right here

16
00:00:39,575 --> 00:00:41,210
0,300 300,665 1105,1395 1395,1530 1530,1635
at MIT,| right here in
|就在这里，在这座大楼里，

17
00:00:41,210 --> 00:00:42,370
0,165 165,375 375,615 615,855 855,1160
{this,very -} building, in fact,|
|

18
00:00:42,930 --> 00:00:44,525
0,305 305,560 560,935 935,1210 1230,1595
this all stemmed from really
这一切都源于我们在这里创造的令人难以置信的科学创新和进步，

19
00:00:44,525 --> 00:00:47,105
0,300 300,635 835,1235 1735,2135 2245,2580
the incredible scientific innovation and

20
00:00:47,105 --> 00:00:48,755
0,540 540,765 765,960 960,1265 1345,1650
advances that we created right

21
00:00:48,755 --> 00:00:49,930
0,225 225,405 405,540 540,735 735,1175
here,| just a few floors
|就在比你们今天坐的地方高几层的地方，

22
00:00:50,100 --> 00:00:51,245
0,335 335,530 530,665 665,905 905,1145
higher than {where,you're -} sitting

23
00:00:51,245 --> 00:00:53,510
0,365 1135,1535 1705,1995 1995,2130 2130,2265
today,| and because of our
|由于我们的背景是 MIT 真正的尖端科学创新，

24
00:00:53,510 --> 00:00:55,570
0,290 610,990 990,1365 1365,1710 1710,2060
background in really cutting edge

25
00:00:55,770 --> 00:00:58,780
0,400 990,1390 2010,2480 2480,2690 2690,3010
scientific innovation stemming from MIT,|
|

26
00:00:58,830 --> 00:01:00,940
0,425 425,635 635,970 1080,1690 1710,2110
Themis is very rooted deeply
Themis 深深植根于科学，就像我说的，创新，

27
00:01:01,380 --> 00:01:03,320
0,400 450,850 1140,1520 1520,1775 1775,1940
in science and like I

28
00:01:03,320 --> 00:01:05,520
0,290 400,800
said, innovation,|
|

29
00:01:05,520 --> 00:01:07,490
0,195 195,470 550,950 1210,1590 1590,1970
we really aim to advance
我们的目标是推动深度学习和人工智能的未来，

30
00:01:07,540 --> 00:01:08,810
0,275 275,515 515,770 770,965 965,1270
the future of deep learning

31
00:01:09,010 --> 00:01:10,395
0,400 420,800 800,1070 1070,1250 1250,1385
and AI| and much of
|我们的许多技术已经从发表的研究成果中成长起来，

32
00:01:10,395 --> 00:01:12,350
0,135 135,425 625,1025 1045,1445 1555,1955
our technology has already grown

33
00:01:12,400 --> 00:01:15,140
0,320 320,605 605,970 1680,2080 2160,2740
from published research,| that we've
|我们在世界各地人工智能[场馆]的顶级同行评议会议上发表的，

34
00:01:15,580 --> 00:01:18,195
0,400 930,1280 1280,1595 1595,2020 2220,2615
published at top tier peer

35
00:01:18,195 --> 00:01:20,060
0,270 270,1025 1045,1320 1320,1530 1530,1865
review conferences in the AI

36
00:01:20,380 --> 00:01:22,065
0,580 660,950 950,1100 1100,1360 1440,1685
[venues] around the world| and
|我们的工作得到了国际知名媒体的报道，

37
00:01:22,065 --> 00:01:23,115
0,180 180,420 420,615 615,795 795,1050
our work has been covered

38
00:01:23,115 --> 00:01:25,440
0,365 445,845 895,1295 1495,1895 2005,2325
by high profile international media

39
00:01:25,440 --> 00:01:28,725
0,500 1480,1880 1900,2300 2530,2930 2980,3285
outlets,| {this,scientific -} innovation, with
|通过这种科学创新，

40
00:01:28,725 --> 00:01:31,095
0,195 195,485 685,1085 1315,1895 2125,2370
this scientific innovation,| Themis, we
|Themis 我们正在应对当今存在的安全关键人工智能中的一些最大挑战，

41
00:01:31,095 --> 00:01:33,015
0,165 165,785 1285,1560 1560,1695 1695,1920
are tackling some of the

42
00:01:33,015 --> 00:01:35,330
0,365 415,815 1255,1620 1620,1950 1950,2315
biggest challenges in safety critical

43
00:01:35,500 --> 00:01:37,260
0,400 420,755 755,1040 1040,1385 1385,1760
AI that exists today,| and
|这实际上源于这样一个事实，

44
00:01:37,260 --> 00:01:38,925
0,300 300,570 570,930 930,1190 1330,1665
really that stems from the

45
00:01:38,925 --> 00:01:39,810
0,240 240,405 405,540 540,705 705,885
fact,| that we want to
|我们希望将所有这些令人惊叹的进步作为本课程的一部分，

46
00:01:39,810 --> 00:01:40,850
0,210 210,405 405,540 540,720 720,1040
take all of these amazing

47
00:01:40,870 --> 00:01:42,015
0,470 470,605 605,785 785,950 950,1145
advances that you're learning as

48
00:01:42,015 --> 00:01:43,340
0,120 120,225 225,405 405,725 925,1325
part of this course| and
|并在现实中作为我们日常生活的一部分来实现，

49
00:01:43,360 --> 00:01:44,780
0,380 380,665 665,905 905,1130 1130,1420
actually achieve them in reality

50
00:01:45,160 --> 00:01:46,140
0,320 320,500 500,605 605,740 740,980
as part of our daily

51
00:01:46,140 --> 00:01:47,835
0,350 670,945 945,1125 1125,1365 1365,1695
lives| and we're working together
|我们正在与许多不同学科的全球领先行业合作伙伴合作，

52
00:01:47,835 --> 00:01:49,580
0,285 285,615 615,995 1195,1470 1470,1745
with leading global industry partners

53
00:01:49,690 --> 00:01:51,855
0,400 630,950 950,1205 1205,1760 1760,2165
across many different disciplines| ranging
|从机器人学、自主性医疗保健等等，

54
00:01:51,855 --> 00:01:54,105
0,240 240,755 1045,1755 1755,2010 2010,2250
from robotics, autonomy health care

55
00:01:54,105 --> 00:01:55,275
0,180 180,450 450,765 765,990 990,1170
and more| to develop a
|开发一系列产品，

56
00:01:55,275 --> 00:01:57,090
0,210 210,450 450,755 1375,1650 1650,1815
line of products,| that will
|以确保安全和值得信赖的人工智能，

57
00:01:57,090 --> 00:01:59,060
0,290 460,810 810,1050 1050,1650 1650,1970
guarantee safe and trustworthy AI|
|

58
00:01:59,440 --> 00:02:01,430
0,305 305,545 545,860 860,1240 1590,1990
and we drive this really
我们的技术工程和机器学习团队一起深入推动这一点，

59
00:02:01,630 --> 00:02:03,500
0,400 420,820 1110,1385 1385,1565 1565,1870
{} deeply with our technical

60
00:02:03,670 --> 00:02:05,450
0,400 540,875 875,1100 1100,1385 1385,1780
engineering and machine learning team,|
|

61
00:02:05,900 --> 00:02:08,005
0,400 480,800 800,1120 1440,1790 1790,2105
and our focus is very
我们的重点是工程，

62
00:02:08,005 --> 00:02:10,135
0,365 595,870 870,1125 1125,1505 1765,2130
much on the engineering,| very
|非常灵活和非常模块化的平台，

63
00:02:10,135 --> 00:02:12,570
0,365 565,840 840,1065 1065,1655 2035,2435
flexible and very modular platforms|
|

64
00:02:13,010 --> 00:02:14,940
0,305 305,610 660,1085 1085,1420 1530,1930
to scale algorithms towards robust
将算法扩展到健壮和值得信赖的人工智能，

65
00:02:15,200 --> 00:02:17,650
0,400 450,1190 1190,1540 2010,2270 2270,2450
and trustworthy AI,| {this,really -}
|这使这种部署能够应对我们社会今天面临的重大挑战，

66
00:02:17,650 --> 00:02:20,575
0,440 640,1035 1035,1640 2140,2540 2590,2925
enables this deployment towards grand

67
00:02:20,575 --> 00:02:22,440
0,335 415,720 720,1025 1105,1485 1485,1865
challenges that our society faces

68
00:02:22,460 --> 00:02:24,670
0,365 365,665 665,1000 1350,1750 1920,2210
with AI today,| specifically the
|特别是，今天的人工智能解决方案的能力根本不太值得信任，

69
00:02:24,670 --> 00:02:26,305
0,285 285,615 615,870 870,1190 1300,1635
ability for AI solutions today

70
00:02:26,305 --> 00:02:27,760
0,270 270,525 525,780 780,1350 1350,1455
are not very trustworthy at

71
00:02:27,760 --> 00:02:29,245
0,260 430,830 880,1170 1170,1335 1335,1485
all,| even if they may
|即使它们在我们作为本课程一部分学习的一些任务中可能表现得非常高。

72
00:02:29,245 --> 00:02:30,400
0,150 150,330 330,600 600,915 915,1155
be very high performance on

73
00:02:30,400 --> 00:02:31,270
0,135 135,225 225,360 360,630 630,870
some of the tasks that

74
00:02:31,270 --> 00:02:32,080
0,165 165,360 360,540 540,690 690,810
we study as part of

75
00:02:32,080 --> 00:02:34,000
0,195 195,530 1030,1410 1410,1710 1710,1920
{this,course -}.| So it's an
|所以，对于 Themis 来说，这是一个令人难以置信的令人兴奋的时刻，

76
00:02:34,000 --> 00:02:36,715
0,380 550,945 945,1340 1750,2150 2200,2715
incredibly exciting time for Themis|
|

77
00:02:36,715 --> 00:02:38,455
0,285 285,600 600,900 900,1205 1435,1740
and specific right now, where
特别是现在风险投资，

78
00:02:38,455 --> 00:02:40,375
0,390 390,900 900,1335 1335,1620 1620,1920
VC backed,| we're located our
|我们的办公室就在 Cambridge ，所以我们是本地的，

79
00:02:40,375 --> 00:02:41,485
0,300 300,540 540,765 765,945 945,1110
offices are right here in

80
00:02:41,485 --> 00:02:43,390
0,510 510,750 750,930 930,1175 1645,1905
Cambridge, so we're local| and
|我们刚刚完成了融资，

81
00:02:43,390 --> 00:02:44,740
0,120 120,345 345,660 660,1010 1030,1350
we have just closed around

82
00:02:44,740 --> 00:02:46,990
0,195 195,470 1180,1545 1545,1965 1965,2250
the funding,| so we're actively
|所以我们正在积极招聘最优秀、最聪明的工程师，像你们所有人一样，

83
00:02:46,990 --> 00:02:48,310
0,320 370,660 660,915 915,1155 1155,1320
hiring the best and the

84
00:02:48,310 --> 00:02:49,930
0,470 520,920 1000,1305 1305,1485 1485,1620
brightest engineers, like all of

85
00:02:49,930 --> 00:02:51,820
0,260 910,1230 1230,1470 1470,1650 1650,1890
you| to realize the future
|实现安全可靠的人工智能的未来，

86
00:02:51,820 --> 00:02:53,485
0,270 270,510 510,750 750,1335 1335,1665
of safe and trustworthy AI|
|

87
00:02:53,485 --> 00:02:54,730
0,255 255,375 375,570 570,900 900,1245
and we hope that really
我们希望今天的演讲激励你，

88
00:02:54,730 --> 00:02:56,970
0,435 435,630 630,1215 1215,1610 1840,2240
today's lecture inspires you| to
|加入我们的使命，建设人工智能的未来。

89
00:02:56,990 --> 00:02:58,470
0,305 305,590 590,875 875,1130 1130,1480
join us on this mission

90
00:02:58,550 --> 00:02:59,665
0,275 275,485 485,680 680,860 860,1115
to build {the,future -} of

91
00:02:59,665 --> 00:03:01,420
0,255 255,435 435,585 585,875 1375,1755
AI.| And with that it's
|说到这里，我非常高兴地介绍一下 Sadhana ，

92
00:03:01,420 --> 00:03:02,965
0,150 150,375 375,710 910,1275 1275,1545
my {great,pleasure -} to introduce

93
00:03:02,965 --> 00:03:04,810
0,515 865,1350 1350,1455 1455,1635 1635,1845
Sadhana,| Sadhana is a machine
|Sadhana 是 Themis 的一位机器学习科学家，

94
00:03:04,810 --> 00:03:05,840
0,240 240,590
learning scientist

95
00:03:05,840 --> 00:03:08,570
0,120 120,560 1240,1790 2020,2420 2440,2730
at Themis,| she's also the
|她也是这门课程的首席助教，

96
00:03:08,570 --> 00:03:10,060
0,240 240,590 670,945 945,1155 1155,1490
lead TA of this course,|
|

97
00:03:10,170 --> 00:03:11,510
0,410 410,530 530,680 680,970 990,1340
intro to deep learning at
MIT 的深度学习入门，

98
00:03:11,510 --> 00:03:13,430
0,350 700,975 975,1245 1245,1545 1545,1920
MIT,| {her,research -} at Themis
|她在 Themis 的研究专注于，

99
00:03:13,430 --> 00:03:15,620
0,555 555,890 970,1275 1275,1580 1930,2190
focuses specifically on,| how we
|我们如何为人工智能构建非常模块化和灵活的方法，

100
00:03:15,620 --> 00:03:17,045
0,135 135,360 360,690 690,1215 1215,1425
can build very modular and

101
00:03:17,045 --> 00:03:19,430
0,305 385,785 1255,1655 1795,2145 2145,2385
flexible methods for AI| and
|并构建我们所说的安全可靠的人工智能。

102
00:03:19,430 --> 00:03:20,900
0,290 610,870 870,1080 1080,1305 1305,1470
building what we call a

103
00:03:20,900 --> 00:03:22,880
0,180 180,360 360,1005 1005,1370 1690,1980
safe and trustworthy AI.| And
|今天，她将教我们，

104
00:03:22,880 --> 00:03:23,900
0,240 240,480 480,585 585,780 780,1020
today she'll be teaching us|
|

105
00:03:23,900 --> 00:03:25,955
0,255 255,590 730,1130 1300,1590 1590,2055
more about specifically the bias
更多关于人工智能算法的偏见和不确定性领域，

106
00:03:25,955 --> 00:03:28,090
0,240 240,450 450,785 805,1445 1735,2135
and the uncertainty realms of

107
00:03:28,320 --> 00:03:30,410
0,400 420,940 1470,1730 1730,1865 1865,2090
AI algorithms,| which are really
|这是两个关键或关键组件，

108
00:03:30,410 --> 00:03:32,500
0,350 490,885 885,1185 1185,1490 1690,2090
two key or critical components|
|

109
00:03:32,550 --> 00:03:34,145
0,400 420,860 860,1085 1085,1370 1370,1595
towards achieving this mission or
实现安全可靠地部署我们周围人工智能的使命或愿景。

110
00:03:34,145 --> 00:03:35,930
0,180 180,485 715,1115 1195,1545 1545,1785
this vision of safe and

111
00:03:35,930 --> 00:03:38,360
0,680 760,1340 1450,1800 1800,2130 2130,2430
trustworthy deployment of AI all

112
00:03:38,360 --> 00:03:40,220
0,225 225,530 1060,1440 1440,1710 1710,1860
{around,us -}.| So thank you
|所以谢谢你们，请给 Sadhana 热烈的掌声。

113
00:03:40,220 --> 00:03:41,855
0,195 195,530 640,900 900,1160 1330,1635
and please give a big,

114
00:03:41,855 --> 00:03:43,370
0,305 535,810 810,975 975,1215 1215,1515
warm round of applause for

115
00:03:43,370 --> 00:03:44,860
0,380
Sadhana.|
|

116
00:03:48,040 --> 00:03:49,140
0,260 260,395 395,530 530,785 785,1100
Thank you so much, Alexander,
Alexander ，非常感谢你的介绍。

117
00:03:49,140 --> 00:03:51,540
0,165 165,240 240,470 1660,2040 2040,2400
for the introduction.| {Hi,everyone. -}
|大家好。我是 Sadhana ，

118
00:03:51,540 --> 00:03:53,025
0,375 375,840 840,1110 1110,1275 1275,1485
I'm Sadhana,| I'm a machine
|我是 Themis AI 的一名机器学习科学家，

119
00:03:53,025 --> 00:03:54,840
0,225 225,575 865,1185 1185,1410 1410,1815
learning scientist here at Themis

120
00:03:54,840 --> 00:03:56,715
0,350 730,1130 1210,1470 1470,1650 1650,1875
AI| and the lead TA
|也是今年这门课程的首席助教，

121
00:03:56,715 --> 00:03:57,710
0,165 165,300 300,510 510,720 720,995
of the course this year,|
|

122
00:03:57,910 --> 00:03:59,385
0,275 275,545 545,920 920,1160 1160,1475
and today I'm super excited
今天，我非常兴奋地代表 Themis 向你们所有人谈论健壮和值得信赖的深度学习。

123
00:03:59,385 --> 00:04:00,290
0,210 210,390 390,540 540,645 645,905
to talk to you all

124
00:04:00,340 --> 00:04:02,415
0,400 600,980 980,1265 1265,1880 1880,2075
about robust and trustworthy deep

125
00:04:02,415 --> 00:04:04,010
0,305 355,690 690,960 960,1170 1170,1595
learning on behalf of Themis.|
|

126
00:04:06,230 --> 00:04:07,830
0,400 480,815 815,1025 1025,1250 1250,1600
So over the past decade,|
在过去的十年里，|

127
00:04:07,850 --> 00:04:09,595
0,395 395,650 650,950 950,1270 1380,1745
we've seen some tremendous growth
我们看到人工智能在安全关键领域取得了巨大的增长，

128
00:04:09,595 --> 00:04:11,635
0,345 345,690 690,1055 1315,1695 1695,2040
in artificial intelligence, across safety

129
00:04:11,635 --> 00:04:13,825
0,315 315,785 1375,1650 1650,1815 1815,2190
critical domains,| in the spheres
|在自主和机器人领域，

130
00:04:13,825 --> 00:04:15,820
0,180 180,780 780,975 975,1445 1705,1995
of autonomy and robotics,| {we,now
|我们现在的模型可以在一秒钟的时间内对自动驾驶等事情做出关键决定，

131
00:04:15,820 --> 00:04:17,275
0,270 270,650 760,1110 1110,1320 1320,1455
-} have models that can

132
00:04:17,275 --> 00:04:18,670
0,195 195,515 535,930 930,1215 1215,1395
make critical decisions about things

133
00:04:18,670 --> 00:04:19,915
0,195 195,405 405,710 850,1110 1110,1245
like self driving at a

134
00:04:19,915 --> 00:04:21,610
0,405 405,665 1105,1380 1380,1530 1530,1695
second's notice,| and these are
|这些模型为全自动车辆和机器人铺平道路，

135
00:04:21,610 --> 00:04:22,750
0,330 330,450 450,660 660,915 915,1140
paving the way for fully

136
00:04:22,750 --> 00:04:25,285
0,570 570,860 910,1200 1200,1610 2260,2535
autonomous vehicles and robots,| and
|而这并没有停止，

137
00:04:25,285 --> 00:04:26,370
0,225 225,360 360,555 555,765 765,1085
that's not where this stops,|
|

138
00:04:26,480 --> 00:04:27,780
0,260 260,410 410,800 800,1010 1010,1300
in the spheres of medicine
在医药和医疗保健领域，

139
00:04:27,830 --> 00:04:29,740
0,275 275,550 1020,1505 1505,1685 1685,1910
and healthcare,| robots are now
|机器人现在已经装备好，可以进行救命手术，

140
00:04:29,740 --> 00:04:31,300
0,330 330,540 540,800 880,1230 1230,1560
equipped to conduct life {saving,surgery

141
00:04:31,300 --> 00:04:33,400
0,380 880,1155 1155,1430 1450,1890 1890,2100
-},| we have algorithms that
|我们有算法来生成对关键药物的预测，

142
00:04:33,400 --> 00:04:35,130
0,270 270,855 855,1050 1050,1335 1335,1730
generate predictions for critical drugs,|
|

143
00:04:35,150 --> 00:04:36,535
0,275 275,485 485,770 770,1115 1115,1385
that may cure diseases that
这些药物可能会治愈我们之前认为无法治愈的疾病，

144
00:04:36,535 --> 00:04:38,250
0,180 180,485 565,870 870,1035 1035,1715
we previously {thought,were -} incurable,|
|

145
00:04:38,750 --> 00:04:40,075
0,395 395,650 650,800 800,1070 1070,1325
and we have models that
我们的模型可以自动诊断疾病，

146
00:04:40,075 --> 00:04:42,295
0,275 325,725 745,1425 1425,1805 1855,2220
can automatically diagnose diseases,| without
|而不需要任何卫生保健专业人员的干预。

147
00:04:42,295 --> 00:04:43,900
0,615 615,870 870,1080 1080,1350 1350,1605
intervention from any health {care,professionals

148
00:04:43,900 --> 00:04:46,615
0,510 510,690 690,950 1870,2235 2235,2715
-} at all.| These advances
|这些进步是革命性的，

149
00:04:46,615 --> 00:04:48,160
0,180 180,960 960,1230 1230,1380 1380,1545
are revolutionary,| and they have
|它们有可能改变我们今天所知的生活。

150
00:04:48,160 --> 00:04:49,315
0,210 210,480 480,690 690,870 870,1155
the potential to change life

151
00:04:49,315 --> 00:04:50,190
0,240 240,375 375,480 480,600 600,875
as we know it today.|
|

152
00:04:51,550 --> 00:04:52,890
0,350 350,680 680,860 860,1130 1130,1340
But there's another question that
但还有一个问题我们需要问，

153
00:04:52,890 --> 00:04:53,895
0,120 120,270 270,405 405,650 730,1005
we need to ask,| which
|那就是这些模型在现实生活中处于什么位置，

154
00:04:53,895 --> 00:04:55,485
0,275 565,885 885,1125 1125,1335 1335,1590
is where are these models

155
00:04:55,485 --> 00:04:57,045
0,240 240,420 420,725 1135,1410 1410,1560
in real life,| a lot
|这些技术中的许多都是五年前、十年前创新的，

156
00:04:57,045 --> 00:04:58,545
0,150 150,330 330,635 865,1110 1110,1500
of these technologies were innovated

157
00:04:58,545 --> 00:05:00,225
0,360 360,660 660,945 945,1325 1405,1680
five, ten years ago,| but
|但你我在日常生活中并没有看到它们，

158
00:05:00,225 --> 00:05:01,305
0,135 135,285 285,555 555,930 930,1080
you and I don't see

159
00:05:01,305 --> 00:05:02,300
0,165 165,300 300,435 435,660 660,995
them in our daily lives,|
|

160
00:05:02,950 --> 00:05:04,500
0,400 570,860 860,1100 1100,1430 1430,1550
{so,what -} is, what's the
那么，创新和使用之间的差距是什么。

161
00:05:04,500 --> 00:05:06,240
0,210 210,540 540,915 915,1310 1360,1740
gap here between innovation and

162
00:05:06,240 --> 00:05:07,480
0,500
deployment.|
|

163
00:05:09,270 --> 00:05:10,340
0,260 260,485 485,785 785,965 965,1070
The reason why you and
你和我不能乘坐自动驾驶汽车或机器人去手术室的原因是这样的，

164
00:05:10,340 --> 00:05:11,390
0,195 195,480 480,600 600,810 810,1050
I can't go by self

165
00:05:11,390 --> 00:05:13,210
0,285 285,650 730,1020 1020,1380 1380,1820
driving cars or robots don't

166
00:05:13,470 --> 00:05:15,280
0,400 540,875 875,1175 1175,1475 1475,1810
typically assist in operating rooms

167
00:05:15,330 --> 00:05:17,435
0,305 305,610 1320,1670 1670,1910 1910,2105
is this,| {these,are -} some
|这些是仅在过去几年关于人工智能失败的一些头条新闻，

168
00:05:17,435 --> 00:05:18,830
0,435 435,690 690,825 825,1185 1185,1395
headlines about the failures of

169
00:05:18,830 --> 00:05:20,015
0,315 315,570 570,675 675,915 915,1185
AI from the last few

170
00:05:20,015 --> 00:05:22,340
0,225 225,575 1435,1755 1755,2070 2070,2325
years alone,| in addition to
|除了这些令人难以置信的进步，

171
00:05:22,340 --> 00:05:25,160
0,260 400,800 1510,2130 2130,2565 2565,2820
these incredible advances,| we've also
|我们还看到出现了灾难性的故障，

172
00:05:25,160 --> 00:05:27,520
0,290 310,1125 1125,1640 1750,2055 2055,2360
seen catastrophic failures| in every
|我刚才提到的每一个安全关键领域，

173
00:05:27,540 --> 00:05:28,730
0,350 350,635 635,830 830,965 965,1190
single one of the safety

174
00:05:28,730 --> 00:05:30,160
0,285 285,690 690,930 930,1125 1125,1430
critical domains I {just,mentioned -},|
|

175
00:05:31,140 --> 00:05:32,975
0,335 335,670 720,1100 1100,1415 1415,1835
these problems range from crashing
这些问题包括自动驾驶汽车的撞车，

176
00:05:32,975 --> 00:05:34,835
0,465 465,755 1105,1365 1365,1560 1560,1860
autonomous vehicles| to health care
|以及并不适用于每个人的医疗保健算法，

177
00:05:34,835 --> 00:05:36,395
0,435 435,615 615,990 990,1305 1305,1560
algorithms that don't actually work

178
00:05:36,395 --> 00:05:38,060
0,335 355,755 835,1170 1170,1380 1380,1665
for everyone,| even though they're
|尽管它们部署在现实世界中，

179
00:05:38,060 --> 00:05:39,005
0,315 315,495 495,660 660,780 780,945
deployed out in the real

180
00:05:39,005 --> 00:05:40,295
0,305 355,720 720,975 975,1125 1125,1290
world,| so everyone can use
|所以每个人都可以使用它们。

181
00:05:40,295 --> 00:05:41,000
0,305
them.|
|

182
00:05:41,790 --> 00:05:43,180
0,400 450,710 710,830 830,1040 1040,1390
Now, at a first glance,|
现在，初看起来，|

183
00:05:43,380 --> 00:05:45,710
0,290 290,560 560,860 860,1480 1980,2330
this seems really demoralizing,| {if,these
这似乎真的让人士气低落，|如果这些都是人工智能的问题所在，

184
00:05:45,710 --> 00:05:46,520
0,225 225,390 390,570 570,705 705,810
-} are all of the

185
00:05:46,520 --> 00:05:47,950
0,180 180,435 435,735 735,1065 1065,1430
things wrong with artificial intelligence,|
|

186
00:05:48,330 --> 00:05:49,340
0,275 275,410 410,560 560,785 785,1010
how are we ever going
我们将如何实现这样的愿景，

187
00:05:49,340 --> 00:05:50,600
0,225 225,465 465,690 690,990 990,1260
to achieve that vision| of
|将我们的人工智能整合到我们的日常生活结构中，

188
00:05:50,600 --> 00:05:52,370
0,290 310,690 690,1070 1180,1515 1515,1770
having our AI integrated into

189
00:05:52,370 --> 00:05:53,465
0,180 180,420 420,660 660,825 825,1095
the fabric of our daily

190
00:05:53,465 --> 00:05:55,145
0,365 625,945 945,1215 1215,1455 1455,1680
lives| in terms of safety
|在安全关键部署方面，

191
00:05:55,145 --> 00:05:58,490
0,335 355,905 2305,2655 2655,2910 2910,3345
critical deployment,| but at Themis,
|但在 Themis ，这正是我们解决的问题类型，

192
00:05:58,490 --> 00:05:59,645
0,240 240,450 450,765 765,1005 1005,1155
this is exactly the type

193
00:05:59,645 --> 00:06:00,640
0,150 150,360 360,570 570,720 720,995
of problem that we solve,|
|

194
00:06:01,200 --> 00:06:02,540
0,380 380,710 710,920 920,1100 1100,1340
we want to bring these
我们希望将这些进步带到现实世界中，

195
00:06:02,540 --> 00:06:03,770
0,480 480,645 645,765 765,945 945,1230
advances to the real world,|
|

196
00:06:03,770 --> 00:06:04,610
0,255 255,375 375,525 525,705 705,840
and the way we do
我们做到这一点的方式是在安全和值得信赖的人工智能领域进行创新，

197
00:06:04,610 --> 00:06:05,810
0,165 165,345 345,510 510,990 990,1200
this is by innovating in

198
00:06:05,810 --> 00:06:07,085
0,150 150,525 525,750 750,1020 1020,1275
the spheres of safe and

199
00:06:07,085 --> 00:06:09,380
0,675 675,1035 1035,1415 1765,2055 2055,2295
trustworthy artificial intelligence| in order
|以便将世界各地研究实验室开发的东西带给像你我这样的客户。

200
00:06:09,380 --> 00:06:10,625
0,210 210,470 640,900 900,1065 1065,1245
to bring the things that

201
00:06:10,625 --> 00:06:11,825
0,165 165,390 390,585 585,825 825,1200
were developed in research labs

202
00:06:11,825 --> 00:06:13,060
0,180 180,345 345,605 685,960 960,1235
around the world to customers

203
00:06:13,350 --> 00:06:14,500
0,305 305,455 455,575 575,850
like you and me.|
|

204
00:06:16,450 --> 00:06:17,930
0,365 365,605 605,755 755,1030 1080,1480
And we do this by,
我们这样做，我们的核心意识形态是，

205
00:06:18,100 --> 00:06:19,560
0,320 320,530 530,1145 1145,1295 1295,1460
our core ideology is that,|
|

206
00:06:19,560 --> 00:06:20,670
0,225 225,480 480,720 720,945 945,1110
we believe that all of
我们相信这张幻灯片上的所有问题都有两个关键概念作为基础，

207
00:06:20,670 --> 00:06:21,800
0,135 135,390 390,645 645,825 825,1130
the problems on this slide

208
00:06:21,820 --> 00:06:23,985
0,400 570,1175 1175,1445 1445,1810 1830,2165
are underlaid by two key

209
00:06:23,985 --> 00:06:26,270
0,545 775,1050 1050,1325 1465,1785 1785,2285
notions,| {the,first -} is bias,|
|首先是偏差，|

210
00:06:27,110 --> 00:06:28,735
0,530 530,740 740,890 890,1150 1230,1625
bias is what happens when
偏差是指机器学习模型在某些人口统计数据上表现得比其他更好，

211
00:06:28,735 --> 00:06:30,040
0,255 255,435 435,755 775,1050 1050,1305
machine learning models do better

212
00:06:30,040 --> 00:06:31,650
0,300 300,510 510,1155 1155,1350 1350,1610
on some demographics than others,|
|

213
00:06:32,390 --> 00:06:33,760
0,365 365,635 635,830 830,1070 1070,1370
{this,results -} in things like
这导致面部检测系统，无法高精度检测某些人脸，

214
00:06:33,760 --> 00:06:35,410
0,435 435,795 795,1070 1180,1485 1485,1650
facial detection systems, not being

215
00:06:35,410 --> 00:06:36,720
0,210 210,480 480,735 735,990 990,1310
able to detect certain faces

216
00:06:36,890 --> 00:06:39,325
0,380 380,755 755,1420 1800,2255 2255,2435
with high accuracy,| Siri not
|Siri 无法识别带有口音的语音，

217
00:06:39,325 --> 00:06:40,620
0,165 165,345 345,525 525,785 895,1295
being able to recognize voices

218
00:06:40,640 --> 00:06:42,595
0,275 275,820 1020,1415 1415,1820 1820,1955
with accents,| or algorithms that
|或者针对不平衡数据集进行训练的算法，

219
00:06:42,595 --> 00:06:44,065
0,150 150,390 390,630 630,1215 1215,1470
are trained on imbalanced data

220
00:06:44,065 --> 00:06:45,670
0,335 445,720 720,885 885,1170 1170,1605
sets,| so what the algorithm
|所以，算法相信的是一个好的解决方案，

221
00:06:45,670 --> 00:06:47,010
0,315 315,570 570,750 750,1005 1005,1340
believes is a good solution|
|

222
00:06:47,450 --> 00:06:49,105
0,530 530,815 815,1055 1055,1370 1370,1655
doesn't actually work for everyone
实际上并不适用于现实世界中的每个人。

223
00:06:49,105 --> 00:06:50,740
0,165 165,270 270,435 435,755
in the real world.|
|

224
00:06:52,030 --> 00:06:53,160
0,275 275,410 410,605 605,905 905,1130
And the second notion that
许多此类问题背后的第二个概念是无法缓解和无法传达的不确定性，

225
00:06:53,160 --> 00:06:54,165
0,405 405,555 555,675 675,825 825,1005
underlies a lot of these

226
00:06:54,165 --> 00:06:56,460
0,285 285,665 805,1200 1200,2025 2025,2295
problems today is unmitigated and

227
00:06:56,460 --> 00:06:59,370
0,860 1000,1400 2290,2565 2565,2715 2715,2910
uncommunicated uncertainty,| {this,is -} when
|这就是模型不知道什么时候可以信任或者不可以信任，

228
00:06:59,370 --> 00:07:00,990
0,320 400,825 825,1100 1120,1410 1410,1620
models don't know when they

229
00:07:00,990 --> 00:07:02,450
0,300 300,615 615,975 975,1155 1155,1460
can or can't be trusted,|
|

230
00:07:03,160 --> 00:07:04,785
0,290 290,545 545,830 830,1040 1040,1625
and this results in scenarios,|
这导致了一些场景，|

231
00:07:04,785 --> 00:07:06,020
0,210 210,345 345,555 555,855 855,1235
such as self driving cars
比如自动驾驶汽车在没有 100% 自信的情况下继续在环境中操作，

232
00:07:06,190 --> 00:07:08,700
0,365 365,695 695,1010 1010,1360 1950,2510
continuing to operate in environments

233
00:07:08,700 --> 00:07:09,870
0,195 195,390 390,555 555,885 885,1170
when {they're,not - -} 100%

234
00:07:09,870 --> 00:07:11,550
0,350 640,885 885,1005 1005,1280 1360,1680
confident| instead of giving control
|而不是将控制权交给用户，

235
00:07:11,550 --> 00:07:14,180
0,180 180,440 1270,1670 1750,2280 2280,2630
to users,| or robots being
|或者机器人在它们以前从未进入过的环境中移动，并且高度不熟悉。

236
00:07:14,380 --> 00:07:15,870
0,365 365,635 635,875 875,1340 1340,1490
moving around in environments that

237
00:07:15,870 --> 00:07:16,790
0,180 180,345 345,540 540,660 660,920
they've never been in before

238
00:07:16,810 --> 00:07:18,590
0,260 260,455 455,695 695,1520 1520,1780
and have high unfamiliarity with.|
|

239
00:07:21,160 --> 00:07:22,125
0,350 350,560 560,695 695,830 830,965
And a lot of the
现代人工智能中的许多问题都是彻头彻尾的偏差和不确定性的组合的结果。

240
00:07:22,125 --> 00:07:24,180
0,275 505,905 985,1365 1365,1725 1725,2055
problems in modern day AI

241
00:07:24,180 --> 00:07:25,215
0,240 240,450 450,690 690,870 870,1035
are the result of a

242
00:07:25,215 --> 00:07:27,495
0,305 565,840 840,1485 1485,1965 1965,2280
combination of unmitigated bias and

243
00:07:27,495 --> 00:07:29,200
0,365
uncertainty.|
|

244
00:07:30,980 --> 00:07:32,550
0,395 395,790 840,1115 1115,1280 1280,1570
So today in this lecture,|
所以今天在这节课中，|

245
00:07:32,750 --> 00:07:33,880
0,320 320,440 440,575 575,800 800,1130
we're going to focus on
我们将重点研究所有这些问题的根本原因，

246
00:07:33,880 --> 00:07:35,620
0,585 585,795 795,1020 1020,1370 1420,1740
investigating the root causes of

247
00:07:35,620 --> 00:07:37,120
0,210 210,375 375,555 555,860 1120,1500
all of these problems,| these
|这是健壮深度学习面临的两大挑战。

248
00:07:37,120 --> 00:07:38,950
0,345 345,660 660,1010 1210,1530 1530,1830
two big challenges to robust

249
00:07:38,950 --> 00:07:41,050
0,270 270,560 1270,1680 1680,1890 1890,2100
deep learning.| We'll also talk
|我们还将讨论它们的解决方案，

250
00:07:41,050 --> 00:07:42,295
0,225 225,530 610,870 870,1050 1050,1245
about solutions for them,| that
|可以为每个人提高所有这些算法的健壮性和安全性。

251
00:07:42,295 --> 00:07:43,660
0,210 210,465 465,705 705,1185 1185,1365
can improve the robustness and

252
00:07:43,660 --> 00:07:45,000
0,290 460,750 750,930 930,1080 1080,1340
safety of all of these

253
00:07:45,050 --> 00:07:47,365
0,515 515,875 875,1270 1800,2090 2090,2315
algorithms for everyone.| {And,we'll -}
|我们将从讨论偏差开始。

254
00:07:47,365 --> 00:07:48,900
0,195 195,405 405,695 745,1035 1035,1535
start by talking about bias.|
|

255
00:07:50,280 --> 00:07:51,320
0,425 425,560 560,680 680,860 860,1040
Bias is a word that
在深度学习的背景下，我们都听说过 Bias 这个词，

256
00:07:51,320 --> 00:07:52,610
0,210 210,435 435,800 910,1170 1170,1290
we've all heard, outside the

257
00:07:52,610 --> 00:07:54,125
0,255 255,525 525,690 690,980 1240,1515
context of deep learning,| but
|但在机器学习的背景下，它是可以量化和数学定义的。

258
00:07:54,125 --> 00:07:55,250
0,150 150,300 300,570 570,900 900,1125
in the context of machine

259
00:07:55,250 --> 00:07:56,885
0,290 430,705 705,840 840,1020 1020,1635
learning, it can be quantified

260
00:07:56,885 --> 00:07:59,060
0,240 240,785 835,1235
and mathematically defined.|
|

261
00:07:59,130 --> 00:08:00,170
0,320 320,560 560,710 710,905 905,1040
Today we'll talk about how
今天，我们将讨论如何做到这一点，

262
00:08:00,170 --> 00:08:01,295
0,135 135,270 270,530 550,855 855,1125
to do this| and methods
|从算法上缓解这种偏差的方法，

263
00:08:01,295 --> 00:08:02,950
0,240 240,735 735,1005 1005,1200 1200,1655
for mitigation of this bias

264
00:08:02,970 --> 00:08:05,030
0,910 990,1280 1280,1505 1505,1880 1880,2060
algorithmically| and how Themis is
|以及 Themis 如何在这些领域进行创新，

265
00:08:05,030 --> 00:08:06,455
0,465 465,660 660,810 810,1100 1180,1425
innovating in these areas| in
|以便将该领域的新算法带到世界各地的行业。

266
00:08:06,455 --> 00:08:07,760
0,165 165,330 330,495 495,815 895,1305
order to bring new algorithms

267
00:08:07,760 --> 00:08:10,445
0,135 135,330 330,680 880,1280 2350,2685
in this space to industries

268
00:08:10,445 --> 00:08:12,200
0,225 225,375 375,635
around the world.|
|

269
00:08:12,240 --> 00:08:14,080
0,400 630,935 935,1115 1115,1445 1445,1840
Afterwards, we'll talk about uncertainty,|
然后，我们将讨论不确定性，|

270
00:08:14,460 --> 00:08:15,800
0,275 275,550 690,965 965,1145 1145,1340
which is, can we teach
也就是，我们能教一个模型知道或不知道给定任务的答案吗，

271
00:08:15,800 --> 00:08:17,015
0,165 165,435 435,750 750,960 960,1215
a model when it does

272
00:08:17,015 --> 00:08:18,190
0,240 240,570 570,765 765,915 915,1175
or doesn't know the answer

273
00:08:18,300 --> 00:08:19,750
0,400 510,740 740,860 860,1100 1100,1450
to to its given task,|
|

274
00:08:20,100 --> 00:08:21,095
0,260 260,425 425,590 590,800 800,995
and we'll talk about the
我们将讨论这对现实世界人工智能的影响。

275
00:08:21,095 --> 00:08:22,940
0,675 675,915 915,1175 1375,1650 1650,1845
ramifications for this for real

276
00:08:22,940 --> 00:08:24,780
0,320 370,770
world AI.|
|

277
00:08:27,900 --> 00:08:29,900
0,400 630,980 980,1325 1325,1610 1610,2000
So what exactly does bias
那么，偏差到底是什么意思，

278
00:08:29,900 --> 00:08:31,295
0,320 580,885 885,1095 1095,1260 1260,1395
mean,| and where is it
|它在人工智能生命周期中处于什么位置，

279
00:08:31,295 --> 00:08:32,645
0,240 240,465 465,675 675,990 990,1350
present in the artificial intelligence

280
00:08:32,645 --> 00:08:34,860
0,785
lifecycle,|
|

281
00:08:34,860 --> 00:08:36,165
0,135 135,300 300,915 915,1125 1125,1305
the most intuitive form of
最直观的偏差形式来自数据。

282
00:08:36,165 --> 00:08:38,535
0,455 505,870 870,1140 1140,1445 2095,2370
bias comes from data.| {We,have
|我们这里有两种不同的主要类型的偏差，

283
00:08:38,535 --> 00:08:39,780
0,240 240,495 495,780 780,1050 1050,1245
-} two different two main

284
00:08:39,780 --> 00:08:41,205
0,225 225,390 390,705 705,1010 1150,1425
types of bias here,| the
|第一种是抽样偏差，

285
00:08:41,205 --> 00:08:43,290
0,275 325,660 660,1125 1125,1595 1825,2085
first is sampling bias,| which
|即从我们的输入数据分布的某些地区过度抽样，

286
00:08:43,290 --> 00:08:44,535
0,150 150,375 375,600 600,870 870,1245
is when we over sample

287
00:08:44,535 --> 00:08:45,840
0,300 300,525 525,840 840,1080 1080,1305
from some regions of our

288
00:08:45,840 --> 00:08:47,700
0,255 255,530 550,950 1330,1620 1620,1860
input data distribution| and under
|而从其他地区抽样不足，

289
00:08:47,700 --> 00:08:49,500
0,330 330,585 585,860 1360,1620 1620,1800
sample {from,others -},| a good
|一个很好的例子是大量的临床数据集，

290
00:08:49,500 --> 00:08:50,505
0,255 255,450 450,675 675,900 900,1005
example of this is a

291
00:08:50,505 --> 00:08:51,710
0,120 120,285 285,555 555,870 870,1205
lot of clinical data sets,|
|

292
00:08:51,940 --> 00:08:53,835
0,400 600,890 890,1180 1260,1580 1580,1895
where they often contain fewer
其中包含的疾病患者的例子往往比健康患者少，

293
00:08:53,835 --> 00:08:56,145
0,395 505,905 1135,1605 1605,1955 2035,2310
examples of diseased patients than

294
00:08:56,145 --> 00:08:57,585
0,240 240,605 745,1005 1005,1230 1230,1440
healthy patients,| because it's much
|因为获得健康患者的数据比获得疾病患者的数据容易得多。

295
00:08:57,585 --> 00:08:59,265
0,330 330,630 630,915 915,1265 1435,1680
easier to acquire data for

296
00:08:59,265 --> 00:09:00,600
0,195 195,545 625,900 900,1095 1095,1335
healthy patients than their disease

297
00:09:00,600 --> 00:09:02,040
0,680
counterparts.|
|

298
00:09:02,610 --> 00:09:04,130
0,305 305,610 720,1070 1070,1280 1280,1520
In addition, we also have
此外，在人工智能生命周期的数据部分，我们也存在选择偏差，

299
00:09:04,130 --> 00:09:05,540
0,360 360,840 840,1065 1065,1200 1200,1410
selection bias at the data

300
00:09:05,540 --> 00:09:06,755
0,350 370,630 630,810 810,1020 1020,1215
portion of the AI life

301
00:09:06,755 --> 00:09:09,340
0,305 1105,1425 1425,1680 1680,2295 2295,2585
cycle,| {think,about -} Apple's series
|想想苹果的一系列语音识别算法，

302
00:09:09,390 --> 00:09:12,220
0,320 320,640 960,1540 2190,2510 2510,2830
voice recognition algorithm,| this model
|这个模型主要是根据完美的美式英语训练的，

303
00:09:12,540 --> 00:09:14,420
0,320 320,590 590,940 1020,1385 1385,1880
is trained largely on flawless

304
00:09:14,420 --> 00:09:16,535
0,350 460,860 1180,1485 1485,1800 1800,2115
American English,| but it's deployed
|但它被部署在现实世界中，

305
00:09:16,535 --> 00:09:17,675
0,225 225,420 420,615 615,915 915,1140
across the real world| to
|以便能够识别来自世界各地的带有口音的声音，

306
00:09:17,675 --> 00:09:18,880
0,90 90,255 255,420 420,665 805,1205
be able to recognize voices

307
00:09:18,900 --> 00:09:20,345
0,290 290,785 785,995 995,1190 1190,1445
with accents from all over

308
00:09:20,345 --> 00:09:21,260
0,180 180,425
the world,|
|

309
00:09:21,850 --> 00:09:23,505
0,365 365,730 870,1145 1145,1280 1280,1655
the distribution of the model's
模型的训练数据的分布与现实世界中这类语言的分布不匹配，

310
00:09:23,505 --> 00:09:25,320
0,210 210,545 625,1095 1095,1415 1435,1815
training data doesn't match the

311
00:09:25,320 --> 00:09:26,630
0,380 430,705 705,870 870,1035 1035,1310
distribution of this type of

312
00:09:27,070 --> 00:09:28,250
0,350 350,545 545,665 665,860 860,1180
language in the real world,|
|

313
00:09:28,570 --> 00:09:30,195
0,350 350,700 780,1100 1100,1340 1340,1625
because American English is highly
因为美国英语被严重高估了，与其他人口统计数据相反，

314
00:09:30,195 --> 00:09:32,040
0,845 985,1320 1320,1560 1560,1695 1695,1845
overrepresented, as opposed to other

315
00:09:32,040 --> 00:09:33,900
0,740
demographics.|
|

316
00:09:35,070 --> 00:09:36,920
0,320 320,620 620,800 800,1120 1500,1850
But that's not where, that's
但这并不是问题所在，偏差和数据也不会止步于此，

317
00:09:36,920 --> 00:09:37,955
0,105 105,270 270,630 630,780 780,1035
not where bias and data

318
00:09:37,955 --> 00:09:40,745
0,395 1045,1365 1365,2010 2010,2375 2485,2790
stops,| {these,biases -} can be
|这些偏差可以传播到模型训练循环本身，

319
00:09:40,745 --> 00:09:42,875
0,570 570,900 900,1265 1285,1635 1635,2130
propagated towards models training cycles

320
00:09:42,875 --> 00:09:44,105
0,395 565,825 825,945 945,1065 1065,1230
themselves,| which is what we'll
|这是我们在本课程的后半部分将重点关注的。

321
00:09:44,105 --> 00:09:45,035
0,195 195,450 450,600 600,720 720,930
focus on in the second

322
00:09:45,035 --> 00:09:47,140
0,270 270,465 465,600 600,875
half of this lecture.|
|

323
00:09:48,130 --> 00:09:49,440
0,260 260,520 600,935 935,1130 1130,1310
And then once the model
然后，一旦模型被实际部署，

324
00:09:49,440 --> 00:09:51,195
0,320 400,800 850,1320 1320,1545 1545,1755
is actually deployed,| which means
|这意味着它被放到了现实世界中，

325
00:09:51,195 --> 00:09:52,350
0,330 330,600 600,780 780,960 960,1155
it's actually put out into

326
00:09:52,350 --> 00:09:53,450
0,165 165,345 345,585 585,810 810,1100
the real world| and customers
|客户或用户可以从它获得预测，

327
00:09:53,590 --> 00:09:55,245
0,275 275,550 570,950 950,1330 1350,1655
or users can actually get

328
00:09:55,245 --> 00:09:56,835
0,165 165,555 555,720 720,995 1315,1590
the predictions from it,| we
|我们可能会看到更多我们以前从未见过的偏差，

329
00:09:56,835 --> 00:09:59,805
0,240 240,605 1435,1770 1770,2295 2295,2970
may see further biases perpetuated

330
00:09:59,805 --> 00:10:00,980
0,240 240,390 390,690 690,885 885,1175
that we haven't seen before,|
|

331
00:10:01,630 --> 00:10:02,805
0,260 260,440 440,620 620,830 830,1175
{the,first -} of these is
第一个是分布的转变，

332
00:10:02,805 --> 00:10:05,055
0,395 475,1055 1525,1905 1905,2070 2070,2250
distribution shifts,| let's say I
|假设我有一个模型，

333
00:10:05,055 --> 00:10:06,270
0,135 135,255 255,515 565,930 930,1215
have a model,| that I
|是我根据过去 20 年的数据训练的，

334
00:10:06,270 --> 00:10:07,560
0,270 270,585 585,825 825,1050 1050,1290
trained on the past twenty

335
00:10:07,560 --> 00:10:08,985
0,195 195,360 360,620 970,1245 1245,1425
years of data,| and then
|然后我把它部署到现实世界中，

336
00:10:08,985 --> 00:10:09,900
0,240 240,450 450,585 585,750 750,915
I deploy it into the

337
00:10:09,900 --> 00:10:11,990
0,150 150,440 910,1215 1215,1740 1740,2090
real world,| {in -} 2023,
|在 2023 ，这个模型可能会做得很好，

338
00:10:12,100 --> 00:10:13,410
0,290 290,500 500,695 695,970 1020,1310
This model will probably do

339
00:10:13,410 --> 00:10:14,895
0,290 430,690 690,810 810,1070 1120,1485
fine,| because the data input
|因为数据输入分布与训练分布中的数据非常相似，

340
00:10:14,895 --> 00:10:16,605
0,365 445,765 765,1035 1035,1385 1465,1710
distribution is quite similar to

341
00:10:16,605 --> 00:10:17,900
0,210 210,480 480,645 645,900 900,1295
data in the training distribution,|
|

342
00:10:18,900 --> 00:10:20,025
0,320 340,615 615,750 750,945 945,1125
but what would happen to
但这种模式在 2033 年会发生什么，

343
00:10:20,025 --> 00:10:22,650
0,135 135,405 405,735 735,2370 2370,2625
this model in 2033,| it
|如果它可能不会很好地发挥作用，

344
00:10:22,650 --> 00:10:24,360
0,290 310,600 600,890 1000,1395 1395,1710
probably would not work as

345
00:10:24,360 --> 00:10:26,250
0,320 700,1020 1020,1290 1290,1635 1635,1890
well,| because the distribution that
|因为数据来自的分布将在这十年内发生显着变化，

346
00:10:26,250 --> 00:10:27,150
0,105 105,255 255,450 450,675 675,900
the data is coming from

347
00:10:27,150 --> 00:10:29,355
0,210 210,530 550,950 1630,1965 1965,2205
would shift significantly across this

348
00:10:29,355 --> 00:10:30,705
0,305 565,825 825,945 945,1065 1065,1350
decade,| {and,if -} we don't
|如果我们不继续用这些输入数据流更新我们的模型，

349
00:10:30,705 --> 00:10:31,875
0,240 240,525 525,735 735,885 885,1170
continue to update our models

350
00:10:31,875 --> 00:10:33,405
0,255 255,515 925,1230 1230,1395 1395,1530
with this input stream of

351
00:10:33,405 --> 00:10:34,545
0,275 475,780 780,870 870,990 990,1140
data,| we're going to have
|我们就会得到过时和不正确的预测。

352
00:10:34,545 --> 00:10:37,480
0,630 630,855 855,1365 1365,1985
obsolete and incorrect predictions.|
|

353
00:10:38,510 --> 00:10:40,690
0,290 290,580 780,1180 1290,1805 1805,2180
And finally, after deployment, there
最后，部署之后，在评估方面，

354
00:10:40,690 --> 00:10:43,390
0,195 195,800 1000,1400 1780,2180 2380,2700
the evaluation aspect,| {so,think -}
|回想一下我们谈论的 Apple Siri 的例子，

355
00:10:43,390 --> 00:10:44,920
0,300 300,600 600,840 840,1125 1125,1530
back to the Apple Siri

356
00:10:44,920 --> 00:10:45,985
0,255 255,450 450,645 645,780 780,1065
example that we've been talking

357
00:10:45,985 --> 00:10:48,715
0,395 835,1235 1645,1980 1980,2205 2205,2730
about,| {} if the evaluation
|如果评估 Siri 的评估指标或评估数据集

358
00:10:48,715 --> 00:10:50,290
0,495 495,720 720,870 870,1275 1275,1575
metric or the evaluation data

359
00:10:50,290 --> 00:10:51,685
0,240 240,450 450,780 780,945 945,1395
set that Siri was evaluated

360
00:10:51,685 --> 00:10:53,725
0,305 475,875 925,1215 1215,1500 1500,2040
on| was also mostly comprised
|也主要由美国英语组成，

361
00:10:53,725 --> 00:10:56,610
0,360 360,725 805,1205 1525,1925 2485,2885
of American English,| then to
|那么在任何人看来，这个模型都会表现得非常好，

362
00:10:56,630 --> 00:10:57,955
0,380 380,650 650,935 935,1190 1190,1325
anybody this model will look

363
00:10:57,955 --> 00:10:59,160
0,135 135,255 255,495 495,840 840,1205
like it does extremely well,

364
00:10:59,330 --> 00:11:00,790
0,290 290,440 440,650 650,1000 1200,1460
right,| it can detect, it
|它可以检测，它可以非常高的准确率识别美国英语的声音，

365
00:11:00,790 --> 00:11:02,440
0,165 165,470 700,1095 1095,1380 1380,1650
can recognize American English voices

366
00:11:02,440 --> 00:11:04,495
0,345 345,660 660,1005 1005,1610 1690,2055
with extremely high accuracy| and
|此部署到现实世界中，

367
00:11:04,495 --> 00:11:05,650
0,255 255,495 495,825 825,990 990,1155
therefore is deployed into the

368
00:11:05,650 --> 00:11:07,300
0,165 165,470 940,1200 1200,1365 1365,1650
{real,world -},| but what about
|但它在子组准确率会怎么样，

369
00:11:07,300 --> 00:11:09,340
0,375 375,855 855,1050 1050,1710 1710,2040
its accuracy on subgroups,| on
|对于带口音的人，那些英语不是他们的第一语言的人，

370
00:11:09,340 --> 00:11:11,400
0,570 570,920 1060,1395 1395,1695 1695,2060
accented voices, on people who

371
00:11:11,780 --> 00:11:12,790
0,275 275,500 500,725 725,860 860,1010
for whom English is not

372
00:11:12,790 --> 00:11:14,320
0,165 165,345 345,650 1090,1365 1365,1530
their first language,| if we
|如果我们不在我们的评估指标中对子组进行测试，

373
00:11:14,320 --> 00:11:15,985
0,405 405,705 705,945 945,1170 1170,1665
don't also test on subgroups

374
00:11:15,985 --> 00:11:17,680
0,120 120,270 270,795 795,1145 1405,1695
in our evaluation metrics,| we're
|我们将面临偏差。

375
00:11:17,680 --> 00:11:20,340
0,120 120,270 270,1125 1125,1640
going to {face,evaluation} bias.|
|

376
00:11:20,640 --> 00:11:22,115
0,260 260,520 690,1055 1055,1235 1235,1475
So now let's talk about
现在，让我们来谈谈现实世界中的另一个例子，

377
00:11:22,115 --> 00:11:23,240
0,270 270,615 615,855 855,960 960,1125
another example in the real

378
00:11:23,240 --> 00:11:24,530
0,300 300,570 570,765 765,1125 1125,1290
world| of how bias can
|关于偏差如何在人工智能生命周期的整个过程中持续存在，

379
00:11:24,530 --> 00:11:26,120
0,615 615,900 900,1125 1125,1365 1365,1590
perpetuate throughout the course of

380
00:11:26,120 --> 00:11:27,640
0,240 240,585 585,930 930,1215 1215,1520
this artificial intelligence life cycle,|
|

381
00:11:30,310 --> 00:11:32,240
0,395 395,845 845,1220 1220,1510 1530,1930
commercial facial detection systems are
商用面部检测系统无处不在，

382
00:11:32,320 --> 00:11:34,095
0,400 570,970 1080,1370 1370,1565 1565,1775
everywhere,| {you,actually -} played around
|在第二个实验中，你玩了其中一些，

383
00:11:34,095 --> 00:11:35,150
0,195 195,360 360,495 495,705 705,1055
with some of them in

384
00:11:35,410 --> 00:11:36,795
0,365 365,680 680,875 875,1085 1085,1385
lab two,| when you trained
|当你在面部检测数据集上训练你的 VAE 时。

385
00:11:36,795 --> 00:11:38,580
0,225 225,515 865,1200 1200,1425 1425,1785
your VAE on a facial

386
00:11:38,580 --> 00:11:40,520
0,330 330,525 525,860
detection data set.|
|

387
00:11:40,520 --> 00:11:41,690
0,165 165,470 490,795 795,945 945,1170
In addition to the lock
除了手机上的锁屏外，

388
00:11:41,690 --> 00:11:42,820
0,300 300,450 450,615 615,825 825,1130
screens on your cell phones,|
|

389
00:11:44,040 --> 00:11:45,695
0,455 455,785 785,1040 1040,1400 1400,1655
facial detection systems are also
面部检测系统还安装在自动滤镜中，

390
00:11:45,695 --> 00:11:47,350
0,255 255,510 510,780 780,1110 1110,1655
present in the automatic filters,|
|

391
00:11:47,370 --> 00:11:48,650
0,260 260,395 395,590 590,910 930,1280
that your phone cameras apply,|
你的手机摄像头就会应用这些，|

392
00:11:48,650 --> 00:11:49,445
0,240 240,420 420,570 570,690 690,795
whenever you try to take
每当你拍照时，

393
00:11:49,445 --> 00:11:51,140
0,105 105,365 985,1335 1335,1515 1515,1695
a picture,| and {they're -}
|它们也被用于刑事调查。

394
00:11:51,140 --> 00:11:52,840
0,225 225,435 435,690 690,1005 1005,1700
also used in criminal investigations.|
|

395
00:11:54,120 --> 00:11:55,985
0,365 365,680 680,1025 1025,1400 1400,1865
These are three commercial facial
这是三个部署的商用面部检测系统，

396
00:11:55,985 --> 00:11:57,700
0,345 345,635 655,930 930,1170 1170,1715
detection systems that were deployed,|
|

397
00:11:57,780 --> 00:12:00,215
0,400 1110,1445 1445,1835 1835,1985 1985,2435
and we'll analyze the biases
我们将在接下来的几分钟内分析所有系统中可能存在的偏差。

398
00:12:00,215 --> 00:12:01,010
0,150 150,285 285,405 405,555 555,795
that might have been present

399
00:12:01,010 --> 00:12:02,510
0,225 225,390 390,555 555,830 1150,1500
in all of them for

400
00:12:02,510 --> 00:12:03,670
0,330 330,555 555,720 720,900 900,1160
in the next few minutes.|
|

401
00:12:05,730 --> 00:12:06,770
0,350 350,560 560,725 725,905 905,1040
So the first thing you
所以，你可能会注意到的第一件事是，

402
00:12:06,770 --> 00:12:07,775
0,135 135,390 390,645 645,825 825,1005
may notice is that,| there
|在这张图中，两个不同的人口统计数据之间存在巨大的准确性差距，

403
00:12:07,775 --> 00:12:09,700
0,150 150,420 420,815 1165,1665 1665,1925
is a huge accuracy gap

404
00:12:09,900 --> 00:12:11,885
0,350 350,605 605,860 860,1600 1710,1985
between two different demographics in

405
00:12:11,885 --> 00:12:14,520
0,275 1135,1535
this plot,|
|

406
00:12:14,520 --> 00:12:16,050
0,380 430,885 885,1080 1080,1320 1320,1530
this accuracy gap can get
这个精度差距最高可达 34% ，

407
00:12:16,050 --> 00:12:18,255
0,240 240,480 480,1095 1095,1400 1930,2205
up to {34% -},| keep
|请记住，这个面部检测是一个二进制分类任务，

408
00:12:18,255 --> 00:12:20,145
0,150 150,425 445,845 865,1265 1435,1890
in mind, that this facial

409
00:12:20,145 --> 00:12:21,770
0,360 360,495 495,645 645,1050 1050,1625
detection is a {binary,classification -}

410
00:12:21,820 --> 00:12:23,400
0,400 900,1145 1145,1265 1265,1430 1430,1580
task,| everything is either a
|每件事要么是脸，要么不是脸，

411
00:12:23,400 --> 00:12:24,390
0,260 280,540 540,720 720,855 855,990
face or {it's,not -} a

412
00:12:24,390 --> 00:12:26,220
0,260 850,1155 1155,1395 1395,1620 1620,1830
face,| this means that a
|这意味着随机初始化的模型应该有 50% 的准确率，

413
00:12:26,220 --> 00:12:28,260
0,570 570,1035 1035,1370 1570,1830 1830,2040
randomly initialized model would be

414
00:12:28,260 --> 00:12:29,730
0,270 270,450 450,675 675,1020 1020,1470
expected {to,have -} an accuracy

415
00:12:29,730 --> 00:12:31,815
0,180 180,800 1540,1800 1800,1965 1965,2085
of 50%,| because it's going
|因为它将随机指定某个东西是否是人脸，

416
00:12:31,815 --> 00:12:33,015
0,120 120,585 585,825 825,1050 1050,1200
to randomly assign whether or

417
00:12:33,015 --> 00:12:33,825
0,150 150,375 375,525 525,615 615,810
not something is a face

418
00:12:33,825 --> 00:12:34,860
0,195 195,455
or not,|
|

419
00:12:35,150 --> 00:12:36,415
0,275 275,410 410,560 560,920 920,1265
some of these facial detection
其中一些面部检测分类器表现仅略好于随机，

420
00:12:36,415 --> 00:12:38,680
0,545 745,1065 1065,1385 1465,1865 1915,2265
classifiers do only barely better

421
00:12:38,680 --> 00:12:40,890
0,255 255,560 610,1010 1150,1425 1425,2210
than random| on these underrepresented
|对这个群体这些代表性不足的数据，这些代表性不足的样本上。

422
00:12:41,420 --> 00:12:43,435
0,400 450,710 710,830 830,1490 1490,2015
data, on these underrepresented samples

423
00:12:43,435 --> 00:12:46,520
0,365 955,1245 1245,1535
in this population.|
|

424
00:12:46,850 --> 00:12:48,240
0,400 540,815 815,950 950,1100 1100,1390
So how did this happen,|
那么这是怎么发生的，|

425
00:12:48,620 --> 00:12:49,600
0,320 320,515 515,680 680,830 830,980
why is there such a
为什么这些不同的人口群体在准确性方面存在如此明显的差距，

426
00:12:49,600 --> 00:12:51,535
0,360 360,540 540,810 810,1430 1660,1935
blatant gap in accuracy between

427
00:12:51,535 --> 00:12:53,515
0,150 150,360 360,1005 1005,1295 1675,1980
these different demographic groups,| and
|这些模型最初是如何部署的，

428
00:12:53,515 --> 00:12:55,075
0,180 180,300 300,545 835,1230 1230,1560
how did these models ever

429
00:12:55,075 --> 00:12:56,140
0,285 285,645 645,780 780,870 870,1065
get deployed in the first

430
00:12:56,140 --> 00:12:58,270
0,350 820,1155 1155,1410 1410,1590 1590,2130
place,| what types of biases
|这些模型中存在哪些类型的偏差。

431
00:12:58,270 --> 00:12:59,610
0,270 270,560 640,900 900,1050 1050,1340
were present in these models.|
|

432
00:13:01,310 --> 00:13:02,755
0,400 570,830 830,950 950,1070 1070,1445
So a lot of facial
许多人脸检测系统表现出非常明显的选择偏见，

433
00:13:02,755 --> 00:13:04,825
0,360 360,635 925,1365 1365,1680 1680,2070
detection systems exhibit very clear

434
00:13:04,825 --> 00:13:06,940
0,375 375,935 1225,1530 1530,1830 1830,2115
selection bias,| {this,model -} was
|这个模型很可能主要是针对浅色皮肤的面孔进行训练，

435
00:13:06,940 --> 00:13:08,695
0,285 285,660 660,1040 1120,1410 1410,1755
likely trained mostly on lighter

436
00:13:08,695 --> 00:13:10,900
0,210 210,515 865,1265 1315,1715 1855,2205
skin faces| and therefore {}
|因此学习这些面孔比学习对深色皮肤的面孔进行分类要有效得多。

437
00:13:10,900 --> 00:13:12,240
0,300 300,600 600,855 855,1050 1050,1340
learned those much more effectively

438
00:13:12,290 --> 00:13:13,405
0,230 230,335 335,500 500,650 650,1115
than it learned to classify

439
00:13:13,405 --> 00:13:15,505
0,345 345,525 525,815 1615,1875 1875,2100
darker skin faces.| But that's
|但这并不是唯一存在的偏差，

440
00:13:15,505 --> 00:13:16,480
0,150 150,300 300,480 480,840 840,975
not the only bias that

441
00:13:16,480 --> 00:13:18,460
0,150 150,440 1120,1380 1380,1590 1590,1980
{was,present -},| the second bias
|面部检测系统中经常出现的第二个偏差是评估偏差，

442
00:13:18,460 --> 00:13:21,220
0,225 225,470 550,950 2080,2445 2445,2760
that's often {} very present

443
00:13:21,220 --> 00:13:22,945
0,300 300,705 705,1035 1035,1310 1390,1725
in facial detection systems is

444
00:13:22,945 --> 00:13:26,695
0,585 585,1145 1585,1985 2395,2795 3415,3750
evaluation bias,| because originally this
|因为最初你在屏幕上看到的这个数据集，

445
00:13:26,695 --> 00:13:27,535
0,270 270,465 465,585 585,720 720,840
data set that you see

446
00:13:27,535 --> 00:13:28,435
0,90 90,240 240,480 480,705 705,900
on the screen| is not
|并不是评估这些模型所依据的数据集，

447
00:13:28,435 --> 00:13:29,260
0,165 165,345 345,540 540,675 675,825
the data set that these

448
00:13:29,260 --> 00:13:31,060
0,255 255,510 510,945 945,1250 1540,1800
models {were,evaluated -} on,| they
|它们是在一个大数据集上进行评估的，

449
00:13:31,060 --> 00:13:32,875
0,150 150,660 660,960 960,1310 1420,1815
were evaluated on one big

450
00:13:32,875 --> 00:13:34,710
0,360 360,720 720,995 1105,1470 1470,1835
bulk data set,| without any
|根本没有对子组进行任何分类，

451
00:13:34,970 --> 00:13:36,810
0,590 590,890 890,1430 1430,1565 1565,1840
classification {into,subgroups -} at all,|
|

452
00:13:37,130 --> 00:13:38,550
0,400 420,770 770,980 980,1130 1130,1420
and therefore you can imagine,|
因此，你可以想象，|

453
00:13:38,570 --> 00:13:39,820
0,290 290,425 425,770 770,1010 1010,1250
if the dataset was also
如果数据集也主要由白皮肤的人脸组成，

454
00:13:39,820 --> 00:13:41,260
0,300 300,620 640,915 915,1230 1230,1440
comprised mostly of lighter skinned

455
00:13:41,260 --> 00:13:43,270
0,260 640,1040 1060,1530 1530,1770 1770,2010
faces,| these accuracy metrics would
|这些精确度指标将被难以置信地夸大，

456
00:13:43,270 --> 00:13:45,100
0,195 195,495 495,1070 1120,1520 1540,1830
be incredibly inflated| and therefore
|因此会产生不必要的信心，

457
00:13:45,100 --> 00:13:47,260
0,195 195,450 450,1185 1185,1520 1870,2160
would cause unnecessary confidence| and
|我们可以将它们部署到现实世界中。

458
00:13:47,260 --> 00:13:48,325
0,135 135,345 345,615 615,825 825,1065
we could deploy them into

459
00:13:48,325 --> 00:13:49,960
0,195 195,375 375,695
the real world.|
|

460
00:13:50,660 --> 00:13:52,180
0,305 305,610 630,890 890,1370 1370,1520
In fact, the biases in
事实上，这些模型中的偏差被发现，

461
00:13:52,180 --> 00:13:53,845
0,165 165,450 450,705 705,945 945,1665
these models were only uncovered,|
|

462
00:13:53,845 --> 00:13:55,860
0,270 270,605 685,1065 1065,1445 1615,2015
once an independent study actually
只有当一项独立研究真正构建了一个数据集，

463
00:13:55,940 --> 00:13:57,430
0,455 455,605 605,815 815,1150 1230,1490
constructed a data set,| that
|专门用于揭示这些类型的偏差，

464
00:13:57,430 --> 00:14:00,085
0,180 180,500 790,1190 1510,1910 2080,2655
is specifically designed to uncover

465
00:14:00,085 --> 00:14:01,255
0,135 135,315 315,480 480,975 975,1170
these sorts of biases,| by
|通过平衡种族和性别。

466
00:14:01,255 --> 00:14:03,060
0,600 600,915 915,1230 1230,1500 1500,1805
balancing across race and gender.|
|

467
00:14:04,810 --> 00:14:06,030
0,380 380,620 620,740 740,950 950,1220
However, there are other ways
然而，数据集可能存在偏见的其他方面，

468
00:14:06,030 --> 00:14:07,275
0,320 400,720 720,945 945,1110 1110,1245
that data sets can be

469
00:14:07,275 --> 00:14:08,160
0,345 345,510 510,630 630,765 765,885
biased,| that we {haven't -}
|我们还没有讨论过。

470
00:14:08,160 --> 00:14:10,080
0,180 180,450 450,800
yet talked about.|
|

471
00:14:11,060 --> 00:14:12,990
0,400 420,725 725,995 995,1385 1385,1930
So, so far we've assumed
到目前为止，我们已经在我们的数据集中假设了一个非常关键的假设，

472
00:14:13,370 --> 00:14:14,800
0,380 380,695 695,920 920,1325 1325,1430
a pretty key assumption in

473
00:14:14,800 --> 00:14:15,760
0,120 120,330 330,600 600,810 810,960
our data set,| which is
|那就是我们数据中的人脸数量与数据中的非人脸数量完全相同，

474
00:14:15,760 --> 00:14:16,720
0,210 210,375 375,540 540,735 735,960
that the number of faces

475
00:14:16,720 --> 00:14:18,010
0,195 195,315 315,590 850,1125 1125,1290
in our data is the

476
00:14:18,010 --> 00:14:19,045
0,225 225,510 510,720 720,825 825,1035
exact same as the number

477
00:14:19,045 --> 00:14:20,035
0,255 255,450 450,690 690,870 870,990
of non faces in our

478
00:14:20,035 --> 00:14:21,600
0,275 745,1005 1005,1125 1125,1275 1275,1565
data,| {but,you -} can imagine,|
|但你可以想象，|

479
00:14:21,680 --> 00:14:22,645
0,320 320,485 485,680 680,800 800,965
especially if you're looking at
特别是如果你在查看安全提要之类的东西，

480
00:14:22,645 --> 00:14:24,160
0,165 165,420 420,750 750,1245 1245,1515
things like security feeds,| this
|情况可能并不总是如此，

481
00:14:24,160 --> 00:14:24,970
0,165 165,360 360,570 570,705 705,810
might not always be the

482
00:14:24,970 --> 00:14:26,365
0,260 580,855 855,1035 1035,1200 1200,1395
case,| you might be faced
|在你的数据集中，你可能会面临比正样本多得多的负样本。

483
00:14:26,365 --> 00:14:27,970
0,240 240,480 480,750 750,1080 1080,1605
with many more negative samples

484
00:14:27,970 --> 00:14:29,350
0,225 225,525 525,1065 1065,1245 1245,1380
than positive samples in your

485
00:14:29,350 --> 00:14:30,860
0,210 210,530
data set.|
|

486
00:14:31,470 --> 00:14:32,825
0,275 275,395 395,640 690,950 950,1355
In the most,| so what's
在最的情况下，|那么这里有什么问题，

487
00:14:32,825 --> 00:14:34,655
0,120 120,390 390,785 1435,1710 1710,1830
the problem here,| in the
|在最极端的情况下，

488
00:14:34,655 --> 00:14:36,245
0,245 295,630 630,965 1105,1380 1380,1590
most extreme case,| we may
|我们可能会为数据集中的每个项目分配标签 non-face ，

489
00:14:36,245 --> 00:14:38,020
0,255 255,435 435,695 1015,1395 1395,1775
assign the label {non-face -}

490
00:14:38,220 --> 00:14:39,155
0,260 260,440 440,680 680,845 845,935
to every item in the

491
00:14:39,155 --> 00:14:40,805
0,180 180,515 985,1260 1260,1395 1395,1650
data set,| because the model
|因为模型很少看到被标记为 face 的项目，

492
00:14:40,805 --> 00:14:42,155
0,315 315,635 655,915 915,1050 1050,1350
sees items that are labeled

493
00:14:42,155 --> 00:14:44,165
0,180 180,485 625,960 960,1800 1800,2010
as faces,| so infrequently that
|以至于它无法学习，

494
00:14:44,165 --> 00:14:45,220
0,105 105,345 345,555 555,780 780,1055
it isn't able to learn,|
|

495
00:14:45,930 --> 00:14:47,945
0,380 380,695 695,1030 1200,1790 1790,2015
an accurate class boundary between
两个分类之间，两个样本之间的准确分类边界。

496
00:14:47,945 --> 00:14:49,010
0,165 165,345 345,635 685,945 945,1065
the two sample, between the

497
00:14:49,010 --> 00:14:50,680
0,135 135,410
two classes.|
|

498
00:14:52,490 --> 00:14:54,040
0,400 690,965 965,1085 1085,1190 1190,1550
So how can we mitigate
那么，我们如何才能缓解这种情况，

499
00:14:54,040 --> 00:14:54,955
0,240 240,450 450,555 555,675 675,915
this,| this is a really
|这是一个非常大的问题，

500
00:14:54,955 --> 00:14:56,200
0,300 300,615 615,840 840,1020 1020,1245
big problem| and it's very
|在许多不同类型的机器学习任务和数据集中都很常见。

501
00:14:56,200 --> 00:14:57,580
0,380 490,855 855,1080 1080,1230 1230,1380
common across a lot of

502
00:14:57,580 --> 00:14:58,660
0,195 195,435 435,660 660,855 855,1080
different types of machine learning

503
00:14:58,660 --> 00:15:00,840
0,350 400,660 660,840 840,1160 1780,2180
tasks and data sets.| {And,the
|我们可以尝试缓解类别失衡的第一种方法是使用样本重新加权，

504
00:15:01,040 --> 00:15:01,900
0,260 260,440 440,620 620,740 740,860
-} first way that we

505
00:15:01,900 --> 00:15:02,980
0,120 120,360 360,600 600,705 705,1080
can {} try to mitigate

506
00:15:02,980 --> 00:15:04,735
0,225 225,780 780,1020 1020,1310 1360,1755
class imbalance is using {sample

507
00:15:04,735 --> 00:15:06,360
0,255 255,695 925,1185 1185,1335 1335,1625
-} re-weighting,| which is when
|即不是以速率从我们的数据集中统一采样，

508
00:15:06,410 --> 00:15:08,290
0,275 275,550 660,1100 1100,1625 1625,1880
instead of uniformly sampling from

509
00:15:08,290 --> 00:15:09,280
0,150 150,390 390,660 660,855 855,990
our data set at a

510
00:15:09,280 --> 00:15:11,010
0,195 195,530 790,1140 1140,1410 1410,1730
rate {},| we instead sample
|而是以与我们数据集中某个类别的发生率成反比的速率进行采样，

511
00:15:11,060 --> 00:15:11,905
0,260 260,380 380,545 545,710 710,845
at a rate that is

512
00:15:11,905 --> 00:15:13,855
0,570 570,1215 1215,1410 1410,1515 1515,1950
inversely proportional to the incidence

513
00:15:13,855 --> 00:15:14,785
0,240 240,360 360,600 600,810 810,930
of a class in {our,data

514
00:15:14,785 --> 00:15:16,585
0,195 195,515 1135,1470 1470,1680 1680,1800
-} set,| so in the
|因此，在前面的例子中，

515
00:15:16,585 --> 00:15:18,580
0,245 355,755 1045,1380 1380,1695 1695,1995
previous example,| if {} the
|如果人脸的可能性很小，

516
00:15:18,580 --> 00:15:20,430
0,645 645,975 975,1290 1290,1560 1560,1850
likelihood if faces were much,|
|

517
00:15:21,020 --> 00:15:21,960
0,260 260,365 365,500 500,665 665,940
if the number of faces
如果人脸的数量比我们数据集中的非人脸的数量少得多，

518
00:15:22,130 --> 00:15:23,260
0,305 305,545 545,830 830,1040 1040,1130
was much lower than the

519
00:15:23,260 --> 00:15:24,460
0,180 180,420 420,645 645,975 975,1200
number of non faces in

520
00:15:24,460 --> 00:15:25,645
0,120 120,345 345,680 790,1035 1035,1185
our data set,| we would
|我们将以比[负]更高的概率对人脸进行采样，

521
00:15:25,645 --> 00:15:26,920
0,305 445,705 705,930 930,1155 1155,1275
sample the faces with a

522
00:15:26,920 --> 00:15:28,740
0,210 210,770 970,1230 1230,1335 1335,1820
higher probability than the negatives,|
|

523
00:15:28,820 --> 00:15:29,905
0,245 245,410 410,590 590,815 815,1085
so that the model sees
这样模型就可以平等地看到这两种类别。

524
00:15:29,905 --> 00:15:31,400
0,240 240,555 555,935
both classes equally.|
|

525
00:15:33,160 --> 00:15:34,530
0,260 260,515 515,910 960,1205 1205,1370
The second example, the second
第二个例子，我们可以缓解类别失衡的第二种方法是通过损失重新评级，

526
00:15:34,530 --> 00:15:35,595
0,195 195,330 330,465 465,855 855,1065
way we can mitigate class

527
00:15:35,595 --> 00:15:37,200
0,585 585,870 870,1110 1110,1380 1380,1605
imbalance is through loss {re-rating,

528
00:15:37,200 --> 00:15:39,375
0,260 820,1095 1095,1260 1260,1550 1810,2175
-},| which is when instead
|即不是让模型中的每个错误对总损失函数做出相等的贡献，

529
00:15:39,375 --> 00:15:41,145
0,285 285,570 570,915 915,1295 1435,1770
of having every single mistake

530
00:15:41,145 --> 00:15:42,540
0,180 180,285 285,495 495,845 1045,1395
that the model makes contribute

531
00:15:42,540 --> 00:15:43,785
0,350 400,630 630,750 750,975 975,1245
equally to our total {loss

532
00:15:43,785 --> 00:15:45,930
0,335 985,1380 1380,1695 1695,1950 1950,2145
-} function,| we're weight the
|我们对样本进行加权，

533
00:15:45,930 --> 00:15:47,745
0,525 525,840 840,1160 1180,1665 1665,1815
samples,| such that samples from
|比如来自未被充分代表的类别的样本对损失函数有更大的贡献，

534
00:15:47,745 --> 00:15:50,070
0,765 765,1115 1465,1830 1830,2145 2145,2325
underrepresented classes contribute more {to,the

535
00:15:50,070 --> 00:15:52,230
0,90 90,285 285,620 1180,1580 1900,2160
-} loss function,| so instead
|因此，与模型将每个单一输入面分配为负数不同，

536
00:15:52,230 --> 00:15:53,490
0,120 120,240 240,435 435,900 900,1260
of the model assigning every

537
00:15:53,490 --> 00:15:56,055
0,380 1180,1470 1470,1760 1990,2295 2295,2565
single input face to a

538
00:15:56,055 --> 00:15:57,990
0,240 240,390 390,665 1345,1755 1755,1935
as a negative,| it'll be
|如果它这样做，它将受到极大的惩罚，

539
00:15:57,990 --> 00:15:59,250
0,315 315,780 780,930 930,1065 1065,1260
highly penalized, if it does

540
00:15:59,250 --> 00:16:01,305
0,320 580,980 1300,1560 1560,1770 1770,2055
so,| because the loss of
|因为面数的损失将比负损失对总损失函数的贡献更大。

541
00:16:01,305 --> 00:16:02,760
0,240 240,510 510,825 825,1140 1140,1455
the faces would contribute more

542
00:16:02,760 --> 00:16:03,860
0,195 195,315 315,510 510,765 765,1100
to the total loss function

543
00:16:03,880 --> 00:16:04,710
0,260 260,380 380,560 560,740 740,830
than the loss of the

544
00:16:04,710 --> 00:16:05,520
0,470
negatives.|
|

545
00:16:07,210 --> 00:16:08,580
0,335 335,530 530,770 770,1115 1115,1370
And the final way that
我们可以缓解分类不平衡的最后一种方法是通过批量选择，

546
00:16:08,580 --> 00:16:10,500
0,165 165,440 700,1170 1170,1365 1365,1920
we can mitigate class imbalance

547
00:16:10,500 --> 00:16:12,720
0,350 580,885 885,1220 1240,1640 1960,2220
is through batch selection,| which
|这是当我们从类中随机选择时，

548
00:16:12,720 --> 00:16:14,010
0,135 135,285 285,465 465,705 705,1290
is when we choose randomly

549
00:16:14,010 --> 00:16:15,450
0,180 180,470 730,990 990,1155 1155,1440
from classes,| so that every
|这样每一批都有每个类相同数量的数据点。

550
00:16:15,450 --> 00:16:16,905
0,315 315,680 820,1095 1095,1230 1230,1455
single batch has an equal

551
00:16:16,905 --> 00:16:18,255
0,300 300,495 495,690 690,1020 1020,1350
number of data points per

552
00:16:18,255 --> 00:16:19,780
0,335
class.|
|

553
00:16:22,300 --> 00:16:25,080
0,400 450,830 830,1115 1115,1600
So is everything solved,|
那么一切都解决了吗，|

554
00:16:25,510 --> 00:16:27,795
0,400 480,755 755,935 935,1240 1980,2285
clearly, there are other forms
显然，也存在其他形式的偏差，

555
00:16:27,795 --> 00:16:29,535
0,165 165,600 600,990 990,1380 1380,1740
of bias that exist,| even
|即使分类完全平衡，

556
00:16:29,535 --> 00:16:31,590
0,365 565,945 945,1325 1345,1725 1725,2055
when the classes are completely

557
00:16:31,590 --> 00:16:33,555
0,530 940,1340 1510,1755 1755,1860 1860,1965
balanced,| because the thing that
|因为我们还没有考虑到的是潜在特征。

558
00:16:33,555 --> 00:16:34,640
0,120 120,375 375,555 555,780 780,1085
we haven't thought about yet

559
00:16:34,840 --> 00:16:37,220
0,350 350,845 845,1180
is latent features.|
|

560
00:16:37,390 --> 00:16:38,670
0,400 420,680 680,830 830,1055 1055,1280
So if you remember from
如果你们还记得实验二和上一节课的内容，

561
00:16:38,670 --> 00:16:40,110
0,255 255,620 640,1005 1005,1230 1230,1440
lab two and the last

562
00:16:40,110 --> 00:16:42,620
0,350 790,1290 1290,1580 1690,2090 2110,2510
lecture,| latent features are the
|潜在特征实际代表，通过模型实际代表这个图像。

563
00:16:42,700 --> 00:16:44,010
0,290 290,575 575,860 860,1100 1100,1310
actual represent, is the actual

564
00:16:44,010 --> 00:16:46,305
0,680 730,1020 1020,1215 1215,1520 1960,2295
representation of this image according

565
00:16:46,305 --> 00:16:48,600
0,180 180,285 285,545 1315,1715 1975,2295
to the model.| {And,so -}
|到目前为止，我们已经缓解了当我们知道我们有代表不足的类时的问题，

566
00:16:48,600 --> 00:16:50,090
0,315 315,660 660,1050 1050,1230 1230,1490
far we've mitigated the problem

567
00:16:50,200 --> 00:16:51,450
0,320 320,530 530,725 725,1010 1010,1250
of when we know that

568
00:16:51,450 --> 00:16:53,670
0,135 135,315 315,1110 1110,1460 1960,2220
we have underrepresented classes,| but
|但我们还没有减轻当我们在同一个类中有大量可变性的问题。

569
00:16:53,670 --> 00:16:54,950
0,135 135,435 435,810 810,1005 1005,1280
we haven't mitigated the problem

570
00:16:54,970 --> 00:16:55,920
0,275 275,440 440,605 605,770 770,950
of when we have a

571
00:16:55,920 --> 00:16:57,750
0,195 195,375 375,980 1150,1545 1545,1830
lot of variability within the

572
00:16:57,750 --> 00:16:59,745
0,255 255,620 1360,1740 1740,1890 1890,1995
same class.| Let's say we
|让我们假设我们的数据集中有相同数量的人脸和反面例子，

573
00:16:59,745 --> 00:17:00,690
0,60 60,180 180,420 420,720 720,945
have an equal number of

574
00:17:00,690 --> 00:17:02,295
0,290 400,690 690,975 975,1365 1365,1605
faces and negative examples in

575
00:17:02,295 --> 00:17:04,160
0,135 135,360 360,695 1285,1575 1575,1865
our data set,| what happens
|如果大多数人脸来自特定的人群，

576
00:17:04,210 --> 00:17:05,220
0,275 275,395 395,635 635,890 890,1010
if the majority of the

577
00:17:05,220 --> 00:17:06,225
0,260 280,555 555,690 690,810 810,1005
faces are from a certain

578
00:17:06,225 --> 00:17:07,590
0,780 780,1050 1050,1170 1170,1260 1260,1365
demographic| or they have a
|或者他们有一套特定的特征，会发生什么，

579
00:17:07,590 --> 00:17:09,435
0,195 195,375 375,495 495,770 1600,1845
certain set of features,| can
|我们还能应用我们刚刚学到的技术，

580
00:17:09,435 --> 00:17:10,530
0,135 135,390 390,675 675,855 855,1095
we still apply the techniques

581
00:17:10,530 --> 00:17:11,510
0,240 240,345 345,465 465,660 660,980
that we just learned about,|
|

582
00:17:12,300 --> 00:17:13,310
0,245 245,455 455,725 725,875 875,1010
the answer is that we
答案是我们不能这样做，

583
00:17:13,310 --> 00:17:14,810
0,255 255,495 495,770 1090,1350 1350,1500
cannot do this,| and the
|问题是目前存在的偏差存在于我们的潜在特征中。

584
00:17:14,810 --> 00:17:16,055
0,255 255,525 525,705 705,840 840,1245
problem is that the bias

585
00:17:16,055 --> 00:17:17,405
0,285 285,570 570,905 955,1230 1230,1350
present right now is in

586
00:17:17,405 --> 00:17:19,380
0,150 150,510 510,785
our latent features.|
|

587
00:17:19,570 --> 00:17:20,790
0,290 290,455 455,590 590,850 930,1220
All of these images are
所有这些图像都被贴上了完全相同的标签，

588
00:17:20,790 --> 00:17:22,065
0,360 360,510 510,675 675,960 960,1275
labeled with the exact same

589
00:17:22,065 --> 00:17:24,290
0,335 535,870 870,1140 1140,1475 1825,2225
label,| so according to as
|所以根据模型，我们所知道的就是它们都是人脸，

590
00:17:24,610 --> 00:17:26,025
0,260 260,520 600,920 920,1130 1130,1415
the model, all we know

591
00:17:26,025 --> 00:17:27,380
0,285 285,540 540,825 825,1005 1005,1355
is that they're all faces,|
|

592
00:17:28,210 --> 00:17:29,570
0,245 245,350 350,530 530,850 960,1360
{so,we -} have no information
因此，我们没有关于这些功能的任何信息，只有标签上的信息，

593
00:17:29,680 --> 00:17:30,860
0,290 290,500 500,695 695,875 875,1180
about any of these features,

594
00:17:30,970 --> 00:17:33,450
0,400 420,710 710,860 860,1120 2160,2480
only from the label,| therefore
|因此，我们不能应用以前用来缓解分类不平衡的任何方法，

595
00:17:33,450 --> 00:17:34,740
0,240 240,645 645,915 915,1140 1140,1290
we can't apply any of

596
00:17:34,740 --> 00:17:35,910
0,150 150,435 435,795 795,1020 1020,1170
the previous approaches that we

597
00:17:35,910 --> 00:17:38,625
0,290 670,1070 1360,1890 1890,2115 2115,2715
used to mitigate class imbalanced,|
|

598
00:17:38,625 --> 00:17:40,125
0,240 240,390 390,690 690,1005 1005,1500
because our classes are balanced,|
因为我们的分类是平衡的，|

599
00:17:40,125 --> 00:17:41,415
0,225 225,360 360,540 540,780 780,1290
but we have feature imbalance
但我们现在有特征不平衡。

600
00:17:41,415 --> 00:17:42,180
0,305
now.|
|

601
00:17:42,640 --> 00:17:44,145
0,380 380,665 665,950 950,1280 1280,1505
However, we can adapt the
然而，我们可以调整前面的方法来解释潜在特征中的偏差，

602
00:17:44,145 --> 00:17:45,765
0,255 255,635 835,1170 1170,1425 1425,1620
previous methods to account for

603
00:17:45,765 --> 00:17:47,505
0,375 375,570 570,870 870,1145 1435,1740
bias in latent features,| which
|这将在几张幻灯片中完成。

604
00:17:47,505 --> 00:17:48,900
0,240 240,485 745,1035 1035,1230 1230,1395
we'll do in just a

605
00:17:48,900 --> 00:17:50,520
0,150 150,440
few slides.|
|

606
00:17:52,060 --> 00:17:53,565
0,400 510,860 860,1175 1175,1370 1370,1505
So let's unpack this a
所以让我们进一步了解一下这个问题，

607
00:17:53,565 --> 00:17:55,020
0,135 135,300 300,575 925,1215 1215,1455
little bit further,| {we,have -}
|我们有可能有偏差的数据集，

608
00:17:55,020 --> 00:17:56,810
0,330 330,710 730,1215 1215,1455 1455,1790
our potentially biased data set,|
|

609
00:17:56,980 --> 00:17:58,095
0,275 275,500 500,695 695,890 890,1115
and we're trying to build
我们试图建立和部署一个模型，

610
00:17:58,095 --> 00:17:59,550
0,315 315,665 715,990 990,1215 1215,1455
and deploy a model,| that
|在传统的训练管道中对人脸进行分类，

611
00:17:59,550 --> 00:18:01,380
0,480 480,630 630,890 1210,1560 1560,1830
classifies the faces in a

612
00:18:01,380 --> 00:18:03,225
0,285 285,585 585,920 1420,1695 1695,1845
traditional training pipeline,| this is
|这就是那条管道的样子，

613
00:18:03,225 --> 00:18:04,215
0,150 150,315 315,585 585,825 825,990
what that pipeline would {look,like

614
00:18:04,215 --> 00:18:05,190
0,285 285,525 525,660 660,810 810,975
-},| We would train our
|我们将训练我们的分类器，

615
00:18:05,190 --> 00:18:06,510
0,585 585,840 840,945 945,1125 1125,1320
classifier| and we would deploy
|并将其部署到现实世界中，

616
00:18:06,510 --> 00:18:07,430
0,120 120,285 285,450 450,615 615,920
it into {the,real -} world,|
|

617
00:18:07,720 --> 00:18:09,285
0,275 275,485 485,755 755,1090 1110,1565
but this training pipeline doesn't
但这种训练管道丝毫不会 debias 我们的输入。

618
00:18:09,285 --> 00:18:10,545
0,195 195,510 510,750 750,1095 1095,1260
{debias -} our inputs in

619
00:18:10,545 --> 00:18:12,020
0,210 210,545
any way.|
|

620
00:18:13,440 --> 00:18:14,585
0,395 395,695 695,875 875,1010 1010,1145
So one thing we could
.因此，我们可以做的一件事是标记我们的偏差特征，然后应用重新采样。

621
00:18:14,585 --> 00:18:16,220
0,275 295,615 615,935 955,1245 1245,1635
do is label our biased

622
00:18:16,220 --> 00:18:18,160
0,320 520,780 780,975 975,1215 1215,1940
features and then apply resampling.|
|

623
00:18:18,510 --> 00:18:19,750
0,260 260,515 515,725 725,950 950,1240
{So\,,let's -} say in reality,|
因此，让我们假设在现实中，|

624
00:18:20,040 --> 00:18:21,155
0,275 275,440 440,665 665,905 905,1115
that this data set was
这个数据集是对头发颜色的偏见，

625
00:18:21,155 --> 00:18:22,910
0,375 375,600 600,810 810,1115 1435,1755
biased on hair color,| most
|大多数数据集由金发的人组成，

626
00:18:22,910 --> 00:18:23,810
0,180 180,285 285,450 450,675 675,900
of the data set is

627
00:18:23,810 --> 00:18:24,695
0,180 180,300 300,435 435,660 660,885
made up of people with

628
00:18:24,695 --> 00:18:26,585
0,285 285,545 715,1115 1285,1650 1650,1890
blonde hair,| with faces with
|黑发和红发的脸没有得到充分的代表，

629
00:18:26,585 --> 00:18:27,485
0,195 195,375 375,510 510,690 690,900
black hair and red {hair,underrepresented

630
00:18:27,485 --> 00:18:30,095
0,815 1675,1995 1995,2190 2190,2340 2340,2610
-},| if we knew this
|如果我们知道这个信息，

631
00:18:30,095 --> 00:18:31,775
0,395 745,1020 1020,1185 1185,1440 1440,1680
information,| we could label the
|我们就可以标记这个数据集中每个人的头发颜色，

632
00:18:31,775 --> 00:18:33,065
0,210 210,495 495,735 735,990 990,1290
hair color of every single

633
00:18:33,065 --> 00:18:34,270
0,300 300,510 510,645 645,870 870,1205
person in this data set,|
|

634
00:18:34,410 --> 00:18:35,480
0,275 275,395 395,560 560,800 800,1070
and we could apply either
我们可以应用样本加权或损失重新加权，

635
00:18:35,480 --> 00:18:36,980
0,350 400,855 855,1065 1065,1305 1305,1500
sample {weighting -} or loss

636
00:18:36,980 --> 00:18:37,925
0,285 285,465 465,645 645,780 780,945
re-weighting,| just as we did
|就像我们之前所做的那样。

637
00:18:37,925 --> 00:18:39,480
0,305
previously.|
|

638
00:18:39,720 --> 00:18:40,715
0,350 350,620 620,800 800,905 905,995
But does anyone want to
但有没有人想告诉我问题出在哪里？

639
00:18:40,715 --> 00:18:41,480
0,120 120,270 270,420 420,555 555,765
tell me what the problem

640
00:18:41,480 --> 00:18:43,160
0,240 240,530
is here?|
|

641
00:18:45,920 --> 00:18:47,555
0,240 240,420 420,690 690,1040 1060,1635
You go through each samples,
你要检查每个样本，要花很多时间。

642
00:18:47,555 --> 00:18:48,395
0,195 195,375 375,510 510,645 645,840
it takes a lot of

643
00:18:48,395 --> 00:18:48,800
0,305
time.|
|

644
00:18:48,970 --> 00:18:50,595
0,400 870,1265 1265,1490 1490,1550 1550,1625
Yeah, so there are a
是的，所以这里有几个问题，

645
00:18:50,595 --> 00:18:51,600
0,165 165,465 465,675 675,765 765,1005
couple problems here,| and that's
|这绝对是其中之一。

646
00:18:51,600 --> 00:18:53,340
0,260 310,570 570,705 705,980 1480,1740
definitely one of them.| {The,first
|第一个问题是，

647
00:18:53,340 --> 00:18:54,525
0,195 195,530 580,855 855,990 990,1185
-} is,| how do we
|我们如何知道头发颜色在这个数据集中是一个有偏见的特征，

648
00:18:54,525 --> 00:18:55,725
0,315 315,585 585,795 795,1035 1035,1200
know that hair color is

649
00:18:55,725 --> 00:18:56,730
0,120 120,450 450,690 690,870 870,1005
a biased feature in this

650
00:18:56,730 --> 00:18:58,790
0,210 210,530 880,1245 1245,1560 1560,2060
data set,| unless we visually
|除非我们肉眼检查这个数据集中的每一个样本，

651
00:18:58,870 --> 00:19:00,660
0,400 420,820 870,1235 1235,1565 1565,1790
inspect every single sample in

652
00:19:00,660 --> 00:19:01,680
0,135 135,345 345,600 600,855 855,1020
this data set,| we're not
|否则我们不会知道哪些是有偏差的特征。

653
00:19:01,680 --> 00:19:02,565
0,210 210,360 360,555 555,765 765,885
going to know what the

654
00:19:02,565 --> 00:19:04,590
0,300 300,555 555,905 1615,1890 1890,2025
biased features are.| And the
|第二件事正是你所说的，

655
00:19:04,590 --> 00:19:05,730
0,195 195,420 420,630 630,915 915,1140
second thing is exactly what

656
00:19:05,730 --> 00:19:07,260
0,150 150,440 610,885 885,1160 1210,1530
you said,| which is once
|那就是一旦我们有了我们有偏差的特征，

657
00:19:07,260 --> 00:19:08,540
0,225 225,435 435,630 630,975 975,1280
we have our biased features,|
|

658
00:19:08,890 --> 00:19:10,620
0,380 380,695 695,920 920,1460 1460,1730
going through and annotating every
浏览和注释每一张带有这个特征的图像是一项极其劳动密集型的任务，

659
00:19:10,620 --> 00:19:12,105
0,330 330,585 585,765 765,1070 1240,1485
image with this feature is

660
00:19:12,105 --> 00:19:13,880
0,240 240,630 630,945 945,1410 1410,1775
an extremely labor intensive task,|
|

661
00:19:14,080 --> 00:19:15,285
0,260 260,395 395,995 995,1115 1115,1205
that is infeasible in the
在现实世界中是不可行的。

662
00:19:15,285 --> 00:19:16,680
0,150 150,455
real world.|
|

663
00:19:17,370 --> 00:19:18,760
0,400 420,710 710,860 860,1055 1055,1390
So now the question is,|
所以现在的问题是，|

664
00:19:19,050 --> 00:19:19,955
0,275 275,425 425,575 575,740 740,905
what if we had a
如果我们有一种方法来自动学习潜在特征，

665
00:19:19,955 --> 00:19:22,130
0,255 255,635 655,1055 1225,1625 1705,2175
way to automatically learn latent

666
00:19:22,130 --> 00:19:24,065
0,290 760,1080 1080,1350 1350,1635 1635,1935
features| and use this learn
|并使用这种学习特征表示法来描述一个模型。

667
00:19:24,065 --> 00:19:25,880
0,300 300,1055 1255,1515 1515,1635 1635,1815
feature representation to {debias -}

668
00:19:25,880 --> 00:19:27,620
0,165 165,410
a model.|
|

669
00:19:29,460 --> 00:19:30,800
0,400 420,680 680,830 830,1100 1100,1340
So what we want is
因此，我们想要的是一种方法来学习这个数据集的特征，

670
00:19:30,800 --> 00:19:31,820
0,120 120,345 345,615 615,840 840,1020
a way to learn the

671
00:19:31,820 --> 00:19:32,950
0,210 210,435 435,585 585,795 795,1130
features of this data set|
|

672
00:19:33,060 --> 00:19:35,210
0,260 260,520 570,970 1140,1540 1770,2150
and then automatically determine the
然后自动确定具有最高特征偏差的样本和具有最低特征偏差的样本，

673
00:19:35,210 --> 00:19:36,845
0,380 670,1140 1140,1275 1275,1395 1395,1635
{} samples with the highest

674
00:19:36,845 --> 00:19:38,315
0,315 315,810 810,1020 1020,1125 1125,1470
feature bias and the samples

675
00:19:38,315 --> 00:19:39,490
0,120 120,210 210,405 405,690 690,1175
with the lowest feature bias,|
|

676
00:19:40,140 --> 00:19:41,435
0,410 410,665 665,935 935,1130 1130,1295
we've already learned a method
我们已经学习了这样做的方法，

677
00:19:41,435 --> 00:19:42,845
0,180 180,375 375,695 1015,1275 1275,1410
of doing this,| in the
|在生成建模课程中，你们都学过变分自编码器，

678
00:19:42,845 --> 00:19:44,540
0,360 360,735 735,995 1255,1515 1515,1695
generative modeling lecture, {you,all -}

679
00:19:44,540 --> 00:19:46,720
0,320 370,705 705,1250 1270,1590 1590,2180
learned about variational {autoencoders -},|
|

680
00:19:46,860 --> 00:19:48,110
0,260 260,410 410,680 680,980 980,1250
which are models that learn
这是一种从数据集种学习潜在特征的模型，

681
00:19:48,110 --> 00:19:49,370
0,225 225,555 555,830 880,1140 1140,1260
the latent features of a

682
00:19:49,370 --> 00:19:51,470
0,180 180,500 1270,1545 1545,1680 1680,2100
data set,| as a recap,
|回顾一下，变分自编码器从学习的潜在空间进行概率采样，

683
00:19:51,470 --> 00:19:53,390
0,525 525,795 795,1290 1290,1605 1605,1920
variational {autoencoders -} work by

684
00:19:53,390 --> 00:19:55,600
0,870 870,1395 1395,1680 1680,1905 1905,2210
probabilistically sampling from a learn

685
00:19:55,920 --> 00:19:57,890
0,425 425,730 1080,1370 1370,1655 1655,1970
latent space,| and then they
|然后它们将这个新的潜在向量解码到原始输入空间，

686
00:19:57,890 --> 00:19:59,350
0,480 480,645 645,840 840,1155 1155,1460
decode this new latent vector

687
00:19:59,550 --> 00:20:00,910
0,400 420,710 710,905 905,1085 1085,1360
into back into the original

688
00:20:01,020 --> 00:20:02,975
0,305 305,610 840,1190 1190,1400 1400,1955
input space,| measure the reconstruction
|测量输入和输出之间的重建损失，

689
00:20:02,975 --> 00:20:04,100
0,345 345,585 585,765 765,1005 1005,1125
loss between the inputs and

690
00:20:04,100 --> 00:20:05,660
0,225 225,705 705,1035 1035,1290 1290,1560
the outputs,| and continue to
|并继续更新它们对潜在空间的表示。

691
00:20:05,660 --> 00:20:07,100
0,240 240,390 390,1035 1035,1290 1290,1440
update their representation of the

692
00:20:07,100 --> 00:20:08,160
0,300 300,590
latent space.|
|

693
00:20:08,590 --> 00:20:09,600
0,260 260,380 380,590 590,830 830,1010
And the reason why we
我们之所以如此关心这个潜在空间，

694
00:20:09,600 --> 00:20:10,635
0,210 210,405 405,645 645,885 885,1035
care so much about this

695
00:20:10,635 --> 00:20:12,300
0,330 330,635 805,1080 1080,1355 1375,1665
latent space| is that we
|是因为我们希望输入中彼此相似的样本

696
00:20:12,300 --> 00:20:13,665
0,255 255,735 735,870 870,1050 1050,1365
want samples that are similar

697
00:20:13,665 --> 00:20:14,625
0,225 225,315 315,510 510,720 720,960
to each other in the

698
00:20:14,625 --> 00:20:16,710
0,365 535,810 810,1470 1470,1740 1740,2085
input| to decode to latent
|解码成在这个潜在空间中彼此非常接近的潜在向量，

699
00:20:16,710 --> 00:20:17,850
0,405 405,585 585,690 690,885 885,1140
vectors that are very close

700
00:20:17,850 --> 00:20:18,630
0,150 150,240 240,435 435,630 630,780
to each other in this

701
00:20:18,630 --> 00:20:20,505
0,315 315,620 1060,1365 1365,1755 1755,1875
latent space,| {and,samples -} that
|而输入中相距较远的样本或不相似的样本

702
00:20:20,505 --> 00:20:21,375
0,165 165,405 405,585 585,720 720,870
are far from each other

703
00:20:21,375 --> 00:20:22,860
0,270 270,675 675,780 780,915 915,1485
or samples that are dissimilar

704
00:20:22,860 --> 00:20:23,805
0,150 150,255 255,495 495,720 720,945
to each other in the

705
00:20:23,805 --> 00:20:26,010
0,365 595,870 870,1415 1615,2100 2100,2205
input| should decode encode to
|应解码编码为在潜在空间中彼此相距较远的潜在向量。

706
00:20:26,010 --> 00:20:27,675
0,300 300,675 675,870 870,1130 1360,1665
latent vectors that are far

707
00:20:27,675 --> 00:20:28,530
0,150 150,270 270,495 495,720 720,855
from each other in the

708
00:20:28,530 --> 00:20:30,120
0,300 300,620
latent space.|
|

709
00:20:32,090 --> 00:20:33,460
0,380 380,695 695,920 920,1085 1085,1370
So now we'll walk through
所以，现在我们将逐步介绍一种 debias 算法，

710
00:20:33,460 --> 00:20:34,540
0,240 240,465 465,735 735,945 945,1080
step by step a {debiasing

711
00:20:34,540 --> 00:20:36,570
0,510 510,915 915,1220 1240,1635 1635,2030
-} algorithm,| that automatically uses
|该算法自动通过变分自编码器使用潜在特征学习，

712
00:20:36,680 --> 00:20:38,515
0,400 540,980 980,1240 1260,1610 1610,1835
the latent features learned by

713
00:20:38,515 --> 00:20:40,560
0,120 120,555 555,855 855,1475 1645,2045
a variational {autoencoder -}, {}|
|

714
00:20:40,640 --> 00:20:42,355
0,400 510,860 860,1190 1190,1460 1460,1715
to under sample and over
对数据集中的区域进行欠采样和过采样。

715
00:20:42,355 --> 00:20:43,720
0,365 445,720 720,990 990,1230 1230,1365
sample from regions in our

716
00:20:43,720 --> 00:20:46,015
0,290 340,740 1420,1770 1770,2055 2055,2295
data set.| {} {Before,I,start -
|在我开始之前，

717
00:20:46,015 --> 00:20:46,810
0,210 210,390 390,525 525,660 660,795
-},| I want to point
|我想指出这个 debias 模型实际上是 Themis 工作的基础，

718
00:20:46,810 --> 00:20:48,040
0,165 165,405 405,660 660,840 840,1230
out that this {debiasing -}

719
00:20:48,040 --> 00:20:49,470
0,210 210,560 610,915 915,1125 1125,1430
model is actually the foundation

720
00:20:49,730 --> 00:20:51,700
0,275 275,785 785,1060 1470,1775 1775,1970
of Themis's work,| this work
|这项工作来自我们几年前发表的一篇论文，

721
00:20:51,700 --> 00:20:52,540
0,180 180,345 345,450 450,570 570,840
comes out of a paper

722
00:20:52,540 --> 00:20:53,560
0,240 240,390 390,645 645,885 885,1020
that we published a few

723
00:20:53,560 --> 00:20:54,745
0,225 225,590 670,915 915,1035 1035,1185
years ago,| that has been
|演示了 debias 商业面部检测算法，

724
00:20:54,745 --> 00:20:56,725
0,570 570,765 765,1145 1195,1560 1560,1980
demonstrated to debias commercial facial

725
00:20:56,725 --> 00:20:59,260
0,450 450,965 1105,1505 2095,2370 2370,2535
{detection,algorithms -},| and it was
|它的影响力如此之大，

726
00:20:59,260 --> 00:21:00,660
0,240 240,735 735,885 885,1080 1080,1400
so impactful,| that we decided
|以至于我们决定提供它，并与公司和行业合作，

727
00:21:00,710 --> 00:21:02,310
0,400 450,710 710,845 845,1120 1200,1600
to make it available and

728
00:21:02,690 --> 00:21:04,135
0,290 290,470 470,740 740,1120 1170,1445
work with {companies,and -} industries,|
|

729
00:21:04,135 --> 00:21:05,200
0,135 135,390 390,540 540,870 870,1065
and that's how Themis was
Themis 就是这样开始的。

730
00:21:05,200 --> 00:21:06,440
0,290
started.|
|

731
00:21:06,700 --> 00:21:08,295
0,400 540,920 920,1115 1115,1370 1370,1595
So let's first start by
所以，让我们首先对 VAE 进行关于这个数据集的训练，

732
00:21:08,295 --> 00:21:09,720
0,270 270,510 510,875 925,1230 1230,1425
training a VAE on this

733
00:21:09,720 --> 00:21:11,475
0,210 210,530 910,1215 1215,1470 1470,1755
data set,| {the,z -} shown
|这张图中显示的 z 最终是我们的潜在空间，

734
00:21:11,475 --> 00:21:13,080
0,255 255,435 435,570 570,1115 1315,1605
here in this diagram ends

735
00:21:13,080 --> 00:21:14,420
0,180 180,420 420,675 675,1035 1035,1340
up being our latent space,|
|

736
00:21:14,800 --> 00:21:16,820
0,395 395,695 695,1040 1040,1330 1620,2020
and the latent space automatically
并且潜在空间自动捕获对分类重要的特征。

737
00:21:16,930 --> 00:21:18,795
0,590 590,910 1050,1325 1325,1535 1535,1865
captures features that were important

738
00:21:18,795 --> 00:21:20,800
0,270 270,755
for classification.|
|

739
00:21:20,870 --> 00:21:22,810
0,400 780,1145 1145,1295 1295,1565 1565,1940
So here's an example, latent
这是一个例子，这个模型捕捉到的潜在特征，

740
00:21:22,810 --> 00:21:24,090
0,240 240,480 480,645 645,915 915,1280
feature that this model captured,|
|

741
00:21:24,530 --> 00:21:25,765
0,365 365,590 590,740 740,890 890,1235
{} {this,is -} the facial
这是输入人脸的面部位置，

742
00:21:25,765 --> 00:21:26,970
0,255 255,450 450,660 660,915 915,1205
position of an input face,|
|

743
00:21:27,440 --> 00:21:29,350
0,400 690,1010 1010,1295 1295,1550 1550,1910
and something that's really crucial
这里真正关键的一点是，

744
00:21:29,350 --> 00:21:30,880
0,375 375,645 645,870 870,1185 1185,1530
here is that,| we never
|我们从来没有告诉模型，

745
00:21:30,880 --> 00:21:33,570
0,285 285,465 465,740 1060,1460 1990,2690
told the model| to calculate,
|计算，编码给定人脸面部位置的特征向量，

746
00:21:34,010 --> 00:21:35,815
0,400 450,965 965,1175 1175,1460 1460,1805
to encode the feature vector

747
00:21:35,815 --> 00:21:37,780
0,365 835,1095 1095,1425 1425,1725 1725,1965
of the facial position of

748
00:21:37,780 --> 00:21:39,295
0,105 105,315 315,680 940,1245 1245,1515
a {given,face -},| it learned
|它会自动学习这一点，

749
00:21:39,295 --> 00:21:41,860
0,365 655,1055 1675,2010 2010,2265 2265,2565
this automatically,| because this feature
|因为这一特征对于模型很重要，

750
00:21:41,860 --> 00:21:43,105
0,375 375,735 735,945 945,1035 1035,1245
is important for the model|
|

751
00:21:43,105 --> 00:21:44,980
0,360 360,690 690,930 930,1140 1140,1875
to develop a good representation
对于开发出一个很好的人脸是什么样子的表示。

752
00:21:44,980 --> 00:21:46,405
0,255 255,405 405,585 585,890 1090,1425
of what a face actually

753
00:21:46,405 --> 00:21:47,740
0,335
is.|
|

754
00:21:48,660 --> 00:21:49,820
0,400 450,725 725,845 845,980 980,1160
So now that we have
现在我们有了我们的潜在结构，

755
00:21:49,820 --> 00:21:51,320
0,165 165,465 465,770 1090,1350 1350,1500
our latent structure,| we can
|我们可以用它来计算输入在每个潜在变量上的分布，

756
00:21:51,320 --> 00:21:52,715
0,195 195,405 405,585 585,1095 1095,1395
use it to calculate a

757
00:21:52,715 --> 00:21:54,530
0,395 505,780 780,1020 1020,1485 1485,1815
distribution of the inputs across

758
00:21:54,530 --> 00:21:56,480
0,330 330,750 750,1010 1540,1815 1815,1950
every latent variable,| and we
|我们可以估计概率分布，

759
00:21:56,480 --> 00:21:58,270
0,260 340,615 615,795 795,1340 1390,1790
can estimate a probability distribution|
|

760
00:21:58,830 --> 00:22:00,755
0,380 380,760 1170,1505 1505,1685 1685,1925
depending on that's based on
根据这个数据集中每一项的特征，

761
00:22:00,755 --> 00:22:02,240
0,210 210,515 655,945 945,1185 1185,1485
the features of every item

762
00:22:02,240 --> 00:22:04,270
0,225 225,375 375,570 570,890 1630,2030
in this data set,| {essentially\,,what
|本质上，这意味着，

763
00:22:04,350 --> 00:22:05,525
0,290 290,485 485,770 770,1010 1010,1175
-} this means is that,|
|

764
00:22:05,525 --> 00:22:07,000
0,165 165,330 330,810 810,960 960,1475
we can calculate the probability
我们可以计算特定特征组合出现在我们的数据集中的概率，

765
00:22:07,080 --> 00:22:08,540
0,245 245,410 410,695 695,1060 1170,1460
that a certain combination of

766
00:22:08,540 --> 00:22:09,905
0,290 550,885 885,1050 1050,1155 1155,1365
features appears in our data

767
00:22:09,905 --> 00:22:11,330
0,335 475,825 825,1035 1035,1140 1140,1425
set| based on the latent
|基于我们刚刚学习的潜在空间。

768
00:22:11,330 --> 00:22:13,030
0,195 195,375 375,540 540,830 1300,1700
space that we just learned,|
|

769
00:22:13,640 --> 00:22:14,450
0,150 150,285 285,420 420,555 555,810
and then we can over
然后我们可以对该数据集较稀疏的区域进行过采样，

770
00:22:14,450 --> 00:22:16,760
0,360 360,950 1390,1890 1890,2100 2100,2310
sample denser, sparser areas of

771
00:22:16,760 --> 00:22:17,855
0,135 135,345 345,615 615,840 840,1095
this data set| and under
|并从该数据集的较密集区域进行欠采样。

772
00:22:17,855 --> 00:22:19,265
0,330 330,570 570,930 930,1170 1170,1410
sample from denser areas of

773
00:22:19,265 --> 00:22:20,640
0,150 150,345 345,665
this data set.|
|

774
00:22:20,770 --> 00:22:22,305
0,400 600,950 950,1040 1040,1220 1220,1535
So let's say our distribution
假设我们的分布是这样的，

775
00:22:22,305 --> 00:22:23,565
0,270 270,510 510,780 780,1050 1050,1260
looks something like this,| {this,is
|这是过于简单化了，

776
00:22:23,565 --> 00:22:25,020
0,90 90,210 210,935 1045,1320 1320,1455
-} an oversimplification,| but for
|但为了可视化的目的，

777
00:22:25,020 --> 00:22:27,990
0,570 570,920 1570,1970 2170,2505 2505,2970
visualization purposes| and the denser
|这个数据集的更密集的部分，

778
00:22:27,990 --> 00:22:29,240
0,390 390,540 540,675 675,900 900,1250
portions of this data set,|
|

779
00:22:29,380 --> 00:22:30,590
0,260 260,455 455,740 740,950 950,1210
we would expect to have
我们希望有一个均匀的肤色和头发颜色的姿势，以及非常好的照明，

780
00:22:31,030 --> 00:22:32,760
0,275 275,845 845,1070 1070,1360 1440,1730
a homogeneous skin color and

781
00:22:32,760 --> 00:22:34,155
0,320 370,645 645,825 825,1125 1125,1395
pose in hair color and

782
00:22:34,155 --> 00:22:36,180
0,225 225,510 510,845 1615,1890 1890,2025
very good lighting,| and then
|然后在这个数据集的较稀疏部分，

783
00:22:36,180 --> 00:22:37,290
0,135 135,285 285,675 675,975 975,1110
in the sparser portions of

784
00:22:37,290 --> 00:22:38,520
0,150 150,375 375,710 790,1035 1035,1230
this data set,| we would
|我们预计会看到不同的肤色、姿势和照明，

785
00:22:38,520 --> 00:22:39,990
0,285 285,510 510,735 735,1070 1180,1470
expect to see diverse skin

786
00:22:39,990 --> 00:22:43,020
0,290 550,1010 1060,1320 1320,1850
color, pose and illumination,|
|

787
00:22:46,280 --> 00:22:47,380
0,365 365,605 605,740 740,890 890,1100
so now that we have
现在我们有了这个分布，

788
00:22:47,380 --> 00:22:48,745
0,285 285,650 700,975 975,1140 1140,1365
this distribution| and we know
|我们知道了我们分布的哪些区域是密集的，哪些区域是稀疏的，

789
00:22:48,745 --> 00:22:50,130
0,210 210,485 535,795 795,1020 1020,1385
what areas of our distribution

790
00:22:50,150 --> 00:22:51,535
0,335 335,665 665,935 935,1115 1115,1385
are dense and which areas

791
00:22:51,535 --> 00:22:53,380
0,300 300,845 1015,1290 1290,1560 1560,1845
are sparse,| we want to
|我们想要从落在这个分布的密集区域中的欠采样，

792
00:22:53,380 --> 00:22:55,225
0,270 270,630 630,1010 1180,1515 1515,1845
under sample areas from the,

793
00:22:55,225 --> 00:22:56,740
0,345 345,675 675,1110 1110,1275 1275,1515
under sample samples that fall

794
00:22:56,740 --> 00:22:58,150
0,210 210,315 315,770 790,1170 1170,1410
in the denser areas of

795
00:22:58,150 --> 00:23:00,000
0,225 225,590 970,1245 1245,1485 1485,1850
this distribution| and over sample
|在这个分布的稀疏区域中的过采样数据点。

796
00:23:00,980 --> 00:23:02,080
0,335 335,575 575,755 755,935 935,1100
data points that fall in

797
00:23:02,080 --> 00:23:03,340
0,120 120,525 525,765 765,1005 1005,1260
the sparser areas of this

798
00:23:03,340 --> 00:23:06,025
0,380 1420,1770 1770,2010 2010,2300 2440,2685
distribution.| {So,for -} example,| we
|例如，|我们可能会对普通的肤色、发色和良好光照的欠采样，

799
00:23:06,025 --> 00:23:07,555
0,135 135,420 420,795 795,1155 1155,1530
would probably under sample points

800
00:23:07,555 --> 00:23:09,790
0,395 775,1175 1315,1650 1650,1965 1965,2235
with the very common skin

801
00:23:09,790 --> 00:23:11,470
0,285 285,600 600,915 915,1310 1360,1680
colors, hair colors and good

802
00:23:11,470 --> 00:23:12,970
0,315 315,570 570,830 880,1230 1230,1500
lighting,| that is extremely present
|在这个数据集中非常常见，

803
00:23:12,970 --> 00:23:14,200
0,165 165,300 300,525 525,860 940,1230
in this data set| and
|对我们在上一张幻灯片中看到的各种图像进行过采样，

804
00:23:14,200 --> 00:23:15,730
0,255 255,570 570,780 780,1040 1150,1530
over sample the diverse images

805
00:23:15,730 --> 00:23:16,600
0,225 225,360 360,570 570,750 750,870
that we saw on the

806
00:23:16,600 --> 00:23:18,595
0,210 210,560 1240,1515 1515,1740 1740,1995
last slide,| and this allows
|这允许我们以公平和不偏不倚的方式进行训练。

807
00:23:18,595 --> 00:23:19,630
0,210 210,405 405,660 660,870 870,1035
us to train in a

808
00:23:19,630 --> 00:23:22,000
0,300 300,570 570,1140 1140,1460
fair and unbiased manner.|
|

809
00:23:24,170 --> 00:23:25,375
0,260 260,425 425,730 750,1025 1025,1205
To dig in a little
为了更深入地了解重采样背后的数学原理，

810
00:23:25,375 --> 00:23:26,520
0,195 195,420 420,675 675,870 870,1145
bit more into the math

811
00:23:26,600 --> 00:23:28,170
0,320 320,545 545,740 740,1280 1280,1570
behind how this resampling works,|
|

812
00:23:28,850 --> 00:23:30,910
0,335 335,605 605,940 1200,1880 1880,2060
this approach basically approximates the
这种方法基本上是通过对单个潜在变量的联合直方图来近似潜在空间，

813
00:23:30,910 --> 00:23:32,395
0,315 315,620 700,1020 1020,1245 1245,1485
latent space via a joint

814
00:23:32,395 --> 00:23:34,450
0,630 630,960 960,1290 1290,1650 1650,2055
histogram over the individual latent

815
00:23:34,450 --> 00:23:37,020
0,560 1000,1400 1540,1830 1830,2120 2170,2570
variables,| {so,we -} have a
|所以我们对每个潜在变量 zi 都有一个直方图，

816
00:23:37,850 --> 00:23:39,660
0,635 635,860 860,1115 1115,1520 1520,1810
histogram for every latent variable

817
00:23:39,860 --> 00:23:41,545
0,320 320,515 515,790 1200,1505 1505,1685
{zi - -},| and what
|这个直方图主要做的是，

818
00:23:41,545 --> 00:23:43,030
0,105 105,525 525,840 840,1215 1215,1485
the histogram essentially does is|
|

819
00:23:43,030 --> 00:23:45,270
0,210 210,890 1030,1380 1380,1730 1840,2240
it discretizes the continuous distribution,|
它离散化连续分布，|

820
00:23:45,590 --> 00:23:46,645
0,245 245,350 350,455 455,590 590,1055
so that we can calculate
这样我们就可以更容易地计算概率。

821
00:23:46,645 --> 00:23:48,920
0,515 565,855 855,1145
probabilities more easily.|
|

822
00:23:49,370 --> 00:23:51,510
0,400 480,880 930,1550 1550,1700 1700,2140
Then we multiply the probabilities
然后我们将所有潜在分布的概率相乘，

823
00:23:51,530 --> 00:23:52,975
0,400 480,830 830,1085 1085,1265 1265,1445
together across all of the

824
00:23:52,975 --> 00:23:55,630
0,405 405,845 2005,2280 2280,2430 2430,2655
latent distributions,| and then after
|然后我们可以了解所有样本在潜在空间中的联合分布。

825
00:23:55,630 --> 00:23:56,850
0,285 285,480 480,645 645,885 885,1220
that we can develop an

826
00:23:57,440 --> 00:23:58,920
0,335 335,530 530,695 695,1000 1080,1480
understanding of the joint distribution

827
00:23:58,940 --> 00:23:59,965
0,275 275,425 425,545 545,665 665,1025
of all of the samples

828
00:23:59,965 --> 00:24:01,820
0,150 150,270 270,570 570,875
in our latent space.|
|

829
00:24:03,170 --> 00:24:04,405
0,320 320,515 515,785 785,1040 1040,1235
Based on this,| we can
在此基础上，|我们可以定义特定数据点的调整后的抽样概率如下，

830
00:24:04,405 --> 00:24:06,250
0,285 285,585 585,930 930,1535 1555,1845
define the adjusted probability for

831
00:24:06,250 --> 00:24:07,930
0,530 640,885 885,1065 1065,1380 1380,1680
sampling for a particular data

832
00:24:07,930 --> 00:24:09,955
0,210 210,420 420,740 1330,1605 1605,2025
point as follows,| {the,probability -}
|选择样本数据点 x 的概率将基于 x 的潜在空间，

833
00:24:09,955 --> 00:24:11,425
0,255 255,570 570,720 720,1025 1135,1470
of selecting a sample data

834
00:24:11,425 --> 00:24:12,745
0,240 240,545 685,945 945,1080 1080,1320
point x will be based

835
00:24:12,745 --> 00:24:13,825
0,240 240,375 375,690 690,900 900,1080
on the latent space of

836
00:24:13,825 --> 00:24:15,220
0,275 655,960 960,1140 1140,1260 1260,1395
x,| such that it is
|从而它是联合近似分布的逆，

837
00:24:15,220 --> 00:24:16,555
0,180 180,660 660,915 915,1065 1065,1335
the inverse of the joint

838
00:24:16,555 --> 00:24:19,205
0,660 660,1055 2155,2430 2430,2625 2625,2850
approximated distribution,| we have a
|我们这里有一个参数 α ，

839
00:24:19,305 --> 00:24:20,715
0,435 435,870 870,1080 1080,1275 1275,1410
parameter α here,| which is
|这是一个偏置参数，

840
00:24:20,815 --> 00:24:22,675
0,225 225,765 765,1265 1405,1680 1680,1860
a biasing parameter,| and as
|随着 α 的增加，

841
00:24:22,675 --> 00:24:24,880
0,480 480,845 1165,1470 1470,1965 1965,2205
α increases,| this probability will
|这个概率将趋于均匀分布，

842
00:24:24,880 --> 00:24:26,490
0,270 270,615 615,930 930,1245 1245,1610
tend to the uniform distribution,|
|

843
00:24:26,870 --> 00:24:28,585
0,260 260,395 395,875 875,1270 1440,1715
and if α increases, we
如果 α 增加，我们倾向于更强烈地去偏置。

844
00:24:28,585 --> 00:24:29,605
0,150 150,270 270,375 375,735 735,1020
tend to de bias more

845
00:24:29,605 --> 00:24:31,080
0,335
strongly.|
|

846
00:24:32,570 --> 00:24:33,760
0,275 275,470 470,680 680,920 920,1190
And this gives us the
这给了我们数据集中样本的最终权重，

847
00:24:33,760 --> 00:24:35,250
0,320 370,770 790,1065 1065,1215 1215,1490
final weight of the sample

848
00:24:35,750 --> 00:24:36,730
0,245 245,380 380,605 605,830 830,980
in our data set,| that
|我们可以动态计算，

849
00:24:36,730 --> 00:24:37,945
0,120 120,345 345,855 855,1020 1020,1215
we can calculate on the

850
00:24:37,945 --> 00:24:39,690
0,305 535,935 985,1260 1260,1440 1440,1745
fly| and use it to
|并在训练时使用它来自适应地重新采样。

851
00:24:39,860 --> 00:24:42,620
0,605 605,1100 1100,1295 1295,1600
adaptively resample while training.|
|

852
00:24:43,770 --> 00:24:45,370
0,305 305,610 780,1085 1085,1295 1295,1600
And so once we apply
因此，一旦我们应用了这种偏差，

853
00:24:45,630 --> 00:24:47,525
0,335 335,935 935,1205 1205,1510 1530,1895
this biasing,| we have pretty
|我们就会有非常显著的结果，

854
00:24:47,525 --> 00:24:49,490
0,365 475,875 1225,1515 1515,1725 1725,1965
remarkable results,| {this,is -} the
|这是原始的图，

855
00:24:49,490 --> 00:24:51,230
0,320 490,855 855,1185 1185,1470 1470,1740
original {} graph,| that shows
|显示了此数据集中深色男性和浅色男性之间的精度差距。

856
00:24:51,230 --> 00:24:53,170
0,315 315,750 750,1010 1030,1430 1540,1940
the accuracy gap between {}

857
00:24:53,220 --> 00:24:54,905
0,365 365,815 815,1175 1175,1430 1430,1685
the darker Males and the

858
00:24:54,905 --> 00:24:56,350
0,330 330,585 585,705 705,855 855,1445
lighter Males in this dataset.|
|

859
00:24:57,770 --> 00:24:59,130
0,320 320,575 575,815 815,1040 1040,1360
Once we apply the debias
一旦我们应用了 debias 算法，

860
00:24:59,330 --> 00:25:00,940
0,560 560,860 860,1070 1070,1430 1430,1610
algorithm,| where as α gets
|随着 α 变得越来越小，

861
00:25:00,940 --> 00:25:02,320
0,285 285,570 570,920 940,1215 1215,1380
smaller,| we're debiasing more and
|我们 debias 的越来越多，

862
00:25:02,320 --> 00:25:03,505
0,290 370,660 660,810 810,960 960,1185
more,| as we just talked
|正如我们刚才所说的，

863
00:25:03,505 --> 00:25:06,430
0,335 1105,1500 1500,1935 1935,2195 2305,2925
about,| this accuracy gap decreases
|这种精度差距显著减小，

864
00:25:06,430 --> 00:25:08,815
0,380 1390,1650 1650,1980 1980,2205 2205,2385
significantly| and that's because we
|这是因为我们倾向于对肤色较深的样本进行采样，

865
00:25:08,815 --> 00:25:10,435
0,180 180,330 330,555 555,935 1135,1620
tend to over sample samples

866
00:25:10,435 --> 00:25:12,120
0,225 225,615 615,795 795,1085 1285,1685
with darker skin color| and
|因此模型会更好地学习它们，

867
00:25:12,260 --> 00:25:13,495
0,320 320,500 500,710 710,1085 1085,1235
therefore the model learns them

868
00:25:13,495 --> 00:25:14,620
0,270 270,570 570,840 840,975 975,1125
better| and tends to do
|并倾向于在它们身上做得更好。

869
00:25:14,620 --> 00:25:16,340
0,225 225,465 465,770
better on them.|
|

870
00:25:16,380 --> 00:25:17,680
0,290 290,560 560,905 905,1025 1025,1300
Keep this algorithm in mind,|
记住这个算法，|

871
00:25:17,730 --> 00:25:18,500
0,260 260,410 410,530 530,665 665,770
because you're going to need
因为你们在实验 3 的比赛中会用到它，

872
00:25:18,500 --> 00:25:19,450
0,135 135,270 270,375 375,585 585,950
it for the lab 3

873
00:25:19,530 --> 00:25:20,975
0,400 600,860 860,1055 1055,1220 1220,1445
competition,| which I'll talk more
|我会在这节课的最后更多地讲到。

874
00:25:20,975 --> 00:25:21,965
0,270 270,540 540,705 705,855 855,990
about towards the end of

875
00:25:21,965 --> 00:25:23,560
0,135 135,425
this lecture.|
|

876
00:25:25,420 --> 00:25:26,820
0,400 420,725 725,995 995,1280 1280,1400
So, so far we've been
到目前为止，我们主要关注面部识别系统和其他几个系统，

877
00:25:26,820 --> 00:25:28,370
0,290 310,690 690,975 975,1290 1290,1550
focusing mainly on facial recognition

878
00:25:28,540 --> 00:25:29,700
0,400 480,725 725,830 830,995 995,1160
systems and a couple of

879
00:25:29,700 --> 00:25:31,665
0,195 195,530 640,1005 1005,1635 1635,1965
other systems| as canonical examples
|作为偏见的典型例子。

880
00:25:31,665 --> 00:25:34,290
0,270 270,695 1435,1835 1915,2370 2370,2625
of bias.| {However\,,bias -} is
|然而，在机器学习中，偏见实际上要普遍得多，

881
00:25:34,290 --> 00:25:36,210
0,375 375,770 790,1110 1110,1680 1680,1920
actually far more widespread in

882
00:25:36,210 --> 00:25:38,085
0,225 225,500 1090,1380 1380,1575 1575,1875
machine learning,| consider the example
|以自动驾驶为例，

883
00:25:38,085 --> 00:25:40,350
0,330 330,885 885,1175 1675,2010 2010,2265
of {autonomous,driving -},| many data
|许多数据集主要包括行驶在阳光明媚的笔直道路上的汽车，

884
00:25:40,350 --> 00:25:42,165
0,300 300,680 730,1230 1230,1545 1545,1815
sets are comprised mainly of

885
00:25:42,165 --> 00:25:43,815
0,285 285,615 615,965 1075,1410 1410,1650
cars driving down straight and

886
00:25:43,815 --> 00:25:45,150
0,240 240,575 625,915 915,1125 1125,1335
sunny roads| in really good
|在非常好的天气条件下，能见度非常高，

887
00:25:45,150 --> 00:25:46,890
0,285 285,680 910,1230 1230,1500 1500,1740
weather conditions with {very,high -}

888
00:25:46,890 --> 00:25:48,570
0,500 820,1095 1095,1245 1245,1440 1440,1680
visibility,| and this is because
|这是因为这些算法中这些汽车的数据

889
00:25:48,570 --> 00:25:49,580
0,180 180,375 375,555 555,705 705,1010
the data for these cars

890
00:25:49,780 --> 00:25:51,405
0,290 290,580 630,1085 1085,1370 1370,1625
for these algorithms| is actually
|是由行驶在道路上的汽车收集的，

891
00:25:51,405 --> 00:25:53,370
0,275 865,1215 1215,1455 1455,1680 1680,1965
just collected by cars driving

892
00:25:53,370 --> 00:25:54,960
0,285 285,620
down roads,|
|

893
00:25:55,220 --> 00:25:57,450
0,400 750,1145 1145,1490 1490,1835 1835,2230
however, in some specific cases,|
然而，在一些特定的情况下，|

894
00:25:57,560 --> 00:25:58,810
0,290 290,395 395,515 515,740 740,1250
you're going to face adverse
你将面临恶劣的天气，糟糕的能见度，接近碰撞的场景，

895
00:25:58,810 --> 00:26:01,855
0,320 610,1010 1450,1785 1785,2330 2710,3045
weather, bad, bad visibility, near

896
00:26:01,855 --> 00:26:03,565
0,345 345,965 1045,1320 1320,1455 1455,1710
collision scenarios,| {and,these -} are
|这些样本实际上是模型需要学习的最重要的样本，

897
00:26:03,565 --> 00:26:04,675
0,315 315,495 495,840 840,975 975,1110
actually the samples that are

898
00:26:04,675 --> 00:26:05,845
0,135 135,395 415,815 835,1080 1080,1170
the most important for the

899
00:26:05,845 --> 00:26:07,390
0,210 210,420 420,665 1045,1320 1320,1545
model to learn,| because they're
|因为它们是最难的样本，

900
00:26:07,390 --> 00:26:08,665
0,105 105,450 450,885 885,1065 1065,1275
the hardest samples| and they're
|也是模型最有可能失败的样本。

901
00:26:08,665 --> 00:26:09,475
0,105 105,435 435,570 570,660 660,810
the samples where the model

902
00:26:09,475 --> 00:26:10,830
0,240 240,510 510,825 825,1080 1080,1355
is most likely to fail.|
|

903
00:26:11,900 --> 00:26:13,295
0,345 345,570 570,735 735,1040 1060,1395
But in a traditional {}
但在传统的自动驾驶管道中，

904
00:26:13,295 --> 00:26:15,245
0,510 510,720 720,1055 1255,1545 1545,1950
autonomous driving pipeline,| these samples
|这些样本往往极低，代表性极低，

905
00:26:15,245 --> 00:26:17,375
0,135 135,395 595,990 990,1385 1765,2130
are often extremely low, have

906
00:26:17,375 --> 00:26:19,610
0,315 315,570 570,1325 1885,2130 2130,2235
extremely low representation,| {so,this -}
|因此，这是一个例子，使用我们刚才谈到的无监督潜在偏差，

907
00:26:19,610 --> 00:26:20,855
0,105 105,285 285,615 615,930 930,1245
is an example where using

908
00:26:20,855 --> 00:26:23,390
0,395 745,1590 1590,1995 1995,2415 2415,2535
the unsupervised latent biasing that

909
00:26:23,390 --> 00:26:24,620
0,120 120,285 285,540 540,890 970,1230
we just talked about,| we
|我们将能够对这些重要数据点进行过采样，

910
00:26:24,620 --> 00:26:25,535
0,120 120,240 240,480 480,720 720,915
would be able to up

911
00:26:25,535 --> 00:26:28,160
0,300 300,665 715,1115 2005,2355 2355,2625
sample these {} important data

912
00:26:28,160 --> 00:26:29,890
0,300 300,585 585,890 910,1310 1330,1730
points| and under sample the
|并对沿着笔直和阳光明媚的道路行驶的数据点进行欠采样。

913
00:26:29,940 --> 00:26:31,420
0,320 320,605 605,860 860,1115 1115,1480
data points of driving down

914
00:26:31,440 --> 00:26:33,480
0,335 335,575 575,800 800,1120
straight and sunny roads.|
|

915
00:26:35,590 --> 00:26:37,695
0,760 930,1280 1280,1535 1535,1820 1820,2105
Similarly, consider the example of
同样，考虑一下大型语言模型的例子，

916
00:26:37,695 --> 00:26:40,485
0,255 255,570 570,935 1675,2075 2425,2790
large language models,| {an -}
|几年前，一篇非常著名的论文表明，

917
00:26:40,485 --> 00:26:41,925
0,360 360,720 720,1035 1035,1260 1260,1440
extremely famous paper a couple

918
00:26:41,925 --> 00:26:43,215
0,240 240,575 685,990 990,1155 1155,1290
years ago showed that,| if
|如果你在一个大型语言模型支持的求职搜索引擎中输入暗示女性或女性的条目，

919
00:26:43,215 --> 00:26:44,600
0,165 165,390 390,705 705,930 930,1385
you put terms that imply

920
00:26:44,830 --> 00:26:46,545
0,400 420,710 710,980 980,1360 1410,1715
female or woman into a

921
00:26:46,545 --> 00:26:48,255
0,240 240,555 555,915 915,1410 1410,1710
large language model powered job

922
00:26:48,255 --> 00:26:49,635
0,240 240,545 835,1155 1155,1260 1260,1380
search engine,| you're going to
|你将获得艺术家等人文学科的角色，

923
00:26:49,635 --> 00:26:51,080
0,150 150,450 450,735 735,1025 1045,1445
get roles such as artist

924
00:26:51,340 --> 00:26:52,580
0,275 275,440 440,575 575,665 665,1240
or things in the humanities,|
|

925
00:26:53,080 --> 00:26:54,840
0,245 245,365 365,640 900,1300 1380,1760
but if you input similar
但如果你输入类似的东西，但是男性的，

926
00:26:54,840 --> 00:26:55,800
0,330 330,525 525,630 630,750 750,960
things, but of the male

927
00:26:55,800 --> 00:26:57,180
0,615 615,825 825,975 975,1155 1155,1380
counterpart,| you put things like
|你把男性之类的东西放进搜索引擎，

928
00:26:57,180 --> 00:26:58,500
0,300 300,615 615,930 930,1170 1170,1320
male into the the search

929
00:26:58,500 --> 00:27:00,000
0,290 730,1065 1065,1185 1185,1335 1335,1500
engine,| you'll end up with
|你最终会得到科学家和工程师的角色，

930
00:27:00,000 --> 00:27:02,090
0,290 310,615 615,920 1330,1710 1710,2090
roles for scientists and engineers,|
|

931
00:27:02,800 --> 00:27:05,385
0,400 660,980 980,1190 1190,1480 2010,2585
so this type of bias
所以这种类型的偏见也会发生，

932
00:27:05,385 --> 00:27:07,065
0,330 330,665 865,1260 1260,1530 1530,1680
also occurs,| regardless of the
|而不管特定模型的手头任务是什么。

933
00:27:07,065 --> 00:27:08,475
0,270 270,570 570,875 895,1215 1215,1410
task at hand for a

934
00:27:08,475 --> 00:27:10,020
0,270 270,665
specific model.|
|

935
00:27:11,190 --> 00:27:12,725
0,290 290,580 750,1130 1130,1310 1310,1535
And finally, let's talk about
最后，让我们来谈谈医疗保健推荐算法，

936
00:27:12,725 --> 00:27:14,900
0,240 240,435 435,995 1105,1625 1885,2175
health care recommendation algorithms,| {these,recommendation
|这些推荐算法往往会放大种族偏见，

937
00:27:14,900 --> 00:27:16,805
0,615 615,1065 1065,1305 1305,1455 1455,1905
-} algorithms tend to amplify

938
00:27:16,805 --> 00:27:18,575
0,360 360,935 1105,1380 1380,1590 1590,1770
racial biases,| a paper from
|几年前的一篇论文显示，

939
00:27:18,575 --> 00:27:19,880
0,90 90,240 240,495 495,845 955,1305
a couple years ago showed

940
00:27:19,880 --> 00:27:21,185
0,285 285,555 555,890 910,1185 1185,1305
that,| black patients need to
|黑人患者的病情需要比白人患者严重得多，

941
00:27:21,185 --> 00:27:23,120
0,135 135,425 805,1385 1465,1755 1755,1935
be significantly sicker than their

942
00:27:23,120 --> 00:27:24,455
0,240 240,870 870,1005 1005,1140 1140,1335
white counterparts| to get the
|才能得到同样水平的护理，

943
00:27:24,455 --> 00:27:26,120
0,240 240,480 480,705 705,1025 1375,1665
same level of care,| and
|这是因为这个模型的数据集存在固有的偏见。

944
00:27:26,120 --> 00:27:27,995
0,375 375,630 630,900 900,1425 1425,1875
that's because of inherent bias

945
00:27:27,995 --> 00:27:28,805
0,165 165,285 285,480 480,675 675,810
in the data set of

946
00:27:28,805 --> 00:27:30,425
0,165 165,455 925,1245 1245,1455 1455,1620
{this,model -}.| And so in
|因此，在所有这些例子中，

947
00:27:30,425 --> 00:27:31,640
0,165 165,315 315,540 540,905 955,1215
all of these examples,| we
|我们可以使用上面的算法偏差缓解方法

948
00:27:31,640 --> 00:27:33,260
0,150 150,375 375,630 630,950 1090,1620
can use the above algorithmic

949
00:27:33,260 --> 00:27:35,060
0,345 345,795 795,1160 1330,1620 1620,1800
bias mitigation method| to try
|来尝试解决这些问题和更多问题。

950
00:27:35,060 --> 00:27:36,470
0,180 180,420 420,705 705,1040 1120,1410
and solve these problems and

951
00:27:36,470 --> 00:27:37,640
0,290
more.|
|

952
00:27:40,360 --> 00:27:42,000
0,400 690,965 965,1130 1130,1355 1355,1640
So we just went through
因此，我们讨论了如何减轻人工智能中的某些形式的偏差，

953
00:27:42,000 --> 00:27:43,365
0,225 225,360 360,780 780,1095 1095,1365
how to mitigate some forms

954
00:27:43,365 --> 00:27:44,810
0,180 180,510 510,765 765,1080 1080,1445
of bias in artificial intelligence|
|

955
00:27:45,040 --> 00:27:46,365
0,290 290,485 485,665 665,940 1050,1325
and where these solutions may
以及这些解决方案可能适用于哪里，

956
00:27:46,365 --> 00:27:48,150
0,195 195,515 1105,1380 1380,1545 1545,1785
be applied,| {and,we -} talked
|我们谈到了 Themis 使用的一种基本算法，

957
00:27:48,150 --> 00:27:50,100
0,350 370,660 660,1130 1360,1785 1785,1950
about a foundational algorithm that

958
00:27:50,100 --> 00:27:51,405
0,330 330,620 730,1005 1005,1140 1140,1305
Themis uses,| that you all
|你们今天也将开发它。

959
00:27:51,405 --> 00:27:52,670
0,255 255,450 450,585 585,870 870,1265
will also be developing today.|
|

960
00:27:53,410 --> 00:27:54,555
0,395 395,650 650,755 755,950 950,1145
And for the next part
在课程的下一部分，

961
00:27:54,555 --> 00:27:55,970
0,90 90,180 180,425 805,1155 1155,1415
of the lecture,| we'll focus
|我们将关注不确定性，

962
00:27:56,020 --> 00:27:57,930
0,400 450,850 1170,1460 1460,1685 1685,1910
on uncertainty| or when a
|或者模型不知道答案的时候，

963
00:27:57,930 --> 00:27:59,175
0,290 340,645 645,885 885,1110 1110,1245
model does not know the

964
00:27:59,175 --> 00:28:00,560
0,245
answer,|
|

965
00:28:00,560 --> 00:28:01,910
0,195 195,360 360,600 600,920 970,1350
we'll talk about why uncertainty
我们将讨论为什么不确定性很重要，

966
00:28:01,910 --> 00:28:03,740
0,315 315,650 940,1260 1260,1575 1575,1830
is important| and how we
|我们如何估计它，

967
00:28:03,740 --> 00:28:05,240
0,260 310,570 570,825 825,1215 1215,1500
can estimate it,| and also
|以及不确定性估计的应用。

968
00:28:05,240 --> 00:28:07,270
0,285 285,660 660,990 990,1340 1540,2030
the applications of uncertainty estimation.|
|

969
00:28:08,600 --> 00:28:10,180
0,380 380,635 635,815 815,1120 1260,1580
So, to start with, what
那么，首先，什么是不确定性，

970
00:28:10,180 --> 00:28:11,680
0,315 315,710 880,1185 1185,1365 1365,1500
is uncertainty| and why is
|为什么需要计算。

971
00:28:11,680 --> 00:28:13,720
0,255 255,570 570,735 735,1130
it necessary to compute.|
|

972
00:28:13,720 --> 00:28:14,580
0,285 285,375 375,480 480,600 600,860
Let's look at the following
让我们看一下下面的例子，

973
00:28:14,690 --> 00:28:16,735
0,400 870,1160 1160,1370 1370,1610 1610,2045
example,| {this,is -} a binary
|这是一个对猫和狗的图像进行训练的二进制分类器，

974
00:28:16,735 --> 00:28:18,190
0,615 615,840 840,1005 1005,1245 1245,1455
classifier that is trained on

975
00:28:18,190 --> 00:28:19,650
0,290 310,630 630,930 930,1185 1185,1460
images of cats and dogs,|
|

976
00:28:20,240 --> 00:28:21,805
0,275 275,545 545,940 990,1340 1340,1565
for every single input,| it
对于每一个输入，|它将输出这两个类的概率分布。

977
00:28:21,805 --> 00:28:23,520
0,275 295,555 555,720 720,1235 1315,1715
will output a probability distribution

978
00:28:23,780 --> 00:28:26,380
0,365 365,680 680,935 935,1240
over these two classes.|
|

979
00:28:27,610 --> 00:28:28,725
0,400 450,785 785,875 875,980 980,1115
Now, let's say I give
现在，让我们假设我给这个模型一个马的图像，

980
00:28:28,725 --> 00:28:29,655
0,150 150,375 375,570 570,750 750,930
this model an image of

981
00:28:29,655 --> 00:28:31,200
0,105 105,365 835,1170 1170,1350 1350,1545
a horse,| it's never seen
|它以前从来没有见过马，

982
00:28:31,200 --> 00:28:32,475
0,105 105,270 270,590 880,1125 1125,1275
a horse before,| {the,horse -}
|这匹马显然既不是猫也不是狗，

983
00:28:32,475 --> 00:28:33,830
0,210 210,510 510,840 840,1065 1065,1355
is clearly neither a cat

984
00:28:33,850 --> 00:28:35,730
0,260 260,380 380,640 1260,1640 1640,1880
nor a dog,| however the
|然而，该模型别无选择，只能输出概率分布，

985
00:28:35,730 --> 00:28:37,110
0,225 225,540 540,855 855,1155 1155,1380
model has no choice, but

986
00:28:37,110 --> 00:28:38,960
0,270 270,645 645,930 930,1430 1450,1850
to output a probability distribution,|
|

987
00:28:39,190 --> 00:28:40,215
0,275 275,545 545,665 665,830 830,1025
because that's how this model
因为这就是该模型的结构。

988
00:28:40,215 --> 00:28:42,040
0,195 195,695
is structured.|
|

989
00:28:42,810 --> 00:28:44,315
0,400 570,845 845,1025 1025,1250 1250,1505
However, what if in addition
然而，如果除了这个预测之外，

990
00:28:44,315 --> 00:28:46,060
0,180 180,330 330,695 985,1365 1365,1745
to this prediction,| we also
|我们还实现了一个置信度估计，

991
00:28:46,290 --> 00:28:48,800
0,350 350,605 605,910 1290,1690 2220,2510
achieved a confidence estimate,| in
|在这种情况下，模型应该能够说，

992
00:28:48,800 --> 00:28:50,105
0,210 210,530 610,870 870,1065 1065,1305
this case, the model which

993
00:28:50,105 --> 00:28:50,950
0,165 165,270 270,405 405,570 570,845
should be able to say,|
|

994
00:28:51,630 --> 00:28:52,910
0,395 395,590 590,905 905,1130 1130,1280
I've never seen anything like
我以前从未见过这样的事情，

995
00:28:52,910 --> 00:28:53,795
0,165 165,420 420,645 645,750 750,885
this before| and I have
|我对这个预测的信心很低，

996
00:28:53,795 --> 00:28:55,235
0,270 270,615 615,965 1015,1275 1275,1440
very low confidence in this

997
00:28:55,235 --> 00:28:56,810
0,365 655,945 945,1170 1170,1410 1410,1575
prediction,| {so,you -} as the
|所以，作为用户，你不应该相信我对这个模型的预测，

998
00:28:56,810 --> 00:28:57,980
0,260 280,555 555,750 750,975 975,1170
user should not trust my

999
00:28:57,980 --> 00:28:59,675
0,270 270,450 450,585 585,860 1420,1695
prediction on this model,| and
|这是不确定性估计背后的核心思想。

1000
00:28:59,675 --> 00:29:01,025
0,285 285,420 420,645 645,975 975,1350
that's the core idea behind

1001
00:29:01,025 --> 00:29:03,080
0,395 535,1025
uncertainty estimation.|
|

1002
00:29:03,700 --> 00:29:04,880
0,365 365,590 590,710 710,875 875,1180
So in the real world,|
因此，在现实世界中，|

1003
00:29:05,050 --> 00:29:06,855
0,400 480,970 990,1250 1250,1510 1530,1805
uncertainty estimation is useful for
不确定性估计对于这样的场景是有用的，

1004
00:29:06,855 --> 00:29:08,700
0,390 390,615 615,935 1375,1650 1650,1845
scenarios like this,| {this,is -}
|这是特斯拉汽车跟在马车后面行驶的一个例子，

1005
00:29:08,700 --> 00:29:10,200
0,225 225,530 580,900 900,1125 1125,1500
an example of a Tesla

1006
00:29:10,200 --> 00:29:11,835
0,285 285,680 880,1200 1200,1410 1410,1635
car driving behind a horse

1007
00:29:11,835 --> 00:29:13,680
0,255 255,755 1195,1470 1470,1635 1635,1845
drawn buggy,| which are very
|这在美国一些地区非常常见，

1008
00:29:13,680 --> 00:29:14,610
0,255 255,465 465,630 630,810 810,930
common in some parts of

1009
00:29:14,610 --> 00:29:16,310
0,105 105,345 345,710 1150,1425 1425,1700
the United States,| it has
|它根本不知道这辆马车是什么，

1010
00:29:16,330 --> 00:29:17,430
0,365 365,635 635,800 800,920 920,1100
no idea what this horse

1011
00:29:17,430 --> 00:29:18,675
0,210 210,510 510,765 765,1050 1050,1245
drawn {buggy,is -},| it first
|它首先认为它是一辆卡车，然后是一辆汽车，然后是一个人，

1012
00:29:18,675 --> 00:29:19,965
0,225 225,465 465,600 600,905 1015,1290
thinks it's a truck and

1013
00:29:19,965 --> 00:29:21,315
0,120 120,285 285,605 955,1230 1230,1350
then a car and then

1014
00:29:21,315 --> 00:29:24,555
0,150 150,455 805,1205 2125,2525 2875,3240
a person,| and it continues
|然后它继续输出预测，

1015
00:29:24,555 --> 00:29:26,270
0,330 330,585 585,1095 1095,1380 1380,1715
to output predictions,| even though
|即使很明显，模型不知道这是什么图像。

1016
00:29:26,650 --> 00:29:27,750
0,245 245,365 365,620 620,920 920,1100
it is very clear that

1017
00:29:27,750 --> 00:29:28,935
0,120 120,380 430,720 720,930 930,1185
the model does not know

1018
00:29:28,935 --> 00:29:30,460
0,210 210,360 360,570 570,905
what this image is.|
|

1019
00:29:32,160 --> 00:29:32,975
0,260 260,440 440,605 605,710 710,815
And now you might be
现在你可能会问，

1020
00:29:32,975 --> 00:29:34,775
0,245 475,875 925,1320 1320,1695 1695,1800
asking,| okay, so what's the
|好吧，这有什么大不了的，

1021
00:29:34,775 --> 00:29:36,220
0,150 150,455 625,885 885,1140 1140,1445
big deal,| it didn't recognize
|它没有认出这辆马车，

1022
00:29:36,390 --> 00:29:37,880
0,245 245,410 410,635 635,1120 1230,1490
the horse drawn buggy,| but
|但它似乎还是开得很成功，

1023
00:29:37,880 --> 00:29:39,460
0,260 400,765 765,1020 1020,1245 1245,1580
it seemed to drive successfully

1024
00:29:39,900 --> 00:29:42,515
0,400 1110,1510 1620,1985 1985,2315 2315,2615
anyway,| {however\,,the -} exact same
|然而，导致这段视频的完全相同的问题

1025
00:29:42,515 --> 00:29:44,000
0,330 330,660 660,990 990,1275 1275,1485
problem that resulted in that

1026
00:29:44,000 --> 00:29:45,980
0,320 670,1020 1020,1290 1290,1610 1660,1980
video| has also resulted in
|也导致了许多自动驾驶汽车相撞。

1027
00:29:45,980 --> 00:29:49,260
0,320 670,1305 1305,1560 1560,2180
numerous autonomous car crashes.|
|

1028
00:29:51,210 --> 00:29:52,580
0,365 365,710 710,875 875,1100 1100,1370
So let's go through why
所以让我们来看看为什么会发生这样的事情，

1029
00:29:52,580 --> 00:29:53,735
0,285 285,510 510,735 735,975 975,1155
something like this might have

1030
00:29:53,735 --> 00:29:55,790
0,275 985,1245 1245,1365 1365,1625 1675,2055
happened,| {there,are -} multiple different
|神经网络中有多种不同类型的不确定性，

1031
00:29:55,790 --> 00:29:57,440
0,360 360,690 690,1040 1090,1380 1380,1650
types of uncertainty in neural

1032
00:29:57,440 --> 00:29:59,405
0,290 730,1035 1035,1260 1260,1515 1515,1965
networks,| which may cause incidents
|这可能会导致像我们刚刚看到的那样的事件，

1033
00:29:59,405 --> 00:30:00,305
0,225 225,375 375,555 555,750 750,900
like the ones that we

1034
00:30:00,305 --> 00:30:01,880
0,180 180,485 1015,1335 1335,1455 1455,1575
just saw,| we'll go through
|我们将通过一个简单的例子来说明两种主要类型的不确定性，

1035
00:30:01,880 --> 00:30:03,560
0,120 120,410 430,830 940,1200 1200,1680
a simple example that illustrates

1036
00:30:03,560 --> 00:30:04,850
0,225 225,450 450,765 765,1050 1050,1290
the two main types of

1037
00:30:04,850 --> 00:30:06,185
0,350 430,690 690,885 885,1080 1080,1335
uncertainty,| that we'll focus on
|我们将在这节课中重点讨论。

1038
00:30:06,185 --> 00:30:08,040
0,180 180,360 360,665
in this lecture.|
|

1039
00:30:09,980 --> 00:30:11,395
0,400 480,860 860,995 995,1250 1250,1415
So let's say I'm trying
假设我正在尝试估计曲线 y 等于 x 的立方，

1040
00:30:11,395 --> 00:30:12,810
0,300 300,570 570,720 720,995 1015,1415
to estimate the curve y

1041
00:30:12,830 --> 00:30:14,185
0,290 290,545 545,965 965,1160 1160,1355
equals x cubed| as part
|作为回归任务的一部分，

1042
00:30:14,185 --> 00:30:15,970
0,135 135,270 270,645 645,1025 1435,1785
of a regression task,| {the,input
|这里的输入 x 是一个实数，

1043
00:30:15,970 --> 00:30:18,250
0,255 255,560 610,1010 1720,2040 2040,2280
-} here, x is some

1044
00:30:18,250 --> 00:30:19,945
0,225 225,530 670,1070 1150,1455 1455,1695
real number,| and we want
|我们希望它输出 f(x) ，

1045
00:30:19,945 --> 00:30:21,310
0,255 255,575 715,1020 1020,1215 1215,1365
it to output f of

1046
00:30:21,310 --> 00:30:23,320
0,240 240,620 1030,1320 1320,1500 1500,2010
x,| which should be ideally
|理想情况下应该是 x 的立方。

1047
00:30:23,320 --> 00:30:24,920
0,270 270,770
x cubed.|
|

1048
00:30:24,990 --> 00:30:26,450
0,400 420,740 740,1055 1055,1310 1310,1460
So right away you might
你可能会立即注意到此数据集中存在一些问题，

1049
00:30:26,450 --> 00:30:27,670
0,240 240,465 465,630 630,870 870,1220
notice that, there are some

1050
00:30:27,990 --> 00:30:29,380
0,400 420,680 680,830 830,1055 1055,1390
issues in this data set,|
|

1051
00:30:29,400 --> 00:30:30,785
0,275 275,395 395,545 545,850 1050,1385
{assume,the -} red points in
假设图像中的红点是你的训练样本，

1052
00:30:30,785 --> 00:30:32,285
0,285 285,635 655,1005 1005,1260 1260,1500
this image are your training

1053
00:30:32,285 --> 00:30:33,780
0,545
samples,|
|

1054
00:30:37,040 --> 00:30:38,665
0,380 380,620 620,875 875,1270 1350,1625
so the box area of
所以这张图的方框区域显示了我们数据集中的数据点，

1055
00:30:38,665 --> 00:30:41,155
0,165 165,455 985,1385 1585,1985 2125,2490
this image shows data points

1056
00:30:41,155 --> 00:30:42,100
0,210 210,345 345,570 570,795 795,945
in our data set,| where
|其中我们有非常高的噪声，

1057
00:30:42,100 --> 00:30:43,560
0,135 135,360 360,690 690,1065 1065,1460
we have really high noise,|
|

1058
00:30:44,060 --> 00:30:45,325
0,335 335,635 635,860 860,1025 1025,1265
{these,points -} do not follow
这些点不遵循曲线 y 等于 x 立方，

1059
00:30:45,325 --> 00:30:46,210
0,195 195,375 375,570 570,720 720,885
the curve {y -} equals

1060
00:30:46,210 --> 00:30:47,620
0,255 255,735 735,990 990,1215 1215,1410
{x,cubed -},| in fact, they
|事实上，它们似乎根本不遵循任何分布，

1061
00:30:47,620 --> 00:30:48,565
0,195 195,375 375,600 600,750 750,945
don't really seem to follow

1062
00:30:48,565 --> 00:30:50,520
0,300 300,600 600,795 795,1055 1555,1955
any {distribution,at -} all,| and
|而且该模型将无法准确计算该区域内点的输出，

1063
00:30:50,990 --> 00:30:52,975
0,245 245,490 1260,1625 1625,1730 1730,1985
the model won't be able

1064
00:30:52,975 --> 00:30:57,130
0,395 925,1505 2095,2430 2430,2705 3835,4155
to compute outputs for points

1065
00:30:57,130 --> 00:30:59,730
0,195 195,390 390,710 1300,1910 2200,2600
in this region accurately,| because
|因为非常相似的输入具有非常不同的输出，

1066
00:31:00,080 --> 00:31:02,070
0,400 420,820 930,1280 1280,1540 1590,1990
very similar inputs have extremely

1067
00:31:02,120 --> 00:31:03,760
0,400 570,1060 1110,1370 1370,1505 1505,1640
different outputs,| which is the
|这是数据不确定性的定义。

1068
00:31:03,760 --> 00:31:06,380
0,260 460,780 780,1100 1180,1580
definition of data uncertainty.|
|

1069
00:31:09,720 --> 00:31:11,060
0,335 335,560 560,755 755,1055 1055,1340
We also have regions in
在这个数据集中，我们还有一些没有数据的地区，

1070
00:31:11,060 --> 00:31:12,005
0,165 165,375 375,615 615,795 795,945
this data set, where we

1071
00:31:12,005 --> 00:31:13,850
0,240 240,510 510,815 1315,1635 1635,1845
have no data,| {so,if -}
|因此，如果我们在模型中查询这部分数据集的预测，

1072
00:31:13,850 --> 00:31:15,005
0,180 180,495 495,630 630,890 910,1155
we queried the model for

1073
00:31:15,005 --> 00:31:16,355
0,120 120,435 435,690 690,965 1075,1350
a prediction in this part

1074
00:31:16,355 --> 00:31:17,390
0,165 165,315 315,465 465,720 720,1035
of in this region of

1075
00:31:17,390 --> 00:31:18,910
0,240 240,465 465,800 940,1230 1230,1520
the data set,| we should
|我们不应该真的期望看到准确的结果，

1076
00:31:18,960 --> 00:31:20,210
0,320 320,635 635,935 935,1130 1130,1250
not really expect to see

1077
00:31:20,210 --> 00:31:21,545
0,225 225,570 570,920 970,1230 1230,1335
an accurate result,| because the
|因为模型以前从未见过这样的事情，

1078
00:31:21,545 --> 00:31:22,745
0,330 330,510 510,810 810,1050 1050,1200
model's never seen anything like

1079
00:31:22,745 --> 00:31:24,590
0,180 180,485 1255,1545 1545,1710 1710,1845
this before,| and this is
|这就是所谓的模型不确定性，

1080
00:31:24,590 --> 00:31:25,990
0,105 105,240 240,510 510,890 1000,1400
what is called model uncertainty,|
|

1081
00:31:26,430 --> 00:31:27,680
0,260 260,380 380,605 605,1025 1025,1250
when the model hasn't seen
当模型没有看到足够多的数据点，

1082
00:31:27,680 --> 00:31:28,870
0,210 210,420 420,645 645,870 870,1190
enough data points| or cannot
|或者不能足够准确地估计输入分布的区域

1083
00:31:29,130 --> 00:31:30,365
0,305 305,530 530,800 800,1010 1010,1235
estimate that area of the

1084
00:31:30,365 --> 00:31:33,050
0,315 315,665 1465,1950 1950,2255 2305,2685
input distribution| accurately enough to
|以输出正确的预测时。

1085
00:31:33,050 --> 00:31:35,860
0,255 255,465 465,800 1270,1760
output a correct prediction.|
|

1086
00:31:37,320 --> 00:31:38,840
0,400 570,845 845,980 980,1240 1260,1520
So what would happen,| if
那么会发生什么情况，|如果我添加以下蓝色训练点，

1087
00:31:38,840 --> 00:31:40,445
0,150 150,440 490,780 780,1070 1270,1605
I added the following blue

1088
00:31:40,445 --> 00:31:43,210
0,300 300,665 895,1295 2005,2385 2385,2765
training points| to the areas
|到模型不确定性较高的数据集区域，

1089
00:31:43,230 --> 00:31:44,405
0,320 320,545 545,770 770,995 995,1175
of the data set with

1090
00:31:44,405 --> 00:31:46,580
0,275 295,695 745,1145 1795,2040 2040,2175
high model uncertainty,| do you
|你认为模型的不确定性会减少吗，

1091
00:31:46,580 --> 00:31:48,250
0,225 225,480 480,795 795,1190 1270,1670
think the model uncertainty would

1092
00:31:48,390 --> 00:31:50,840
0,400 840,1115 1115,1235 1235,1480
decrease,| raise your hand.|
|请举手。|

1093
00:31:52,020 --> 00:31:52,955
0,335 335,530 530,635 635,770 770,935
Does anyone think it would
有人认为它不会改变吗？

1094
00:31:52,955 --> 00:31:54,760
0,180 180,485
not change?|
|

1095
00:31:55,520 --> 00:31:56,710
0,400 450,695 695,845 845,1040 1040,1190
Okay, so yeah, most of
好的，是的，你们大多数人都是对的，

1096
00:31:56,710 --> 00:31:58,590
0,105 105,285 285,620 1030,1430 1480,1880
you were correct,| {model,uncertainty -}
|模型不确定性通常可以被减少，

1097
00:31:58,670 --> 00:32:00,295
0,290 290,580 600,980 980,1340 1340,1625
can typically be reduced| by
|通过将数据添加到任何区域，

1098
00:32:00,295 --> 00:32:02,050
0,270 270,555 555,875 925,1325 1435,1755
adding in data into any

1099
00:32:02,050 --> 00:32:03,670
0,285 285,555 555,860 970,1335 1335,1620
region,| but specifically regions with
|尤其是模型不确定性较高的区域。

1100
00:32:03,670 --> 00:32:05,980
0,255 255,585 585,980
high model uncertainty.|
|

1101
00:32:06,020 --> 00:32:07,795
0,275 275,550 1050,1325 1325,1550 1550,1775
And now, what happens if
现在，如果我们将这些蓝色数据点添加到这个数据集中会发生什么，

1102
00:32:07,795 --> 00:32:09,580
0,275 415,815 865,1260 1260,1545 1545,1785
we add these blue data

1103
00:32:09,580 --> 00:32:12,330
0,345 345,740 910,1310 2080,2415 2415,2750
points into this data set,|
|

1104
00:32:12,860 --> 00:32:14,550
0,365 365,695 695,1010 1010,1325 1325,1690
would anyone expect the data
有人预计数据的不确定性会降低吗，

1105
00:32:14,570 --> 00:32:16,360
0,400 510,910 930,1330 1410,1670 1670,1790
uncertainty to decrease,| {} raise
|请举手。

1106
00:32:16,360 --> 00:32:17,660
0,105 105,350
your hand.|
|

1107
00:32:18,920 --> 00:32:20,760
0,410 410,700 750,1055 1055,1360 1440,1840
That's correct, {so,data -} uncertainty
没错，数据不确定性是不可减少的，

1108
00:32:21,050 --> 00:32:23,155
0,335 335,1300 1500,1775 1775,1910 1910,2105
is irreducible,| in the {real,world
|在真实的世界中，

1109
00:32:23,155 --> 00:32:25,225
0,335 535,935 1225,1530 1530,1800 1800,2070
-},| the blue points and
|这张图像上的蓝点和嘈杂的红点对应于机器人传感器之类的东西，

1110
00:32:25,225 --> 00:32:26,710
0,305 385,870 870,1065 1065,1290 1290,1485
the noisy red points on

1111
00:32:26,710 --> 00:32:28,465
0,225 225,560 850,1250 1300,1560 1560,1755
this image correspond to things

1112
00:32:28,465 --> 00:32:30,445
0,335 535,840 840,1325 1555,1890 1890,1980
like robot sensors,| let's say
|比方说，我有一个机器人，

1113
00:32:30,445 --> 00:32:31,705
0,245 385,660 660,795 795,1005 1005,1260
I I have a robot,|
|

1114
00:32:31,705 --> 00:32:33,550
0,375 375,720 720,1115 1495,1740 1740,1845
that's trained to has a
它被训练成有一个传感器来测量深度，

1115
00:32:33,550 --> 00:32:35,740
0,470 520,780 780,1040 1090,1490 1600,2190
sensor that is making {measurements,of

1116
00:32:35,740 --> 00:32:38,755
0,320 610,1010 1390,1790 2410,2685 2685,3015
-} depth,| if the sensor
|如果传感器中有噪音，

1117
00:32:38,755 --> 00:32:40,060
0,180 180,405 405,585 585,845 895,1305
has noise in it,| there's
|我就无法向系统中添加更多数据来降低噪音，

1118
00:32:40,060 --> 00:32:41,125
0,210 210,480 480,675 675,825 825,1065
no way that I can

1119
00:32:41,125 --> 00:32:42,880
0,300 300,600 600,870 870,1205 1405,1755
add any more data into

1120
00:32:42,880 --> 00:32:44,170
0,255 255,555 555,855 855,1080 1080,1290
the system to reduce that

1121
00:32:44,170 --> 00:32:45,595
0,290 370,645 645,900 900,1215 1215,1425
noise,| unless I replace my
|除非我完全更换传感器。

1122
00:32:45,595 --> 00:32:46,880
0,375 375,695
sensor entirely.|
|

1123
00:32:49,770 --> 00:32:51,170
0,260 260,425 425,710 710,1000 1050,1400
So now let's assign some
因此，现在让我们为我们刚刚谈到的不确定性类型指定一些名称，

1124
00:32:51,170 --> 00:32:52,805
0,350 610,990 990,1230 1230,1395 1395,1635
names to the types of

1125
00:32:52,805 --> 00:32:53,915
0,330 330,600 600,735 735,885 885,1110
uncertainty that we just talked

1126
00:32:53,915 --> 00:32:55,865
0,335 655,975 975,1200 1200,1505 1645,1950
about,| {the,blue -} area or
|蓝色区域或数据高度不确定的区域，被称为偶然不确定性，

1127
00:32:55,865 --> 00:32:57,040
0,165 165,375 375,630 630,855 855,1175
the area of high data

1128
00:32:57,060 --> 00:32:59,300
0,400 540,845 845,1085 1085,1370 1370,2240
uncertainty is known as aleatoric

1129
00:32:59,300 --> 00:33:01,940
0,380 1180,1440 1440,1605 1605,2415 2415,2640
uncertainty,| it is irreducible, as
|它是不可减少的，正如我们刚才提到的，

1130
00:33:01,940 --> 00:33:03,530
0,150 150,300 300,590 1150,1425 1425,1590
we just mentioned,| and it
|它可以直接从数据中学习，

1131
00:33:03,530 --> 00:33:05,045
0,150 150,345 345,680 760,1160 1210,1515
can be directly learned from

1132
00:33:05,045 --> 00:33:06,200
0,305 325,600 600,795 795,945 945,1155
data,| which we'll talk about
|我们稍后会谈到这一点。

1133
00:33:06,200 --> 00:33:07,760
0,135 135,210 210,360 360,680
in a little bit.|
|

1134
00:33:08,650 --> 00:33:10,755
0,290 290,485 485,790 930,1330 1860,2105
The green areas of, the
我们所说的绿色盒子，绿色区域，

1135
00:33:10,755 --> 00:33:11,850
0,150 150,455 505,765 765,885 885,1095
green boxes that we talked

1136
00:33:11,850 --> 00:33:13,580
0,350 430,735 735,945 945,1250 1330,1730
about,| which were model uncertainty,
|也就是模型的不确定性，被称为认知不确定性，

1137
00:33:13,840 --> 00:33:15,860
0,305 305,560 560,860 860,1655 1655,2020
are known as epistemic uncertainty,|
|

1138
00:33:16,450 --> 00:33:17,865
0,290 290,560 560,920 920,1175 1175,1415
and this cannot be learned
这不能直接从数据中得知，

1139
00:33:17,865 --> 00:33:19,710
0,330 330,570 570,705 705,965 1465,1845
directly from the data,| {however\,,it
|然而，通过在我们的系统中向这些地区添加更多数据，可以减少这种情况。

1140
00:33:19,710 --> 00:33:21,120
0,300 300,555 555,840 840,1155 1155,1410
-} can be reduced by

1141
00:33:21,120 --> 00:33:22,350
0,255 255,510 510,750 750,1020 1020,1230
adding more data into our

1142
00:33:22,350 --> 00:33:24,680
0,290 340,675 675,915 915,1220
systems into these regions.|
|

1143
00:33:30,410 --> 00:33:31,900
0,400 600,860 860,1040 1040,1310 1310,1490
Okay, so first let's go
好的，那么首先让我们来看看偶然不确定性，

1144
00:33:31,900 --> 00:33:35,050
0,225 225,960 960,1310 2440,2840 2860,3150
through aleatoric uncertainty,| {so,the -}
|因此，估计偶然不确定性的目标是

1145
00:33:35,050 --> 00:33:37,590
0,240 240,590 970,1410 1410,2190 2190,2540
goal of estimating aleatoric uncertainty

1146
00:33:37,790 --> 00:33:39,040
0,305 305,485 485,725 725,1010 1010,1250
is| to learn a set
|学习一组与输入相对应的方差，

1147
00:33:39,040 --> 00:33:40,960
0,240 240,915 915,1155 1155,1460 1660,1920
of variances that correspond to

1148
00:33:40,960 --> 00:33:42,480
0,225 225,590
the input,|
|

1149
00:33:42,490 --> 00:33:43,305
0,260 260,395 395,560 560,710 710,815
keep in mind that we
请记住，我们看到的不是数据分布，

1150
00:33:43,305 --> 00:33:44,520
0,120 120,360 360,705 705,1005 1005,1215
are not looking at a

1151
00:33:44,520 --> 00:33:45,945
0,270 270,650 790,1095 1095,1260 1260,1425
data distribution| and we are
|因为我们人类不会估计差异，

1152
00:33:45,945 --> 00:33:47,340
0,225 225,495 495,720 720,990 990,1395
as humans are not estimating

1153
00:33:47,340 --> 00:33:48,765
0,165 165,585 585,930 930,1185 1185,1425
the variance,| we're training the
|我们训练模型来完成这项任务，

1154
00:33:48,765 --> 00:33:49,880
0,270 270,525 525,645 645,810 810,1115
model to do this task,|
|

1155
00:33:50,470 --> 00:33:51,530
0,305 305,470 470,605 605,770 770,1060
{and,so -} what that means
这通常意味着，

1156
00:33:51,610 --> 00:33:53,505
0,400 420,820 1260,1550 1550,1730 1730,1895
is typically,| when we train
|当我们训练一个模型时，

1157
00:33:53,505 --> 00:33:54,435
0,135 135,395 445,705 705,825 825,930
a model,| we give it
|我们给它一个输入 x ，

1158
00:33:54,435 --> 00:33:55,845
0,195 195,420 420,695 895,1185 1185,1410
an input x| and we
|我们期望一个输出 ŷ ，

1159
00:33:55,845 --> 00:33:57,380
0,270 270,600 600,915 915,1185 1185,1535
expect an output {ŷ -},|
|

1160
00:33:57,430 --> 00:33:58,545
0,260 260,395 395,530 530,875 875,1115
which is the prediction of
这是模型的预测，

1161
00:33:58,545 --> 00:33:59,860
0,135 135,395
the model,|
|

1162
00:34:00,100 --> 00:34:01,905
0,400 540,940 960,1250 1250,1520 1520,1805
now we also predict an
现在我们还预测了额外的 σ 平方，

1163
00:34:01,905 --> 00:34:03,660
0,305 475,960 960,1295 1375,1635 1635,1755
additional σ squared,| so we
|因此我们在我们的模型中增加了另一层，

1164
00:34:03,660 --> 00:34:04,830
0,225 225,525 525,840 840,1050 1050,1170
add another layer to our

1165
00:34:04,830 --> 00:34:06,290
0,290 700,945 945,1035 1035,1170 1170,1460
model,| {we,have -} the same
|我们有相同的输出大小，

1166
00:34:06,310 --> 00:34:07,875
0,275 275,550 660,965 965,1415 1415,1565
output size| that predicts a
|可以预测每个产出的方差。

1167
00:34:07,875 --> 00:34:09,940
0,420 420,615 615,875 985,1385
variance for every output.|
|

1168
00:34:11,230 --> 00:34:12,360
0,365 365,575 575,770 770,995 995,1130
So the reason why we
因此，我们这样做的原因是，

1169
00:34:12,360 --> 00:34:13,470
0,120 120,330 330,555 555,780 780,1110
do this is that,| we
|我们预计数据集中具有高数据不确定性的区域将具有更高的方差，

1170
00:34:13,470 --> 00:34:14,745
0,330 330,585 585,890 910,1155 1155,1275
expect that areas in our

1171
00:34:14,745 --> 00:34:16,130
0,210 210,465 465,720 720,1020 1020,1385
data set with high data

1172
00:34:16,180 --> 00:34:17,760
0,400 750,1040 1040,1220 1220,1370 1370,1580
uncertainty are going to have

1173
00:34:17,760 --> 00:34:19,640
0,300 300,860
higher variance,|
|

1174
00:34:20,650 --> 00:34:21,900
0,290 290,455 455,730 780,1070 1070,1250
and the crucial thing to
这里要记住的关键一点是，

1175
00:34:21,900 --> 00:34:23,190
0,240 240,590 670,930 930,1080 1080,1290
remember here is that,| this
|这种方差并不是恒定的，

1176
00:34:23,190 --> 00:34:25,620
0,465 465,750 750,1100 1120,1520 2080,2430
variance is not constant,| {it,depends
|这取决于 x 的值，

1177
00:34:25,620 --> 00:34:26,910
0,315 315,555 555,705 705,980 1000,1290
-} on the value of

1178
00:34:26,910 --> 00:34:28,590
0,290 580,870 870,1160 1210,1515 1515,1680
x,| we typically tend to
|我们通常倾向于认为方差是一个单独的数字，

1179
00:34:28,590 --> 00:34:29,730
0,135 135,270 270,705 705,945 945,1140
think of variance as a

1180
00:34:29,730 --> 00:34:31,380
0,225 225,510 510,765 765,1335 1335,1650
single number,| that parameterizes an
|它将整个分布参数化，

1181
00:34:31,380 --> 00:34:33,900
0,350 460,860 1570,1970 2020,2310 2310,2520
entire distribution,| however, in this
|然而，在这种情况下，

1182
00:34:33,900 --> 00:34:35,310
0,320 520,795 795,960 960,1140 1140,1410
case,| we may have areas
|我们的输入分布的区域可能具有非常高的方差，

1183
00:34:35,310 --> 00:34:36,735
0,240 240,465 465,780 780,1125 1125,1425
of our input distribution with

1184
00:34:36,735 --> 00:34:38,310
0,270 270,570 570,1145 1195,1455 1455,1575
really high variance,| and we
|而我们可能具有非常低的方差，

1185
00:34:38,310 --> 00:34:39,240
0,135 135,270 270,480 480,705 705,930
may have areas with very

1186
00:34:39,240 --> 00:34:41,295
0,255 255,770 1030,1410 1410,1680 1680,2055
{low,variance -},| so our variance
|所以我们的方差不能独立于输入，

1187
00:34:41,295 --> 00:34:42,780
0,285 285,635 715,1065 1065,1275 1275,1485
cannot be independent of the

1188
00:34:42,780 --> 00:34:44,130
0,315 315,540 540,780 780,1125 1125,1350
input,| and it depends on
|它取决于我们的输入 x 。

1189
00:34:44,130 --> 00:34:46,220
0,255 255,525 525,800
our input x.|
|

1190
00:34:47,250 --> 00:34:48,365
0,400 420,695 695,815 815,935 935,1115
So now that we have
所以现在我们有了这个模型，

1191
00:34:48,365 --> 00:34:49,220
0,210 210,465 465,660 660,750 750,855
this model,| we have an
|我们有了一个额外的层，

1192
00:34:49,220 --> 00:34:50,705
0,260 280,675 675,1035 1035,1245 1245,1485
extra layer attached to it,|
|

1193
00:34:50,705 --> 00:34:52,145
0,300 300,540 540,735 735,1185 1185,1440
{in,addition -} to predicting {ŷ
除了预测 ŷ ，我们还预测了 σ 平方，

1194
00:34:52,145 --> 00:34:53,405
0,270 270,570 570,810 810,1020 1020,1260
-}, we also predict a

1195
00:34:53,405 --> 00:34:55,340
0,345 345,695 1405,1680 1680,1785 1785,1935
σ squared,| how do we
|我们如何训练这个模型。

1196
00:34:55,340 --> 00:34:57,220
0,225 225,420 420,710
train this model.|
|

1197
00:34:58,390 --> 00:35:00,120
0,305 305,560 560,860 860,1210 1440,1730
Our current loss function does
我们的当前损失函数不考虑任何点的变化，

1198
00:35:00,120 --> 00:35:01,560
0,210 210,435 435,720 720,1020 1020,1440
not take into account variance

1199
00:35:01,560 --> 00:35:02,850
0,165 165,390 390,740 820,1110 1110,1290
at any point,| {this,is -}
|这是用于训练回归模型的典型均方误差损失函数，

1200
00:35:02,850 --> 00:35:04,230
0,180 180,470 490,825 825,1110 1110,1380
your typical mean squared error

1201
00:35:04,230 --> 00:35:05,430
0,285 285,600 600,825 825,960 960,1200
loss function that is used

1202
00:35:05,430 --> 00:35:07,230
0,225 225,390 390,750 750,1130 1540,1800
to train regression models,| and
|不可能通过这个损失函数的训练，

1203
00:35:07,230 --> 00:35:08,685
0,240 240,420 420,740 910,1245 1245,1455
there's no way training from

1204
00:35:08,685 --> 00:35:10,110
0,165 165,375 375,695 985,1260 1260,1425
this loss function,| that we
|我们知道我们估计的方差是否准确。

1205
00:35:10,110 --> 00:35:11,295
0,195 195,480 480,810 810,1020 1020,1185
can learn whether or not

1206
00:35:11,295 --> 00:35:12,510
0,165 165,465 465,615 615,900 900,1215
the variance that we're estimating

1207
00:35:12,510 --> 00:35:14,240
0,315 315,710
is accurate.|
|

1208
00:35:15,210 --> 00:35:16,445
0,365 365,620 620,845 845,1025 1025,1235
So in addition to adding
因此，除了增加另一层来估计偶然不确定性外，

1209
00:35:16,445 --> 00:35:18,665
0,285 285,605 805,1200 1200,1485 1485,2220
another layer to estimate aleatoric

1210
00:35:18,665 --> 00:35:20,840
0,335 865,1265 1435,1800 1800,2025 2025,2175
uncertainty correctly,| we also have
|我们还必须改变我们的损失函数。

1211
00:35:20,840 --> 00:35:22,030
0,165 165,390 390,630 630,855 855,1190
to change our loss function.|
|

1212
00:35:24,340 --> 00:35:25,700
0,350 350,545 545,725 725,1025 1025,1360
So the mean squared error
所以均方误差实际上学习的是多元高斯函数，

1213
00:35:25,990 --> 00:35:29,595
0,400 1500,2015 2015,2285 2285,2650 2850,3605
actually learns {} a multivariate

1214
00:35:29,595 --> 00:35:31,215
0,450 450,630 630,795 795,1085 1285,1620
Gaussian| with a mean {yi
|它有一个均值 yi ，一个常方差，

1215
00:35:31,215 --> 00:35:33,480
0,335 505,855 855,1205 1285,1895 2005,2265
-} and constant variance,| and
|我们想把这个损失函数推广到，当我们没有常方差时，

1216
00:35:33,480 --> 00:35:34,650
0,135 135,300 300,480 480,975 975,1170
we want to generalize this

1217
00:35:34,650 --> 00:35:35,910
0,225 225,560 730,975 975,1095 1095,1260
loss function to when we

1218
00:35:35,910 --> 00:35:37,460
0,165 165,315 315,570 570,950 970,1550
{don't -} have constant variance,|
|

1219
00:35:40,010 --> 00:35:40,840
0,290 290,425 425,545 545,695 695,830
and the way we do
我们做到这一点的方法是将损失函数改为负对数可能性，

1220
00:35:40,840 --> 00:35:41,905
0,195 195,420 420,615 615,870 870,1065
this is by changing the

1221
00:35:41,905 --> 00:35:43,120
0,180 180,515 625,855 855,960 960,1215
loss function to the negative

1222
00:35:43,120 --> 00:35:44,875
0,300 300,950 1210,1470 1470,1590 1590,1755
log likelihood,| {we,can -} think
|我们现在可以认为这是均方误差损失到非常数方差的推广，

1223
00:35:44,875 --> 00:35:45,850
0,210 210,420 420,585 585,780 780,975
about this for now as

1224
00:35:45,850 --> 00:35:47,425
0,135 135,890 910,1185 1185,1335 1335,1575
a generalization of the mean

1225
00:35:47,425 --> 00:35:49,060
0,300 300,570 570,935 1075,1365 1365,1635
squared error loss to non

1226
00:35:49,060 --> 00:35:51,235
0,375 375,1160 1660,1920 1920,2055 2055,2175
constant variances,| so now that
|现在我们在损失函数中有了 σ 平方项，

1227
00:35:51,235 --> 00:35:52,285
0,105 105,225 225,390 390,750 750,1050
we have a σ squared

1228
00:35:52,285 --> 00:35:53,550
0,330 330,555 555,720 720,945 945,1265
term in our loss function,|
|

1229
00:35:53,990 --> 00:35:55,855
0,260 260,470 470,785 785,1150 1290,1865
we can determine how accurately
我们可以确定我们预测的 σ 和 y 对分布精度，

1230
00:35:55,855 --> 00:35:57,085
0,225 225,600 600,840 840,1050 1050,1230
the σ and the y

1231
00:35:57,085 --> 00:35:59,080
0,165 165,360 360,905 1135,1740 1740,1995
that we're predicting,| parameterize {the,distribution
|对我们的输入分布进行参数化。

1232
00:35:59,080 --> 00:36:00,750
0,350 580,885 885,1050 1050,1290 1290,1670
-} that is our input.|
|

1233
00:36:04,510 --> 00:36:05,715
0,400 480,755 755,890 890,1025 1025,1205
So now that we know
现在我们知道了如何估计偶然不确定性，

1234
00:36:05,715 --> 00:36:07,370
0,165 165,425 475,735 735,1335 1335,1655
how to estimate aleatoric uncertainty,|
|

1235
00:36:07,750 --> 00:36:08,760
0,365 365,470 470,575 575,740 740,1010
let's look at a real
让我们来看一个真实世界的例子，

1236
00:36:08,760 --> 00:36:11,210
0,330 330,710 1660,1950 1950,2145 2145,2450
world example,| {for,this -} task,
|在本任务中，我们将重点介绍语义分割，

1237
00:36:11,230 --> 00:36:13,430
0,350 350,590 590,920 920,1550 1550,2200
we'll focus on semantic segmentation,|
|

1238
00:36:13,900 --> 00:36:14,960
0,260 260,410 410,590 590,770 770,1060
which is when we label
也就是用相应的类标记图像的每个像素，

1239
00:36:14,980 --> 00:36:16,430
0,400 420,920 920,1070 1070,1190 1190,1450
every pixel of an image

1240
00:36:16,510 --> 00:36:19,095
0,260 260,440 440,1145 1145,1450 2310,2585
with its corresponding class,| we
|我们这样做是为了理解场景，

1241
00:36:19,095 --> 00:36:20,565
0,135 135,375 375,675 675,995 1075,1470
do this for scene understanding|
|

1242
00:36:20,565 --> 00:36:21,615
0,330 330,540 540,660 660,810 810,1050
and because it is more
因为它比典型的目标检测算法更细粒度。

1243
00:36:21,615 --> 00:36:22,790
0,300 300,615 615,780 780,900 900,1175
fine grained than a typical

1244
00:36:22,960 --> 00:36:25,120
0,320 320,755 755,1300
object detection algorithm.|
|

1245
00:36:25,300 --> 00:36:26,660
0,320 320,605 605,950 950,1100 1100,1360
So the inputs of this
所以这个数据集的输入就是所谓的城市景观数据集，

1246
00:36:26,710 --> 00:36:27,915
0,245 245,365 365,575 575,905 905,1205
to this data set are

1247
00:36:27,915 --> 00:36:28,890
0,210 210,450 450,735 735,855 855,975
known as it's from a

1248
00:36:28,890 --> 00:36:30,750
0,180 180,390 390,615 615,1280 1510,1860
data set called citycapes,| and
|它的输入是场景的 RGB 图像，

1249
00:36:30,750 --> 00:36:32,510
0,315 315,630 630,870 870,1485 1485,1760
the inputs are RGB images

1250
00:36:32,530 --> 00:36:35,085
0,335 335,670 1500,1760 1760,2225 2225,2555
of scenes,| {the,labels -} are
|标签是整个图像的像素级别注释，

1251
00:36:35,085 --> 00:36:37,065
0,450 450,705 705,1295 1435,1740 1740,1980
pixel wise annotations of this

1252
00:36:37,065 --> 00:36:38,690
0,285 285,635 715,1020 1020,1275 1275,1625
entire image,| of which label
|每个像素都属于它的标签，

1253
00:36:38,710 --> 00:36:41,205
0,380 380,860 860,1235 1235,1540 2220,2495
every pixel belongs to,| and
|并且输出试图模仿标签，

1254
00:36:41,205 --> 00:36:42,345
0,255 255,600 600,780 780,930 930,1140
the outputs try to mimic

1255
00:36:42,345 --> 00:36:43,500
0,120 120,510 510,735 735,915 915,1155
the labels,| there are also
|也有预测的像素级别掩码，

1256
00:36:43,500 --> 00:36:46,250
0,390 390,750 750,960 960,1610 2350,2750
predicted pixel wise masks,| so
|所以为什么我们会期望这个数据集具有高度的自然偶然不确定性，

1257
00:36:46,870 --> 00:36:48,105
0,335 335,545 545,755 755,1010 1010,1235
why would we expect that

1258
00:36:48,105 --> 00:36:49,640
0,225 225,495 495,780 780,1115 1135,1535
this data set has high

1259
00:36:49,810 --> 00:36:52,005
0,290 290,950 950,1270 1650,1955 1955,2195
natural aleatoric uncertainty,| and which
|你认为这个数据集的哪些部分会有偶然不确定性。

1260
00:36:52,005 --> 00:36:53,060
0,255 255,420 420,540 540,735 735,1055
parts of this data set

1261
00:36:53,140 --> 00:36:54,000
0,230 230,335 335,500 500,680 680,860
do you think would have

1262
00:36:54,000 --> 00:36:55,700
0,585 585,890
aleatoric uncertainty.|
|

1263
00:36:58,880 --> 00:37:01,030
0,365 365,970 1020,1415 1415,1760 1760,2150
Because labeling every single pixel
因为标记图像的每一个像素都是一项劳动密集型的任务，

1264
00:37:01,030 --> 00:37:02,080
0,105 105,210 210,435 435,750 750,1050
of an image is such

1265
00:37:02,080 --> 00:37:03,760
0,210 210,390 390,825 825,1190 1420,1680
a labor intensive task,| and
|而且很难做到准确，

1266
00:37:03,760 --> 00:37:05,020
0,240 240,465 465,765 765,1080 1080,1260
it's also very hard to

1267
00:37:05,020 --> 00:37:06,775
0,260 280,860 1060,1320 1320,1515 1515,1755
do accurately,| we would expect
|所以我们预计这张图像中对象之间的边界具有高度的偶然不确定性，

1268
00:37:06,775 --> 00:37:08,580
0,180 180,330 330,905 985,1385 1405,1805
that the boundaries between, {}

1269
00:37:09,140 --> 00:37:10,885
0,400 930,1205 1205,1340 1340,1475 1475,1745
between objects in this image

1270
00:37:10,885 --> 00:37:13,300
0,315 315,570 570,1260 1260,1595 2155,2415
have high aleatoric uncertainty,| and
|这正是我们所看到的，

1271
00:37:13,300 --> 00:37:14,490
0,255 255,495 495,720 720,885 885,1190
that's exactly what we see,|
|

1272
00:37:14,570 --> 00:37:15,535
0,260 260,410 410,575 575,710 710,965
{if,you -} train a model
如果你训练一个模型来预测这个数据集上的偶然不确定性，

1273
00:37:15,535 --> 00:37:17,365
0,240 240,420 420,1065 1065,1385 1525,1830
to predict aleatoric uncertainty on

1274
00:37:17,365 --> 00:37:19,090
0,195 195,405 405,725 1015,1515 1515,1725
this data set,| corners and
|角落和边界有最高的偶然不确定性，

1275
00:37:19,090 --> 00:37:20,905
0,510 510,735 735,900 900,1170 1170,1815
boundaries have the highest aleatoric

1276
00:37:20,905 --> 00:37:22,540
0,365 805,1080 1080,1275 1275,1470 1470,1635
uncertainty,| because even if your
|因为即使你的像素像一行或一列分开，

1277
00:37:22,540 --> 00:37:23,815
0,360 360,570 570,795 795,1035 1035,1275
pixels are like one row

1278
00:37:23,815 --> 00:37:25,230
0,255 255,495 495,720 720,1035 1035,1415
off or one column off,|
|

1279
00:37:25,430 --> 00:37:26,875
0,290 290,755 755,1070 1070,1310 1310,1445
that introduces noise into the
这也会向模型中引入噪音，

1280
00:37:26,875 --> 00:37:27,620
0,245
model,|
|

1281
00:37:27,620 --> 00:37:28,850
0,210 210,420 420,675 675,915 915,1230
the model can still learn
在这种噪音面前，模型仍然可以学习，

1282
00:37:28,850 --> 00:37:29,675
0,240 240,360 360,510 510,660 660,825
in the face of this

1283
00:37:29,675 --> 00:37:31,130
0,305 415,660 660,780 780,1055 1075,1455
noise,| but it does exist
|但它确实存在，不能减少。

1284
00:37:31,130 --> 00:37:32,320
0,225 225,375 375,660 660,855 855,1190
and it can't be reduced.|
|

1285
00:37:36,240 --> 00:37:37,640
0,400 600,875 875,1010 1010,1145 1145,1400
So now that we know
既然我们已经了解了数据不确定性或偶然不确定性，

1286
00:37:37,640 --> 00:37:39,935
0,315 315,650 700,1100 1330,1590 1590,2295
about data uncertainty or aleatoric

1287
00:37:39,935 --> 00:37:41,915
0,335 1015,1395 1395,1545 1545,1785 1785,1980
uncertainty,| let's move on to
|让我们继续学习认知不确定性。

1288
00:37:41,915 --> 00:37:44,555
0,245 265,615 615,1365 1365,1715 2365,2640
learning about epistemic uncertainty.| {As,a
|总而言之，认知不确定性可以最好地描述为模型本身的不确定性，

1289
00:37:44,555 --> 00:37:46,760
0,150 150,575 715,1485 1485,1835 1945,2205
-} recap, epistemic uncertainty can

1290
00:37:46,760 --> 00:37:48,340
0,210 210,510 510,840 840,1200 1200,1580
best be described as uncertainty

1291
00:37:48,420 --> 00:37:50,375
0,305 305,485 485,760 900,1300 1710,1955
in the model itself,| and
|并且可以通过在模型中添加数据来减少。

1292
00:37:50,375 --> 00:37:51,755
0,120 120,315 315,960 960,1140 1140,1380
it is reducible by adding

1293
00:37:51,755 --> 00:37:53,720
0,345 345,570 570,675 675,935
data to the model.|
|

1294
00:37:56,360 --> 00:37:58,405
0,365 365,620 620,1220 1220,1570 1680,2045
So with epistemic uncertainty,| essentially
因此，在认知不确定性的情况下，|本质上我们试图问的是，

1295
00:37:58,405 --> 00:37:59,425
0,225 225,420 420,600 600,780 780,1020
what we're trying to ask

1296
00:37:59,425 --> 00:38:01,120
0,395 565,885 885,1080 1080,1350 1350,1695
is,| is the model {unconfident
|这个模型对预测有信心吗。

1297
00:38:01,120 --> 00:38:03,600
0,560 640,945 945,1125 1125,1490 2080,2480
-} about a prediction.| So
|一个非常简单和非常聪明的方法是，

1298
00:38:03,650 --> 00:38:05,370
0,275 275,530 530,910 930,1325 1325,1720
a really simple and very

1299
00:38:05,510 --> 00:38:06,540
0,320 320,500 500,620 620,755 755,1030
smart way to do this

1300
00:38:06,620 --> 00:38:08,320
0,400 720,1100 1100,1250 1250,1460 1460,1700
is,| let's say I train
|假设我用随机初始化多次训练同一个网络，

1301
00:38:08,320 --> 00:38:10,140
0,225 225,465 465,800 1000,1400 1420,1820
the same network multiple times

1302
00:38:10,160 --> 00:38:12,520
0,305 305,575 575,1300 1770,2120 2120,2360
with random initializations| and I
|我要求它预测，我在相同输入调用它。

1303
00:38:12,520 --> 00:38:13,690
0,225 225,465 465,630 630,870 870,1170
ask it to predict the

1304
00:38:13,690 --> 00:38:15,055
0,285 285,650 760,1050 1050,1215 1215,1365
exact, I call it on

1305
00:38:15,055 --> 00:38:17,335
0,150 150,425 445,845 1285,1685 1915,2280
the same input.| {So,let's -}
|假设我给模型 1 一个完全相同的输入，

1306
00:38:17,335 --> 00:38:18,355
0,90 90,225 225,420 420,690 690,1020
say I give model 1

1307
00:38:18,355 --> 00:38:19,690
0,255 255,465 465,750 750,1050 1050,1335
the exact same input,| and
|蓝色的 X 是这个模型的输出，

1308
00:38:19,690 --> 00:38:21,040
0,300 300,555 555,855 855,1125 1125,1350
the blue X is the

1309
00:38:21,040 --> 00:38:22,800
0,195 195,300 300,465 465,770
output of this model,|
|

1310
00:38:23,270 --> 00:38:24,310
0,275 275,545 545,815 815,935 935,1040
and then I do the
然后我再用模型 2 做同样的事情，

1311
00:38:24,310 --> 00:38:25,465
0,135 135,315 315,615 615,900 900,1155
same thing again with model

1312
00:38:25,465 --> 00:38:27,895
0,365 1525,1785 1785,1965 1965,2205 2205,2430
2,| and then again with
|然后用模型 3 ，

1313
00:38:27,895 --> 00:38:29,560
0,270 270,635
model 3,|
|

1314
00:38:29,630 --> 00:38:30,900
0,305 305,500 500,665 665,905 905,1270
and again with model 4,|
然后在模型 4 ，|

1315
00:38:31,100 --> 00:38:32,515
0,320 320,640 660,995 995,1205 1205,1415
these models all have the
这些模型都具有完全相同的超参数，完全相同的体系结构，

1316
00:38:32,515 --> 00:38:34,360
0,285 285,555 555,945 945,1445 1555,1845
exact same hyper parameters, the

1317
00:38:34,360 --> 00:38:35,890
0,225 225,450 450,740 1090,1350 1350,1530
exact same architecture,| and their
|相同的训练方式，

1318
00:38:35,890 --> 00:38:37,110
0,240 240,420 420,585 585,855 855,1220
train in the same way,|
|

1319
00:38:37,310 --> 00:38:38,800
0,275 275,545 545,940 1020,1295 1295,1490
{the,only -} difference between them
它们之间的唯一区别是，

1320
00:38:38,800 --> 00:38:40,030
0,210 210,420 420,660 660,1035 1035,1230
is that,| their weights are
|它们的权重都是随机初始化的，

1321
00:38:40,030 --> 00:38:42,190
0,240 240,780 780,1340 1510,1890 1890,2160
all randomly initialized,| so where
|因此它们从哪里开始是不同的。

1322
00:38:42,190 --> 00:38:43,530
0,210 210,495 495,810 810,1050 1050,1340
they start from is different.|
|

1323
00:38:45,330 --> 00:38:46,280
0,275 275,395 395,575 575,800 800,950
And the reason why we
我们之所以能用它来确定认知的不确定性，

1324
00:38:46,280 --> 00:38:47,440
0,135 135,330 330,570 570,825 825,1160
can use this to determine

1325
00:38:47,850 --> 00:38:49,970
0,770 770,1120 1470,1790 1790,1985 1985,2120
epistemic uncertainty is,| because we
|是因为我们预计，在我们的网络中有熟悉的输入时，

1326
00:38:49,970 --> 00:38:52,000
0,225 225,590 760,1080 1080,1400 1630,2030
would expect, that with familiar

1327
00:38:52,350 --> 00:38:54,260
0,500 500,725 725,860 860,1150 1530,1910
inputs in our network,| our
|我们的网络应该都收敛到相同的答案，

1328
00:38:54,260 --> 00:38:55,940
0,375 375,645 645,920 970,1485 1485,1680
networks should all converge to

1329
00:38:55,940 --> 00:38:57,410
0,195 195,390 390,600 600,920 1210,1470
around the same answer,| and
|我们应该会看到对数或我们预测的输出的很小的变化。

1330
00:38:57,410 --> 00:38:58,670
0,150 150,330 330,540 540,860 910,1260
we should see very little

1331
00:38:58,670 --> 00:39:00,485
0,570 570,825 825,1130 1270,1545 1545,1815
variance in the the log

1332
00:39:00,485 --> 00:39:01,610
0,255 255,480 480,780 780,930 930,1125
or the outputs that we're

1333
00:39:01,610 --> 00:39:03,100
0,530
predicting.|
|

1334
00:39:03,100 --> 00:39:04,360
0,380 430,690 690,810 810,1020 1020,1260
However, if a model has
然而，如果一个模型以前从未见过特定的输入，

1335
00:39:04,360 --> 00:39:05,665
0,255 255,495 495,645 645,920 1030,1305
never seen a specific input

1336
00:39:05,665 --> 00:39:06,985
0,275 445,720 720,960 960,1170 1170,1320
before,| or that input is
|或者该输入很难学习，

1337
00:39:06,985 --> 00:39:08,680
0,285 285,615 615,825 825,1085 1405,1695
very hard to learn,| all
|所有这些模型应该预测略有不同的答案，

1338
00:39:08,680 --> 00:39:10,165
0,165 165,345 345,650 700,1095 1095,1485
of these models should predict

1339
00:39:10,165 --> 00:39:11,980
0,390 390,720 720,1055 1405,1680 1680,1815
slightly different answers,| and the
|并且它们的方差应该高于它们预测类似输入的情况。

1340
00:39:11,980 --> 00:39:13,030
0,360 360,525 525,720 720,900 900,1050
variance of them should be

1341
00:39:13,030 --> 00:39:14,095
0,290 460,720 720,840 840,945 945,1065
higher than if they were

1342
00:39:14,095 --> 00:39:16,640
0,420 420,660 660,995 1195,1595
predicting a similar input.|
|

1343
00:39:18,520 --> 00:39:20,010
0,380 380,710 710,905 905,1385 1385,1490
So creating an ensemble of
因此，创建一个网络集合非常简单，

1344
00:39:20,010 --> 00:39:21,630
0,260 280,585 585,825 825,1160 1300,1620
networks is quite simple, quite

1345
00:39:21,630 --> 00:39:23,730
0,320 580,980 1300,1575 1575,1785 1785,2100
simple,| {you,start -} out with
|你从定义你想要的 num_ensembles 开始，

1346
00:39:23,730 --> 00:39:25,065
0,420 420,525 525,675 675,825 825,1335
defining the {num_ensembles - -}

1347
00:39:25,065 --> 00:39:26,265
0,165 165,425 445,765 765,1005 1005,1200
you want,| you create them
|你以完全相同的方式创建它们，

1348
00:39:26,265 --> 00:39:27,560
0,195 195,435 435,720 720,990 990,1295
all the exact same way,|
|

1349
00:39:27,730 --> 00:39:28,635
0,260 260,395 395,560 560,740 740,905
and then you fit them
然后你把它们都放在相同的训练数据。

1350
00:39:28,635 --> 00:39:29,850
0,195 195,390 390,585 585,885 885,1215
all on the same training

1351
00:39:29,850 --> 00:39:32,540
0,350 370,770 1090,1410 1410,1730
data and training data.|
|

1352
00:39:33,090 --> 00:39:34,750
0,275 275,440 440,730 1020,1340 1340,1660
And then afterwards, when at
然后，当在推理时，

1353
00:39:34,770 --> 00:39:37,280
0,530 530,880 1170,1505 1505,1840 2220,2510
inference time,| we call all
|我们调用所有模型，集合中的每个模型，关于我们的特定输入，

1354
00:39:37,280 --> 00:39:38,480
0,150 150,270 270,530 550,900 900,1200
of the models, every model

1355
00:39:38,480 --> 00:39:40,460
0,240 240,390 390,1040 1240,1620 1620,1980
in the ensemble on our

1356
00:39:40,460 --> 00:39:43,115
0,380 550,950 1720,2010 2010,2300 2380,2655
specific input,| and then we
|然后我们可以将我们的新预测视为所有集合的平均值，

1357
00:39:43,115 --> 00:39:44,710
0,180 180,485 745,1035 1035,1215 1215,1595
can treat our new prediction

1358
00:39:44,730 --> 00:39:46,235
0,290 290,470 470,760 900,1250 1250,1505
as the average of all

1359
00:39:46,235 --> 00:39:47,855
0,180 180,315 315,965 1045,1365 1365,1620
of the ensembles,| {this,results -}
|这通常会产生更稳健和更准确的预测，

1360
00:39:47,855 --> 00:39:49,145
0,180 180,285 285,540 540,930 930,1290
in a usually more robust

1361
00:39:49,145 --> 00:39:51,515
0,345 345,615 615,965 1705,2100 2100,2370
and accurate prediction,| and we
|我们可以将不确定性视为所有这些预测的方差，

1362
00:39:51,515 --> 00:39:53,090
0,165 165,345 345,585 585,935 1225,1575
can treat the uncertainty as

1363
00:39:53,090 --> 00:39:54,290
0,255 255,705 705,900 900,1065 1065,1200
the variance of all of

1364
00:39:54,290 --> 00:39:57,125
0,165 165,710 1780,2160 2160,2490 2490,2835
these predictions,| again, remember that
|记住，如果我们看到熟悉的输入或具有低认知不确定性的输入，

1365
00:39:57,125 --> 00:39:58,685
0,285 285,450 450,645 645,965 1135,1560
if we saw familiar inputs

1366
00:39:58,685 --> 00:40:00,185
0,270 270,540 540,690 690,885 885,1500
or inputs with low epistemic

1367
00:40:00,185 --> 00:40:01,745
0,335 685,960 960,1170 1170,1410 1410,1560
uncertainty,| we should expect to
|我们应该预期会有很小的方差，

1368
00:40:01,745 --> 00:40:03,470
0,165 165,450 450,750 750,1295 1465,1725
have very {little,variance -},| and
|如果我们有一个非常不熟悉的输入，或者是分布之外的东西，

1369
00:40:03,470 --> 00:40:04,490
0,135 135,285 285,495 495,765 765,1020
if we had a very

1370
00:40:04,490 --> 00:40:06,185
0,750 750,1050 1050,1245 1245,1470 1470,1695
unfamiliar input or something that

1371
00:40:06,185 --> 00:40:07,565
0,180 180,375 375,630 630,995 1105,1380
was out of distribution| or
|或者是模型以前没有见过的东西，

1372
00:40:07,565 --> 00:40:08,690
0,195 195,360 360,555 555,945 945,1125
something the model hasn't seen

1373
00:40:08,690 --> 00:40:10,190
0,290 610,885 885,1035 1035,1215 1215,1500
before,| we should have very
|我们应该有非常高的认知不确定性或方差。

1374
00:40:10,190 --> 00:40:12,250
0,300 300,960 960,1280 1300,1575 1575,2060
high epistemic uncertainty or variance.|
|

1375
00:40:15,260 --> 00:40:16,750
0,400 540,905 905,1010 1010,1235 1235,1490
So what's the problem with
那么，这有什么问题，

1376
00:40:16,750 --> 00:40:18,460
0,285 285,615 615,950 1270,1560 1560,1710
this,| can anyone raise their
|有没有人能举手告诉我，

1377
00:40:18,460 --> 00:40:19,060
0,135 135,270 270,390 390,510 510,600
hand and tell me,| what
|训练一组网络有什么问题。

1378
00:40:19,060 --> 00:40:19,990
0,90 90,300 300,525 525,735 735,930
a problem with training an

1379
00:40:19,990 --> 00:40:22,420
0,465 465,570 570,825 825,1220
ensemble of networks is.|
|

1380
00:40:25,440 --> 00:40:26,510
0,290 290,485 485,620 620,980 980,1070
So training an ensemble of
训练一组网络是计算成本高昂的，

1381
00:40:26,510 --> 00:40:28,330
0,255 255,600 600,915 915,1440 1440,1820
networks is really compute expensive,|
|

1382
00:40:29,250 --> 00:40:30,425
0,335 335,545 545,695 695,935 935,1175
{even,if -} your model is
即使你的模型不是很大，

1383
00:40:30,425 --> 00:40:32,195
0,195 195,435 435,755 985,1385 1405,1770
not very large,| training five
|训练五个或十个副本往往会占用计算和时间，

1384
00:40:32,195 --> 00:40:33,260
0,390 390,525 525,660 660,840 840,1065
copies of it or ten

1385
00:40:33,260 --> 00:40:35,140
0,360 360,480 480,740 1120,1575 1575,1880
copies of it tends to,

1386
00:40:35,580 --> 00:40:37,100
0,395 395,665 665,830 830,1250 1250,1520
it takes up compute and

1387
00:40:37,100 --> 00:40:38,915
0,320 610,1010 1030,1410 1410,1575 1575,1815
time,| and that's just not
|当我们在特定任务上进行训练时，这是不太可行的。

1388
00:40:38,915 --> 00:40:40,660
0,285 285,905 955,1260 1260,1485 1485,1745
really feasible when we're training

1389
00:40:41,460 --> 00:40:43,660
0,320 320,620 620,1000
on specific tasks.|
|

1390
00:40:43,730 --> 00:40:45,580
0,400 450,755 755,1060 1350,1685 1685,1850
However, the key insight for
然而，一组的关键洞察力是，

1391
00:40:45,580 --> 00:40:47,430
0,615 615,870 870,1065 1065,1275 1275,1850
ensembles is that,| by introducing
|通过在我们的网络中引入某种随机性或随机性的方法，

1392
00:40:47,480 --> 00:40:49,375
0,320 320,635 635,950 950,1640 1640,1895
some method of randomness or

1393
00:40:49,375 --> 00:40:51,835
0,960 960,1245 1245,1425 1425,1715 2155,2460
stochasticity into our networks,| we're
|我们能够估计认知的不确定性，

1394
00:40:51,835 --> 00:40:53,790
0,195 195,545 565,885 885,1590 1590,1955
able to estimate epistemic uncertainty,|
|

1395
00:40:54,740 --> 00:40:56,215
0,400 570,875 875,1055 1055,1190 1190,1475
{so,another -} way that we've
因此，我们已经看到的另一种将随机性引入网络的方法是使用 dropout 层，

1396
00:40:56,215 --> 00:40:59,335
0,305 565,965 1405,2040 2040,2940 2940,3120
seen about introducing stochasticity into

1397
00:40:59,335 --> 00:41:01,090
0,305 445,720 720,870 870,1145 1255,1755
networks is by using dropout

1398
00:41:01,090 --> 00:41:03,235
0,440 940,1290 1290,1455 1455,1800 1800,2145
layers,| we've seen dropout layers
|我们把 dropout 层看作是一种减少过度拟合的方法，

1399
00:41:03,235 --> 00:41:04,435
0,165 165,300 300,555 555,885 885,1200
as a method of reducing

1400
00:41:04,435 --> 00:41:06,625
0,755 985,1260 1260,1425 1425,1950 1950,2190
overfitting,| because we randomly drop
|因为我们随机地丢弃了层中的不同节点，

1401
00:41:06,625 --> 00:41:08,515
0,285 285,635 1135,1590 1590,1725 1725,1890
out different nodes in our,

1402
00:41:08,515 --> 00:41:09,940
0,165 165,315 315,605 865,1155 1155,1425
in our layer,| and then
|然后我们继续通过它们传播信息，

1403
00:41:09,940 --> 00:41:11,530
0,345 345,630 630,810 810,1260 1260,1590
we continue to propagate information

1404
00:41:11,530 --> 00:41:12,625
0,225 225,435 435,630 630,780 780,1095
through them| and it prevents
|这阻止了模型记忆数据。

1405
00:41:12,625 --> 00:41:15,060
0,330 330,615 615,1110 1110,1475
models from memorizing data.|
|

1406
00:41:15,190 --> 00:41:17,925
0,400 1080,1400 1400,1655 1655,1990 2370,2735
However, in the {} case
然而，在认知不确定性的情况下，

1407
00:41:17,925 --> 00:41:20,025
0,270 270,930 930,1265 1735,1980 1980,2100
of epistemic uncertainty,| we can
|我们可以在模型中的每一层之后添加 dropout 层，

1408
00:41:20,025 --> 00:41:21,620
0,225 225,630 630,945 945,1215 1215,1595
add dropout layers after every

1409
00:41:21,640 --> 00:41:23,270
0,400 600,965 965,1190 1190,1340 1340,1630
single layer in our model,|
|

1410
00:41:23,620 --> 00:41:25,080
0,290 290,500 500,820 1020,1265 1265,1460
and in addition, we can
此外，我们可以在测试时保持这些 dropout 层处于启用状态，

1411
00:41:25,080 --> 00:41:26,415
0,225 225,420 420,750 750,1005 1005,1335
keep these dropout layers enabled

1412
00:41:26,415 --> 00:41:28,470
0,300 300,585 585,935 1105,1505 1795,2055
at test time,| {usually,we -}
|通常，我们在测试时不会使 dropout 层处于启用状态，

1413
00:41:28,470 --> 00:41:29,745
0,255 255,420 420,765 765,1020 1020,1275
don't keep dropout layers enabled

1414
00:41:29,745 --> 00:41:30,675
0,210 210,405 405,630 630,825 825,930
at test time,| because we
|因为我们不想在推理时丢失有关网络进程或权重的任何信息，

1415
00:41:30,675 --> 00:41:31,610
0,210 210,315 315,435 435,615 615,935
don't want to lose any

1416
00:41:31,720 --> 00:41:33,800
0,400 480,815 815,1150 1260,1505 1505,2080
information about the the network's

1417
00:41:34,900 --> 00:41:36,390
0,395 395,650 650,845 845,1265 1265,1490
process or any weights when

1418
00:41:36,390 --> 00:41:38,445
0,195 195,345 345,765 765,1100 1660,2055
we're at inference time,| however,
|然而，当我们估计认知不确定性时，

1419
00:41:38,445 --> 00:41:40,520
0,255 255,605 655,1065 1065,1725 1725,2075
when we're estimating epistemic uncertainty,|
|

1420
00:41:40,840 --> 00:41:41,895
0,290 290,485 485,695 695,875 875,1055
we do want to keep
我们确实希望在测试时保持 dropout 启用，

1421
00:41:41,895 --> 00:41:43,340
0,375 375,675 675,930 930,1140 1140,1445
dropout enabled at test time,|
|

1422
00:41:43,480 --> 00:41:44,570
0,260 260,485 485,620 620,800 800,1090
because that's how we can
因为这也是我们引入随机性和推理时间的方式。

1423
00:41:45,040 --> 00:41:46,980
0,320 320,910 990,1490 1490,1730 1730,1940
introduce randomness, inference time as

1424
00:41:46,980 --> 00:41:48,200
0,290
well.|
|

1425
00:41:48,270 --> 00:41:49,205
0,245 245,350 350,485 485,665 665,935
So what we do here
所以我们在这里做的是我们有一个模型，

1426
00:41:49,205 --> 00:41:50,360
0,255 255,420 420,630 630,885 885,1155
is we have one model,|
|

1427
00:41:50,360 --> 00:41:51,395
0,285 285,405 405,585 585,810 810,1035
it's the same model the
从头到尾都是一样的模型，

1428
00:41:51,395 --> 00:41:52,940
0,210 210,420 420,725 1135,1380 1380,1545
entire way through,| {we,add -}
|我们以特定的概率添加 dropout 层，

1429
00:41:52,940 --> 00:41:54,125
0,405 405,675 675,795 795,915 915,1185
dropout layers with a specific

1430
00:41:54,125 --> 00:41:55,840
0,635 835,1125 1125,1275 1275,1425 1425,1715
probability,| and then we run
|然后我们运行多个前向传递，

1431
00:41:55,980 --> 00:41:57,770
0,400 450,815 815,1180 1410,1655 1655,1790
multiple forward passes| and at
|在每一次前向传递中，

1432
00:41:57,770 --> 00:41:59,705
0,255 255,555 555,890 1150,1515 1515,1935
every forward passes,| different layers
|不同的层在一层中获得不同的节点被丢弃，

1433
00:41:59,705 --> 00:42:01,100
0,305 415,765 765,1170 1170,1290 1290,1395
get different nodes in a

1434
00:42:01,100 --> 00:42:02,525
0,195 195,420 420,630 630,950 1180,1425
layer get dropped out,| so
|所以我们有随机性的度量。

1435
00:42:02,525 --> 00:42:04,180
0,120 120,300 300,605 925,1290 1290,1655
we have that measure of

1436
00:42:04,260 --> 00:42:06,800
0,635 635,845 845,1690
randomness and stochasticity.|
|

1437
00:42:07,930 --> 00:42:09,255
0,320 320,605 605,830 830,1010 1010,1325
So again, in order to
再说一次，为了实现这一点，

1438
00:42:09,255 --> 00:42:10,605
0,345 345,690 690,945 945,1080 1080,1350
implement this,| what we have
|我们拥有的是一个完全相同的模型，

1439
00:42:10,605 --> 00:42:12,405
0,315 315,555 555,875 1225,1575 1575,1800
is a model with the

1440
00:42:12,405 --> 00:42:13,905
0,275 355,660 660,965 1075,1350 1350,1500
exact one model,| {and,then -}
|然后当我们运行我们的向前传递时，

1441
00:42:13,905 --> 00:42:15,225
0,165 165,570 570,840 840,1050 1050,1320
when we're running our forward

1442
00:42:15,225 --> 00:42:16,950
0,365 685,945 945,1095 1095,1365 1365,1725
passes,| we can simply run
|我们可以简单地运行 T 次向前传递，

1443
00:42:16,950 --> 00:42:18,330
0,300 300,585 585,945 945,1230 1230,1380
T forward passes,| where T
|其中 T 通常是一个类似于 20 的数字，

1444
00:42:18,330 --> 00:42:19,215
0,120 120,315 315,480 480,645 645,885
is usually a number like

1445
00:42:19,215 --> 00:42:21,105
0,305 625,1020 1020,1320 1320,1530 1530,1890
twenty,| {we -} keep dropout
|我们在测试时保持 dropout 开启，

1446
00:42:21,105 --> 00:42:22,575
0,330 330,585 585,810 810,1145 1195,1470
enabled at test time,| and
|然后使用这些样本的平均值作为新的预测，

1447
00:42:22,575 --> 00:42:24,105
0,180 180,420 420,755 775,1170 1170,1530
then we use the mean

1448
00:42:24,105 --> 00:42:26,660
0,270 270,575 835,1395 1395,1775 2155,2555
of these samples as the

1449
00:42:26,920 --> 00:42:28,335
0,305 305,700 720,995 995,1115 1115,1415
new prediction| and the variance
|并使用这些样本的方差作为认知不确定性的衡量标准。

1450
00:42:28,335 --> 00:42:30,060
0,225 225,575 655,1170 1170,1470 1470,1725
of these samples as a

1451
00:42:30,060 --> 00:42:32,340
0,225 225,465 465,1080 1080,1430
measure of epistemic uncertainty.|
|

1452
00:42:34,300 --> 00:42:36,330
0,400 1080,1370 1370,1520 1520,1715 1715,2030
So both of the methods
所以我们刚才谈到的这两种方法都涉及到抽样，而且抽样费用很高。Ense抽样是非常昂贵的，但即使你有一个相当大的模型。

1453
00:42:36,330 --> 00:42:37,490
0,240 240,435 435,660 660,855 855,1160
we talked about just now

1454
00:42:37,570 --> 00:42:39,615
0,380 380,1000 1110,1400 1400,1790 1790,2045
involves sampling, and sampling is

1455
00:42:39,615 --> 00:42:41,240
0,365 445,840 840,1125 1125,1320 1320,1625
expensive. {Ense,sampling -} is very

1456
00:42:41,290 --> 00:42:42,720
0,400 690,965 965,1145 1145,1310 1310,1430
expensive, but even if you

1457
00:42:42,720 --> 00:42:43,880
0,120 120,270 270,480 480,780 780,1160
have a pretty large model.|
|

1458
00:42:45,400 --> 00:42:47,470
0,290 610,915 915,1365 1365,1755 1755,2070
Having our introducing dropout layers
让我们引入退学层和召唤20个前锋传球也可能是非常不可行的。在博物馆，我们致力于开发评估认知不确定性的创新方法，这种方法不依赖于采样之类的东西，因此它们更具普遍性，可以被更多的行业和人使用。

1459
00:42:47,470 --> 00:42:48,900
0,180 180,435 435,750 750,1065 1065,1430
and calling twenty forward passes

1460
00:42:49,100 --> 00:42:50,350
0,365 365,590 590,710 710,935 935,1250
might also be something that's

1461
00:42:50,350 --> 00:42:52,980
0,165 165,890 1390,1790 1810,2130 2130,2630
pretty infeasible. {And,at -} themus,

1462
00:42:53,150 --> 00:42:55,675
0,350 350,610 930,1330 1350,1750 2190,2525
we're dedicated to developing innovative

1463
00:42:55,675 --> 00:42:57,780
0,315 315,695 715,1125 1125,1770 1770,2105
methods of estimating epistemic uncertainty

1464
00:42:58,160 --> 00:42:59,545
0,320 320,710 710,950 950,1175 1175,1385
that don't rely on things

1465
00:42:59,545 --> 00:43:00,865
0,210 210,735 735,960 960,1095 1095,1320
like sampling so that they're

1466
00:43:00,865 --> 00:43:02,920
0,165 165,1055 1105,1410 1410,1650 1650,2055
more generalizable and they're usable

1467
00:43:02,920 --> 00:43:04,680
0,195 195,500 1030,1305 1305,1470 1470,1760
by more industries and people.|
|

1468
00:43:05,680 --> 00:43:07,120
0,350 490,765 765,975 975,1170 1170,1440
So a method that we've
因此，我们开发了一种方法来估计我们研究过的估计认知不确定性的方法，就是通过生成性建模。我们已经讨论过VA几次了，假设我用我们之前讨论过的完全相同的数据集训练了AV，也就是说，只有狗和猫。这个模型的潜在空间将由与狗和猫有关的特征组成。如果我给它一个原型狗，它应该能够产生一个很好的代表这只狗，它应该有相当低的重建损失。

1469
00:43:07,120 --> 00:43:08,980
0,290 400,800 850,1250 1450,1710 1710,1860
developed to estimate a method

1470
00:43:08,980 --> 00:43:10,705
0,150 150,345 345,620 670,1070 1450,1725
that we've studied to estimate

1471
00:43:10,705 --> 00:43:12,780
0,645 645,995 1375,1650 1650,1800 1800,2075
epistemic uncertainty is by using

1472
00:43:12,860 --> 00:43:15,265
0,515 515,1030 1590,1940 1940,2225 2225,2405
generative modeling. {So,we've -} talked

1473
00:43:15,265 --> 00:43:16,465
0,225 225,510 510,765 765,945 945,1200
about V A couple times

1474
00:43:16,465 --> 00:43:18,490
0,335 625,1025 1195,1590 1590,1770 1770,2025
now, but let's say I

1475
00:43:18,490 --> 00:43:20,680
0,255 480,770 1120,1520 1570,1920 1920,2190
trained AV on the exact

1476
00:43:20,680 --> 00:43:21,490
0,210 210,405 405,570 570,675 675,810
same data set we were

1477
00:43:21,490 --> 00:43:22,735
0,240 240,465 465,740 880,1125 1125,1245
talking about earlier, which is

1478
00:43:22,735 --> 00:43:24,970
0,240 240,585 585,870 870,1175 1975,2235
only dogs and {cats.,The -}

1479
00:43:24,970 --> 00:43:26,220
0,330 330,585 585,810 810,960 960,1250
latent space of this model

1480
00:43:26,300 --> 00:43:28,015
0,275 275,425 425,880 1020,1370 1370,1715
would be comprised of features

1481
00:43:28,015 --> 00:43:29,305
0,315 315,585 585,795 795,1020 1020,1290
that relate to dogs {and,cats.

1482
00:43:29,305 --> 00:43:30,520
0,305 565,810 810,915 915,1065 1065,1215
-} And if I give

1483
00:43:30,520 --> 00:43:32,395
0,135 135,330 330,990 990,1280 1570,1875
it a prototypical dog, it

1484
00:43:32,395 --> 00:43:33,325
0,165 165,285 285,480 480,675 675,930
should be able to generate

1485
00:43:33,325 --> 00:43:34,990
0,300 300,540 540,765 765,1425 1425,1665
a pretty good representation of

1486
00:43:34,990 --> 00:43:36,265
0,180 180,470 760,1005 1005,1125 1125,1275
this dog, and it should

1487
00:43:36,265 --> 00:43:38,130
0,210 210,495 495,825 825,1500 1500,1865
have pretty low reconstruction loss.|
|

1488
00:43:40,580 --> 00:43:41,695
0,365 365,590 590,740 740,935 935,1115
Now, if I gave the
现在，如果我给这个VA举一个同样的马的例子，这匹马将被解码到的潜在向量对于这个网络的解码者来说是无法理解的。解码器将不知道如何将潜在向量投影回原始输入空间，因此我们应该预期在这里会看到更糟糕的重建。我们应该看到，重建损失比我们给模型一个熟悉的输入或它习惯看到的东西要高得多。

1489
00:43:41,695 --> 00:43:43,230
0,225 225,575 775,1110 1110,1290 1290,1535
same example of the horse

1490
00:43:43,340 --> 00:43:45,510
0,260 260,425 425,575 575,820 1770,2170
to this V A, the

1491
00:43:45,920 --> 00:43:47,260
0,410 410,710 710,965 965,1130 1130,1340
latent vector that this horse

1492
00:43:47,260 --> 00:43:48,730
0,180 180,300 300,780 780,1070 1180,1470
would be decoded to would

1493
00:43:48,730 --> 00:43:50,635
0,180 180,1200 1200,1380 1380,1470 1470,1905
be incomprehensible to the decoder

1494
00:43:50,635 --> 00:43:52,225
0,135 135,300 300,605 985,1230 1230,1590
of this network. {The,decoder -}

1495
00:43:52,225 --> 00:43:53,230
0,270 270,390 390,600 600,825 825,1005
wouldn't be able to know

1496
00:43:53,230 --> 00:43:54,550
0,180 180,390 390,735 735,1005 1005,1320
how to project the latent

1497
00:43:54,550 --> 00:43:56,070
0,290 490,840 840,1095 1095,1260 1260,1520
vector back into the original

1498
00:43:56,180 --> 00:43:58,105
0,335 335,670 1020,1420 1440,1745 1745,1925
input space, and therefore we

1499
00:43:58,105 --> 00:43:59,080
0,195 195,390 390,555 555,750 750,975
should expect to see a

1500
00:43:59,080 --> 00:44:01,405
0,315 315,690 690,1410 1410,1790 2020,2325
much worse reconstruction {here.,And -}

1501
00:44:01,405 --> 00:44:02,665
0,180 180,345 345,635 865,1140 1140,1260
we should see that the

1502
00:44:02,665 --> 00:44:04,200
0,495 495,765 765,990 990,1215 1215,1535
reconstruction loss is much higher

1503
00:44:04,220 --> 00:44:05,170
0,245 245,365 365,530 530,755 755,950
than if we gave the

1504
00:44:05,170 --> 00:44:06,655
0,225 225,480 480,770 1030,1320 1320,1485
model a familiar input or

1505
00:44:06,655 --> 00:44:07,495
0,195 195,360 360,465 465,615 615,840
something that it was used

1506
00:44:07,495 --> 00:44:08,880
0,195 195,455
to seeing.|
|

1507
00:44:12,970 --> 00:44:14,280
0,400 420,695 695,920 920,1055 1055,1310
So now let's move on
现在让我们来看看我认为最令人兴奋的估计认知不确定性的方法，我们今天要谈到的。因此，在前面的两个例子中，EM采样是计算密集型的，但生成式建模也可以是计算密集型的。比方说，你的任务实际上不需要变分自动编码器，然后你无缘无故地训练整个解码器，除了估计认知不确定性，那么如果我们有一种方法，不依赖于生成性建模或采样来估计认知不确定性，会怎么样？

1508
00:44:14,280 --> 00:44:15,465
0,375 375,630 630,795 795,1005 1005,1185
to what I think is

1509
00:44:15,465 --> 00:44:16,850
0,120 120,360 360,690 690,1005 1005,1385
the most exciting method of

1510
00:44:17,200 --> 00:44:19,440
0,460 660,1400 1400,1750 1770,2030 2030,2240
estimating epistemic uncertainty that we'll

1511
00:44:19,440 --> 00:44:21,690
0,165 165,375 375,680 1510,1910 1960,2250
talk about today. {So,in -}

1512
00:44:21,690 --> 00:44:22,790
0,180 180,330 330,480 480,735 735,1100
both of the examples before,

1513
00:44:23,080 --> 00:44:24,920
0,395 395,950 950,1175 1175,1460 1460,1840
EM sampling is compute intensive,

1514
00:44:25,180 --> 00:44:26,730
0,275 275,665 665,1070 1070,1325 1325,1550
but generative modeling can also

1515
00:44:26,730 --> 00:44:28,170
0,120 120,390 390,830 940,1305 1305,1440
be compute intensive. Let's say

1516
00:44:28,170 --> 00:44:29,385
0,135 135,465 465,765 765,1035 1035,1215
you don't actually need a

1517
00:44:29,385 --> 00:44:30,750
0,405 405,645 645,1095 1095,1230 1230,1365
variational auto encoder for your

1518
00:44:30,750 --> 00:44:32,355
0,290 670,945 945,1155 1155,1350 1350,1605
task, then you're training an

1519
00:44:32,355 --> 00:44:33,920
0,285 285,930 930,1140 1140,1290 1290,1565
entire decoder for no reason

1520
00:44:34,180 --> 00:44:35,565
0,335 335,605 605,830 830,1115 1115,1385
and other than to estimate

1521
00:44:35,565 --> 00:44:38,295
0,120 120,675 675,1025 1975,2375 2485,2730
the epistemic uncertainty, so what

1522
00:44:38,295 --> 00:44:39,060
0,135 135,285 285,420 420,555 555,765
if we had a method

1523
00:44:39,060 --> 00:44:40,215
0,240 240,390 390,615 615,915 915,1155
that did not rely on

1524
00:44:40,215 --> 00:44:42,135
0,420 420,900 900,1215 1215,1740 1740,1920
generative modeling or sampling in

1525
00:44:42,135 --> 00:44:43,650
0,150 150,455 505,825 825,990 990,1515
order to estimate the epistemic

1526
00:44:43,650 --> 00:44:44,600
0,320
uncertainty?|
|

1527
00:44:44,730 --> 00:44:46,090
0,410 410,680 680,935 935,1085 1085,1360
That's exactly what a method
这正是我们在Themis开发的方法所做的。因此，我们认为学习是一个以证据为基础的过程。所以，如果你还记得之前我们训练合奏的时候，我们在同一个输入上调用多个合奏，我们收到了多个预测，我们计算了那个方差。

1528
00:44:46,410 --> 00:44:47,735
0,275 275,560 560,850 870,1160 1160,1325
that we've developed here at

1529
00:44:47,735 --> 00:44:50,555
0,330 330,665 1405,1805 2275,2595 2595,2820
themis does. {So,we -} view

1530
00:44:50,555 --> 00:44:52,190
0,305 355,645 645,840 840,1145 1255,1635
learning as an evidence based

1531
00:44:52,190 --> 00:44:53,735
0,380 790,1035 1035,1140 1140,1305 1305,1545
{process.,So -} if you remember

1532
00:44:53,735 --> 00:44:55,000
0,210 210,485 595,855 855,990 990,1265
from earlier when we were

1533
00:44:55,440 --> 00:44:57,140
0,335 335,515 515,1100 1100,1385 1385,1700
training the ensemble and calling

1534
00:44:57,140 --> 00:44:58,930
0,380 400,1125 1125,1365 1365,1515 1515,1790
multiple ensembles on the same

1535
00:44:58,950 --> 00:45:01,445
0,400 810,1160 1160,1445 1445,1780 1830,2495
input, we received multiple predictions

1536
00:45:01,445 --> 00:45:03,130
0,240 240,405 405,885 885,1125 1125,1685
and we calculated that variance.|
|

1537
00:45:04,260 --> 00:45:05,960
0,400 930,1175 1175,1310 1310,1490 1490,1700
Now, the way we frame
现在，我们框架证据学习的方式是，如果我们假设这些数据点，那些预测，实际上是从分布本身得出的。如果我们能够估计这种高阶证据分布的参数，我们就能够自动学习这种方差或认知不确定性的这种测量，而不需要进行任何抽样或生成性建模。而这正是证据不确定性的作用所在。

1538
00:45:05,960 --> 00:45:07,505
0,600 600,860 910,1230 1230,1410 1410,1545
evidential learning is what if

1539
00:45:07,505 --> 00:45:08,960
0,225 225,540 540,855 855,1155 1155,1455
we assume that those data

1540
00:45:08,960 --> 00:45:10,970
0,315 315,600 600,1220 1330,1710 1710,2010
points, those predictions, were actually

1541
00:45:10,970 --> 00:45:12,550
0,270 270,450 450,630 630,980 1180,1580
drawn from a distribution themselves.

1542
00:45:13,230 --> 00:45:14,420
0,290 290,455 455,730 750,1025 1025,1190
{If,we -} could estimate the

1543
00:45:14,420 --> 00:45:16,010
0,480 480,720 720,900 900,1215 1215,1590
parameters of this higher order

1544
00:45:16,010 --> 00:45:18,170
0,780 780,1160 1690,1935 1935,2040 2040,2160
evidential distribution, we would be

1545
00:45:18,170 --> 00:45:19,520
0,195 195,405 405,615 615,855 855,1350
able to learn this variance

1546
00:45:19,520 --> 00:45:20,960
0,240 240,405 405,630 630,840 840,1440
or this measure of epistemic

1547
00:45:20,960 --> 00:45:23,450
0,350 790,1190 1600,1950 1950,2220 2220,2490
uncertainty automatically without doing any

1548
00:45:23,450 --> 00:45:25,490
0,510 510,735 735,1125 1125,1640 1780,2040
sampling or generative {modeling.,And -}

1549
00:45:25,490 --> 00:45:27,100
0,255 255,540 540,765 765,1290 1290,1610
that's exactly what evidential uncertainty

1550
00:45:27,150 --> 00:45:28,620
0,400
does.|
|

1551
00:45:30,920 --> 00:45:32,530
0,400 870,1145 1145,1265 1265,1385 1385,1610
So now that we have
现在我们的工具箱中有许多方法来估计认知不确定性，让我们回到现实世界中的例子。

1552
00:45:32,530 --> 00:45:33,910
0,285 285,585 585,795 795,945 945,1380
many methods in our toolbox

1553
00:45:33,910 --> 00:45:37,330
0,380 1210,1620 1620,2310 2310,2660 3040,3420
for estimating epistemic uncertainty, let's

1554
00:45:37,330 --> 00:45:38,305
0,150 150,390 390,585 585,735 735,975
go back to our real

1555
00:45:38,305 --> 00:45:40,120
0,315 315,695
world example.|
|

1556
00:45:40,120 --> 00:45:42,010
0,270 270,530 940,1335 1335,1680 1680,1890
Let's say again, the input
让我们再说一次，输入和以前一样，它是城市中某个场景的RGB图像，输出也是该图像中每个像素所属的像素级掩码，它属于哪个类。

1557
00:45:42,010 --> 00:45:42,870
0,150 150,300 300,435 435,585 585,860
is the same as before,

1558
00:45:42,950 --> 00:45:44,680
0,320 320,545 545,1100 1100,1360 1380,1730
it's an rgb image of

1559
00:45:44,680 --> 00:45:45,780
0,285 285,525 525,675 675,810 810,1100
some scene in a city,

1560
00:45:46,310 --> 00:45:47,965
0,350 350,700 750,1085 1085,1400 1400,1655
and the output again is

1561
00:45:47,965 --> 00:45:49,330
0,180 180,570 570,795 795,1125 1125,1365
a pixel level mask of

1562
00:45:49,330 --> 00:45:50,605
0,150 150,435 435,900 900,1080 1080,1275
what every pixel in this

1563
00:45:50,605 --> 00:45:52,030
0,305 385,825 825,975 975,1155 1155,1425
image belongs to, which class

1564
00:45:52,030 --> 00:45:53,660
0,240 240,570 570,860
it belongs to.|
|

1565
00:45:53,700 --> 00:45:55,070
0,335 335,670 690,965 965,1145 1145,1370
Which parts of the data
你预计数据集的哪些部分会有很高的认知不确定性？在本例中，我们来看看模型本身的输出。该模型在语义切分方面做得很好。然而，它把人行道弄错了。它会将某些人行道指定给道路，而其他部分的人行道标记不正确。

1566
00:45:55,070 --> 00:45:56,210
0,255 255,465 465,660 660,930 930,1140
set would you expect to

1567
00:45:56,210 --> 00:45:58,760
0,195 195,495 495,1230 1230,1580 2290,2550
have high epistemic uncertainty? In

1568
00:45:58,760 --> 00:46:00,125
0,195 195,530 790,1065 1065,1200 1200,1365
this example, take a look

1569
00:46:00,125 --> 00:46:01,280
0,180 180,455 535,840 840,1020 1020,1155
at the output of the

1570
00:46:01,280 --> 00:46:03,095
0,260 310,710 1060,1320 1320,1530 1530,1815
model itself. {The,model -} does

1571
00:46:03,095 --> 00:46:05,050
0,335 355,705 705,960 960,1380 1380,1955
mostly well on semantic {segmentation.,However,

1572
00:46:05,550 --> 00:46:07,100
0,365 365,590 590,785 785,1025 1025,1550
-} it gets the {sidewalk,wrong.

1573
00:46:07,100 --> 00:46:09,350
0,350 970,1370 1600,2010 2010,2130 2130,2250
-} It assigns some of

1574
00:46:09,350 --> 00:46:10,300
0,120 120,450 450,570 570,675 675,950
the sidewalk to the road,

1575
00:46:10,500 --> 00:46:11,870
0,290 290,515 515,850 930,1220 1220,1370
and other parts of the

1576
00:46:11,870 --> 00:46:14,220
0,315 315,465 465,765 765,1550
sidewalk are labeled incorrectly.|
|

1577
00:46:15,610 --> 00:46:17,805
0,400 510,785 785,1060 1230,1565 1565,2195
And we can, using epistemic
我们可以，利用认知的不确定性，我们可以明白为什么会这样。变色的人行道区域具有高度的认知不确定性。这可能是因为该模型以前从未见过具有多种不同颜色的人行道的示例，或者可能它没有在具有人行道的示例上进行过训练。无论哪种方式，认知的不确定性将图像的这一特定区域隔离为高度不确定的区域。

1578
00:46:17,805 --> 00:46:19,305
0,335 655,900 900,1035 1035,1245 1245,1500
uncertainty, we can see why

1579
00:46:19,305 --> 00:46:20,910
0,210 210,485 865,1140 1140,1380 1380,1605
this is. {The,areas -} of

1580
00:46:20,910 --> 00:46:22,520
0,135 135,585 585,765 765,915 915,1610
the sidewalk that are discolored

1581
00:46:22,540 --> 00:46:24,330
0,335 335,590 590,905 905,1175 1175,1790
have high levels of epistemic

1582
00:46:24,330 --> 00:46:26,280
0,350 940,1275 1275,1470 1470,1665 1665,1950
{uncertainty.,Maybe -} this is because

1583
00:46:26,280 --> 00:46:27,375
0,195 195,390 390,615 615,840 840,1095
the model has never seen

1584
00:46:27,375 --> 00:46:28,620
0,225 225,510 510,735 735,870 870,1245
an example of a sidewalk

1585
00:46:28,620 --> 00:46:29,745
0,165 165,405 405,690 690,945 945,1125
with multiple different colors in

1586
00:46:29,745 --> 00:46:31,125
0,105 105,365 745,1020 1020,1200 1200,1380
it before, or maybe it

1587
00:46:31,125 --> 00:46:32,595
0,270 270,420 420,705 705,1085 1105,1470
hasn't been trained on examples

1588
00:46:32,595 --> 00:46:35,090
0,255 255,780 780,1085 1795,2145 2145,2495
with {sidewalks,generally. -} Either way,

1589
00:46:35,920 --> 00:46:38,205
0,725 725,1040 1040,1310 1310,1895 1895,2285
epistemic uncertainty has isolated this

1590
00:46:38,205 --> 00:46:39,620
0,390 390,780 780,1035 1035,1155 1155,1415
specific area of the image

1591
00:46:39,670 --> 00:46:40,920
0,275 275,410 410,665 665,965 965,1250
as an area of high

1592
00:46:40,920 --> 00:46:42,420
0,380
uncertainty.|
|

1593
00:46:45,040 --> 00:46:46,700
0,400 420,815 815,1175 1175,1340 1340,1660
So today, we've gone through
因此，今天，我们已经经历了两个主要挑战，以实现稳健的深度学习。我们已经谈到了偏差，这是当模型被敏感的特征输入和不确定性扭曲时发生的事情，也就是我们可以衡量某个模型的置信度水平时发生的事情。现在，我们将讨论TMUS如何使用这些概念来构建转变模型的产品，使它们更具风险意识，以及我们如何在安全和值得信赖的人工智能方面改变人工智能的格局。

1594
00:46:47,320 --> 00:46:49,365
0,395 395,790 840,1240 1410,1760 1760,2045
two major challenges for robust

1595
00:46:49,365 --> 00:46:50,970
0,210 210,485 805,1170 1170,1350 1350,1605
deep learning. We've talked about

1596
00:46:50,970 --> 00:46:52,290
0,525 525,780 780,915 915,1065 1065,1320
bias, which is what happens

1597
00:46:52,290 --> 00:46:53,700
0,255 255,510 510,840 840,1230 1230,1410
when models are skewed by

1598
00:46:53,700 --> 00:46:55,850
0,290 310,710 730,1250 1390,1770 1770,2150
sensitive feature inputs and uncertainty,

1599
00:46:56,140 --> 00:46:57,255
0,275 275,545 545,830 830,980 980,1115
which is when we can

1600
00:46:57,255 --> 00:46:58,640
0,225 225,540 540,825 825,1065 1065,1385
measure a level of confidence

1601
00:46:58,780 --> 00:47:01,100
0,260 260,380 380,575 575,910 1920,2320
of a certain model. {Now,we'll

1602
00:47:01,120 --> 00:47:02,430
0,335 335,500 500,710 710,935 935,1310
-} talk about how themus

1603
00:47:02,430 --> 00:47:03,930
0,255 255,525 525,1095 1095,1290 1290,1500
uses these concepts to build

1604
00:47:03,930 --> 00:47:05,685
0,345 345,740 820,1125 1125,1430 1510,1755
products that transform models to

1605
00:47:05,685 --> 00:47:06,830
0,135 135,315 315,510 510,780 780,1145
make them more risk aware,

1606
00:47:06,970 --> 00:47:08,265
0,290 290,470 470,740 740,980 980,1295
and how we're changing the

1607
00:47:08,265 --> 00:47:09,645
0,335 415,735 735,945 945,1155 1155,1380
AI landscape in terms of

1608
00:47:09,645 --> 00:47:11,980
0,270 270,525 525,1110 1110,1445
safe and trustworthy AI.|
|

1609
00:47:14,230 --> 00:47:15,780
0,400 420,710 710,1070 1070,1310 1310,1550
So at themis, we believe
因此，在Themis，我们相信不确定性和偏见的缓解开启了一系列新的解决方案，以安全和负责任的人工智能来解决这些问题。

1610
00:47:15,780 --> 00:47:17,480
0,285 285,645 645,900 900,1200 1200,1700
that uncertainty and bias mitigation

1611
00:47:17,500 --> 00:47:20,265
0,605 605,1000 1500,1865 1865,2230 2460,2765
unlock a host of new

1612
00:47:20,265 --> 00:47:21,740
0,305 385,645 645,990 990,1170 1170,1475
solutions to solving these problems

1613
00:47:21,880 --> 00:47:23,630
0,305 305,545 545,830 830,1180 1350,1750
with safe and responsible AI.|
|

1614
00:47:24,490 --> 00:47:25,815
0,275 275,410 410,605 605,1070 1070,1325
We can use bias and
我们可以使用偏见和不确定性来降低人工智能生命周期每个部分的风险。让我们从标记数据开始。今天我们讨论了不确定度，这是一种检测错误标记的样本，突出标记噪音的方法，通常可能会告诉标签员重新标记他们获得的图像或样本。这可能是错误的。

1615
00:47:25,815 --> 00:47:27,570
0,335 505,750 750,1170 1170,1470 1470,1755
uncertainty to mitigate risk in

1616
00:47:27,570 --> 00:47:28,725
0,320 340,660 660,855 855,975 975,1155
every part of the AI

1617
00:47:28,725 --> 00:47:30,525
0,240 240,545 1045,1425 1425,1590 1590,1800
life cycle. Let's start with

1618
00:47:30,525 --> 00:47:32,505
0,435 435,725 1045,1445 1495,1770 1770,1980
labeling data. {Today,,we -} talked

1619
00:47:32,505 --> 00:47:34,395
0,210 210,870 870,1205 1525,1785 1785,1890
about allatoric uncertainty, which is

1620
00:47:34,395 --> 00:47:35,955
0,135 135,390 390,690 690,960 960,1560
a method to detect mislabeled

1621
00:47:35,955 --> 00:47:37,820
0,485 595,870 870,1145 1165,1515 1515,1865
samples, to highlight label noise,

1622
00:47:37,900 --> 00:47:39,195
0,260 260,380 380,640 660,1025 1025,1295
and to generally maybe tell

1623
00:47:39,195 --> 00:47:42,140
0,525 525,905 1585,2175 2175,2435 2545,2945
labelers to relabel images or

1624
00:47:42,160 --> 00:47:43,695
0,530 530,725 725,935 935,1180 1230,1535
samples that they've {gotten.,That -}

1625
00:47:43,695 --> 00:47:45,220
0,180 180,345 345,635
may be wrong.|
|

1626
00:47:45,340 --> 00:47:46,215
0,245 245,365 365,575 575,770 770,875
In the second part of
在本周期的第二部分中，我们甚至在对任何数据进行模型训练之前就已经分析了数据。我们可以分析这个数据集中存在的偏差，EM告诉创建者他们是否应该添加更多的样本，哪些人口统计数据，数据集中的哪些领域在当前数据集中没有得到充分代表，甚至在我们对他们进行模型训练之前。

1627
00:47:46,215 --> 00:47:47,940
0,135 135,425 565,855 855,1140 1140,1725
this cycle, we have analyzing

1628
00:47:47,940 --> 00:47:49,530
0,210 210,470 760,1125 1125,1365 1365,1590
the data before a model

1629
00:47:49,530 --> 00:47:50,730
0,225 225,465 465,780 780,1005 1005,1200
is even trained on any

1630
00:47:50,730 --> 00:47:52,095
0,320 370,645 645,780 780,1185 1185,1365
data. {We,can -} analyze the

1631
00:47:52,095 --> 00:47:53,220
0,345 345,525 525,690 690,915 915,1125
bias that is present in

1632
00:47:53,220 --> 00:47:54,980
0,210 210,525 525,890 1030,1395 1395,1760
this data set and EM

1633
00:47:55,120 --> 00:47:56,280
0,260 260,410 410,755 755,980 980,1160
tell the creators whether or

1634
00:47:56,280 --> 00:47:57,165
0,150 150,315 315,465 465,660 660,885
not they should add more

1635
00:47:57,165 --> 00:47:59,190
0,525 525,825 825,1530 1530,1755 1755,2025
samples, which demographics, which areas

1636
00:47:59,190 --> 00:48:00,045
0,225 225,315 315,495 495,705 705,855
of the data set are

1637
00:48:00,045 --> 00:48:01,635
0,765 765,1020 1020,1170 1170,1350 1350,1590
underrepresented in the current data

1638
00:48:01,635 --> 00:48:03,105
0,335 475,825 825,1050 1050,1275 1275,1470
set before we even train

1639
00:48:03,105 --> 00:48:04,800
0,105 105,285 285,480 480,755
a model on them.|
|

1640
00:48:05,160 --> 00:48:06,410
0,275 275,550 630,995 995,1085 1085,1250
And then let's go to
然后让我们去训练这个模型。一旦我们真正训练了一个模型，如果它已经在有偏差的数据集上进行了训练，我们可以在训练过程中使用我们今天讨论的方法自适应地消除它的偏见。

1641
00:48:06,410 --> 00:48:08,255
0,255 255,450 450,710 1300,1590 1590,1845
training the model. {Once,we're -}

1642
00:48:08,255 --> 00:48:09,230
0,210 210,435 435,615 615,795 795,975
actually training a model, if

1643
00:48:09,230 --> 00:48:10,535
0,210 210,435 435,705 705,1005 1005,1305
it's already been trained on

1644
00:48:10,535 --> 00:48:12,080
0,240 240,585 585,825 825,1175 1285,1545
a biased data set, we

1645
00:48:12,080 --> 00:48:13,565
0,150 150,330 330,645 645,855 855,1485
can de bias it adaptively

1646
00:48:13,565 --> 00:48:14,975
0,270 270,635 685,1020 1020,1215 1215,1410
during training using the methods

1647
00:48:14,975 --> 00:48:16,030
0,195 195,330 330,540 540,765 765,1055
that we talked about today.|
|

1648
00:48:17,300 --> 00:48:19,450
0,275 275,550 1230,1490 1490,1750 1770,2150
And afterwards, we can also
然后，我们还可以验证或认证部署的机器学习模型，确保实际存在的模型如它们声称的那样安全和公正。我们可以做到这一点的方法是，通过利用认知的不确定性或偏差来计算模型将会做的样本或数据点。模型上最差的样本具有最困难的学习或在模型数据集中最不能被代表的数据集样本。

1649
00:48:19,450 --> 00:48:23,305
0,690 690,1040 1930,2600 3100,3615 3615,3855
verify or certify deployed machine

1650
00:48:23,305 --> 00:48:25,270
0,210 210,545 1105,1485 1485,1770 1770,1965
learning models, making sure that

1651
00:48:25,270 --> 00:48:26,440
0,240 240,435 435,675 675,960 960,1170
models that are actually out

1652
00:48:26,440 --> 00:48:27,910
0,225 225,405 405,680 910,1245 1245,1470
there are as safe and

1653
00:48:27,910 --> 00:48:29,215
0,540 540,705 705,900 900,1125 1125,1305
unbiased as they claim they

1654
00:48:29,215 --> 00:48:30,295
0,275 445,705 705,810 810,945 945,1080
are. {And,the -} way we

1655
00:48:30,295 --> 00:48:31,105
0,120 120,270 270,450 450,630 630,810
can do this is by

1656
00:48:31,105 --> 00:48:33,660
0,540 540,1245 1245,1625 1765,2070 2070,2555
leveraging epistemic uncertainty or bias

1657
00:48:33,830 --> 00:48:35,200
0,290 290,560 560,815 815,1235 1235,1370
in order to calculate the

1658
00:48:35,200 --> 00:48:36,520
0,420 420,615 615,840 840,1125 1125,1320
samples or data points that

1659
00:48:36,520 --> 00:48:37,255
0,120 120,315 315,510 510,630 630,735
the model will {do.,The -}

1660
00:48:37,255 --> 00:48:38,590
0,180 180,515 655,915 915,1110 1110,1335
worst on the model has

1661
00:48:38,590 --> 00:48:40,290
0,285 285,680 730,1080 1080,1365 1365,1700
the the most trouble learning

1662
00:48:40,340 --> 00:48:42,145
0,400 450,785 785,1120 1260,1700 1700,1805
or data set samples that

1663
00:48:42,145 --> 00:48:43,810
0,165 165,345 345,555 555,1440 1440,1665
are the most underrepresented in

1664
00:48:43,810 --> 00:48:45,140
0,120 120,345 345,645 645,980
a model data set.|
|

1665
00:48:45,210 --> 00:48:46,130
0,290 290,425 425,560 560,755 755,920
If we can test the
如果我们可以在这些样本上测试模型，特别是模型的最硬样本，并且它做得很好，那么我们就知道模型可能已经以一种公平和公正的方式进行了训练，以减少不确定性。

1666
00:48:46,130 --> 00:48:47,500
0,195 195,435 435,675 675,1080 1080,1370
model on these samples, specifically

1667
00:48:47,700 --> 00:48:48,935
0,275 275,605 605,950 950,1115 1115,1235
the hardest samples for the

1668
00:48:48,935 --> 00:48:50,440
0,245 535,810 810,960 960,1170 1170,1505
model, and it does well,

1669
00:48:50,670 --> 00:48:51,515
0,275 275,410 410,575 575,740 740,845
then we know that the

1670
00:48:51,515 --> 00:48:53,090
0,180 180,420 420,725 775,1175 1225,1575
model has probably been trained

1671
00:48:53,090 --> 00:48:54,395
0,195 195,330 330,555 555,765 765,1305
in a fair and unbiased

1672
00:48:54,395 --> 00:48:57,080
0,305 415,720 720,1215 1215,1595
manner that mitigates uncertainty.|
|

1673
00:48:57,310 --> 00:48:58,650
0,290 290,815 815,1010 1010,1145 1145,1340
And lastly, we can think
最后，我们可以考虑一下，EM，我们，我们正在Themis开发一款名为AI Guardian的产品，这本质上是人工智能算法和用户之间的一层。它的工作原理是，这是一种算法，如果你驾驶一辆自动驾驶汽车，它会说，嘿，这个模型实际上并不知道它周围的世界现在发生了什么。作为使用者，你应该控制这辆自动驾驶汽车，我们也可以将这一点应用于自动驾驶之外的长矛。

1674
00:48:58,650 --> 00:49:00,195
0,320 400,800 940,1245 1245,1395 1395,1545
about, EM, we, we are

1675
00:49:00,195 --> 00:49:01,530
0,270 270,525 525,750 750,960 960,1335
developing a product at themis

1676
00:49:01,530 --> 00:49:03,585
0,330 585,780 780,1310 1450,1740 1740,2055
called AI guardian, and that's

1677
00:49:03,585 --> 00:49:04,980
0,240 240,480 480,755 835,1140 1140,1395
essentially a layer between the

1678
00:49:04,980 --> 00:49:07,035
0,315 315,680 1300,1740 1740,1920 1920,2055
artificial intelligence algorithm and the

1679
00:49:07,035 --> 00:49:08,370
0,275 655,915 915,1020 1020,1155 1155,1335
user. {And,the -} way this

1680
00:49:08,370 --> 00:49:09,675
0,290 310,710 760,1035 1035,1170 1170,1305
works is this is a

1681
00:49:09,675 --> 00:49:10,920
0,135 135,360 360,735 735,990 990,1245
type of algorithm that if

1682
00:49:10,920 --> 00:49:12,320
0,255 255,465 465,630 630,1110 1110,1400
you're driving an autonomous vehicle,

1683
00:49:12,670 --> 00:49:14,805
0,305 305,610 750,1150 1260,1660 1800,2135
would say, hey, the model

1684
00:49:14,805 --> 00:49:16,275
0,405 405,705 705,1025 1105,1350 1350,1470
doesn't actually know what is

1685
00:49:16,275 --> 00:49:17,280
0,240 240,465 465,570 570,765 765,1005
happening in the world around

1686
00:49:17,280 --> 00:49:18,420
0,150 150,300 300,590 610,945 945,1140
it right {now.,As -} the

1687
00:49:18,420 --> 00:49:19,635
0,260 340,630 630,780 780,990 990,1215
user, you should take control

1688
00:49:19,635 --> 00:49:21,255
0,120 120,225 225,705 705,995 1345,1620
of this autonomous vehicle and

1689
00:49:21,255 --> 00:49:22,410
0,120 120,270 270,480 480,785 835,1155
we can apply this to

1690
00:49:22,410 --> 00:49:24,170
0,420 420,690 690,1260 1260,1470 1470,1760
spears outside autonomy as well.|
|

1691
00:49:27,050 --> 00:49:28,060
0,305 305,530 530,680 680,845 845,1010
So you'll notice that I
所以你会注意到我跳过了一个，呃，周期的一部分。我跳过了关于构建模型的部分，这是因为今天我们将稍微关注一下新兴市场著名的人工智能产品Capa，这是一个模型不可知的风险评估框架。所以Capa是一个开源库。你们今天将在你们的实验室中使用它，EM，它可以转换模型，让它们意识到风险。因此，这是一个典型的培训渠道。你们在课程中已经看过很多次了。到目前为止，我们已经有了数据，我们有了模型，它被输入到训练算法中，最后我们得到了一个训练有素的模型，它为每一次输入输出一个预测。

1692
00:49:28,060 --> 00:49:29,725
0,255 255,540 540,920 1000,1350 1350,1665
skipped one, uh, part of

1693
00:49:29,725 --> 00:49:31,150
0,270 270,575 685,1035 1035,1290 1290,1425
the cycle. {I,skipped -} the

1694
00:49:31,150 --> 00:49:32,370
0,180 180,435 435,735 735,960 960,1220
part about building the model

1695
00:49:32,480 --> 00:49:33,760
0,260 260,545 545,755 755,1010 1010,1280
and that's because today we're

1696
00:49:33,760 --> 00:49:34,495
0,75 75,180 180,360 360,555 555,735
going to focus a little

1697
00:49:34,495 --> 00:49:37,150
0,225 225,545 865,1265 1795,2195 2215,2655
bit on EM famous AI's

1698
00:49:37,150 --> 00:49:39,040
0,320 340,705 705,1370 1450,1725 1725,1890
product called capa, which is

1699
00:49:39,040 --> 00:49:40,885
0,210 210,480 480,1095 1095,1400 1570,1845
a model agnostic framework for

1700
00:49:40,885 --> 00:49:43,660
0,275 295,785 1495,1895 2065,2625 2625,2775
risk {estimation.,So -} capa is

1701
00:49:43,660 --> 00:49:44,995
0,195 195,420 420,615 615,920 1090,1335
an open {source,library. -} You

1702
00:49:44,995 --> 00:49:45,910
0,120 120,360 360,600 600,765 765,915
all will actually use it

1703
00:49:45,910 --> 00:49:47,370
0,105 105,225 225,420 420,740 1060,1460
in your lab today, EM,

1704
00:49:47,480 --> 00:49:49,195
0,400 600,1040 1040,1325 1325,1580 1580,1715
that transforms models so that

1705
00:49:49,195 --> 00:49:51,130
0,255 255,510 510,905 1555,1815 1815,1935
{they're,risk -} aware. So this

1706
00:49:51,130 --> 00:49:52,530
0,180 180,420 420,720 720,1050 1050,1400
is a typical training pipeline.

1707
00:49:52,550 --> 00:49:53,530
0,335 335,425 425,560 560,770 770,980
You've seen this many {times,in

1708
00:49:53,530 --> 00:49:54,480
0,135 135,255 255,465 465,675 675,950
-} the course. By now

1709
00:49:54,740 --> 00:49:55,990
0,275 275,500 500,740 740,1010 1010,1250
we have our data, we

1710
00:49:55,990 --> 00:49:57,085
0,135 135,255 255,500 610,885 885,1095
have the model, and it's

1711
00:49:57,085 --> 00:49:58,315
0,105 105,270 270,450 450,725 775,1230
Fed into the training algorithm

1712
00:49:58,315 --> 00:49:59,080
0,195 195,315 315,420 420,555 555,765
and we get a trained

1713
00:49:59,080 --> 00:50:00,265
0,240 240,435 435,570 570,810 810,1185
model at the end that

1714
00:50:00,265 --> 00:50:01,740
0,330 330,495 495,875 955,1215 1215,1475
outputs a prediction for every

1715
00:50:01,850 --> 00:50:02,940
0,400
input.|
|

1716
00:50:04,320 --> 00:50:05,630
0,260 260,425 425,965 965,1190 1190,1310
But with capa, what we
但有了CAPA，我们可以做的是，通过在任何培训工作流程中添加一行，我们可以将此模型转变为具有风险意识的变体，本质上为您计算偏差、不确定性和标签噪声。因为今天，正如你现在所听到的，有如此多的方法来估计不确定性和偏差，有时某些方法比其他方法更好。真的很难确定你试图估计的是哪种不确定性，以及如何估计。所以卡帕会帮你处理这件事。通过在您的培训工作流程中插入一行，您可以获得一个风险感知模型，然后您可以进一步分析该模型。

1717
00:50:05,630 --> 00:50:06,560
0,135 135,330 330,540 540,705 705,930
can do is by adding

1718
00:50:06,560 --> 00:50:08,165
0,285 285,585 585,915 915,1260 1260,1605
a single line into any

1719
00:50:08,165 --> 00:50:10,115
0,330 330,965 1315,1575 1575,1740 1740,1950
training workflow, we can turn

1720
00:50:10,115 --> 00:50:11,360
0,195 195,465 465,765 765,975 975,1245
this model into a risk

1721
00:50:11,360 --> 00:50:13,655
0,300 300,830 1030,1350 1350,1665 1665,2295
aware variant that essentially calculates

1722
00:50:13,655 --> 00:50:15,845
0,665 865,1265 1375,1650 1650,1875 1875,2190
biases, uncertainty and label noise

1723
00:50:15,845 --> 00:50:18,250
0,255 255,545 955,1355 1405,1805 2005,2405
for you. {Because,today, -} as

1724
00:50:19,050 --> 00:50:20,180
0,305 305,440 440,620 620,890 890,1130
you've heard by now, there

1725
00:50:20,180 --> 00:50:21,530
0,195 195,450 450,705 705,1020 1020,1350
are so many methods of

1726
00:50:21,530 --> 00:50:23,660
0,440 550,945 945,1215 1215,1670 1840,2130
estimating uncertainty and bias, and

1727
00:50:23,660 --> 00:50:25,265
0,290 580,900 900,1185 1185,1425 1425,1605
sometimes certain methods are better

1728
00:50:25,265 --> 00:50:26,765
0,180 180,455 715,1050 1050,1230 1230,1500
than others. It's really hard

1729
00:50:26,765 --> 00:50:28,130
0,240 240,525 525,840 840,1080 1080,1365
to determine what kind of

1730
00:50:28,130 --> 00:50:29,735
0,375 375,720 720,915 915,1230 1230,1605
uncertainty you're trying to estimate

1731
00:50:29,735 --> 00:50:30,820
0,285 285,465 465,630 630,795 795,1085
and how to do {so.,So

1732
00:50:30,960 --> 00:50:32,090
0,260 260,620 620,815 815,1010 1010,1130
-} capa takes care of

1733
00:50:32,090 --> 00:50:33,710
0,210 210,495 495,830 880,1155 1155,1620
this {for,you. -} By inserting

1734
00:50:33,710 --> 00:50:34,865
0,255 255,525 525,750 750,930 930,1155
one line into your training

1735
00:50:34,865 --> 00:50:36,575
0,545 595,885 885,1175 1225,1530 1530,1710
workflow, you can achieve a

1736
00:50:36,575 --> 00:50:37,850
0,270 270,585 585,885 885,1140 1140,1275
risk aware model that you

1737
00:50:37,850 --> 00:50:40,060
0,135 135,360 360,660 660,1220
can then further analyze.|
|

1738
00:50:41,500 --> 00:50:42,645
0,305 305,590 590,845 845,995 995,1145
And so this is the
这就是我一直在谈论的一条线。EM在构建模型之后，您可以只创建一个包装器，也可以调用Capa拥有大量库的包装器。然后，除了实现预测，从你的模型接收预测之外，你还可以得到你试图估计的任何偏差或不确定性指标。

1739
00:50:42,645 --> 00:50:43,485
0,165 165,360 360,510 510,720 720,840
one line that I've been

1740
00:50:43,485 --> 00:50:45,225
0,275 325,725 835,1185 1185,1500 1500,1740
talking about. {EM,,after -} you

1741
00:50:45,225 --> 00:50:46,230
0,180 180,375 375,645 645,885 885,1005
build your model, you can

1742
00:50:46,230 --> 00:50:47,505
0,195 195,450 450,660 660,1095 1095,1275
just create a wrapper or

1743
00:50:47,505 --> 00:50:48,630
0,135 135,300 300,510 510,690 690,1125
you can call a wrapper

1744
00:50:48,630 --> 00:50:50,445
0,210 210,615 615,890 1060,1460 1480,1815
that capa has an extensive

1745
00:50:50,445 --> 00:50:52,200
0,335 385,785 1105,1380 1380,1545 1545,1755
library {of.,And -} then, in

1746
00:50:52,200 --> 00:50:54,885
0,285 285,650 1030,1455 1455,1880 2350,2685
addition to achieving prediction, receiving

1747
00:50:54,885 --> 00:50:56,325
0,465 465,615 615,735 735,995 1195,1440
predictions from your model, you

1748
00:50:56,325 --> 00:50:57,885
0,195 195,480 480,810 810,1140 1140,1560
can also receive whatever bias

1749
00:50:57,885 --> 00:50:59,175
0,240 240,555 555,960 960,1095 1095,1290
or uncertainty metric that you're

1750
00:50:59,175 --> 00:51:00,920
0,210 210,525 525,905
trying to estimate.|
|

1751
00:51:03,180 --> 00:51:04,630
0,365 365,575 575,755 755,1190 1190,1450
And the way capsule works
胶囊的工作方式是通过包装我们想要估计的每个不确定性指标的模型来实现这一点，我们可以根据需要应用和创建最小的模型修改，同时保留初始架构和预测能力。在全能不确定性的情况下，这可能会增加一层新的东西。在变分自动编码器的情况下，这可能是创建和训练解码器并在运行中计算重建损失。

1752
00:51:04,680 --> 00:51:05,720
0,275 275,395 395,545 545,785 785,1040
is it does this by

1753
00:51:05,720 --> 00:51:08,135
0,465 465,770 1390,1665 1665,1940 2020,2415
wrapping models for every uncertainty

1754
00:51:08,135 --> 00:51:09,425
0,495 495,675 675,810 810,990 990,1290
metric that we want to

1755
00:51:09,425 --> 00:51:11,060
0,395 565,840 840,1065 1065,1365 1365,1635
estimate, we can apply and

1756
00:51:11,060 --> 00:51:12,830
0,255 255,465 465,840 840,1095 1095,1770
create the minimal model modifications

1757
00:51:12,830 --> 00:51:14,945
0,380 490,890 1120,1440 1440,1950 1950,2115
as necessary while preserving the

1758
00:51:14,945 --> 00:51:16,690
0,210 210,545 775,1065 1065,1455 1455,1745
initial architecture and predictive capabilities.

1759
00:51:17,700 --> 00:51:18,995
0,260 260,380 380,560 560,725 725,1295
{In,the -} case of allatoric

1760
00:51:18,995 --> 00:51:20,210
0,335 475,765 765,900 900,1020 1020,1215
uncertainty, this could be adding

1761
00:51:20,210 --> 00:51:21,470
0,165 165,300 300,590 880,1140 1140,1260
a new {layer.,In -} the

1762
00:51:21,470 --> 00:51:22,535
0,165 165,315 315,420 420,795 795,1065
case of a variational auto

1763
00:51:22,535 --> 00:51:23,990
0,510 510,705 705,855 855,1095 1095,1455
encoder, this could be creating

1764
00:51:23,990 --> 00:51:25,550
0,270 270,495 495,690 690,1275 1275,1560
and training the decoder and

1765
00:51:25,550 --> 00:51:27,110
0,450 450,585 585,1050 1050,1350 1350,1560
calculating the reconstruction loss on

1766
00:51:27,110 --> 00:51:28,580
0,135 135,410
the fly.|
|

1767
00:51:29,990 --> 00:51:31,320
0,400 450,710 710,830 830,1010 1010,1330
And this is an example
这是一个CAPA处理我们今天讨论的其中一个数据集的例子，那就是增加了噪声的立方体数据集。还有另一项简单的分类任务。我之所以想展示这张图片，是为了展示使用CAPSA，我们可以在几乎不增加额外工作的情况下实现所有这些不确定性估计。

1768
00:51:31,340 --> 00:51:33,760
0,320 320,880 1200,1600 1620,2020 2160,2420
of capa working on one

1769
00:51:33,760 --> 00:51:34,495
0,105 105,195 195,360 360,570 570,735
of the data sets that

1770
00:51:34,495 --> 00:51:35,695
0,150 150,360 360,570 570,845 925,1200
we talked about today, which

1771
00:51:35,695 --> 00:51:36,715
0,135 135,285 285,570 570,780 780,1020
was the cubic data set

1772
00:51:36,715 --> 00:51:38,010
0,300 300,630 630,885 885,1050 1050,1295
with added noise in it.

1773
00:51:38,120 --> 00:51:39,930
0,365 365,680 680,965 965,1235 1235,1810
{And,also -} another simple classification

1774
00:51:39,950 --> 00:51:41,490
0,400 630,905 905,1025 1025,1205 1205,1540
{task.,And -} the reason why

1775
00:51:41,750 --> 00:51:42,655
0,275 275,455 455,635 635,770 770,905
I wanted to show this

1776
00:51:42,655 --> 00:51:43,765
0,255 255,525 525,690 690,855 855,1110
image is to show that

1777
00:51:43,765 --> 00:51:45,175
0,330 330,780 780,915 915,1110 1110,1410
using capsa, we can achieve

1778
00:51:45,175 --> 00:51:47,040
0,285 285,495 495,765 765,1145 1375,1865
all of these uncertainty estimates

1779
00:51:47,120 --> 00:51:48,970
0,350 350,695 695,1070 1070,1450 1500,1850
with very little additional added

1780
00:51:48,970 --> 00:51:50,440
0,350
work.|
|

1781
00:51:53,840 --> 00:51:55,165
0,400 450,815 815,1055 1055,1190 1190,1325
So using all of the
所以，使用我今天刚刚谈到的所有产品，使用Capa Themis，就是解锁了跨领域安全部署深度学习模型的钥匙。我们现在可以回答早些时候头条新闻提出的许多问题，即人类应该在什么时候控制自动驾驶汽车？在商业自动驾驶管道中，哪些类型的数据代表不足？由于Themis正在开发的产品，我们现在已经对这些问题有了有根据的答案。

1782
00:51:55,165 --> 00:51:56,095
0,225 225,420 420,555 555,720 720,930
products that I just talked

1783
00:51:56,095 --> 00:51:57,870
0,225 225,515 655,930 930,1170 1170,1775
about today and using capa

1784
00:51:58,040 --> 00:51:59,680
0,455 455,650 650,1250 1250,1400 1400,1640
themis is unlocking the key

1785
00:51:59,680 --> 00:52:01,000
0,270 270,510 510,720 720,975 975,1320
to deploy deep learning models

1786
00:52:01,000 --> 00:52:03,325
0,380 520,885 885,1250 1930,2190 2190,2325
safely across fields. {We,can -}

1787
00:52:03,325 --> 00:52:04,435
0,210 210,540 540,810 810,975 975,1110
now answer a lot of

1788
00:52:04,435 --> 00:52:05,575
0,120 120,395 445,705 705,825 825,1140
the questions that the headlines

1789
00:52:05,575 --> 00:52:07,140
0,180 180,405 405,755 1015,1290 1290,1565
were raising earlier, which is

1790
00:52:07,310 --> 00:52:08,605
0,365 365,575 575,710 710,965 965,1295
when should a human take

1791
00:52:08,605 --> 00:52:09,990
0,330 330,540 540,615 615,1095 1095,1385
control of an autonomous vehicle?

1792
00:52:10,550 --> 00:52:11,695
0,305 305,530 530,725 725,935 935,1145
What types of data are

1793
00:52:11,695 --> 00:52:13,795
0,720 720,1005 1005,1305 1305,1845 1845,2100
underrepresented in commercial autonomous driving

1794
00:52:13,795 --> 00:52:15,930
0,545 925,1200 1200,1440 1440,1770 1770,2135
pipelines? We now have educated

1795
00:52:16,100 --> 00:52:17,845
0,400 450,680 680,830 830,1150 1470,1745
answers to these questions due

1796
00:52:17,845 --> 00:52:19,030
0,150 150,390 390,645 645,975 975,1185
to products that themis is

1797
00:52:19,030 --> 00:52:20,380
0,320
developing.|
|

1798
00:52:21,440 --> 00:52:22,615
0,320 320,530 530,860 860,1010 1010,1175
And in spheres such as
在医学和医疗保健等领域，我们现在可以回答这样的问题：一个模型什么时候对危及生命的诊断是不确定的？在将此信息传达给患者之前，应在什么时候将此诊断传递给医学专业人员？或者药物发现算法可能对哪类患者有偏见？

1799
00:52:22,615 --> 00:52:24,205
0,305 385,660 660,870 870,1205 1315,1590
medicine and health care, we

1800
00:52:24,205 --> 00:52:25,405
0,135 135,300 300,585 585,945 945,1200
can now answer questions such

1801
00:52:25,405 --> 00:52:26,760
0,275 415,750 750,960 960,1095 1095,1355
as when is a model

1802
00:52:26,870 --> 00:52:28,240
0,400 450,710 710,830 830,1010 1010,1370
uncertain about a life threatening

1803
00:52:28,240 --> 00:52:30,130
0,500 820,1110 1110,1260 1260,1395 1395,1890
diagnosis? When should this diagnosis

1804
00:52:30,130 --> 00:52:31,200
0,270 270,540 540,735 735,810 810,1070
be passed to a medical

1805
00:52:31,250 --> 00:52:33,250
0,400 720,1070 1070,1400 1400,1745 1745,2000
professional before this information is

1806
00:52:33,250 --> 00:52:35,190
0,390 390,480 480,555 555,800 1540,1940
conveyed to a patient? Or

1807
00:52:35,390 --> 00:52:36,730
0,335 335,575 575,755 755,1025 1025,1340
what types of patients might

1808
00:52:36,730 --> 00:52:38,500
0,285 285,650 700,1170 1170,1395 1395,1770
drug discovery algorithms be biased

1809
00:52:38,500 --> 00:52:39,980
0,350
against?|
|

1810
00:52:39,980 --> 00:52:41,915
0,150 150,440 670,1050 1050,1430 1630,1935
And today, the, the application
而今天，你们将关注的应用程序是面部检测。在今天的实验室中，您将使用CAPSA来彻底分析我们以某些方式为您扰乱的常见面部检测数据集，以便您可以自己发现它们。我们强烈鼓励您参加比赛，比赛的细节在实验室中描述。但从根本上讲，它是关于分析这些数据集，创建风险意识模型，以减少特定培训渠道中的偏见和不确定性。

1811
00:52:41,915 --> 00:52:42,740
0,165 165,300 300,465 465,630 630,825
that you guys will focus

1812
00:52:42,740 --> 00:52:44,590
0,320 430,720 720,915 915,1320 1320,1850
on is on facial detection.

1813
00:52:44,910 --> 00:52:46,415
0,350 350,530 530,995 995,1145 1145,1505
You'll use capsa in today's

1814
00:52:46,415 --> 00:52:49,210
0,275 535,935 1435,1965 1965,2460 2460,2795
lab to thoroughly analyze a

1815
00:52:49,440 --> 00:52:51,520
0,400 720,1175 1175,1535 1535,1745 1745,2080
common facial detection data set

1816
00:52:51,570 --> 00:52:52,745
0,260 260,500 500,905 905,1025 1025,1175
that we've perturbed in some

1817
00:52:52,745 --> 00:52:53,780
0,225 225,495 495,735 735,915 915,1035
ways for you so that

1818
00:52:53,780 --> 00:52:54,620
0,120 120,315 315,555 555,720 720,840
you can discover them on

1819
00:52:54,620 --> 00:52:56,525
0,135 135,410 850,1200 1200,1550 1600,1905
your own. {And,we, -} we

1820
00:52:56,525 --> 00:52:57,875
0,305 355,705 705,915 915,1110 1110,1350
highly encourage you to compete

1821
00:52:57,875 --> 00:52:59,975
0,150 150,395 415,815 1285,1685 1765,2100
in the competition, which the

1822
00:52:59,975 --> 00:53:00,950
0,225 225,450 450,690 690,855 855,975
details are described in the

1823
00:53:00,950 --> 00:53:02,555
0,260 400,645 645,890 1000,1365 1365,1605
{lab.,But -} basically it's about

1824
00:53:02,555 --> 00:53:04,300
0,585 585,810 810,1020 1020,1350 1350,1745
analyzing this data set, creating

1825
00:53:04,350 --> 00:53:06,710
0,380 380,755 755,1150 1500,1865 1865,2360
risk aware models that mitigate

1826
00:53:06,710 --> 00:53:08,300
0,405 405,675 675,1040 1150,1425 1425,1590
bias and uncertainty in the

1827
00:53:08,300 --> 00:53:10,320
0,290 310,660 660,1010
specific training pipeline.|
|

1828
00:53:11,410 --> 00:53:12,735
0,290 290,485 485,680 680,1070 1070,1325
And so at themis our
因此，在Themis，我们的目标是在各个行业和世界各地设计、推进和部署值得信赖的人工智能。嗯，我们热衷于科学创新。我们发布了像你们今天将使用的开源工具，我们的产品改变了人工智能工作流程，使人工智能对每个人都更安全。我们与全球各地的行业合作，我们正在为即将到来的夏季和全职职位招聘。因此，如果你感兴趣，请发一封电子邮件给Fomousus I Dot io的求职人员，或者通过向深度学习简历投递来申请，我们会看到这些简历并回复你。谢谢。

1829
00:53:12,735 --> 00:53:14,175
0,225 225,435 435,630 630,935 1075,1440
goal is to design, advance,

1830
00:53:14,175 --> 00:53:16,250
0,315 315,645 645,1290 1290,1625 1675,2075
and deploy trustworthy AI across

1831
00:53:16,390 --> 00:53:17,600
0,335 335,575 575,770 770,935 935,1210
industries and around the world.

1832
00:53:18,100 --> 00:53:20,180
0,400 630,1055 1055,1565 1565,1775 1775,2080
{EM,,we're -} passionate about scientific

1833
00:53:20,440 --> 00:53:22,290
0,400 720,1040 1040,1340 1340,1610 1610,1850
{innovation.,We -} release open source

1834
00:53:22,290 --> 00:53:23,340
0,285 285,510 510,645 645,795 795,1050
tools like the ones you'll

1835
00:53:23,340 --> 00:53:25,100
0,180 180,500 910,1230 1230,1455 1455,1760
use today, and our products

1836
00:53:25,420 --> 00:53:26,985
0,365 365,605 605,1100 1100,1310 1310,1565
transform AI workflows and make

1837
00:53:26,985 --> 00:53:29,000
0,360 360,725 775,1275 1275,1595 1615,2015
artificial intelligence safer {for,everyone. -}

1838
00:53:29,590 --> 00:53:31,140
0,275 275,515 515,875 875,1250 1250,1550
We partner with industries around

1839
00:53:31,140 --> 00:53:32,880
0,210 210,500 790,1185 1185,1515 1515,1740
the globe and we're hiring

1840
00:53:32,880 --> 00:53:34,185
0,225 225,420 420,705 705,1005 1005,1305
for the upcoming summer and

1841
00:53:34,185 --> 00:53:35,610
0,195 195,360 360,585 585,905 1195,1425
for {full,time -} roles. So

1842
00:53:35,610 --> 00:53:36,870
0,75 75,405 405,780 780,1080 1080,1260
if you're interested, please send

1843
00:53:36,870 --> 00:53:38,115
0,195 195,480 480,735 735,1080 1080,1245
an email to careers at

1844
00:53:38,115 --> 00:53:39,870
0,330 330,555 555,720 720,1145 1375,1755
famousus I dot io or

1845
00:53:39,870 --> 00:53:41,270
0,360 360,645 645,945 945,1125 1125,1400
apply by submitting your resume

1846
00:53:41,560 --> 00:53:42,680
0,245 245,350 350,515 515,770 770,1120
to the deep learning resume

1847
00:53:42,700 --> 00:53:43,770
0,350 350,590 590,815 815,920 920,1070
drop and we'll see those

1848
00:53:43,770 --> 00:53:44,820
0,405 405,570 570,705 705,900 900,1050
resumes and {get,back -} to

1849
00:53:44,820 --> 00:53:46,540
0,230 250,555 555,860
you. Thank you.|
|
