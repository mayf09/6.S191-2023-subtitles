1
00:00:09,090 --> 00:00:12,070
嗨，大家好，欢迎回到深度学习导论，

2
00:00:12,810 --> 00:00:15,100
昨天我们有一个非常棒的开讲日，

3
00:00:15,180 --> 00:00:17,915
所以我们希望在整个星期保持同样的势头，

4
00:00:17,915 --> 00:00:18,970
从今天开始，

5
00:00:19,680 --> 00:00:22,720
今天，我们很高兴能够讨论，

6
00:00:23,010 --> 00:00:25,100
也是在这门课上我最喜欢的话题之一，

7
00:00:25,100 --> 00:00:30,790
那就是我们如何制造能够实现视觉的计算机。

8
00:00:32,320 --> 00:00:34,310
现在，我相信视觉，

9
00:00:34,630 --> 00:00:40,310
具体地说，就像我说的，视觉是我们所有人最重要的人类感官之一，

10
00:00:40,900 --> 00:00:44,340
事实上，有视力的人很大程度上依赖于视觉，

11
00:00:44,340 --> 00:00:45,440
在我们的日常生活中，

12
00:00:45,490 --> 00:00:49,400
从四处走动，环游世界，

13
00:00:49,810 --> 00:00:53,720
到与同事和同龄人互动和感知其他情感，

14
00:00:53,950 --> 00:00:57,860
今天，我们将学习如何使用深度学习和机器学习

15
00:00:58,600 --> 00:01:00,680
来构建强大的视觉系统，

16
00:01:01,030 --> 00:01:04,485
这些系统可以看到和预测什么是什么，

17
00:01:04,485 --> 00:01:07,550
只需查看原始的视觉输入，

18
00:01:07,840 --> 00:01:09,470
我喜欢把这句话看作是

19
00:01:09,550 --> 00:01:15,650
对实现视觉真正意味着什么的一个非常简洁和甜蜜的定义。

20
00:01:16,240 --> 00:01:21,710
但在其核心，视觉实际上远远不止是理解什么是在哪里，

21
00:01:22,060 --> 00:01:23,540
它也更深入，

22
00:01:23,620 --> 00:01:24,830
比如这个场景，

23
00:01:24,850 --> 00:01:27,020
我们可以建立计算机视觉系统，

24
00:01:27,160 --> 00:01:31,130
当然，它可以识别这个环境中的所有对象，

25
00:01:31,330 --> 00:01:35,600
首先从黄色的出租车或停在路边的货车开始，

26
00:01:36,220 --> 00:01:40,970
但我们也需要在更深的层面上了解这些物体，

27
00:01:41,020 --> 00:01:42,225
不仅是它们在哪里，

28
00:01:42,225 --> 00:01:46,200
而且是预测未来，预测下一步可能发生的事情，

29
00:01:46,200 --> 00:01:52,140
例如，黄色出租车未来更有可能移动和动态，

30
00:01:52,140 --> 00:01:53,510
因为它位于车道中央，

31
00:01:53,830 --> 00:01:56,780
而不是停在路边的白色面包车，

32
00:01:56,980 --> 00:01:58,910
即使你只看着一张图片，

33
00:01:59,320 --> 00:02:02,670
你的大脑也可以推断出所有这些非常微妙的线索，

34
00:02:02,670 --> 00:02:04,755
它一直延伸到道路上的行人，

35
00:02:04,755 --> 00:02:10,040
甚至红绿灯和场景中其他地方的更微妙的线索，

36
00:02:10,720 --> 00:02:14,985
现在，在场景中解释所有这些细节是一个非同寻常的挑战，

37
00:02:14,985 --> 00:02:18,360
但作为人类，我们在一瞬间就做到了这一点，

38
00:02:18,360 --> 00:02:20,670
可能我把这个幻灯片的图片放上来，

39
00:02:20,670 --> 00:02:24,980
你们所以人一瞬间就可以推理出很多微秒的细节，

40
00:02:25,180 --> 00:02:26,870
在没有我指出的情况下，

41
00:02:27,370 --> 00:02:29,085
但今天课堂上的问题是，

42
00:02:29,085 --> 00:02:32,030
我们如何构建机器学习和深度学习算法，

43
00:02:32,320 --> 00:02:37,200
可以实现对我们世界的同样类型和微妙的理解。

44
00:02:38,210 --> 00:02:43,500
尤其是深度学习真正引领了这场计算机视觉革命，

45
00:02:43,580 --> 00:02:45,570
并实现了计算机的现场定位，

46
00:02:46,520 --> 00:02:52,110
例如，使机器人能够在其环境中捕捉到这些关键的视觉线索，

47
00:02:53,030 --> 00:02:56,580
这对于真正与我们人类一起导航世界至关重要，

48
00:02:56,960 --> 00:03:00,565
你今天将要学习的这些算法已经变得如此主流，

49
00:03:00,565 --> 00:03:03,240
事实上，它们适用于你所有的智能手机，

50
00:03:03,410 --> 00:03:04,350
放在你的口袋里，

51
00:03:04,580 --> 00:03:06,600
处理你拍摄的每一张图像，

52
00:03:06,740 --> 00:03:10,380
增强这些图像，检测人脸，等等。

53
00:03:10,790 --> 00:03:12,940
我们看到了一些令人兴奋的进步，

54
00:03:12,940 --> 00:03:14,460
从生物学和医学，

55
00:03:14,630 --> 00:03:16,350
我们将在今天晚些时候讨论，

56
00:03:16,760 --> 00:03:19,770
到自动驾驶和可访问性。

57
00:03:20,630 --> 00:03:21,505
正如我所说的，

58
00:03:21,505 --> 00:03:27,505
深度学习在过去十年左右的时间里像风暴一样席卷了整个领域，

59
00:03:27,505 --> 00:03:30,480
因为它的能力就像我们昨天讨论的那样，

60
00:03:30,800 --> 00:03:37,470
它能够直接从原始数据和原始图像输入中学习它在环境中看到的东西，

61
00:03:38,120 --> 00:03:40,590
并明确地学习如何执行，

62
00:03:41,120 --> 00:03:42,240
就像我们昨天谈到的，

63
00:03:42,650 --> 00:03:46,590
环境中那些图像的特征提取，

64
00:03:46,700 --> 00:03:49,380
其中一个例子是通过面部检测和识别，

65
00:03:50,030 --> 00:03:54,270
你们所有人都将在今天和明天的实验中练习，

66
00:03:54,320 --> 00:03:57,240
作为这门课总决赛的一部分。

67
00:03:58,100 --> 00:04:01,320
计算机视觉的另一个真正的例子是

68
00:04:01,370 --> 00:04:03,600
在自动驾驶和自动驾驶车辆中，

69
00:04:03,710 --> 00:04:05,640
我们可以将一幅图像作为输入，

70
00:04:05,660 --> 00:04:08,400
或者可能将视频作为输入，多幅图像，

71
00:04:09,050 --> 00:04:10,560
并处理所有这些数据，

72
00:04:10,700 --> 00:04:13,495
这样我们就可以训练汽车学习，

73
00:04:13,495 --> 00:04:18,780
如何驾驶方向盘或控制油门或执行制动命令，

74
00:04:19,100 --> 00:04:20,610
这个整个控制系统，

75
00:04:20,980 --> 00:04:25,200
汽车的转向、油门和刹车可以端到端地执行，

76
00:04:25,400 --> 00:04:29,580
方法是将车辆的图像和传感模式作为输入，

77
00:04:29,750 --> 00:04:32,550
并学习如何预测这些驱动命令，

78
00:04:32,840 --> 00:04:35,020
实际上，这种端到端的方法，

79
00:04:35,020 --> 00:04:37,200
让一个神经网络来做所有这些事情，

80
00:04:37,400 --> 00:04:42,175
与绝大多数自动驾驶汽车公司完全不同，

81
00:04:42,175 --> 00:04:45,030
例如，如果你看一下 Waymo ，那是一种截然不同的方法，

82
00:04:45,260 --> 00:04:48,300
但我们将在今天的课上讨论这些方法，

83
00:04:48,470 --> 00:04:53,760
事实上，这是我们在 MIT 建造的一辆车，

84
00:04:54,050 --> 00:04:56,820
在我的实验室里，就在这个房间上方几层，

85
00:04:57,260 --> 00:05:00,450
再次，我们将分享这项令人难以置信的工作的一些细节。

86
00:05:00,740 --> 00:05:02,970
当然，它并不止步于自动驾驶，

87
00:05:03,230 --> 00:05:07,230
这些你们将在今天的课程中直接学习的相同算法，

88
00:05:07,670 --> 00:05:11,760
可以一直扩展到影响医疗保健、医疗决策，

89
00:05:12,230 --> 00:05:15,120
最终甚至在这些无障碍应用程序中，

90
00:05:15,170 --> 00:05:19,350
我们看到计算机视觉算法帮助视障人士，

91
00:05:19,520 --> 00:05:20,910
举个例子，在这个项目中，

92
00:05:21,020 --> 00:05:25,780
研究人员已经建立了深度学习设备，可以检测步道，

93
00:05:26,250 --> 00:05:30,310
这样视力受损的跑步者就可以得到声音反馈，

94
00:05:30,690 --> 00:05:34,330
这样他们也可以在外出跑步时导航。

95
00:05:35,410 --> 00:05:36,480
就像我说的，

96
00:05:36,480 --> 00:05:41,030
我们经常把今天课程中将要讨论的许多任务视为理所当然的，

97
00:05:41,260 --> 00:05:44,150
因为我们在日常生活中无缝地完成了这些任务，

98
00:05:44,770 --> 00:05:48,560
但今天这门课的问题是，

99
00:05:48,970 --> 00:05:49,760
在它的核心，

100
00:05:49,930 --> 00:05:53,610
我们如何制造一台计算机来做这些不可思议的事情，

101
00:05:53,610 --> 00:05:55,940
我们每天都认为理所当然的。

102
00:05:56,690 --> 00:05:58,920
具体地说，我们将从这个问题开始，

103
00:05:59,900 --> 00:06:02,125
计算机到底是如何看东西的，

104
00:06:02,125 --> 00:06:03,850
更详细的是，

105
00:06:03,850 --> 00:06:06,000
计算机是如何处理图像的，

106
00:06:06,170 --> 00:06:07,050
如果我们认为，

107
00:06:07,250 --> 00:06:09,660
[网站]是通过图像进入计算机的，

108
00:06:10,220 --> 00:06:13,170
那么计算机如何开始处理这些图像。

109
00:06:14,060 --> 00:06:17,310
对计算机来说，图像只是数字，

110
00:06:18,020 --> 00:06:21,420
例如，假设我们这里有一张 Abraham Lincoln 的照片，

111
00:06:22,560 --> 00:06:25,420
好的，这张图片是由所谓的像素组成的，

112
00:06:25,560 --> 00:06:28,750
每个像素在这张图像中只是一个点，

113
00:06:29,250 --> 00:06:30,875
因为这是一个灰度图像，

114
00:06:30,875 --> 00:06:34,120
所以每个像素都只是一个数字，

115
00:06:34,680 --> 00:06:39,880
现在我们可以将我们的图像表示为这个二维数字矩阵，

116
00:06:40,470 --> 00:06:42,730
因为，就像我说的，这是一幅灰度图像，

117
00:06:42,900 --> 00:06:47,290
每个像素只对应于该矩阵位置上的一个数字，

118
00:06:47,860 --> 00:06:50,580
现在假设，例如，我们没有灰度图像，

119
00:06:50,580 --> 00:06:53,420
我们有一个彩色图像，它将是 RGB 图像，

120
00:06:53,920 --> 00:06:57,945
所以现在每个像素将不只由一个数字组成，而是由三个数字组成，

121
00:06:57,945 --> 00:07:02,475
所以你可以把它想象成三维矩阵，而不是二维矩阵，

122
00:07:02,475 --> 00:07:07,640
你有三个二维矩阵堆叠在一起。

123
00:07:08,410 --> 00:07:13,520
所以现在有了这个基本的图像数字表示的基础，

124
00:07:13,540 --> 00:07:15,210
我们可以开始考虑，

125
00:07:15,210 --> 00:07:20,030
我们如何或者我们可以建立什么类型的计算机视觉算法，

126
00:07:20,200 --> 00:07:23,600
来将这些系统作为输入，以及它们可以执行什么操作。

127
00:07:23,680 --> 00:07:26,540
所以我想跟你们谈的第一件事是，

128
00:07:26,920 --> 00:07:31,340
我们到底想要训练这些系统来完成什么样的图像任务，

129
00:07:31,900 --> 00:07:35,415
大体上讲，有两大类任务，

130
00:07:35,415 --> 00:07:37,580
我们在昨天的课程中稍微谈到了这一点，

131
00:07:37,600 --> 00:07:40,310
但为了在今天的课程中更具体一些，

132
00:07:40,570 --> 00:07:43,910
这两项任务要么是分类，要么是回归，

133
00:07:44,530 --> 00:07:51,150
对于回归，你的预测值会有一个连续的值，

134
00:07:51,150 --> 00:07:53,330
这可以是数轴上的任何实数，

135
00:07:53,560 --> 00:07:59,390
但在分类中，你的预测可能是 k 或 n 个不同的类别之一，

136
00:08:00,040 --> 00:08:01,610
这些是离散的不同类别。

137
00:08:01,630 --> 00:08:05,060
所以，让我们首先考虑图像分类的任务，

138
00:08:06,010 --> 00:08:11,025
在这个任务中，我们想要为每个单独的图像预测一个单独的标签，

139
00:08:11,025 --> 00:08:16,820
我们预测的这个标签将是可以考虑的 n 个不同的可能标签之一，

140
00:08:17,110 --> 00:08:20,660
例如，假设我们有一系列美国总统的图像，

141
00:08:21,130 --> 00:08:23,060
我们想要建立一个分类管道，

142
00:08:23,080 --> 00:08:27,710
来告诉我们在屏幕上看到的这张图像中是哪位总统，

143
00:08:29,420 --> 00:08:34,960
在这种情况下，我们模型的目标基本上是输出一个概率分数，

144
00:08:35,100 --> 00:08:39,010
这个图像包含这些不同总统之一的概率，

145
00:08:39,600 --> 00:08:42,140
最大的分数最终将是

146
00:08:42,140 --> 00:08:45,130
我们推断为图像中正确的总统的那个。

147
00:08:46,080 --> 00:08:50,980
所以，为了正确地执行这项任务，并正确地对这些图像进行分类，

148
00:08:51,810 --> 00:08:56,495
我们的计算机视觉模型需要能够告诉我们

149
00:08:56,495 --> 00:09:01,540
这张特定的例如 Abraham Lincoln 图像的独特之处，

150
00:09:01,890 --> 00:09:07,690
与 George Washington 的不同图像和 Obama 的不同图像。

151
00:09:08,310 --> 00:09:13,120
现在，考虑图像分类或图像处理的整个问题的另一种方式，

152
00:09:14,070 --> 00:09:16,930
从高层次上是，从特征的角度，

153
00:09:17,340 --> 00:09:22,360
或者将这些视为你的数据的模式或特定类别的特征，

154
00:09:22,710 --> 00:09:25,240
分类可以简单的完成，

155
00:09:25,530 --> 00:09:29,050
通过检测数据中所有这些不同的模式，

156
00:09:29,310 --> 00:09:33,730
并识别某些模式何时出现在其他模式上，

157
00:09:34,350 --> 00:09:38,950
例如，如果某个特定类别的特征出现在图像中，

158
00:09:39,300 --> 00:09:42,410
那么你可能会推断该图像属于该类别，

159
00:09:42,410 --> 00:09:44,530
例如，如果你想要检测汽车，

160
00:09:44,670 --> 00:09:48,995
你可能会在你的数据中查找车轮、车牌或前灯等数据模式，

161
00:09:48,995 --> 00:09:50,770
如果这些东西出现在你的图像中，

162
00:09:51,120 --> 00:09:53,290
那么你可以相当自信地说，

163
00:09:53,370 --> 00:09:56,590
你的图像是一辆汽车，而不是这些其他类别。

164
00:09:57,120 --> 00:09:59,230
所以，如果我们构建一条计算机视觉管道，

165
00:09:59,790 --> 00:10:01,865
我们有两个主要步骤需要考虑，

166
00:10:01,865 --> 00:10:07,390
第一步是我们需要知道我们在数据中寻找什么特征或模式，

167
00:10:07,620 --> 00:10:10,870
第二步是我们需要检测这些模式，

168
00:10:10,950 --> 00:10:11,855
一旦我们检测到它们，

169
00:10:11,855 --> 00:10:13,870
我们就可以推断我们所在的类别。

170
00:10:15,300 --> 00:10:19,640
现在，解决这个问题的一种方法是利用我们特定领域的知识，

171
00:10:19,640 --> 00:10:21,620
所以，如果我们对我们的领域有所了解，

172
00:10:21,620 --> 00:10:23,740
例如关于人脸的知识，

173
00:10:24,360 --> 00:10:27,160
我们可以用这些知识来定义我们的特征，

174
00:10:27,720 --> 00:10:28,715
什么构成了一张脸，

175
00:10:28,715 --> 00:10:31,930
我们知道脸是由眼睛、鼻子和耳朵组成的，

176
00:10:32,340 --> 00:10:35,200
我们可以定义每个组件的外观，

177
00:10:35,460 --> 00:10:36,640
在定义我们的特征时，

178
00:10:37,260 --> 00:10:39,410
但这种方法有一个很大的问题，

179
00:10:39,410 --> 00:10:45,010
记得图像就是这些数字的三维数组，

180
00:10:45,810 --> 00:10:50,230
即使在同一类型的物体中，它们也可能有很大的变化，

181
00:10:50,670 --> 00:10:53,105
这些变化可以包括任何东西，

182
00:10:53,105 --> 00:10:58,750
从遮挡到照明，旋转、平移，在类别变动中。

183
00:10:59,780 --> 00:11:01,165
这里的问题是，

184
00:11:01,165 --> 00:11:04,800
我们的分类流水线需要处理

185
00:11:05,300 --> 00:11:09,150
所有这些不同类型的变化并保持不变的能力，

186
00:11:09,920 --> 00:11:13,530
同时仍然对所有种类间变化敏感，

187
00:11:13,580 --> 00:11:16,290
不同种类之间发生的变化。

188
00:11:17,210 --> 00:11:21,060
尽管我们的管道可以使用我们作为人类的特征，

189
00:11:21,290 --> 00:11:25,080
基于我们的一些先验知识，手动定义的特征，

190
00:11:25,730 --> 00:11:27,780
问题被分解为，

191
00:11:28,220 --> 00:11:32,370
这些特征变得非常不健壮，

192
00:11:32,630 --> 00:11:38,010
当考虑到图像在现实世界中的大量不同变化时。

193
00:11:39,080 --> 00:11:41,635
所以在实践中，就像我说的，

194
00:11:41,635 --> 00:11:45,685
你的算法需要能够承受所有这些不同类型的变化，

195
00:11:45,685 --> 00:11:47,380
然后自然的问题是，

196
00:11:47,380 --> 00:11:50,860
我们如何建立一个计算机视觉算法来做到这一点，

197
00:11:50,860 --> 00:11:53,730
并仍然保持这种水平的健壮性，

198
00:11:53,960 --> 00:11:56,340
我们想要的是一种提取特征的方法，

199
00:11:56,420 --> 00:12:02,160
既能检测数据中的那些特征，那些模式，

200
00:12:02,810 --> 00:12:05,730
又能以分层的方式做到这一点，

201
00:12:06,170 --> 00:12:12,360
从头开始，从像素级别到具有语义意义的东西，

202
00:12:12,560 --> 00:12:15,690
例如，人脸的眼睛或鼻子。

203
00:12:16,930 --> 00:12:19,185
我们在上一节课中学到了，

204
00:12:19,185 --> 00:12:22,400
我们可以使用神经网络来解决这类问题，

205
00:12:23,050 --> 00:12:27,200
神经网络能够直接从数据中学习特征，

206
00:12:28,170 --> 00:12:31,990
而且，最重要的是，学习一组分层的功能，

207
00:12:32,280 --> 00:12:34,900
在之前的功能基础上构建，

208
00:12:35,070 --> 00:12:37,390
构建越来越复杂的功能集。

209
00:12:39,760 --> 00:12:45,660
现在，我们将确切地了解神经网络如何在图像领域做到这一点，

210
00:12:45,770 --> 00:12:46,980
作为这节课的一部分，

211
00:12:47,570 --> 00:12:50,040
具体地说，神经网络将允许我们，

212
00:12:50,480 --> 00:12:53,790
从视觉数据中学习这些视觉特征，

213
00:12:54,050 --> 00:12:55,840
如果我们巧妙地构建这些视觉特征，

214
00:12:55,840 --> 00:12:57,250
这里的关键点是，

215
00:12:57,250 --> 00:13:01,170
我们在昨天的课程中了解到的模型和体系结构，

216
00:13:01,610 --> 00:13:02,910
到目前为止，

217
00:13:03,380 --> 00:13:10,950
我们将看到它们如何不适合或扩展到今天的图像问题域，

218
00:13:11,120 --> 00:13:15,720
以及我们如何更巧妙地构建和构造神经网络来克服这些问题。

219
00:13:15,980 --> 00:13:20,520
让我们先回顾一下我们在第一课中谈到的内容，

220
00:13:20,930 --> 00:13:23,940
也就是我们学习的全连接网络，

221
00:13:24,140 --> 00:13:27,805
这些网络有多个隐藏层，

222
00:13:27,805 --> 00:13:33,030
在给定的隐藏层中的每个神经元都连接到它的前一层中的每个神经元，

223
00:13:33,500 --> 00:13:39,090
所以，它接收作为这些完全连接的层的函数的所有先前层的输入，

224
00:13:39,580 --> 00:13:44,785
现在，让我们假设我们想要直接使用完全连接的网络，而不做任何修改，

225
00:13:44,785 --> 00:13:46,710
就像我们在第一课中了解到的那样，

226
00:13:47,210 --> 00:13:49,140
使用图像处理管道，

227
00:13:49,370 --> 00:13:53,280
直接使用图像并将其提供给完全连接网络，

228
00:13:53,720 --> 00:13:55,050
我们能做些类似的事情吗。

229
00:13:55,550 --> 00:13:56,940
事实上，在这种情况下，我们可以，

230
00:13:56,960 --> 00:13:58,500
我们要做的是，

231
00:13:58,760 --> 00:14:01,860
记住，因为我们的图像是一个二维数组，

232
00:14:02,000 --> 00:14:05,700
我们必须做的第一件事是将它折叠成一个一维数字序列，

233
00:14:05,990 --> 00:14:10,495
因为一个完全连接网络不是接受一个二维数组，

234
00:14:10,495 --> 00:14:12,090
而是接受一个一维序列。

235
00:14:13,190 --> 00:14:15,180
所以，我们必须做的第一件事是，

236
00:14:15,620 --> 00:14:19,680
将二维数组展平为像素值的向量，

237
00:14:19,700 --> 00:14:21,000
并将其提供给我们的网络，

238
00:14:21,710 --> 00:14:29,710
在这种情况下，我们第一层中的每个神经元都连接到输入层中的所有神经元，

239
00:14:29,710 --> 00:14:31,830
所以，在原始图像展平后，

240
00:14:32,330 --> 00:14:34,555
我们将所有这些像素提供给第一层，

241
00:14:34,555 --> 00:14:39,210
在这里，你应该已经意识到一个非常重要的概念，

242
00:14:39,650 --> 00:14:45,210
那就是真正定义我们图像的每一条空间信息，

243
00:14:45,470 --> 00:14:48,940
使图像成为图像的信息都已经完全消失了，

244
00:14:48,940 --> 00:14:50,560
在我们开始这个问题之前，

245
00:14:50,560 --> 00:14:54,270
因为我们已经将二维图像展平为一维数组，

246
00:14:54,320 --> 00:14:57,390
所以我们已经完全摧毁了空间信息的所有概念。

247
00:14:58,570 --> 00:15:02,720
此外，我们确实有大量的参数，

248
00:15:03,190 --> 00:15:05,150
因为这个系统是完全连接的，

249
00:15:05,200 --> 00:15:07,490
举个例子，在一个非常非常小的图像中，

250
00:15:07,780 --> 00:15:10,200
它甚至是 100 乘以 100 像素，

251
00:15:10,200 --> 00:15:12,290
在今天的标准下，这是一个令人难以置信的小图像，

252
00:15:12,670 --> 00:15:15,920
但这将需要在第一层中的 10000 个神经元，

253
00:15:16,090 --> 00:15:19,190
它将连接到，比如第二层中的 10000 个神经元，

254
00:15:19,690 --> 00:15:24,710
仅在这一层中的参数数量就将是 10000 个平方参数，

255
00:15:24,880 --> 00:15:26,460
这将是非常低效的，

256
00:15:26,460 --> 00:15:28,905
你可以想象，如果您想要将此网络扩展到

257
00:15:28,905 --> 00:15:32,055
我们今天必须处理的合理大小的图像，

258
00:15:32,055 --> 00:15:33,530
所以在实践中是不可行的，

259
00:15:34,060 --> 00:15:36,045
相反，我们需要问问自己，

260
00:15:36,045 --> 00:15:39,920
我们如何才能建立和维护一些非常独特的空间结构，

261
00:15:39,970 --> 00:15:43,920
这种空间结构在我们的输入中，

262
00:15:43,920 --> 00:15:45,860
以及最重要的是，在我们的模型中。

263
00:15:47,830 --> 00:15:53,505
要做到这一点，让我们将二维图像表示为其原始形式，

264
00:15:53,505 --> 00:15:56,030
即一个二维数组，

265
00:15:56,950 --> 00:16:02,325
我们可以使用空间结构连接我们的输入的一种方式是，

266
00:16:02,325 --> 00:16:08,445
将我们输入的这些部分连接到隐藏层中的神经元，

267
00:16:08,445 --> 00:16:13,005
例如，假设这里你可以看到的隐藏层中的每个神经元，

268
00:16:13,005 --> 00:16:15,830
它们将只看到或做出反应，

269
00:16:16,000 --> 00:16:22,800
对于上一层中的一组或某一块神经元，

270
00:16:22,800 --> 00:16:25,425
所以你也可以认为这是一个感受野，

271
00:16:25,425 --> 00:16:30,075
或者你下一层中的单个神经元在上一层中所能关注的，

272
00:16:30,075 --> 00:16:31,310
不是整个图像，

273
00:16:31,690 --> 00:16:34,580
而是你上一张图像中的一个小的感受野，

274
00:16:35,410 --> 00:16:39,080
现在注意输入层的区域，

275
00:16:39,730 --> 00:16:41,600
你可以在左侧看到，

276
00:16:42,340 --> 00:16:45,345
如何影响右侧的单个神经元，

277
00:16:45,345 --> 00:16:47,430
这只是下一层中的一个神经元，

278
00:16:47,430 --> 00:16:53,625
当然，你可以想象在整个输入中定义这些连接，

279
00:16:53,625 --> 00:16:56,570
每一次，你的输入上都有块，

280
00:16:56,950 --> 00:17:00,530
对应于另一层上的单一神经元，

281
00:17:01,420 --> 00:17:02,790
我们可以应用相同的原理，

282
00:17:02,790 --> 00:17:08,760
将整个图像中的这些块连接到后续层中的单个神经元。

283
00:17:08,760 --> 00:17:11,090
我们可以通过滑动块来做到这一点，

284
00:17:12,160 --> 00:17:14,510
在输入图像上逐个像素，

285
00:17:14,890 --> 00:17:18,920
我们将在输出层上使用另一幅图像进行响应。

286
00:17:20,010 --> 00:17:27,590
通过这种方式，我们基本上保留了我们输入所固有的所有非常关键和丰富的空间信息，

287
00:17:27,590 --> 00:17:30,250
但请记住，这里的最终任务是，

288
00:17:30,390 --> 00:17:33,110
不仅仅是保存空间信息，

289
00:17:33,110 --> 00:17:35,720
我们最终想要学习特征，学习这些模式，

290
00:17:35,720 --> 00:17:38,200
以便我们能够检测和分类这些图像，

291
00:17:38,730 --> 00:17:45,130
我们可以通过[挥动]输入块之间的连接来做到这一点，

292
00:17:45,640 --> 00:17:51,000
而且，为了检测这些特定的特征是什么。

293
00:17:51,800 --> 00:17:53,560
让我在这里举一个实际的例子，

294
00:17:53,560 --> 00:17:59,370
所以在实践中，我描述的这个运算，我描述的这个块和滑动运算，

295
00:17:59,480 --> 00:18:02,700
实际上是一个数据运算，以前被称为卷积，

296
00:18:03,470 --> 00:18:05,460
我们首先认为这是一个高水平，

297
00:18:05,780 --> 00:18:09,870
假设我们有一个 4x4 像素的块，

298
00:18:10,310 --> 00:18:12,180
所以你可以看到这个 4x4 像素的小块，

299
00:18:12,320 --> 00:18:15,720
在左手边用红色表示为一个红色的方框，

300
00:18:16,250 --> 00:18:18,600
让我们假设，例如，

301
00:18:19,040 --> 00:18:20,550
因为我们有一个 4x4 的块，

302
00:18:20,630 --> 00:18:25,230
这将由这一层中的 16 个不同的权重组成，

303
00:18:26,000 --> 00:18:28,720
我们将应用相同的 4x4 ，

304
00:18:28,720 --> 00:18:30,160
让我们称这不再是一个块，

305
00:18:30,160 --> 00:18:31,620
让我们使用术语 filter ，

306
00:18:31,760 --> 00:18:35,680
我们将应用相同的 4x4 filter 作为输入，

307
00:18:35,680 --> 00:18:41,010
并使用该操作的结果来定义下一层中神经元的状态，

308
00:18:41,390 --> 00:18:46,680
现在我们要将 filter 向右移动两个像素，

309
00:18:47,300 --> 00:18:53,130
这将定义未来层中相邻位置的下一个神经元，

310
00:18:53,660 --> 00:18:54,625
我们继续这样做，

311
00:18:54,625 --> 00:18:56,760
你可以看到在右手边，

312
00:18:57,230 --> 00:18:59,650
你不仅滑过了输入图像，

313
00:18:59,650 --> 00:19:03,505
还滑过了第二层的输出神经元，

314
00:19:03,505 --> 00:19:07,800
这就是我们如何开始在非常高的水平上考虑卷积。

315
00:19:09,020 --> 00:19:11,280
但你可能想知道，

316
00:19:11,750 --> 00:19:14,320
不只是卷积运算是如何工作的，

317
00:19:14,320 --> 00:19:18,030
我认为这里真正要缩小范围的主要事情是，

318
00:19:18,260 --> 00:19:23,815
卷积如何让我们了解我们说讨论的数据中的这些特征，这些模式，

319
00:19:23,815 --> 00:19:25,780
因为这最终是我们的最终目标，

320
00:19:25,780 --> 00:19:28,350
这才是我们这门课的真正目标，就是提取这些模式。

321
00:19:28,940 --> 00:19:34,200
所以让我们通过一个具体的例子来让这个问题变得非常具体，

322
00:19:35,230 --> 00:19:40,610
例如，假设我们想要构建一个卷积算法，

323
00:19:40,900 --> 00:19:44,590
来检测或者对图像中的 X 进行分类，

324
00:19:44,880 --> 00:19:46,390
这是图像中的字母 X ，

325
00:19:47,010 --> 00:19:51,725
这里，为了简单，假设我们只有黑白图像，

326
00:19:51,725 --> 00:19:56,080
所以在这张图像中的每一个像素将用 0 或 1 表示，

327
00:19:56,370 --> 00:19:59,050
为了简单，这张图中没有灰度级，

328
00:19:59,460 --> 00:20:04,180
所以我们把黑色表示为 -1 ，白色表示为 1 ，

329
00:20:04,710 --> 00:20:11,120
所以要分类，我们不能简单地比较左手边和右手边，

330
00:20:11,120 --> 00:20:12,880
因为这两个都是 X ，

331
00:20:13,080 --> 00:20:14,290
但是你可以看到，

332
00:20:15,030 --> 00:20:19,130
因为右手边的那个稍微旋转了一下，

333
00:20:19,130 --> 00:20:21,620
它不会直接和左边的 X 对齐，

334
00:20:21,620 --> 00:20:22,750
尽管它是一个 X ，

335
00:20:22,980 --> 00:20:25,450
我们希望在这两个图像中都检测到 X ，

336
00:20:25,920 --> 00:20:30,310
所以我们需要考虑如何更巧妙地检测到定义 X 的那些特征。

337
00:20:30,660 --> 00:20:33,220
所以让我们看看我们如何使用卷积来做到这一点，

338
00:20:33,690 --> 00:20:34,990
例如，在这种情况下，

339
00:20:35,280 --> 00:20:41,590
我们不是希望我们的模型逐块地比较这个 X 的图像，

340
00:20:42,420 --> 00:20:48,130
我们寻找的重要块正是这些将定义我们的 X 的特征，

341
00:20:48,660 --> 00:20:54,690
所以如果我们的模型能够在输入中找到这些大致相同位置的粗略特征块，

342
00:20:54,860 --> 00:20:57,000
我们可以确定，或者我们可以推断，

343
00:20:57,500 --> 00:21:03,265
这两个图像是相同类型或相同字母，

344
00:21:03,265 --> 00:21:07,710
它可以比简单地测量这两张图像之间的相似性好得多，

345
00:21:08,000 --> 00:21:09,820
因为我们是在块级别操作的，

346
00:21:09,820 --> 00:21:13,915
所以把每个块想象成一个微缩的图像，

347
00:21:13,915 --> 00:21:16,470
一个小的二维值数组，

348
00:21:16,580 --> 00:21:23,430
我们可以使用 filter 来捕捉这些小块或小图像何时出现，

349
00:21:23,990 --> 00:21:25,555
所以在 X 的情况下，

350
00:21:25,555 --> 00:21:28,770
这些 filter 可能代表语义的东西，

351
00:21:28,820 --> 00:21:32,455
例如对角线或交叉点，

352
00:21:32,455 --> 00:21:35,340
它们捕捉到了 X 的所有重要特征，

353
00:21:36,260 --> 00:21:44,400
所以我们可能会在 X 的任何图像中捕捉到我们字母的臂部和中心的这些特征，

354
00:21:44,480 --> 00:21:48,840
无论图像如何平移或旋转等等，

355
00:21:49,070 --> 00:21:52,480
注意，即使在这些较小的矩阵中，

356
00:21:52,480 --> 00:21:55,370
这些是权重的 filter ，

357
00:21:55,370 --> 00:22:00,890
这些也只是这些块中每个像素的数值，

358
00:22:00,890 --> 00:22:02,380
只是一个简单的数值，

359
00:22:02,430 --> 00:22:05,350
在某种效果上也有图像，

360
00:22:05,610 --> 00:22:08,470
而所有这些都是这个问题留下的，

361
00:22:09,000 --> 00:22:11,410
在我们讨论的这个想法中，

362
00:22:11,940 --> 00:22:13,870
我们定义了一种操作，

363
00:22:14,430 --> 00:22:16,570
它可以获取这些微小的块，

364
00:22:16,890 --> 00:22:19,450
并尝试提取，检测，

365
00:22:19,470 --> 00:22:23,080
这些块何时出现在图像中，以及何时可能不出现。

366
00:22:24,730 --> 00:22:28,370
这就把我们带回了卷积的概念，

367
00:22:28,480 --> 00:22:31,790
所以卷积正是可以解决这个问题的运算，

368
00:22:32,230 --> 00:22:36,080
卷积保存输入数据的空间信息，

369
00:22:36,250 --> 00:22:42,135
通过学习保存在输入数据中小的方块区域中的图像特征。

370
00:22:42,135 --> 00:22:46,730
所以，为了给出另一个具体的例子来执行这个操作，

371
00:22:47,290 --> 00:22:50,630
我们需要进行逐个元素的乘法，

372
00:22:50,740 --> 00:22:57,165
在 filter 矩阵，那些小块以及输入图像的块之间，

373
00:22:57,165 --> 00:22:59,100
所以你有两个块，

374
00:22:59,100 --> 00:23:04,610
你有权重矩阵块，你想要检测的东西，你可以在这里的左上角看到，

375
00:23:04,990 --> 00:23:11,090
你还有第二个块，这是你希望在输入图像中与之进行比较的东西，

376
00:23:11,710 --> 00:23:12,860
问题是，

377
00:23:13,060 --> 00:23:16,760
你在它们之间观察到的这两块有多相似，

378
00:23:17,410 --> 00:23:22,065
例如，这会产生一个 3x3 的矩阵，

379
00:23:22,065 --> 00:23:26,240
因为你要在两个小的 3x3 矩阵进行逐个元素的乘法，

380
00:23:26,260 --> 00:23:29,360
你会得到另外一个 3x3 的矩阵，

381
00:23:29,920 --> 00:23:31,635
在这种情况下，所有的矩阵，

382
00:23:31,635 --> 00:23:36,080
这个结果矩阵的所有元素都是 1 ，

383
00:23:36,610 --> 00:23:44,520
因为在 filter 中的每个位置，在图像块中的每个位置，都是匹配的，

384
00:23:44,520 --> 00:23:47,510
所以，当我们做逐个像素乘法时，我们全部得到 1 ，

385
00:23:48,190 --> 00:23:53,810
最后一步是我们要累加这个矩阵的结果，逐个像素乘法，

386
00:23:54,580 --> 00:23:57,440
在这种情况下，结果是 9 ，

387
00:23:58,060 --> 00:24:00,890
每个都是 1 ，它是一个 3x3 矩阵，所以结果是 9 。

388
00:24:02,640 --> 00:24:06,225
现在，让我们再考虑一个例子，

389
00:24:06,225 --> 00:24:08,330
现在我们有这张绿色的图像，

390
00:24:08,410 --> 00:24:11,420
我们想要检测黄色的 filter ，

391
00:24:12,640 --> 00:24:18,260
假设我们想用这个 3x3 filter 来计算这个 5x5 图像的卷积，

392
00:24:18,850 --> 00:24:19,440
要做到这一点，

393
00:24:19,440 --> 00:24:23,060
我们需要覆盖整个图像，

394
00:24:23,290 --> 00:24:25,875
通过逐段滑动这个 filter ，

395
00:24:25,875 --> 00:24:31,070
并比较整个图像中这个 filter 的相似性或卷积，

396
00:24:32,180 --> 00:24:34,290
再次，我们通过同样的机制做到这一点，

397
00:24:34,640 --> 00:24:35,700
在每个位置，

398
00:24:35,750 --> 00:24:40,020
我们计算那个块和在图片中位置的逐个元素乘法，

399
00:24:40,220 --> 00:24:42,030
将所有结果条目相加，

400
00:24:42,320 --> 00:24:44,940
并将其传递到下一层。

401
00:24:45,320 --> 00:24:46,360
所以让我们来看一下，

402
00:24:46,360 --> 00:24:48,990
首先，让我们从左上角开始，

403
00:24:49,550 --> 00:24:53,190
我们将 filter 放置在图像的左上角，

404
00:24:53,420 --> 00:24:54,700
我们对元素进行相乘，

405
00:24:54,700 --> 00:24:56,460
我们将所有结果相加，

406
00:24:56,480 --> 00:24:57,300
我们得到 4 ，

407
00:24:57,440 --> 00:25:00,930
这个 4 将被放入下一层，

408
00:25:01,250 --> 00:25:03,600
下一层同样是另一幅图像，

409
00:25:04,010 --> 00:25:07,170
但它被确定为我们卷积运算的结果，

410
00:25:07,790 --> 00:25:10,230
我们滑动那个 filter 到下一个位置，

411
00:25:10,430 --> 00:25:13,750
下一个位置提供了图像中的下一个值，

412
00:25:13,750 --> 00:25:16,950
我们可以一遍又一遍地重复这个过程，

413
00:25:17,330 --> 00:25:20,400
直到我们的 filter 覆盖了整个图像，

414
00:25:20,900 --> 00:25:26,760
我们还完整地填写了输出特征图的结果，

415
00:25:26,870 --> 00:25:29,370
输出特征图就是，你可以认为是，

416
00:25:29,480 --> 00:25:33,960
我们的 filter 每个元素与输入图像中每个位置的匹配程度有多高。

417
00:25:35,810 --> 00:25:40,860
现在我们已经了解了定义卷积运算的机制，

418
00:25:41,480 --> 00:25:43,290
让我们看看如何使用不同的 filter ，

419
00:25:43,820 --> 00:25:47,970
来检测数据中的不同类型的模式，

420
00:25:48,440 --> 00:25:51,120
举个例子，让我们来看看这张女性的脸部照片，

421
00:25:52,130 --> 00:25:58,030
以及对这张照片应用三种不同滤镜的结果，

422
00:25:58,030 --> 00:25:59,640
这样你就可以看到准确的 filter ，

423
00:26:00,200 --> 00:26:01,615
它们都是 3x3 的 filter ，

424
00:26:01,615 --> 00:26:06,060
所以你可以在相应的右下角看到准确的 filter ，

425
00:26:06,500 --> 00:26:08,380
通过应用这三种不同的 filter ，

426
00:26:08,380 --> 00:26:11,035
你可以看到我们如何实现截然不同的结果，

427
00:26:11,035 --> 00:26:15,690
通过简单地改变这些 3x3 矩阵中权重，

428
00:26:15,950 --> 00:26:20,070
你就可以看到我们可以检测到的不同类型特征的可变性，

429
00:26:20,120 --> 00:26:24,420
例如，我们可以设计 filter 来锐化图像，

430
00:26:24,650 --> 00:26:27,600
使图像中的边缘更清晰，

431
00:26:27,710 --> 00:26:30,300
我们可以设计 filter 来提取边缘，

432
00:26:30,740 --> 00:26:32,980
我们可以进行更强的边缘检测，

433
00:26:32,980 --> 00:26:35,820
通过再次修改所有这些 filter 中的权重。

434
00:26:37,420 --> 00:26:40,730
所以现在我希望你们所有人都能体会到这些力量，

435
00:26:40,780 --> 00:26:43,470
首先，是这些 filter 操作，

436
00:26:43,470 --> 00:26:46,160
以及我们数学地定义它们，

437
00:26:46,300 --> 00:26:51,020
在这些更小的基于块的运算和矩阵的形式，

438
00:26:51,160 --> 00:26:52,730
然后我们可以在图像上滑动，

439
00:26:53,200 --> 00:26:55,190
这些概念是如此强大，

440
00:26:55,270 --> 00:26:59,660
因为首先，它们保留了我们原始输入的空间信息，

441
00:26:59,920 --> 00:27:02,870
同时仍在执行特征提取，

442
00:27:03,520 --> 00:27:04,830
现在，你可以想到，

443
00:27:05,000 --> 00:27:08,380
不是像我们在上一张幻灯片中所说的那样定义这些 filter ，

444
00:27:08,380 --> 00:27:09,820
如果我们尝试学习它们，

445
00:27:09,820 --> 00:27:14,605
记住，这些 filter 是我们数据中重要模式的某种代理，

446
00:27:14,605 --> 00:27:19,770
所以我们的神经网络可以尝试学习那些小块 filter 中的元素，

447
00:27:19,970 --> 00:27:21,790
作为神经网络中的权重，

448
00:27:21,790 --> 00:27:25,380
学习这些元素本质上等同于

449
00:27:25,400 --> 00:27:29,310
挑选和学习定义一个类别与另一个类别的模式。

450
00:27:30,900 --> 00:27:35,590
现在我们已经得到了这次行动和理解，

451
00:27:35,640 --> 00:27:37,180
我们可以进一步推进这一步，

452
00:27:37,470 --> 00:27:40,150
我们可以利用这种奇异的卷积运算，

453
00:27:40,470 --> 00:27:46,210
并开始考虑如何在这种运算的基础上构建完整的层，卷积层，

454
00:27:46,230 --> 00:27:50,170
这样我们就可以开始想象卷积网络和神经网络。

455
00:27:50,990 --> 00:27:53,610
首先，我们来看看所谓的，

456
00:27:54,980 --> 00:28:00,085
通过创建卷积层和卷积网络，你最终创建的就是，

457
00:28:00,085 --> 00:28:02,310
所谓的 CNN ，卷积神经网络。

458
00:28:03,020 --> 00:28:06,210
这将是今天课程的核心架构，

459
00:28:06,620 --> 00:28:08,845
让我们考虑一个非常简单的 CNN ，

460
00:28:08,845 --> 00:28:11,190
它是为图像分类而设计的，

461
00:28:11,750 --> 00:28:16,440
这里的任务是直接从原始数据中学习特征，

462
00:28:17,120 --> 00:28:23,250
并使用这些学习的特征对我们想要执行的目标检测任务进行分类。

463
00:28:24,650 --> 00:28:27,450
现在 CNN 有三个主要操作，

464
00:28:27,530 --> 00:28:29,580
我们将在这里一步一步地介绍它们，

465
00:28:29,630 --> 00:28:34,495
但在这节课的其余部分，我们会深入到每一个操作。

466
00:28:34,495 --> 00:28:36,180
所以第一步是卷积，

467
00:28:36,260 --> 00:28:39,480
我们在今天的课上已经看过很多，

468
00:28:39,740 --> 00:28:43,110
卷积是用来生成这些特征图的，

469
00:28:43,220 --> 00:28:45,420
所以它们让我们输入先前的图像，

470
00:28:45,860 --> 00:28:48,940
以及它们想要检测的一些 filter ，

471
00:28:48,940 --> 00:28:55,140
并输出 filter 与原始图像关系的特征图。

472
00:28:55,710 --> 00:28:57,700
第二步像昨天一样，

473
00:28:58,020 --> 00:29:01,030
对这些特征映射的结果应用非线性函数，

474
00:29:01,410 --> 00:29:05,140
向我们的神经网络注入一些非线性激活，

475
00:29:05,250 --> 00:29:07,060
使其能够处理非线性数据。

476
00:29:07,890 --> 00:29:09,460
第三步是池化，

477
00:29:09,570 --> 00:29:11,770
这本质上是一种向下采样操作，

478
00:29:12,240 --> 00:29:16,870
允许我们的图像，允许我们的网络处理越来越大规模的图像，

479
00:29:17,070 --> 00:29:19,870
通过逐渐缩小其大小，

480
00:29:19,980 --> 00:29:23,650
以便我们的 filter 可以在感受野内逐渐增长。

481
00:29:25,000 --> 00:29:30,140
最后，将所有这些结果特征提供给某个神经网络，

482
00:29:30,880 --> 00:29:32,750
以推断种类分数，

483
00:29:33,010 --> 00:29:37,220
当我们到达这个完全连接的层时，

484
00:29:37,630 --> 00:29:39,650
记住我们已经提取了我们的特征，

485
00:29:40,180 --> 00:29:43,670
你可以认为这不再是一个二维图像，

486
00:29:44,050 --> 00:29:47,570
我们现在可以使用我们在第一课中学到的方法，

487
00:29:47,620 --> 00:29:52,040
直接获取神经网络检测到的学习特征，

488
00:29:52,420 --> 00:29:54,750
并根据这些学习特征，

489
00:29:54,750 --> 00:29:57,380
根据它们是否被检测到，

490
00:29:57,610 --> 00:29:58,700
我们所在的种类。

491
00:29:59,890 --> 00:30:05,280
现在让我们更详细地逐一介绍这些操作，

492
00:30:05,280 --> 00:30:10,070
看看我们如何才能建立起 CNN 的这个非常基本的架构。

493
00:30:10,910 --> 00:30:14,940
所以首先，让我们回到过去，再考虑一次卷积运算，

494
00:30:15,020 --> 00:30:17,850
这是 CNN 的核心，

495
00:30:18,500 --> 00:30:21,930
和以前一样，这个隐藏层中的每个神经元都将，

496
00:30:22,100 --> 00:30:25,470
被计算为其输入的加权和，

497
00:30:26,120 --> 00:30:29,710
使用偏置和非线性激活，

498
00:30:29,710 --> 00:30:33,475
听起来应该和昨天课程的第一课非常相似，

499
00:30:33,475 --> 00:30:34,320
但现在不同的是，

500
00:30:34,700 --> 00:30:36,510
当我们要做第一步的时候，

501
00:30:37,040 --> 00:30:39,400
我们不是只用我们的权重做点积，

502
00:30:39,400 --> 00:30:41,755
而是用我们的权重进行卷积，

503
00:30:41,755 --> 00:30:45,290
这就是简单的逐个元素乘法和加法，

504
00:30:45,290 --> 00:30:46,600
还有那个滑动操作。

505
00:30:47,370 --> 00:30:49,780
这里真正特别的是，

506
00:30:49,890 --> 00:30:53,770
我真正想强调的是本地连接，

507
00:30:54,150 --> 00:30:58,450
这一隐藏层中的每一个神经元只能看到

508
00:30:58,590 --> 00:31:02,105
其前一层中的某一块输入，

509
00:31:02,105 --> 00:31:05,320
所以，如果我只指向输出层的这一个神经元，

510
00:31:05,820 --> 00:31:10,265
这个神经元只能看到这个红色方块的输入，

511
00:31:10,265 --> 00:31:13,240
它看不到图像其余部分中的任何其他输入，

512
00:31:14,030 --> 00:31:19,050
能够将这些模型缩放到超大规模图像中，这一点非常重要，

513
00:31:19,520 --> 00:31:20,590
现在你可以想象，

514
00:31:20,590 --> 00:31:22,350
随着你越来越深入你的网络，

515
00:31:22,730 --> 00:31:26,760
最终，因为在下一层，你将关注更大的块，

516
00:31:27,110 --> 00:31:30,540
这将不仅包括这个红色方块的数据，

517
00:31:30,680 --> 00:31:34,470
而且还包括一个你可以想象到的更大的红色方块。

518
00:31:36,780 --> 00:31:40,450
现在让我们定义一下进程的实际计算，

519
00:31:40,770 --> 00:31:42,500
对于隐藏层中的神经元，

520
00:31:42,500 --> 00:31:47,740
它的输入是那些在上一层落在它的块中的神经元，

521
00:31:47,880 --> 00:31:52,685
我们可以应用这个权重矩阵，表示为 4x4 的 filter ，

522
00:31:52,685 --> 00:31:54,280
你可以在左侧看到，

523
00:31:54,630 --> 00:31:57,160
在这种情况下，我们进行按元素相乘，

524
00:31:58,020 --> 00:32:02,140
我们累加输出，应用偏置，然后添加非线性，

525
00:32:02,880 --> 00:32:06,845
这是所有神经网络的核心步骤，

526
00:32:06,845 --> 00:32:10,900
我们在今天和本周的课程中学到的。

527
00:32:11,640 --> 00:32:15,820
记住这个按元素的乘法和加法运算，

528
00:32:15,960 --> 00:32:18,230
这是滑动运算，这叫做卷积，

529
00:32:18,230 --> 00:32:20,770
这是这些层的基础。

530
00:32:21,180 --> 00:32:25,130
所以这定义了卷积层中的神经元是如何连接的，

531
00:32:25,130 --> 00:32:26,770
它们是如何数学表达的，

532
00:32:27,180 --> 00:32:29,630
但在一个卷积层内，

533
00:32:29,630 --> 00:32:32,380
了解这一点也非常重要，

534
00:32:32,820 --> 00:32:37,480
单个层实际上可以尝试检测多组 filter ，

535
00:32:37,770 --> 00:32:41,740
也许你想在一个图像中检测多个特征，而不只是一个特征，

536
00:32:41,760 --> 00:32:44,225
但是，如果你在检测人脸，

537
00:32:44,225 --> 00:32:46,150
你不仅仅想检测眼睛，

538
00:32:46,260 --> 00:32:49,780
你想要检测眼睛，鼻子，嘴巴，耳朵，

539
00:32:50,040 --> 00:32:53,650
所有这些都是定义人脸的关键模式，

540
00:32:53,700 --> 00:32:55,360
可以帮助你对人脸进行分类。

541
00:32:56,580 --> 00:32:58,960
所以我们需要考虑的实际上是，

542
00:32:59,010 --> 00:33:04,030
卷积运算可以输出大量不同的图像，

543
00:33:04,110 --> 00:33:08,650
这个[体积]的每个切片代表了一个不同的 filter ，

544
00:33:08,910 --> 00:33:12,160
可以在我们的原始输入中识别出来，

545
00:33:12,330 --> 00:33:19,300
每个 filter 也将与我们图像中的特定图案或特征相对应，

546
00:33:20,050 --> 00:33:25,130
再次想想这些神经元中的连接，它们的感受野，

547
00:33:25,600 --> 00:33:31,550
它们在上一层中连接到的该节点的输入内的位置，

548
00:33:31,930 --> 00:33:33,650
这些参数真正定义了，

549
00:33:34,480 --> 00:33:38,120
我喜欢认为的信息的空间排列，

550
00:33:38,530 --> 00:33:42,950
它在整个网络中传播，特别是在卷积层中传播。

551
00:33:44,060 --> 00:33:47,260
现在，我想总结一下我们所看到的，

552
00:33:47,260 --> 00:33:51,480
以及这些类型的神经网络中的连接是如何定义的，

553
00:33:52,130 --> 00:33:58,800
让我们来看看卷积网络的输出是怎样的，

554
00:33:59,750 --> 00:34:05,875
我们正在朝着真正理解卷积神经网络并定义它们的方向前进，

555
00:34:05,875 --> 00:34:09,810
这就是，我们刚刚谈到的是 CNN 的主要组成部分，

556
00:34:10,250 --> 00:34:13,540
这就是定义这些卷积层的卷积运算，

557
00:34:13,540 --> 00:34:16,195
剩下的步骤也非常关键，

558
00:34:16,195 --> 00:34:18,250
但我想暂停一下，

559
00:34:18,250 --> 00:34:23,970
确保每个人在卷积运算和卷积层的定义上都是一致的。

560
00:34:28,420 --> 00:34:28,820
太棒了。

561
00:34:28,870 --> 00:34:31,380
好的，这里的下一步是，

562
00:34:31,380 --> 00:34:35,540
提取我们的卷积层提取的那些结果特征图，

563
00:34:35,830 --> 00:34:40,320
并将非线性应用于卷积层的输出体积，

564
00:34:40,320 --> 00:34:42,020
所以正如我们在第一节课中讨论的，

565
00:34:42,220 --> 00:34:44,300
应用这些非线性是非常关键的，

566
00:34:44,470 --> 00:34:47,625
因为它允许我们处理非线性数据，

567
00:34:47,625 --> 00:34:51,380
而且因为图像数据特别是高度非线性的，

568
00:34:51,700 --> 00:34:59,030
这是卷积神经网络在实践中实际运行的关键组成部分，

569
00:35:00,280 --> 00:35:02,510
特别是，对于卷积神经网络，

570
00:35:02,650 --> 00:35:09,200
对于这些模型来说，非常常见的激活函数是 ReLU 激活函数，

571
00:35:10,300 --> 00:35:13,130
我们在昨天的第一课和第二课中谈到了这一点，

572
00:35:13,420 --> 00:35:16,370
ReLU 激活函数，你可以在右侧看到它，

573
00:35:16,750 --> 00:35:19,310
可以将此函数视为逐个像素的操作，

574
00:35:19,330 --> 00:35:22,790
该操作将所有负值替换为零，

575
00:35:22,840 --> 00:35:24,770
它使所有正值保持不变，

576
00:35:25,090 --> 00:35:27,530
当一个值是正值时，它是恒等式函数，

577
00:35:27,970 --> 00:35:31,460
但当它是负值时，它把一切都压回到零，

578
00:35:32,430 --> 00:35:34,535
这可以看作是一个阈值函数，

579
00:35:34,535 --> 00:35:38,440
阈值是任何等于零，任何小于零的值都会回到零，

580
00:35:38,880 --> 00:35:45,640
所以这里的负值表示卷积中的负检测，

581
00:35:46,230 --> 00:35:49,055
你可能只想说没有检测到，

582
00:35:49,055 --> 00:35:52,835
你可以认为这是一种直观的理解机制，

583
00:35:52,835 --> 00:35:57,130
为什么 ReLU 激活函数在卷积神经网络中如此受欢迎，

584
00:35:57,180 --> 00:36:02,920
另一种流行的看法是， ReLU 激活函数，

585
00:36:03,210 --> 00:36:04,070
好的，这不是一种[信仰]，

586
00:36:04,070 --> 00:36:08,050
它们非常容易计算，而且计算效率很高，

587
00:36:08,310 --> 00:36:10,130
它们的渐变被非常清晰地定义，

588
00:36:10,130 --> 00:36:14,380
除了分段非线性外，它们都是常量，

589
00:36:15,210 --> 00:36:17,440
这使得它们在这些领域非常受欢迎。

590
00:36:19,810 --> 00:36:23,870
现在， CNN 的下一个关键操作是池化，

591
00:36:24,130 --> 00:36:27,045
池化是一项处于其核心的操作，

592
00:36:27,045 --> 00:36:28,220
它只有一个目的，

593
00:36:28,240 --> 00:36:31,430
那就是逐渐降低图像的维度，

594
00:36:31,570 --> 00:36:35,660
随着你在卷积层中越走越深，

595
00:36:35,980 --> 00:36:38,550
现在你可以真正开始思考这一点，

596
00:36:38,550 --> 00:36:42,350
当你降低你的特征的维度时，

597
00:36:42,760 --> 00:36:47,355
你现在是在增加你的过滤器的维度，

598
00:36:47,355 --> 00:36:50,090
因为你滑过较小图像的每个 filter

599
00:36:50,410 --> 00:36:55,490
都捕捉到了以前在该网络中发生的更大的感受野，

600
00:36:56,200 --> 00:36:58,515
所以，一种非常常见的池化技术就是

601
00:36:58,515 --> 00:37:01,190
所谓的最大池化或简称 max 池化，

602
00:37:01,570 --> 00:37:04,335
max 池化就是，它听起来像什么，

603
00:37:04,335 --> 00:37:06,950
所以它基本上是用这些小块来运作的，

604
00:37:07,120 --> 00:37:08,330
滑过一张图像，

605
00:37:08,710 --> 00:37:10,970
但不是进行卷积运算，

606
00:37:11,290 --> 00:37:15,170
这些块简单地获取块位置的最大值，

607
00:37:15,520 --> 00:37:20,420
所以，这可以把这看作是位置的激活的最大值，

608
00:37:21,520 --> 00:37:23,510
并且只传播最大值，

609
00:37:24,220 --> 00:37:27,710
我鼓励大家想一想其他方法，

610
00:37:27,790 --> 00:37:32,000
让我们可以执行比 max 池化更好的池化操作，

611
00:37:32,020 --> 00:37:33,350
有很多常见的方法，

612
00:37:33,370 --> 00:37:35,325
但你可以想出一些，

613
00:37:35,325 --> 00:37:38,450
例如，或者是算术平均池化或平均池化，

614
00:37:38,530 --> 00:37:40,250
也许你不想只取最大值，

615
00:37:40,390 --> 00:37:47,930
你可以将所有这些像素的平均值折叠为结果中的单个值，

616
00:37:48,850 --> 00:37:52,950
但这些都是卷积神经网络的核心关键操作，

617
00:37:52,950 --> 00:37:56,040
现在我们准备开始将它们组合在一起，

618
00:37:56,040 --> 00:38:00,570
从头到尾形成和构建 CNN ，

619
00:38:00,570 --> 00:38:04,340
有了 CNN ，我们可以一个接一个地对这些操作进行分层，

620
00:38:04,750 --> 00:38:07,395
首先从卷积非线性开始，

621
00:38:07,395 --> 00:38:10,425
然后池化并反复重复这些操作，

622
00:38:10,425 --> 00:38:12,770
以了解这些特征的层次结构，

623
00:38:12,850 --> 00:38:15,480
这就是我们如何获得这样的图片，

624
00:38:15,480 --> 00:38:17,540
我们昨天的课程一开始就是这样，

625
00:38:17,740 --> 00:38:20,720
学习这些特征的分层分解，

626
00:38:21,010 --> 00:38:25,010
通过逐步堆叠和堆叠这些 filter ，

627
00:38:25,030 --> 00:38:29,000
每个 filter 可以使用它学习到的所有以前的 filter 。

628
00:38:30,780 --> 00:38:36,220
所以，为图像分类而建立的 CNN 可以分为两个部分，

629
00:38:36,240 --> 00:38:38,110
首先是特征学习管道，

630
00:38:38,160 --> 00:38:41,260
我们学习我们想要检测的特征，

631
00:38:41,430 --> 00:38:45,790
然后第二部分是检测这些特征并进行分类。

632
00:38:48,010 --> 00:38:53,360
现在，从模型的第一部分输出的卷积层和池化层，

633
00:38:53,740 --> 00:38:56,040
这些卷积和池化层的目标是，

634
00:38:56,040 --> 00:39:01,215
输出从我们的输入中提取的高级特征，

635
00:39:01,215 --> 00:39:05,355
但下一步是使用这些特征并检测它们的存在，

636
00:39:05,355 --> 00:39:06,710
以便对图像进行分类。

637
00:39:07,300 --> 00:39:14,010
所以，我们可以将这些输出特征输入到我们在第一课中学到的完全连接层中，

638
00:39:14,010 --> 00:39:17,450
因为这些现在只是一个一维特征数组，

639
00:39:17,980 --> 00:39:21,270
我们可以使用这些特征来检测我们所在的类别，

640
00:39:21,270 --> 00:39:25,910
我们可以使用一个称为 softmax 函数的函数来实现这一点，

641
00:39:26,640 --> 00:39:30,020
你可以将 softmax 函数看作是一个简单的正规化函数，

642
00:39:30,100 --> 00:39:34,430
其输出表示绝对概率分布的输出，

643
00:39:34,660 --> 00:39:36,810
所以，另一种考虑方式是，

644
00:39:36,810 --> 00:39:39,860
如果你有一个数字数组，你想要折叠，

645
00:39:40,000 --> 00:39:42,080
这些数字可以是任何实数形式，

646
00:39:42,340 --> 00:39:44,835
你想把它压缩成某种概率分布，

647
00:39:44,835 --> 00:39:46,880
概率分布有几个属性，

648
00:39:47,620 --> 00:39:50,120
即它的所有值都必须求和为 1 ，

649
00:39:50,470 --> 00:39:53,000
它也必须始终介于 0 和 1 之间，

650
00:39:53,470 --> 00:39:57,465
所以，维护这两个属性是 softmax 操作所做的事情，

651
00:39:57,465 --> 00:39:58,970
你可以在这里看到它的方程式，

652
00:39:59,140 --> 00:40:01,370
它有效地使一切都是正的，

653
00:40:01,510 --> 00:40:04,290
然后它使结果在彼此之间正规化，

654
00:40:04,290 --> 00:40:06,740
这保持了我刚才提到的两个属性。

655
00:40:08,570 --> 00:40:10,470
好的，让我们把所有这些放在一起，

656
00:40:10,490 --> 00:40:14,640
看看我们如何编写我们的第一个卷积神经网络，

657
00:40:14,660 --> 00:40:16,960
完全从头开始。

658
00:40:16,960 --> 00:40:21,420
所以，让我们首先定义我们的特征提取头，

659
00:40:22,280 --> 00:40:24,360
它从卷积层开始，

660
00:40:24,410 --> 00:40:27,690
这里是 32 个 filter 或 32 个特征，

661
00:40:27,800 --> 00:40:31,770
你可以想象这第一层，这第一层的结果是学习，

662
00:40:31,850 --> 00:40:36,270
不是一个 filter ，不是我们图像中的一个模式，而是 32 个模式，

663
00:40:36,710 --> 00:40:41,670
好的，所以这 32 个结果将被传递到池化层，

664
00:40:41,960 --> 00:40:46,380
然后传递到下一组卷积运算，

665
00:40:46,940 --> 00:40:50,575
下一组卷积运算现在将包含 64 个特征，

666
00:40:50,575 --> 00:40:57,000
将逐步增长和扩展我们在这张图像中识别的模式集，

667
00:40:58,330 --> 00:41:02,510
接下来，我们最终可以扁平化那些我们已经识别的结果特征，

668
00:41:02,860 --> 00:41:05,235
并将所有这些都提供给我们的紧密层，

669
00:41:05,235 --> 00:41:07,670
也就是我们在第一课中学到的完全相连的层，

670
00:41:07,990 --> 00:41:11,505
这将使我们能够预测最终，比如 10 个种类，

671
00:41:11,505 --> 00:41:15,110
如果我们的图像中有十个不同的最终可能种类，

672
00:41:15,280 --> 00:41:17,370
这一层将说明这一点，

673
00:41:17,370 --> 00:41:22,940
并允许我们使用 softmax 输出这十个种类的概率分布。

674
00:41:24,910 --> 00:41:26,240
到目前为止，我们已经讨论了，

675
00:41:26,860 --> 00:41:31,490
如何使用 CNN 来执行图像分类任务，

676
00:41:31,570 --> 00:41:36,080
但实际上，我在今天的课程中特别想强调的一件事是，

677
00:41:36,370 --> 00:41:42,200
我们到目前为止讨论的相同的体系结构和相同的构建块是可扩展的，

678
00:41:42,280 --> 00:41:49,280
它们可以扩展到我们可以想象的许多不同的应用程序和模型类型，

679
00:41:49,420 --> 00:41:53,480
例如，当我们考虑 CNN 进行分类时，

680
00:41:53,980 --> 00:41:55,910
我们看到它实际上有两个部分，

681
00:41:56,140 --> 00:41:59,540
第一部分是特征提取，学习要寻找什么特征，

682
00:41:59,620 --> 00:42:03,590
第二部分是这些特征的分类和检测。

683
00:42:05,060 --> 00:42:09,900
现在，卷积神经网络真正强大的地方是，

684
00:42:10,580 --> 00:42:14,850
因为观察到特征学习部分，

685
00:42:14,930 --> 00:42:19,740
神经网络的第一部分是非常灵活的，

686
00:42:20,000 --> 00:42:22,350
你可以取走神经网络的第一部分，

687
00:42:22,730 --> 00:42:24,360
砍掉它后面的部分，

688
00:42:24,440 --> 00:42:27,870
然后把一堆不同的头放进后面的部分，

689
00:42:27,950 --> 00:42:30,780
第一部分的目标是提取这些特征，

690
00:42:31,130 --> 00:42:33,880
如何处理这些特性完全由你自己决定，

691
00:42:33,880 --> 00:42:39,840
但你仍然可以利用第一部分的灵活性和强大功能来学习所有这些核心特性，

692
00:42:39,860 --> 00:42:45,720
例如，这部分将寻找所有不同的图像分类域，

693
00:42:45,890 --> 00:42:48,270
在我们提取特征之后的部分，

694
00:42:48,350 --> 00:42:51,100
我们也可以引入新体系结构，

695
00:42:51,100 --> 00:42:56,065
获取这些特征并执行分割或图像说明之类的任务，

696
00:42:56,065 --> 00:42:57,720
像我们在昨天课程中看到的。

697
00:42:59,230 --> 00:43:01,700
例如，在分类的情况下，

698
00:43:01,900 --> 00:43:04,550
仅将分类故事联系起来，

699
00:43:04,810 --> 00:43:09,030
这在医疗保健、医疗决策等领域产生重大影响，

700
00:43:09,030 --> 00:43:13,250
深度学习模型被应用于医学扫描分析，

701
00:43:13,720 --> 00:43:16,640
通过一系列不同的医学图像。

702
00:43:18,530 --> 00:43:25,530
现在，分类告诉我们对图像所包含内容的离散预测，

703
00:43:25,700 --> 00:43:29,190
但我们也可以更深入地研究这个问题，

704
00:43:29,270 --> 00:43:35,310
比如，想象一下，我们不仅要识别这张图片是一辆出租车，

705
00:43:35,630 --> 00:43:36,690
你可以在这里看到，

706
00:43:37,070 --> 00:43:38,370
更重要的是，

707
00:43:38,600 --> 00:43:42,990
我们可能希望我们的神经网络不仅能告诉我们这是一辆出租车，

708
00:43:43,220 --> 00:43:49,290
还能识别并在出租车的这个位置上画一个特定的边界框，

709
00:43:49,400 --> 00:43:51,150
所以这是一个两阶段的问题，

710
00:43:51,440 --> 00:43:53,430
第一，我们需要画一个盒子，

711
00:43:53,990 --> 00:43:56,610
第二，我们需要对盒子里的东西进行分类，

712
00:43:56,990 --> 00:43:58,740
所以这是一个回归问题，

713
00:43:59,090 --> 00:44:00,090
盒子在哪里，

714
00:44:00,710 --> 00:44:05,730
这是一个连续的问题，也是一个在盒子里的分类问题。

715
00:44:06,350 --> 00:44:11,430
这是一个比我们在今天的课程中所讨论的要困难得多的问题，

716
00:44:11,540 --> 00:44:14,845
因为在我们的场景中可能有很多物体，

717
00:44:14,845 --> 00:44:15,900
而不仅仅是一个物体，

718
00:44:16,130 --> 00:44:17,635
所以我们必须解释这样一个事实，

719
00:44:17,635 --> 00:44:21,540
也许我们可以约束任意多个对象。

720
00:44:23,440 --> 00:44:27,950
现在我们的网络需要灵活到那个程度，

721
00:44:28,150 --> 00:44:32,115
需要能够推断场景中对象的动态数量，

722
00:44:32,115 --> 00:44:34,530
如果场景只是一辆出租车，

723
00:44:34,530 --> 00:44:37,250
那么它应该只输出一个边界框，

724
00:44:37,480 --> 00:44:43,430
但另一方面，如果图像有许多对象，甚至可能是不同类别的对象，

725
00:44:43,480 --> 00:44:48,260
我们需要一个模型来为这些不同的示例中的每一个绘制一个边界框，

726
00:44:48,940 --> 00:44:54,530
并将它们的预测分类标签独立地与每个示例相关联。

727
00:44:56,230 --> 00:44:59,240
实际上，这相当复杂，

728
00:44:59,530 --> 00:45:01,640
因为这些方框可以在图像中的任何位置，

729
00:45:01,930 --> 00:45:04,040
盒子放在哪里没有限制，

730
00:45:04,390 --> 00:45:06,440
它们也可以有不同的大小，

731
00:45:06,580 --> 00:45:08,840
它们也可以具有不同的比率，

732
00:45:08,920 --> 00:45:10,490
有些可能很高，有些可能很宽。

733
00:45:10,990 --> 00:45:14,360
首先，让我们考虑一种非常幼稚的方式来做这件事，

734
00:45:14,530 --> 00:45:19,280
让我们在该图像上的某个位置放置一个随机框，

735
00:45:19,390 --> 00:45:22,340
例如，我们只需随机选择一个位置，一个随机大小，

736
00:45:22,690 --> 00:45:24,110
我们就会在那里放置一个盒子，

737
00:45:24,790 --> 00:45:28,130
这个盒子，就像我说的，有一个随机的位置，随机的大小，

738
00:45:28,150 --> 00:45:29,360
然后我们可以拿起那个盒子，

739
00:45:29,500 --> 00:45:33,225
只把那个随机的盒子送入我们的卷积神经网络，

740
00:45:33,225 --> 00:45:36,260
它被训练来进行分类，只是分类，

741
00:45:37,950 --> 00:45:40,025
这个神经网络可以检测到，

742
00:45:40,025 --> 00:45:44,105
第一，盒子里有没有一类物体，

743
00:45:44,105 --> 00:45:45,730
如果是的话，是什么种类，

744
00:45:45,960 --> 00:45:47,030
然后我们可以做的是，

745
00:45:47,030 --> 00:45:50,375
我们可以一遍又一遍地重复这个过程，

746
00:45:50,375 --> 00:45:55,790
对我们图像中的所有这些随机框，很多随机框，

747
00:45:55,790 --> 00:45:57,280
我们不断地采样一个新的盒子，

748
00:45:57,780 --> 00:45:59,510
通过我们的卷积神经网络输入它，

749
00:45:59,510 --> 00:46:01,235
然后问这个问题，盒子里是什么，

750
00:46:01,235 --> 00:46:03,730
如果里面有什么东西，那是什么，

751
00:46:04,080 --> 00:46:09,100
我们继续前进，直到我们用尽了图像中所有的方框。

752
00:46:09,750 --> 00:46:10,745
但这里的问题是，

753
00:46:10,745 --> 00:46:14,390
我们必须处理的潜在输入太多了，

754
00:46:14,390 --> 00:46:18,700
这对于在实时系统中运行是完全不切实际的，

755
00:46:18,870 --> 00:46:20,530
例如，用今天的计算机，

756
00:46:20,610 --> 00:46:22,390
这导致了太多的比例，

757
00:46:22,590 --> 00:46:27,340
特别是对于我们今天处理的图像的分辨率类型。

758
00:46:28,770 --> 00:46:30,370
所以，不是选择随机的框，

759
00:46:30,540 --> 00:46:33,370
让我们尝试并使用一个非常简单的启发式，

760
00:46:33,810 --> 00:46:38,800
来识别图像中一些具有很大可变性的位置，

761
00:46:38,820 --> 00:46:43,870
其中可能存在对象的可能性很高，

762
00:46:44,040 --> 00:46:47,210
这些可能具有有意义的见解或有意义的对象，

763
00:46:47,210 --> 00:46:49,060
可以在我们的图像中使用，

764
00:46:49,350 --> 00:46:50,410
我们可以利用这些信息，

765
00:46:50,580 --> 00:46:56,090
将那些高关注度的位置输入到我们的卷积神经网络中，

766
00:46:56,090 --> 00:46:59,960
然后我们可以大大加快管道的第一部分，

767
00:46:59,960 --> 00:47:01,540
因为现在我们不仅仅是挑选随机的盒子，

768
00:47:01,560 --> 00:47:06,250
也许我们使用一些简单的启发式方法来确定图像的有趣部分可能在哪里，

769
00:47:06,630 --> 00:47:09,430
但这实际上仍然非常慢，在实践中，

770
00:47:09,690 --> 00:47:12,850
我们必须独立地为模型提供每个区域的数据，

771
00:47:13,260 --> 00:47:14,795
而且它非常脆弱，

772
00:47:14,795 --> 00:47:21,580
因为最终模型中查看潜在对象可能所在位置的部分

773
00:47:21,630 --> 00:47:25,730
与检测这些对象的部分是分开的，

774
00:47:25,730 --> 00:47:27,130
理想情况下，我们希望有一个模型，

775
00:47:27,480 --> 00:47:33,730
既能找出要注意的地方，又能进行分类。

776
00:47:35,570 --> 00:47:39,880
所以在这个物体检测领域已经提出了很多变种，

777
00:47:39,880 --> 00:47:42,000
但我想，为了今天的课程，

778
00:47:42,410 --> 00:47:45,360
向你们介绍最流行的其中之一。

779
00:47:46,570 --> 00:47:51,920
这是一个被称为 R-CNN 或更快的 R-CNN 的模型，

780
00:47:52,390 --> 00:47:57,380
它试图不仅学习如何对这些盒子进行分类，

781
00:47:57,850 --> 00:48:02,760
而且学习如何提出这些盒子可能在哪里，

782
00:48:02,760 --> 00:48:08,450
这样你就可以学习如何提供或在哪里提供到下游的神经网络。

783
00:48:09,160 --> 00:48:13,950
这意味着我们可以将图像输入到所谓的区域提案网络，

784
00:48:14,300 --> 00:48:19,240
这些网络的目标是提出图像中你应该注意的某些区域，

785
00:48:19,240 --> 00:48:23,310
然后将这些区域仅提供给下游 CNN ，

786
00:48:23,930 --> 00:48:25,540
因此这里的目标是，

787
00:48:25,540 --> 00:48:30,330
直接尝试学习或提取所有这些关键区域，

788
00:48:30,890 --> 00:48:33,235
并通过模型的后面部分对其进行处理，

789
00:48:33,235 --> 00:48:37,710
这些区域中的每一个都使用它们自己的独立特征提取器来处理，

790
00:48:38,410 --> 00:48:41,220
然后，可以使用分类器将它们全部聚集在一起，

791
00:48:41,220 --> 00:48:44,990
并执行特征检测和目标检测。

792
00:48:45,280 --> 00:48:50,510
好的是，它只需要在网络中通过一次，

793
00:48:50,530 --> 00:48:52,395
所以它非常快，

794
00:48:52,395 --> 00:48:54,590
可以很容易地实时运行，

795
00:48:54,640 --> 00:48:58,635
并且在许多行业应用中也非常常用，

796
00:48:58,635 --> 00:49:00,650
它甚至可以在你的智能手机上运行。

797
00:49:02,220 --> 00:49:03,310
所以在分类中，

798
00:49:03,690 --> 00:49:05,230
我们只是看到了我们如何预测，

799
00:49:05,880 --> 00:49:10,450
不仅仅是一张图像，每张图像只有一个物体，

800
00:49:10,560 --> 00:49:12,100
我们看到一个物体检测，

801
00:49:12,480 --> 00:49:17,500
可能会在你的图像中用边界框推断多个物体，

802
00:49:18,210 --> 00:49:20,120
还有一种类型的任务，

803
00:49:20,120 --> 00:49:21,110
我想指出，

804
00:49:21,110 --> 00:49:22,480
它叫做分割。

805
00:49:22,950 --> 00:49:25,210
分割是分类的任务，

806
00:49:25,680 --> 00:49:27,890
但现在是在每一个单独的像素上做的，

807
00:49:27,890 --> 00:49:32,350
这使用了目标检测的思想，把方框的边界限制到了极致，

808
00:49:32,790 --> 00:49:34,270
现在，我们不再画盒子，

809
00:49:34,470 --> 00:49:36,070
我们甚至不会考虑盒子，

810
00:49:36,180 --> 00:49:42,985
我们将学习如何对这张图像中的每一个像素进行分类，

811
00:49:42,985 --> 00:49:45,960
所以我们要做的是大量的分类，

812
00:49:46,160 --> 00:49:47,130
我们会这样做。

813
00:49:48,290 --> 00:49:49,735
好的，首先让我展示一下这个例子，

814
00:49:49,735 --> 00:49:51,940
所以在左手边，这是什么样子，

815
00:49:51,940 --> 00:49:54,090
当你输入原始 RGB 图像时，

816
00:49:54,230 --> 00:49:56,845
右手边的目标是学习，

817
00:49:56,845 --> 00:49:59,130
对于左手边的每个像素，

818
00:49:59,510 --> 00:50:01,975
那个像素是什么种类的，

819
00:50:01,975 --> 00:50:06,210
所以，这与仅仅确定我们的形象上的框框形成了某种对比，

820
00:50:06,230 --> 00:50:08,280
现在我们孤立地看每一个像素，

821
00:50:08,870 --> 00:50:10,110
你可以看到，例如，

822
00:50:10,460 --> 00:50:18,685
你知道牛的像素与天空或草地的像素明显不同，

823
00:50:18,685 --> 00:50:23,580
这是语义分割网络的关键组成部分，

824
00:50:24,290 --> 00:50:29,160
这里的输出是通过再次使用这些卷积操作，

825
00:50:29,840 --> 00:50:31,560
然后是池化操作来创建的，

826
00:50:31,730 --> 00:50:35,460
这些操作学习了一个编码器，你可以在左侧想到它，

827
00:50:35,750 --> 00:50:38,190
这些是从我们的 RGB 图像中学习特征，

828
00:50:39,020 --> 00:50:41,040
学习如何将它们放到一个空间中，

829
00:50:41,300 --> 00:50:45,450
以便它可以重建到一个新的语义标签空间中，

830
00:50:45,650 --> 00:50:47,875
所以你可以想象一种缩小的规模，

831
00:50:47,875 --> 00:50:51,190
然后是进步扩展到语义空间，

832
00:50:51,540 --> 00:50:53,405
但当你进行扩展时，

833
00:50:53,405 --> 00:50:56,585
这一点很重要，当然，你不能抹杀那些信息，

834
00:50:56,585 --> 00:50:58,690
你需要在某种程度上颠倒所有这些操作，

835
00:50:59,280 --> 00:51:01,810
所以现在不是用池化来做卷积，

836
00:51:02,010 --> 00:51:06,550
你现在可以用反向的池化或扩展来做卷积，

837
00:51:07,050 --> 00:51:10,060
你可以在每个标签上扩展您的特征集。

838
00:51:11,010 --> 00:51:16,370
这是一个代码片段的例子，它定义了这些层，

839
00:51:16,370 --> 00:51:19,840
你可以插入这些层，将它们与卷积层结合，

840
00:51:19,920 --> 00:51:24,040
你就可以建立这些完全卷积的网络来完成这种类型的任务。

841
00:51:24,450 --> 00:51:28,040
当然，这也可以应用于医疗保健中的许多其他应用，

842
00:51:28,040 --> 00:51:31,060
特别是分割出癌症区域，

843
00:51:31,590 --> 00:51:35,710
甚至识别感染疟疾的血液部分。

844
00:51:36,330 --> 00:51:39,340
最后一个例子是自动驾驶汽车，

845
00:51:39,780 --> 00:51:43,120
让我们假设我们想要建立一个用于自动导航的神经网络，

846
00:51:43,410 --> 00:51:44,830
特别是建立一个模型，

847
00:51:45,210 --> 00:51:47,050
假设它可以接受图像作为输入，

848
00:51:47,520 --> 00:51:52,640
以及它认为它所在位置的一些非常粗略的地图，

849
00:51:52,640 --> 00:51:57,250
把这个想象成一个谷歌地图截图，

850
00:51:57,330 --> 00:51:58,550
给神经网络，

851
00:51:58,550 --> 00:52:01,000
这是地图的 GPS 位置。

852
00:52:01,890 --> 00:52:07,240
它不想直接推断场景的分类或语义分类，

853
00:52:07,500 --> 00:52:15,080
而是现在直接推断如何驾驶和驾驶这辆车进入未来，

854
00:52:15,080 --> 00:52:20,510
现在，这是整个控制命令空间的全概率分布，

855
00:52:20,510 --> 00:52:23,020
这是一个非常大的连续概率空间，

856
00:52:23,430 --> 00:52:26,440
问题是，我们如何建立一个神经网络来学习这种功能，

857
00:52:26,880 --> 00:52:28,565
我在这里强调的，

858
00:52:28,565 --> 00:52:31,340
所有这些不同类型的体系结构的关键点是，

859
00:52:31,340 --> 00:52:34,750
所有这些体系结构使用完全相同的编码器，

860
00:52:34,770 --> 00:52:40,420
从分类到检测再到语义分割，再到这里，我们没有做任何改变，

861
00:52:40,710 --> 00:52:46,690
它们都在使用相同的卷积、非线性和池化，

862
00:52:46,980 --> 00:52:48,185
唯一的区别是，

863
00:52:48,185 --> 00:52:50,740
在我们执行这些特征提取之后，

864
00:52:51,090 --> 00:52:54,185
我们如何获取这些特征并学习我们的最终任务，

865
00:52:54,185 --> 00:52:57,670
例如，在概率控制命令的情况下，

866
00:52:57,900 --> 00:52:59,770
我们想要获取这些学习的特征，

867
00:53:00,540 --> 00:53:06,910
并了解如何预测全连续概率分布的参数，

868
00:53:07,140 --> 00:53:08,890
就像你在右手边看到的，

869
00:53:09,300 --> 00:53:12,640
以及我们期望的目的地的确定性控制，

870
00:53:13,170 --> 00:53:15,820
同样，就像我们在这节课一开始谈到的那样，

871
00:53:16,050 --> 00:53:21,100
这个模型直接从图像到方向盘角度，

872
00:53:21,150 --> 00:53:23,540
本质上是汽车的一个模型，

873
00:53:23,540 --> 00:53:25,430
它完全是从头到尾学到的，

874
00:53:25,430 --> 00:53:31,420
我们从来没有告诉汽车什么是车道标志或者道路规则，

875
00:53:31,800 --> 00:53:34,720
它能够观察大量的人类驾驶数据，

876
00:53:35,400 --> 00:53:37,090
提取这些模式，这些特征，

877
00:53:37,320 --> 00:53:40,930
从优秀的人类司机与糟糕的人类司机的不同之处，

878
00:53:41,460 --> 00:53:45,550
并学习如何模仿发生的相同类型的动作，

879
00:53:45,990 --> 00:53:50,650
这样，在没有任何人类干预或人类规则的情况下，

880
00:53:50,850 --> 00:53:52,510
我们加入这个系统的，

881
00:53:52,860 --> 00:53:55,325
他们可以简单地观察所有这些数据，

882
00:53:55,325 --> 00:53:57,610
完全从零开始学习如何驾驶。

883
00:53:58,120 --> 00:54:00,380
所以，一个人可以进入汽车，

884
00:54:00,760 --> 00:54:01,880
输入想要的目的地，

885
00:54:03,190 --> 00:54:07,380
这个端到端的 CNN 会启动控制命令，

886
00:54:07,380 --> 00:54:08,960
将他们带到他们的目的地。

887
00:54:10,860 --> 00:54:12,190
我将结束今天的演讲，

888
00:54:12,750 --> 00:54:16,720
通过所说的几个 CNN 应用，

889
00:54:16,860 --> 00:54:18,410
我们今天已经谈到了其中的几个，

890
00:54:18,410 --> 00:54:20,860
但是 CNN 的应用是巨大的，

891
00:54:21,390 --> 00:54:23,890
远远超出了我今天提供的这些例子，

892
00:54:24,210 --> 00:54:29,320
它们都与特征提取和检测这一核心概念联系在一起，

893
00:54:29,610 --> 00:54:31,460
在进行特征提取之后，

894
00:54:31,460 --> 00:54:33,970
你可以切断网络的其余部分，

895
00:54:34,140 --> 00:54:35,885
并将其应用于许多不同的头部，

896
00:54:35,885 --> 00:54:39,125
以执行你可能关心的许多不同的任务和应用，

897
00:54:39,125 --> 00:54:40,220
我们今天已经谈到了一些，

898
00:54:40,220 --> 00:54:43,510
但在不同的领域确实有很多。

899
00:54:43,980 --> 00:54:45,275
有了这个总结，

900
00:54:45,275 --> 00:54:52,235
很快我们就会讨论生成建模，

901
00:54:52,235 --> 00:54:58,810
这是今天和本周系列讲座的核心，

902
00:54:59,460 --> 00:55:02,890
在那之后，稍后我们将有一个软件实验，

903
00:55:02,910 --> 00:55:06,160
我很高兴你们所有人都能开始参与。

904
00:55:06,270 --> 00:55:08,650
是的，我们可以休息五分钟，

905
00:55:09,030 --> 00:55:10,840
然后从那里继续讲课。

906
00:55:11,200 --> 00:55:11,610
谢谢。

