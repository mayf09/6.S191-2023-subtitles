1
00:00:09,160 --> 00:00:12,740
Robotics is a really cool and important direction for the future,

2
00:00:13,090 --> 00:00:18,230
I I really believe that we are moving towards a world,

3
00:00:18,340 --> 00:00:21,770
where so many routine tasks are taken off your plate,

4
00:00:22,420 --> 00:00:26,600
fresh produce turn turns up at your doorsteps delivered by drones,

5
00:00:27,160 --> 00:00:30,020
garbage bins take themselves out,

6
00:00:30,160 --> 00:00:34,280
smart infrastructure ensures that the garbage gets removed,

7
00:00:34,810 --> 00:00:39,350
robots help with recycling, with shelving, with cleaning windows,

8
00:00:41,770 --> 00:00:44,415
robots can do so many physical things for us.

9
00:00:44,415 --> 00:00:45,170
And by the way,

10
00:00:45,310 --> 00:00:49,190
can you count how many robots there are in this, in this image,

11
00:00:49,750 --> 00:00:51,530
anybody want to take a guess,

12
00:00:52,510 --> 00:00:54,470
how many robots do I have in this image?

13
00:00:59,690 --> 00:01:01,500
Okay, that, so that's, that's really close,

14
00:01:01,700 --> 00:01:03,330
it, it turns out it's 19,

15
00:01:03,620 --> 00:01:04,810
so here are all the robots,

16
00:01:04,810 --> 00:01:06,660
we have flying robots,

17
00:01:06,770 --> 00:01:07,770
we have cars,

18
00:01:07,880 --> 00:01:09,730
we have shopping cart robots,

19
00:01:09,730 --> 00:01:11,575
we have robots that carry,

20
00:01:11,575 --> 00:01:13,110
we have robots that shelf,

21
00:01:13,130 --> 00:01:15,390
we have robots that clean.

22
00:01:15,740 --> 00:01:17,065
And so I really believe,

23
00:01:17,065 --> 00:01:20,940
that in the future we will have AI assistance,

24
00:01:21,200 --> 00:01:23,910
whether they are embodied or not,

25
00:01:24,620 --> 00:01:26,880
to act as our guardian angels

26
00:01:27,530 --> 00:01:30,145
to provide advice to ensure that,

27
00:01:30,145 --> 00:01:32,940
we maximize and optimize our lives,

28
00:01:33,050 --> 00:01:34,950
to live well and work effectively.

29
00:01:35,360 --> 00:01:39,990
And these agents will help us with cognitive and physical work.

30
00:01:41,090 --> 00:01:44,370
And so today, we can say that,

31
00:01:45,110 --> 00:01:50,910
with AI we will see such a wide breadth of applications,

32
00:01:51,020 --> 00:01:57,420
for instance, these technologies have the potential to reduce and eliminate car accidents,

33
00:01:57,440 --> 00:02:01,540
they have the potential to better diagnose monitor and treat disease

34
00:02:01,540 --> 00:02:04,890
as you have seen in some of the previous lectures,

35
00:02:04,940 --> 00:02:07,500
we, these technologies have the potential

36
00:02:07,910 --> 00:02:10,350
to keep your information private and safe,

37
00:02:10,880 --> 00:02:16,650
to transport people and goods more effectively and faster and cheaper,

38
00:02:16,970 --> 00:02:22,620
to really make it easier to communicate globally by providing instantaneous translations

39
00:02:23,240 --> 00:02:25,675
to, to develop education to everyone,

40
00:02:25,675 --> 00:02:30,060
to allow human workers to focus on big picture tasks

41
00:02:30,500 --> 00:02:33,120
with machines taking on the routine tasks.

42
00:02:33,620 --> 00:02:37,710
And so this future is really enabled by three interconnected fields

43
00:02:38,330 --> 00:02:40,080
and on one hand we have robots,

44
00:02:40,100 --> 00:02:44,260
now robots, I like to think of robots as as the machines,

45
00:02:44,260 --> 00:02:46,260
that put computing in motion

46
00:02:46,670 --> 00:02:54,960
and give give our our machines in the world the ability to navigate and to manipulate the world,

47
00:02:55,460 --> 00:02:56,940
we have artificial intelligence,

48
00:02:57,980 --> 00:03:04,060
which enables machines to see To hear and to communicate and to make decisions like humans,

49
00:03:04,500 --> 00:03:05,800
and then we have machine learning,

50
00:03:06,720 --> 00:03:12,740
and to me, machine learning is about learning from and making predictions on data,

51
00:03:13,330 --> 00:03:17,930
and this, this kind of application of machine learning is broad,

52
00:03:18,040 --> 00:03:20,600
it, it applies to cognitive tasks,

53
00:03:20,650 --> 00:03:22,760
it applies to physical tasks,

54
00:03:23,230 --> 00:03:24,860
but regardless of the tasks,

55
00:03:25,240 --> 00:03:30,770
we can characterize how machine learning works as using data

56
00:03:31,540 --> 00:03:36,800
to answer questions that are either descriptive, predictive or prescriptive,

57
00:03:37,180 --> 00:03:38,510
so you can look at data

58
00:03:38,680 --> 00:03:41,510
to see what happened or to see what is this,

59
00:03:42,100 --> 00:03:43,130
you can look at data

60
00:03:43,390 --> 00:03:45,380
to see what will happen in the future

61
00:03:45,760 --> 00:03:47,960
and what the world might look like in the future,

62
00:03:48,700 --> 00:03:53,690
or you can use data to ask where should I go, what should I do.

63
00:03:56,550 --> 00:04:01,840
And so when we think about these questions in the context of a robot,

64
00:04:02,220 --> 00:04:06,680
we have to, we have to kind of get on the same page about what a robot is.

65
00:04:06,680 --> 00:04:11,530
And, and so think of a robot as a programmable mechanical device,

66
00:04:12,300 --> 00:04:15,460
that takes input with its sensors,

67
00:04:15,840 --> 00:04:17,480
reasons about this input,

68
00:04:17,480 --> 00:04:20,740
and then generates an action in the physical world.

69
00:04:22,110 --> 00:04:24,940
And robots are made of a body and the brain,

70
00:04:25,830 --> 00:04:29,710
the body consisting of actuators and sensors

71
00:04:30,540 --> 00:04:34,430
determine the range of tasks that the robot can do,

72
00:04:34,430 --> 00:04:37,690
so the robot can only do what its body is capable of doing,

73
00:04:38,100 --> 00:04:42,790
robot on wheels will not be able to do the task of climbing stairs,

74
00:04:43,200 --> 00:04:44,710
so we have to think about that body,

75
00:04:45,300 --> 00:04:46,865
and we have at,

76
00:04:46,865 --> 00:04:50,320
we have a lot of machine learning based research,

77
00:04:50,940 --> 00:04:54,550
that allows us to that that examines,

78
00:04:54,930 --> 00:04:59,290
how to design optimally a robot body for a particular task.

79
00:05:00,360 --> 00:05:03,490
Now, in order for the body to do what it's meant to do,

80
00:05:03,690 --> 00:05:04,490
we need the brain,

81
00:05:04,490 --> 00:05:07,870
we need the machine learning and the reason and the decision making engine,

82
00:05:08,250 --> 00:05:11,620
and this is what we are going to talk about today.

83
00:05:13,320 --> 00:05:15,670
Now in the context of robots,

84
00:05:15,810 --> 00:05:18,065
we have three types of learning

85
00:05:18,065 --> 00:05:23,015
and you have seen different aspects of these methodologies throughout the course.

86
00:05:23,015 --> 00:05:24,280
We have supervised learning,

87
00:05:24,690 --> 00:05:27,760
and so in this, in this method of learning,

88
00:05:28,020 --> 00:05:32,950
eh, we use data to have to find the relationship between input and output.

89
00:05:33,720 --> 00:05:35,590
We have unsupervised learning

90
00:05:35,670 --> 00:05:38,060
and in the context of unsupervised learning,

91
00:05:38,060 --> 00:05:40,090
we can use data to have patterns,

92
00:05:40,320 --> 00:05:42,940
to find patterns and classifications of the data.

93
00:05:43,440 --> 00:05:45,550
And then we have reinforcement learning,

94
00:05:46,170 --> 00:05:50,860
which is about learning to perform a task by maximizing reward.

95
00:05:52,380 --> 00:05:53,290
So for robots,

96
00:05:53,700 --> 00:05:59,290
we end up with a cycle that most often consists of three steps,

97
00:05:59,730 --> 00:06:01,745
we have perception, a perception step,

98
00:06:01,745 --> 00:06:03,545
we have a planning and reasoning step

99
00:06:03,545 --> 00:06:04,960
and we have an action step.

100
00:06:05,640 --> 00:06:09,640
And so this is what we are going to talk about today.

101
00:06:09,720 --> 00:06:11,800
So let me start with some examples

102
00:06:11,970 --> 00:06:18,490
of how we can use machine learning to enhance the perception capability of robots.

103
00:06:19,890 --> 00:06:22,625
And so this is addressing the question what is this,

104
00:06:22,625 --> 00:06:24,940
and this question is really important,

105
00:06:25,110 --> 00:06:30,250
because for instance, training robot cars to recognize all objects on roads,

106
00:06:30,390 --> 00:06:32,770
including ducks, cars, people,

107
00:06:33,150 --> 00:06:36,280
is really critical for autonomous driving

108
00:06:36,900 --> 00:06:38,855
and so how does this work,

109
00:06:38,855 --> 00:06:41,585
well, let me let me give you a high level view

110
00:06:41,585 --> 00:06:45,100
of how a robot car can actually recognize the scene.

111
00:06:47,290 --> 00:06:53,450
So, in order to, in order to use deep learning for the perception task of robots,

112
00:06:53,590 --> 00:06:57,020
we use data, this is manually labeled data,

113
00:06:57,580 --> 00:07:01,010
that gets fed into a convolutional neural network

114
00:07:01,570 --> 00:07:05,130
and then the labels are used to classify what the data is,

115
00:07:05,130 --> 00:07:06,555
so for instance for this image,

116
00:07:06,555 --> 00:07:10,400
we may have classifications like car, duck road

117
00:07:10,930 --> 00:07:11,930
and we do this,

118
00:07:11,980 --> 00:07:15,860
so that when the system when the car sees a new image,

119
00:07:15,970 --> 00:07:17,270
for example this one,

120
00:07:17,440 --> 00:07:21,320
the car could say, oh, this is this is duck's road,

121
00:07:21,580 --> 00:07:30,135
now in order to, in order to actually provide solutions for this object classification problem,

122
00:07:30,135 --> 00:07:34,500
we have to, we have to employ multiple algorithms

123
00:07:34,500 --> 00:07:39,320
and the first algorithm that we have to employ is called image segmentation,

124
00:07:39,700 --> 00:07:44,690
in image segmentation, we take as we take inputs as input images

125
00:07:45,310 --> 00:07:51,350
and we group together the pixels that belong to the same object in the image,

126
00:07:51,880 --> 00:07:53,930
and this is a kind of a lexical step,

127
00:07:54,280 --> 00:07:58,010
and then we need to label and recognize these images,

128
00:07:58,150 --> 00:07:59,780
this is a semantic step,

129
00:08:00,280 --> 00:08:01,815
and so the exciting thing is that,

130
00:08:01,815 --> 00:08:06,110
we already have very good algorithms,

131
00:08:06,190 --> 00:08:09,290
that that can segment images very fast,

132
00:08:09,340 --> 00:08:12,840
so, we can, we can do this, we can do this,

133
00:08:12,840 --> 00:08:14,360
so we can, we can take an image

134
00:08:14,620 --> 00:08:17,205
and we can find the object in the image very fast,

135
00:08:17,205 --> 00:08:18,650
we don't know what the objects are,

136
00:08:18,910 --> 00:08:20,930
so in order to know what the objects are,

137
00:08:21,820 --> 00:08:23,180
well, you know what we do, right,

138
00:08:23,800 --> 00:08:28,550
so we employ thousands of people to label the objects that we have.

139
00:08:28,600 --> 00:08:30,710
And so this is exciting,

140
00:08:30,730 --> 00:08:34,100
but, but labeling is a very labor intensive activity

141
00:08:34,180 --> 00:08:37,260
and a significant challenge to machine learning,

142
00:08:37,260 --> 00:08:38,840
let's keep this thought for later.

143
00:08:40,310 --> 00:08:47,490
Now the the most popular benchmark for measuring the accuracy of image classification is ImageNet

144
00:08:47,990 --> 00:08:52,015
and here we see the leaderboards of imageet

145
00:08:52,015 --> 00:09:00,420
and we see performance of various, variations of, of image classification algorithms,

146
00:09:00,530 --> 00:09:05,280
that perform well into ninety, ninety percent accuracy

147
00:09:05,450 --> 00:09:07,260
and this is really quite exciting,

148
00:09:08,200 --> 00:09:08,960
it's exciting,

149
00:09:09,040 --> 00:09:13,850
but if those algorithms were to run on a car,

150
00:09:15,070 --> 00:09:15,990
that's not good enough,

151
00:09:17,010 --> 00:09:19,750
because, because the car is a safety critical system

152
00:09:20,220 --> 00:09:24,040
and in fact we cannot afford to have any errors

153
00:09:24,300 --> 00:09:28,390
in how images get recognized in the car, on the car,

154
00:09:28,410 --> 00:09:36,095
here's an example of of an object object detector that is running on on a car

155
00:09:36,095 --> 00:09:39,310
and we we see the image from three cameras,

156
00:09:39,420 --> 00:09:40,990
there is a camera pointing forward,

157
00:09:41,010 --> 00:09:42,880
there is a camera pointing to the left,

158
00:09:43,260 --> 00:09:45,400
there is a camera pointing to the right

159
00:09:45,840 --> 00:09:50,255
and so you can see that the car takes this, this input

160
00:09:50,255 --> 00:09:51,410
and it does pretty well,

161
00:09:51,410 --> 00:09:52,450
see there's a blue car,

162
00:09:53,100 --> 00:09:54,640
there are some bicyclists,

163
00:09:54,810 --> 00:09:57,070
there are, there's a bicyclist,

164
00:09:57,330 --> 00:09:58,630
here's another car,

165
00:09:58,740 --> 00:10:00,400
so the system does pretty well

166
00:10:00,570 --> 00:10:04,925
and manages to find the gap in the road to make a turn,

167
00:10:04,925 --> 00:10:07,780
but it's not 100% correct,

168
00:10:08,130 --> 00:10:11,890
so in particular there is this moving truck

169
00:10:12,750 --> 00:10:17,170
and when this moving and when the image of the moving truck is passed through,

170
00:10:17,480 --> 00:10:21,720
the object recognition piece of the system,

171
00:10:22,850 --> 00:10:26,640
we, we end up with a lot of interesting things,

172
00:10:26,840 --> 00:10:28,860
[] is recognized as a fence,

173
00:10:30,120 --> 00:10:35,020
so these are, these are the kinds of extreme errors

174
00:10:35,850 --> 00:10:40,150
or the way we denote them, corner cases,

175
00:10:40,740 --> 00:10:45,250
that we need to pay attention to when we train machine learning,

176
00:10:45,900 --> 00:10:48,350
for safety critical applications, like driving.

177
00:10:49,240 --> 00:10:51,080
Well, it turns out that in fact,

178
00:10:51,490 --> 00:10:56,150
if you use deep neural network solutions for image classification,

179
00:10:57,130 --> 00:10:58,970
the the solutions work really well,

180
00:10:59,170 --> 00:11:02,750
because they are trained on a huge dataset ImageNet,

181
00:11:03,830 --> 00:11:07,915
but the solutions capture more than the essence of the object,

182
00:11:07,915 --> 00:11:12,720
the solutions also capture the context in which the object appears,

183
00:11:13,130 --> 00:11:14,580
and so MIT researchers,

184
00:11:14,900 --> 00:11:17,940
lead by Josh Tenenbaum and Boris Katz,

185
00:11:18,140 --> 00:11:20,850
did an experiment a few years ago,

186
00:11:21,380 --> 00:11:23,160
where they took regular objects,

187
00:11:24,150 --> 00:11:25,810
and they put them in a different context,

188
00:11:25,860 --> 00:11:28,480
so, for instance, they took their shoes and put them on the bed,

189
00:11:28,890 --> 00:11:31,990
they took the pots and pens and put them in the bathroom,

190
00:11:32,490 --> 00:11:35,290
and with a significant change in context,

191
00:11:35,940 --> 00:11:42,850
the performance of the top performing ImageNet algorithms dropped by as much as 40-50%,

192
00:11:43,350 --> 00:11:44,500
which is really extraordinary.

193
00:11:45,280 --> 00:11:46,795
And I'm sharing this with you,

194
00:11:46,795 --> 00:11:49,860
not to discourage you from using these algorithms,

195
00:11:49,940 --> 00:11:51,570
but to point out that,

196
00:11:52,160 --> 00:11:55,240
when you, when you deploy an algorithm,

197
00:11:55,240 --> 00:11:58,380
and especially when you deploy it in a safety critical application,

198
00:11:58,580 --> 00:12:01,780
it's important to understand the scope of the algorithm,

199
00:12:01,780 --> 00:12:03,270
it's important to understand,

200
00:12:03,410 --> 00:12:05,080
what works and what doesn't work,

201
00:12:05,080 --> 00:12:06,900
when you can apply the algorithm

202
00:12:06,950 --> 00:12:10,710
and you when you shouldn't apply the the algorithm.

203
00:12:11,620 --> 00:12:14,715
And so, so keep this in mind

204
00:12:14,715 --> 00:12:20,060
as you think about deploying or building and deploying deep neural network solutions.

205
00:12:20,920 --> 00:12:27,590
There's another thing that is very critical for autonomous driving and for robots,

206
00:12:28,150 --> 00:12:32,540
you have heard a beautiful lecture on adversarial attacks,

207
00:12:32,920 --> 00:12:35,510
well, it turns out, you can attack very easily,

208
00:12:35,950 --> 00:12:43,220
the images that get fed from the camera streams of cars to the decision making engine of the car,

209
00:12:43,420 --> 00:12:47,300
and in fact, it's quite easy to take a stop sign

210
00:12:47,620 --> 00:12:50,690
and perturb it a little bit, perturb it,

211
00:12:50,980 --> 00:12:55,425
in, in such a way that you can't even tell with the naked eye,

212
00:12:55,425 --> 00:12:58,010
that there is a difference between the two images,

213
00:12:58,880 --> 00:13:00,550
and with those small perturbations,

214
00:13:00,600 --> 00:13:03,910
you can turn the stop sign into a yield sign,

215
00:13:04,140 --> 00:13:10,330
and you can imagine what kind of chaos this would create on a, on a physical road.

216
00:13:11,410 --> 00:13:17,540
So machine learning is very powerful for building perception systems for robots,

217
00:13:18,040 --> 00:13:21,705
but as we employ machine learning in the context of robots,

218
00:13:21,705 --> 00:13:24,530
it's important to keep in mind the scope,

219
00:13:24,730 --> 00:13:26,540
when they work, when they don't work,

220
00:13:26,890 --> 00:13:29,960
and then it's important to think about

221
00:13:30,880 --> 00:13:36,650
what might, what kind of guardrails we might put in place at the decision time,

222
00:13:37,240 --> 00:13:40,040
so that we have robust behavior.

223
00:13:41,410 --> 00:13:42,470
So what could we do,

224
00:13:42,550 --> 00:13:47,630
about the possibility of adversarial perturbations on the stop sign.

225
00:13:48,550 --> 00:13:51,560
Well, let's talk a little bit about decision making

226
00:13:51,940 --> 00:13:56,960
and let's talk about how the car figures out what to do given input.

227
00:13:57,250 --> 00:14:02,360
Reinforcement learning is causing a huge revolution in robotics,

228
00:14:03,160 --> 00:14:05,625
and so and why is that,

229
00:14:05,625 --> 00:14:10,710
well, the reason reinforcement learning is causing a huge revolution in robotics

230
00:14:10,710 --> 00:14:17,270
is because we have built fast simulation systems and simulation methodologies,

231
00:14:17,530 --> 00:14:20,930
that allow us to run thousands of simulations in parallel

232
00:14:21,790 --> 00:14:24,380
in order to train a reinforcement learning policy,

233
00:14:24,910 --> 00:14:32,990
and we are also decreasing the gap between the hardware platforms and the simulation engines.

234
00:14:34,410 --> 00:14:39,940
So you have seen reinforcement learning earlier in the in the boot camp,

235
00:14:40,740 --> 00:14:44,990
and so reinforcement learning is concerned with

236
00:14:44,990 --> 00:14:49,150
how intelligent agents ought to take action in an environment

237
00:14:49,470 --> 00:14:53,500
in order to maximize the notion of a cumulative reward.

238
00:14:54,310 --> 00:14:58,550
And so reinforcement learning is really about learning to act,

239
00:14:58,630 --> 00:15:01,130
and this differs from supervised learning

240
00:15:01,930 --> 00:15:08,600
is not needing a labeled input or is not needing labeled input, output pairs,

241
00:15:09,790 --> 00:15:10,850
so in this example,

242
00:15:10,930 --> 00:15:13,580
the agent has to learn to avoid fire

243
00:15:13,630 --> 00:15:14,895
and it's very simple,

244
00:15:14,895 --> 00:15:17,780
it gets a negative reward, if it goes to fire

245
00:15:17,800 --> 00:15:20,790
and it gets a positive reward, when it gets to water,

246
00:15:20,790 --> 00:15:24,600
and that's essentially what what this approach is like,

247
00:15:24,600 --> 00:15:26,690
you, you do trial and error

248
00:15:27,070 --> 00:15:32,370
and, and eventually the positive rewards dominate the negative rewards

249
00:15:32,370 --> 00:15:38,480
and that that directs the, the agent towards the, the best action.

250
00:15:39,500 --> 00:15:41,610
And so here is an example,

251
00:15:41,870 --> 00:15:45,000
where we have a reinforcement learning agent,

252
00:15:45,560 --> 00:15:48,780
that is trying to drive on a racetrack

253
00:15:49,040 --> 00:15:54,325
and you can see that it starts off and initially it makes mistakes,

254
00:15:54,325 --> 00:16:00,120
but eventually it learns how to, how to take the turns at high speeds,

255
00:16:00,590 --> 00:16:04,885
and interestingly you can do, you can take this idea

256
00:16:04,885 --> 00:16:06,330
and you can run it in parallel,

257
00:16:06,590 --> 00:16:08,670
so you can take thousands of cars

258
00:16:09,080 --> 00:16:11,970
and you can, you can put them on the same track

259
00:16:12,470 --> 00:16:14,910
and in the beginning they all make mistakes,

260
00:16:15,710 --> 00:16:19,080
but eventually they get the solution right

261
00:16:19,160 --> 00:16:27,690
and eventually that [joint] policy ends up being the policy that reliably controls the the vehicles,

262
00:16:28,460 --> 00:16:33,480
so, it's very, it's really a very exciting area,

263
00:16:33,800 --> 00:16:38,610
reinforcement learning, much like deep learning, has been invented decades ago,

264
00:16:39,170 --> 00:16:41,310
but it works well now,

265
00:16:41,600 --> 00:16:44,725
because of the advent of, of computation,

266
00:16:44,725 --> 00:16:48,810
we have so much more computes today, than we had 40 years ago,

267
00:16:49,280 --> 00:16:54,210
we have so much more data for deep neural networks today than we had 40 years ago,

268
00:16:54,380 --> 00:16:58,140
and so these techniques that did not do so well back then,

269
00:16:58,340 --> 00:17:03,810
all of a sudden are creating extraordinary possibilities and capabilities in our agents.

270
00:17:04,910 --> 00:17:06,810
Now this is a simple simulation,

271
00:17:07,130 --> 00:17:10,350
in order to get the simulation to drive a real robot,

272
00:17:10,640 --> 00:17:12,270
we actually need to think about,

273
00:17:12,320 --> 00:17:14,640
the dynamics of the robot,

274
00:17:14,900 --> 00:17:16,330
and so in other words,

275
00:17:16,330 --> 00:17:20,035
we have to take into account what the vehicle looks like,

276
00:17:20,035 --> 00:17:22,980
what are its kinematics, what are its dynamics,

277
00:17:23,000 --> 00:17:27,810
and so here is a vehicle that is running the policy learned in simulation,

278
00:17:27,980 --> 00:17:29,160
so it's really cool,

279
00:17:29,990 --> 00:17:33,205
because really we are now able to train in simulation,

280
00:17:33,205 --> 00:17:37,590
and if the model that we have for the vehicle dynamics is good enough,

281
00:17:37,760 --> 00:17:41,130
we can take the policy that was learned very fast in simulation

282
00:17:41,420 --> 00:17:43,530
and we can deploy it on the vehicle,

283
00:17:43,880 --> 00:17:47,430
and here are two vehicles racing with each other,

284
00:17:47,510 --> 00:17:49,270
the white car and the green car,

285
00:17:49,270 --> 00:17:50,890
so check out what the white car is doing,

286
00:17:50,890 --> 00:17:51,750
see how it snuck,

287
00:17:51,950 --> 00:17:53,080
wow, it's really great,

288
00:17:53,080 --> 00:17:54,390
I wish I could race like this,

289
00:17:55,970 --> 00:17:58,440
so this is really, really exciting,

290
00:17:59,300 --> 00:18:03,300
now in this case, the vehicles have limited field of view

291
00:18:04,190 --> 00:18:08,340
and they get the position of the other vehicles on the track,

292
00:18:08,960 --> 00:18:13,290
but only only, so they get this position from an external localization system,

293
00:18:13,880 --> 00:18:18,340
but they only know where the the vehicles within their field of view are,

294
00:18:18,340 --> 00:18:20,430
so look at this, that's great.

295
00:18:21,440 --> 00:18:25,380
So okay, so what can we do with with these methodologies,

296
00:18:25,400 --> 00:18:30,570
so we've seen how we can use deep learning to understand

297
00:18:30,680 --> 00:18:33,450
the the view of the vehicle from cameras,

298
00:18:33,950 --> 00:18:37,410
we've seen an example of learning to steer,

299
00:18:37,820 --> 00:18:38,790
what can we do.

300
00:18:39,140 --> 00:18:44,310
Well, I think that these, these advancements in robotics

301
00:18:44,510 --> 00:18:49,285
are really enabling the possibility that you saw in the first slide,

302
00:18:49,285 --> 00:18:52,440
the possibility of creating many robots that can do many tasks

303
00:18:52,850 --> 00:18:55,830
and much more complicated tasks than what we see here.

304
00:18:56,090 --> 00:18:59,980
And so what I want to talk about next is the autopilot,

305
00:18:59,980 --> 00:19:03,940
how do we take these pieces together to enable the autopilot,

306
00:19:03,940 --> 00:19:06,635
meaning to enable a self driving vehicle,

307
00:19:06,635 --> 00:19:07,880
I don't mean so,

308
00:19:07,880 --> 00:19:09,910
this autopilot is not the Tesla,

309
00:19:10,140 --> 00:19:15,160
autopilot, It's just the idea that you have a full self driving vehicle,

310
00:19:16,170 --> 00:19:17,330
now, in order to do this,

311
00:19:17,330 --> 00:19:19,060
we need to advance the brain more,

312
00:19:19,410 --> 00:19:24,580
we need to do more about the learning, reasoning and planning part of the car.

313
00:19:24,840 --> 00:19:26,705
So let me, let me ask you,

314
00:19:26,705 --> 00:19:30,790
do you know when was the first autonomous coast to coast drive in the United States,

315
00:19:31,350 --> 00:19:32,140
any guesses?

316
00:19:36,870 --> 00:19:37,990
Not you, I know, you know.

317
00:19:42,440 --> 00:19:43,020
2000 interesting.

318
00:19:46,340 --> 00:19:48,030
Actually, it was in 1995,

319
00:19:49,100 --> 00:19:53,550
in 1995, a Carnegie Mellon project called Navlab,

320
00:19:55,700 --> 00:20:02,370
built a car that that actually was was driven by a machine learning engine called Alvinn,

321
00:20:02,980 --> 00:20:08,430
and Alvinn drove this car all the way from Washington DC to Los Angeles,

322
00:20:09,470 --> 00:20:14,520
and the car was in autonomous mode for a large part of the highway driving,

323
00:20:14,780 --> 00:20:18,360
but there was always a student there ready to to take control

324
00:20:18,470 --> 00:20:22,110
and the car did not, did not drive in autonomous mode,

325
00:20:22,460 --> 00:20:30,900
when there were, when it was raining or when there was a lot of congestion or when the car had to had to take exits,

326
00:20:31,430 --> 00:20:33,360
so this is what the car did,

327
00:20:33,710 --> 00:20:37,620
it went from Washington DC, all the way to Los Angeles,

328
00:20:39,240 --> 00:20:41,950
now 1995 is a long time ago, right,

329
00:20:42,030 --> 00:20:44,380
I mean, it's before many of you were born.

330
00:20:44,820 --> 00:20:46,960
So it's really extraordinary to think about,

331
00:20:47,400 --> 00:20:52,000
what is needed in terms of advancement, in terms of progress

332
00:20:52,110 --> 00:20:55,805
in order to get from where we were back then to the point,

333
00:20:55,805 --> 00:20:58,120
where we can actually see deployed autonomous vehicles,

334
00:20:58,500 --> 00:20:59,150
and by the way,

335
00:20:59,150 --> 00:21:02,290
you should come and check out the MIT autonomous vehicles,

336
00:21:02,910 --> 00:21:06,155
which Alexander has built over the past five years,

337
00:21:06,155 --> 00:21:09,070
which are very powerful and can drive in our neighborhood,

338
00:21:10,730 --> 00:21:11,970
and we'll talk about how they drive.

339
00:21:13,070 --> 00:21:20,160
But interestingly, this was not the first time when we had cars racing in autonomous modes on, on highways,

340
00:21:21,380 --> 00:21:27,900
in fact, do you know when was the first autonomous highway drive in the world, anywhere in the world,

341
00:21:32,670 --> 00:21:35,020
so it was in 1986,

342
00:21:35,870 --> 00:21:41,280
and in 1986, German engineer Ernst Dickmann started thinking about,

343
00:21:41,480 --> 00:21:45,270
how he could turn his Van into an autonomous vehicle,

344
00:21:46,220 --> 00:21:49,075
and so he put computers and cameras on the Van

345
00:21:49,075 --> 00:21:53,845
and began running tests on an empty section of the German autobahn,

346
00:21:53,845 --> 00:21:58,320
which had not been open for for public driving,

347
00:21:59,030 --> 00:22:04,170
and he was able to actually get his Van to drive on that empty road,

348
00:22:05,000 --> 00:22:10,040
but interestingly, when he started developing this work,

349
00:22:11,350 --> 00:22:14,570
computers needed about 10 minutes to analyze an image,

350
00:22:16,080 --> 00:22:17,200
okay, can you imagine,

351
00:22:17,790 --> 00:22:21,700
okay, so how do you, how do you go from that

352
00:22:21,870 --> 00:22:26,350
to enabling an autonomous vehicle to drive at 90 kilometers an hour,

353
00:22:27,740 --> 00:22:29,860
well, what they did was,

354
00:22:29,860 --> 00:22:34,470
they, they, they developed some very fast solutions

355
00:22:34,820 --> 00:22:41,335
for paring down the image to only the aspects that they needed to look at

356
00:22:41,335 --> 00:22:43,480
and they assumed that there were no obstacles in the world,

357
00:22:43,480 --> 00:22:45,630
which made the problem much easier,

358
00:22:45,800 --> 00:22:49,320
because all the car had to do was to stay on on the road,

359
00:22:49,790 --> 00:22:55,140
so it's really super interesting to think about how visual processing improved

360
00:22:55,880 --> 00:23:00,090
from one frame per ten minutes to a hundred frames per second,

361
00:23:01,210 --> 00:23:04,575
and this has been a game changer for autonomous cars,

362
00:23:04,575 --> 00:23:08,270
and we're getting back to the connection between hardware and software,

363
00:23:08,800 --> 00:23:13,160
we need both in order to get good solutions for real problems.

364
00:23:14,980 --> 00:23:19,310
The other thing that happened in autonomous driving was that the lidar sensors,

365
00:23:19,780 --> 00:23:22,820
decreased the uncertainty and increased safety.

366
00:23:23,320 --> 00:23:28,815
And today we have many companies and groups that are deploying self driving cars,

367
00:23:28,815 --> 00:23:31,365
this is an example from Singapore,

368
00:23:31,365 --> 00:23:33,710
it's, it's a vehicle we deployed

369
00:23:33,790 --> 00:23:39,045
and in fact, we had the public ride our vehicle in 2014,

370
00:23:39,045 --> 00:23:40,490
we have vehicles at MIT,

371
00:23:40,660 --> 00:23:44,780
we have a lot of other groups that are developing these vehicles,

372
00:23:45,550 --> 00:23:49,370
now, before we had lidar, we had sonar

373
00:23:49,750 --> 00:23:51,830
and nothing worked when we had sonar,

374
00:23:53,230 --> 00:23:57,830
because, when you, when you work with sonar,

375
00:23:58,180 --> 00:24:02,270
the sonar beams just kind of go forward and then they bounce,

376
00:24:02,860 --> 00:24:06,950
and if the angle is about plus minus seven degrees,

377
00:24:07,720 --> 00:24:09,500
you will hear the ping back,

378
00:24:09,820 --> 00:24:13,580
but if the angle, so if the sonar bounces on a surface,

379
00:24:13,660 --> 00:24:19,490
that's more than seven degrees angled from from the direction of the sonar,

380
00:24:19,690 --> 00:24:22,310
that sonar ping will bounce off

381
00:24:22,510 --> 00:24:25,735
and it will bounce on other objects and walls

382
00:24:25,735 --> 00:24:30,180
and you will get wrong direction measurements, wrong distance measurements.

383
00:24:31,550 --> 00:24:33,780
So with lidar, that problem went away,

384
00:24:33,830 --> 00:24:39,210
so all of a sudden, a powerful, accurate sensor made a huge difference,

385
00:24:39,230 --> 00:24:43,680
all the algorithms that were developed on sonar and didn't work started working,

386
00:24:43,700 --> 00:24:45,210
when the lidar was introduced,

387
00:24:45,320 --> 00:24:46,200
it was really exciting.

388
00:24:48,010 --> 00:24:53,330
Okay, now when we think about autonomous driving,

389
00:24:53,470 --> 00:24:56,580
there are several key parameters that emerge,

390
00:24:56,580 --> 00:25:00,890
as we think about what the capabilities of these systems are,

391
00:25:01,540 --> 00:25:03,090
and one, one question is,

392
00:25:03,090 --> 00:25:05,895
how complex is the environment where the car is moving,

393
00:25:05,895 --> 00:25:08,775
if it's an empty road like in the German case,

394
00:25:08,775 --> 00:25:10,310
then the problem is much easier,

395
00:25:11,650 --> 00:25:13,050
then we have to ask ourselves,

396
00:25:13,050 --> 00:25:16,850
how, how complex are the interactions between the car and the environment,

397
00:25:17,050 --> 00:25:18,710
and we also have to think about,

398
00:25:19,000 --> 00:25:20,955
how complex is the reasoning of the car,

399
00:25:20,955 --> 00:25:22,460
how fast is the car going,

400
00:25:23,290 --> 00:25:27,020
and underlying all these questions is a fundamental question,

401
00:25:27,400 --> 00:25:29,150
and this fundamental question is,

402
00:25:29,320 --> 00:25:31,220
how does the car cope with uncertainties,

403
00:25:32,800 --> 00:25:34,580
now you have seen that,

404
00:25:34,660 --> 00:25:38,210
machine learning has uncertainty associated with it,

405
00:25:38,710 --> 00:25:43,520
so as you consider deploying machine learning on safety critical applications,

406
00:25:43,660 --> 00:25:44,840
it is super important

407
00:25:45,250 --> 00:25:55,670
to consider the connection between between your context, the uncertainties of the models that you're deploying and what the actual application requires.

408
00:25:56,520 --> 00:25:57,285
And I will tell you,

409
00:25:57,285 --> 00:26:02,540
that today we have very effective and deployable solutions for robot cars

410
00:26:02,590 --> 00:26:05,655
that move safely in easy environments,

411
00:26:05,655 --> 00:26:08,900
where there aren't many static nor moving obstacles,

412
00:26:09,760 --> 00:26:12,740
and you can, you can see from this example,

413
00:26:12,910 --> 00:26:15,440
this is, this is an example of the MIT car

414
00:26:15,730 --> 00:26:17,960
and its you can see this,

415
00:26:18,040 --> 00:26:22,440
this car operating autonomously without any issues at Fort Devens,

416
00:26:22,440 --> 00:26:24,920
where there aren't too many obstacles,

417
00:26:25,340 --> 00:26:28,385
and the car is perfectly capable of avoiding the obstacle,

418
00:26:28,385 --> 00:26:29,410
by the way, that's my car,

419
00:26:29,640 --> 00:26:34,790
so I'm very glad that the car is very capable of avoiding obstacles,

420
00:26:34,790 --> 00:26:36,550
and in fact, I was so convinced,

421
00:26:36,570 --> 00:26:39,250
I said, okay, we can use my car as the obstacle.

422
00:26:40,230 --> 00:26:42,460
But the sensors don't work well in weather,

423
00:26:42,750 --> 00:26:49,150
and the uncertainty of the perception system increases significantly if it rains hard or it snows,

424
00:26:49,530 --> 00:26:53,320
and the uncertainty of the vehicle prior also increases,

425
00:26:53,700 --> 00:26:55,900
in the case of extreme congestion,

426
00:26:56,010 --> 00:26:58,570
where you have erratic driving with vehicles,

427
00:26:58,800 --> 00:27:00,160
with people with scooters,

428
00:27:00,420 --> 00:27:02,890
even with cows on the road,

429
00:27:03,180 --> 00:27:07,660
and this is a video I took during a taxi ride in Bangalore,

430
00:27:08,070 --> 00:27:08,920
there come the cows.

431
00:27:11,520 --> 00:27:15,070
So, there are so many important preconditions,

432
00:27:15,570 --> 00:27:21,670
and many of these preconditions revolve around uncertainty in perception, planning, learning, reasoning and execution,

433
00:27:22,080 --> 00:27:24,070
before we can get to RoboTaxi,

434
00:27:25,760 --> 00:27:28,350
we can have many other robot solutions,

435
00:27:29,030 --> 00:27:32,700
that are much, that can happen today,

436
00:27:33,500 --> 00:27:34,975
and so I want to tell you,

437
00:27:34,975 --> 00:27:40,410
that many companies and research teams are deploying and developing self driving cars

438
00:27:41,000 --> 00:27:44,310
and many of them follow a very simple solution,

439
00:27:44,570 --> 00:27:47,920
which you can adopt and turn your car into a self driving car,

440
00:27:47,920 --> 00:27:49,170
so here's what you have to do,

441
00:27:49,800 --> 00:27:53,910
you take your car, you extend it to drive by wires,

442
00:27:53,910 --> 00:27:59,690
so that your computer can talk to to the steering and the acceleration, the throttle controls,

443
00:28:00,650 --> 00:28:04,525
so then you further extend this car with sensors,

444
00:28:04,525 --> 00:28:08,400
and most of the sensors we use are cameras and lidars,

445
00:28:09,350 --> 00:28:12,300
and then there are a suite of software models, modules,

446
00:28:12,560 --> 00:28:14,425
and this includes a perception module,

447
00:28:14,425 --> 00:28:20,310
that provides support for making maps and for detecting a static and dynamic obstacles,

448
00:28:20,900 --> 00:28:22,980
and then we have an estimation module,

449
00:28:23,570 --> 00:28:27,415
that identifies where the robot is located,

450
00:28:27,415 --> 00:28:38,610
and it does so by comparing what what the perception system sees now against a map that was created by looking at the infrastructure,

451
00:28:39,530 --> 00:28:43,080
and finally, there is a learning planning and control system

452
00:28:43,220 --> 00:28:47,010
that figures out what the car should do based on where it is,

453
00:28:47,450 --> 00:28:49,530
so this is it, this is the recipe,

454
00:28:50,270 --> 00:28:53,790
you can take your vehicle and turn it into an autonomous vehicle,

455
00:28:54,140 --> 00:28:55,290
and as you do so,

456
00:28:55,310 --> 00:28:59,130
you really have to think about foundationally,

457
00:28:59,920 --> 00:29:04,550
what are the computational units that you have to make, you have to create,

458
00:29:05,560 --> 00:29:07,580
so you have to process the sensor data,

459
00:29:07,990 --> 00:29:09,620
you have to detect obstacles,

460
00:29:09,730 --> 00:29:11,240
you have to localize the vehicle,

461
00:29:11,410 --> 00:29:12,800
you have to plan,

462
00:29:13,030 --> 00:29:14,090
and then you have to move.

463
00:29:15,370 --> 00:29:21,890
And so there are so many works that address each of these sub tasks that are involved in autonomous navigation,

464
00:29:22,330 --> 00:29:25,215
and, and some of these works are model based,

465
00:29:25,215 --> 00:29:28,160
and some of the works are machine learning based,

466
00:29:29,290 --> 00:29:31,725
but what's really interesting is that,

467
00:29:31,725 --> 00:29:37,700
in this autonomous driving pipeline, the classical autonomous driving pipeline,

468
00:29:38,880 --> 00:29:40,300
there are a lot of parameters,

469
00:29:41,070 --> 00:29:44,170
so for every solution of each of these individual problems,

470
00:29:44,490 --> 00:29:50,710
you have to hand engineer parameters for any type of road situation that the car will encounter,

471
00:29:51,330 --> 00:29:54,980
and then you will have to think about how the modules get stitched together,

472
00:29:54,980 --> 00:29:59,600
you need to define parameters that connect the modules together,

473
00:29:59,600 --> 00:30:03,140
and this is very tough to do in a robust way,

474
00:30:03,140 --> 00:30:05,530
and it brings brittleness to these solutions,

475
00:30:06,090 --> 00:30:09,590
in, in fact, you have to really think about, what are the parameters,

476
00:30:09,590 --> 00:30:10,840
if you have nighttime driving

477
00:30:11,220 --> 00:30:12,940
or if you have rainy weather

478
00:30:12,960 --> 00:30:16,235
or you're, you're in the country, on a country road

479
00:30:16,235 --> 00:30:18,230
or in the city on a city road,

480
00:30:18,230 --> 00:30:20,620
or you're on the road with no lane markings,

481
00:30:22,380 --> 00:30:25,390
so these are really challenging things,

482
00:30:25,740 --> 00:30:31,330
that the first solutions for autonomous driving had to reason through.

483
00:30:32,180 --> 00:30:35,520
Now, in Alexander's PhD thesis,

484
00:30:35,520 --> 00:30:39,650
his idea was to utilize a large dataset

485
00:30:40,120 --> 00:30:44,480
to learn a representation of what humans did in similar situations

486
00:30:45,070 --> 00:30:47,300
and develop autonomous driving solutions,

487
00:30:47,470 --> 00:30:52,970
that drive more like humans than than the the traditional pipeline,

488
00:30:53,020 --> 00:30:56,630
which is much more roboticy if you, if you like.

489
00:30:58,500 --> 00:30:59,680
So then the question is,

490
00:30:59,730 --> 00:31:05,860
how can we use machine learning to go directly from sensors to actuation,

491
00:31:06,450 --> 00:31:09,790
in other words, can we compress all the stuff in the middle

492
00:31:10,050 --> 00:31:13,780
and use learning to connect directly perception and action.

493
00:31:16,220 --> 00:31:19,230
So the solutions that we employed

494
00:31:20,030 --> 00:31:22,650
build on things we have already talked about,

495
00:31:23,660 --> 00:31:26,520
we can use deep learning and reinforcement learning

496
00:31:26,870 --> 00:31:35,310
to take us from images of roads onto steering and throttle on what to do,

497
00:31:37,440 --> 00:31:39,760
so this is really great,

498
00:31:40,020 --> 00:31:43,240
because you can train on certain kinds of roads,

499
00:31:44,180 --> 00:31:52,060
and, and you can then take your vehicle and put the vehicle in completely different driving environments and driving situations,

500
00:31:52,410 --> 00:31:53,920
and you don't need new parameters,

501
00:31:54,360 --> 00:31:55,450
you don't need retraining，

502
00:31:55,800 --> 00:31:59,620
you can go exactly directly to what the car has to do.

503
00:32:01,570 --> 00:32:03,920
So in other words, we can learn a model

504
00:32:04,570 --> 00:32:06,080
to go from raw perception,

505
00:32:06,250 --> 00:32:09,980
and here you can think of this as pixels from a camera,

506
00:32:11,130 --> 00:32:15,460
and the other thing we feed the vehicle is noisy street view maps,

507
00:32:15,750 --> 00:32:18,040
so these are not the high definition maps,

508
00:32:18,060 --> 00:32:24,010
that are usually created by autonomous driving labs and companies.

509
00:32:24,830 --> 00:32:25,950
And so you can do this

510
00:32:26,480 --> 00:32:33,720
to directly infer a full continuous probability distribution over the space of all control,

511
00:32:34,130 --> 00:32:41,850
and here the red lines indicate the inferred trajectories of the vehicle projected out onto the image frame,

512
00:32:42,530 --> 00:32:47,040
and the opacity represents the learned density function by our model,

513
00:32:47,720 --> 00:32:50,820
and so this is done by training a deep learning model,

514
00:32:51,290 --> 00:32:59,670
and the deep learning model can output the parameters of this conditional distribution directly from human driving data.

515
00:33:02,620 --> 00:33:05,960
Okay, so more precisely,

516
00:33:06,400 --> 00:33:12,380
the input to our learning system consists of camera feeds from three cameras,

517
00:33:12,850 --> 00:33:15,830
camera that looks forward and two cameras that look sideways,

518
00:33:16,600 --> 00:33:19,820
and also the street view maps,

519
00:33:21,310 --> 00:33:27,170
and from this, from this data, this data is processed,

520
00:33:27,460 --> 00:33:30,230
and it's, from this data,

521
00:33:30,460 --> 00:33:38,900
we can learn to maximize the likelihood of particular control signals for particular situations,

522
00:33:40,370 --> 00:33:47,340
and amazingly, the solution also allows us to localize the vehicle,

523
00:33:48,580 --> 00:33:49,830
so it's really super exciting.

524
00:33:52,850 --> 00:33:57,850
Okay, so we can, we can, we can get this human like control,

525
00:33:57,850 --> 00:34:00,445
but this human like control requires a lot of data,

526
00:34:00,445 --> 00:34:01,860
and I've told you at the beginning,

527
00:34:02,090 --> 00:34:03,840
that we have to be careful with the data,

528
00:34:04,040 --> 00:34:05,820
because there are a lot of corner cases,

529
00:34:06,740 --> 00:34:11,190
there are a lot of typical near accident situations,

530
00:34:11,540 --> 00:34:14,580
for which it's difficult to generate real data.

531
00:34:16,080 --> 00:34:17,260
So, for instance,

532
00:34:18,890 --> 00:34:19,300
let's see,

533
00:34:19,920 --> 00:34:22,100
for instance, if you have,

534
00:34:23,620 --> 00:34:28,550
if you want to ensure that the car will not be responsible for an accident

535
00:34:28,570 --> 00:34:32,270
and will know what to do when it comes to road situations like this,

536
00:34:32,470 --> 00:34:33,650
it would be pretty expensive

537
00:34:34,180 --> 00:34:37,550
to take a car and crash that car in order to generate the data.

538
00:34:39,130 --> 00:34:41,300
So instead, what we do is,

539
00:34:41,830 --> 00:34:44,330
we do the training in simulation,

540
00:34:45,460 --> 00:34:50,420
and so Alexander developed the vista simulator,

541
00:34:51,040 --> 00:34:57,430
and the Vista simulator can model multiple agents, multiple types of sensors

542
00:34:57,430 --> 00:35:00,990
and multiple types of agent to agent interaction,

543
00:35:02,030 --> 00:35:08,070
and, and so the Vista simulator has been recently open sourced,

544
00:35:08,420 --> 00:35:12,370
you can get the code from vista.csail.mit.edu,

545
00:35:12,370 --> 00:35:15,900
and a lot of people are already using the system.

546
00:35:16,580 --> 00:35:18,210
So what we get from Vista,

547
00:35:19,030 --> 00:35:24,540
is the ability to simulate different physical sensing modalities,

548
00:35:24,540 --> 00:35:31,280
that means including 2D cameras, 3D LiDAR, event cameras and, and so forth,

549
00:35:32,170 --> 00:35:41,600
and then you get the possibility to, to simulate different types of environmental, situations and perturbations,

550
00:35:41,740 --> 00:35:45,020
you can simulate weather, you can simulate different lighting,

551
00:35:45,610 --> 00:35:48,030
you can simulate, simulate different types of roads

552
00:35:48,030 --> 00:35:52,670
and you can also simulate different types of of interactions.

553
00:35:53,110 --> 00:35:54,860
So here's how we use Vista,

554
00:35:57,840 --> 00:36:04,480
we can take one high quality dataset,

555
00:36:05,550 --> 00:36:07,990
taken from a human driven vehicle,

556
00:36:08,850 --> 00:36:12,310
We can take this very high definition dataset

557
00:36:12,600 --> 00:36:16,660
and then in simulation, we can turn it into anything we want,

558
00:36:16,830 --> 00:36:18,880
we can turn it into erratic driving,

559
00:36:18,930 --> 00:36:22,810
we can turn it into near accident simulation, situation,

560
00:36:23,040 --> 00:36:25,690
we can turn it into anything, anything you want,

561
00:36:26,040 --> 00:36:30,310
so for instance, here you can see our original data

562
00:36:30,660 --> 00:36:35,140
and you can see how this original data can be mapped in simulation

563
00:36:36,300 --> 00:36:42,280
and in a way that looks very realistically into a new simulated trajectory,

564
00:36:42,450 --> 00:36:43,570
that is erratic

565
00:36:43,830 --> 00:36:48,040
and that now exists as part of our training set in Vista.

566
00:36:49,820 --> 00:36:52,770
And so we can, we can do this,

567
00:36:53,450 --> 00:36:54,540
we can use this data

568
00:36:54,860 --> 00:36:56,730
and then we can learn a policy,

569
00:36:56,900 --> 00:36:59,040
we can evaluate this policy offline

570
00:36:59,270 --> 00:37:01,800
and ultimately deploy it on the vehicle.

571
00:37:02,300 --> 00:37:07,230
And this works by first updating the state of all agents,

572
00:37:07,760 --> 00:37:10,290
based on the vehicle dynamics and interactions,

573
00:37:10,790 --> 00:37:17,220
and then by recreating the world from the new viewpoints of the agents,

574
00:37:17,420 --> 00:37:18,330
once you move them,

575
00:37:18,800 --> 00:37:25,170
the world will look different to the agent than in the original driving dataset.

576
00:37:25,550 --> 00:37:28,560
And finally we can,

577
00:37:28,880 --> 00:37:33,810
we can then render the image from the different agents viewpoints

578
00:37:34,070 --> 00:37:37,560
and we can perform the control.

579
00:37:39,220 --> 00:37:41,115
So, so this is really cool,

580
00:37:41,115 --> 00:37:43,730
there are several other simulation engines

581
00:37:44,080 --> 00:37:46,580
and there are simulation engines,

582
00:37:46,780 --> 00:37:54,230
that rely on on Imitation Learning or on Domain Randomization or there is CARLA,

583
00:37:54,430 --> 00:37:56,630
that's very effective, that seem to real,

584
00:37:57,580 --> 00:38:00,740
however, our solution works better than all the other solutions

585
00:38:00,820 --> 00:38:03,770
and here you can see the results of comparing,

586
00:38:04,210 --> 00:38:12,110
what happens in Vista with the, with what happens in the existing simulators in the state of the art.

587
00:38:12,940 --> 00:38:16,130
So the top line shows crash locations in red

588
00:38:16,480 --> 00:38:21,080
and the bottom line shows mean trajectory variation in color,

589
00:38:21,590 --> 00:38:25,405
and you can, you can see that our solution really does the best,

590
00:38:25,405 --> 00:38:35,310
and in fact, the solution is able to do things that other simulation based learning, based control cannot do,

591
00:38:36,290 --> 00:38:38,010
for instance, in our solution,

592
00:38:38,270 --> 00:38:43,860
we are able to recover from orientations that point the vehicle off the road,

593
00:38:44,720 --> 00:38:49,740
or we are able to to recover from being in the wrong in the wrong lane.

594
00:38:50,740 --> 00:38:54,945
So here's a vehicle that is executing the learning base control,

595
00:38:54,945 --> 00:38:57,770
and here's Alexander with his vehicle,

596
00:38:57,880 --> 00:39:02,270
that was trained using data from urban driving,

597
00:39:02,800 --> 00:39:05,120
and now he's driving to the soccer field,

598
00:39:05,720 --> 00:39:07,570
and you can see that,

599
00:39:07,800 --> 00:39:11,585
he, he's able to drive to, to get this car, to drive him to the soccer field

600
00:39:11,585 --> 00:39:13,370
without doing any training,

601
00:39:13,370 --> 00:39:16,180
without ever having seen this road

602
00:39:16,440 --> 00:39:20,405
and explicitly providing data about this road,

603
00:39:20,405 --> 00:39:21,760
so this is pretty cool, right.

604
00:39:22,920 --> 00:39:26,350
All right, so, okay, I'm going to open the hood for you,

605
00:39:27,200 --> 00:39:32,730
and so I'm going to show you what happens inside the decision engine of this solution,

606
00:39:33,800 --> 00:39:36,600
so let me orient you in this image,

607
00:39:37,640 --> 00:39:40,680
bottom right, you see the map of the environment,

608
00:39:41,270 --> 00:39:43,800
top left, you see the camera input stream,

609
00:39:44,210 --> 00:39:48,480
bottom left, you see the attention map of the vehicle,

610
00:39:48,590 --> 00:39:51,420
and then in the middle you see the decision engine,

611
00:39:51,950 --> 00:39:56,850
the decision engine has about 100,000 neurons and about half a million parameters.

612
00:39:57,920 --> 00:40:03,790
And I will challenge you to to figure out,

613
00:40:03,790 --> 00:40:10,990
if there are any patterns that associate the state of neurons with the behavior of the vehicle,

614
00:40:10,990 --> 00:40:12,220
it's really hard to see,

615
00:40:12,220 --> 00:40:13,500
because there are so many of them,

616
00:40:13,670 --> 00:40:18,480
there's just so much stuff that is happening in parallel at the same time.

617
00:40:19,390 --> 00:40:21,800
And then have a look at the attention map,

618
00:40:22,090 --> 00:40:25,520
so it turns out this vehicle is looking at the bushes on the roads

619
00:40:25,540 --> 00:40:27,890
in order on the road in order to make decisions,

620
00:40:29,860 --> 00:40:32,210
still, it seems to do a pretty good job,

621
00:40:32,650 --> 00:40:34,190
but we asked ourselves,

622
00:40:34,450 --> 00:40:35,330
can we do better,

623
00:40:35,980 --> 00:40:39,920
can we have more reliable learning based solutions.

624
00:40:40,900 --> 00:40:49,040
And so yesterday Ramin introduced liquid networks and introduced neural circuit policies,

625
00:40:50,020 --> 00:40:54,210
and so I just want to drill down a little bit more into this area,

626
00:40:54,710 --> 00:40:56,370
because you can now compare,

627
00:40:56,600 --> 00:41:02,430
you can now understand how the the original engine worked,

628
00:41:02,480 --> 00:41:06,900
and you can compare that against what we get from liquid networks.

629
00:41:07,910 --> 00:41:11,430
And so look at this, we have nineteen neurons,

630
00:41:11,510 --> 00:41:18,480
and now it's much easier to look at patterns of activation of these neurons

631
00:41:18,800 --> 00:41:22,050
and associate them with the behavior of the vehicle,

632
00:41:22,860 --> 00:41:26,520
and the attention map is so much cleaner, right,

633
00:41:26,520 --> 00:41:30,915
the vehicle is looking at the road horizon and at the sides of the road,

634
00:41:30,915 --> 00:41:33,080
which is what we all do when we drive a vehicle.

635
00:41:35,060 --> 00:41:41,070
Now remember how, so remember that Ramin told us that,

636
00:41:41,120 --> 00:41:44,910
this this model called liquid time-constant network,

637
00:41:45,680 --> 00:41:50,340
is a continuous time network,

638
00:41:53,150 --> 00:41:56,820
this model changes what the neuron computes

639
00:41:57,230 --> 00:42:01,410
and in particular we start with a well behaved state space model

640
00:42:01,520 --> 00:42:03,870
to increase the neuron stability during learning,

641
00:42:04,580 --> 00:42:08,125
and then we have nonlinearities over the synaptic inputs

642
00:42:08,125 --> 00:42:10,170
to increase the expressivity of the model

643
00:42:11,060 --> 00:42:15,630
and to also increase the model state during training and inference,

644
00:42:16,040 --> 00:42:19,765
and by plugging these equations into each other,

645
00:42:19,765 --> 00:42:23,400
we can see the equation of the LTC neuron,

646
00:42:24,810 --> 00:42:26,830
where here the function is,

647
00:42:28,470 --> 00:42:33,070
where the function here determines not only the state of the neuron,

648
00:42:34,450 --> 00:42:44,000
but also this function can be is controlled by new input at the time of execution of inference,

649
00:42:44,650 --> 00:42:47,900
so what's really cool about this model is that,

650
00:42:48,520 --> 00:42:55,130
it is able to dynamically adapt after training based on the inputs that it sees,

651
00:42:55,890 --> 00:42:59,020
and this is something very powerful about liquid networks.

652
00:42:59,850 --> 00:43:03,580
Now, in addition to changing the neuron equation,

653
00:43:04,800 --> 00:43:06,430
we also change the wiring

654
00:43:06,690 --> 00:43:08,960
and this new type of wiring,

655
00:43:08,960 --> 00:43:14,740
essentially gives function to the neurons in a deep neural network,

656
00:43:15,090 --> 00:43:16,540
every neurons is the same,

657
00:43:16,890 --> 00:43:18,640
in our architecture,

658
00:43:19,200 --> 00:43:20,765
we have input neurons,

659
00:43:20,765 --> 00:43:22,960
we have we have control neurons,

660
00:43:22,980 --> 00:43:24,370
we have inter neurons

661
00:43:24,600 --> 00:43:26,140
and they each do different things.

662
00:43:26,460 --> 00:43:27,560
And so with this in mind,

663
00:43:27,560 --> 00:43:33,250
we can look again at the beautiful solution that is enabled by liquid networks,

664
00:43:33,510 --> 00:43:37,600
and the solution keeps the car on on the roads

665
00:43:37,680 --> 00:43:42,880
and only requires 19 neurons to deliver that kind of function.

666
00:43:43,680 --> 00:43:44,770
And you can see here,

667
00:43:44,850 --> 00:43:48,880
that the attention of the solution is extremely focused

668
00:43:49,350 --> 00:43:51,070
as compared to other models,

669
00:43:51,240 --> 00:43:54,730
like CNN or CT-RNN or LSTM,

670
00:43:55,380 --> 00:43:56,860
which are much more noisy.

671
00:43:58,080 --> 00:44:03,160
So I hope you now have a better understanding of how liquid networks work

672
00:44:03,210 --> 00:44:05,110
and what their properties are,

673
00:44:06,240 --> 00:44:10,370
now we can take this model and apply it to many other problems.

674
00:44:10,810 --> 00:44:12,800
So here's a problem we call Kenyon Run,

675
00:44:13,360 --> 00:44:16,640
where we have taken a liquid network

676
00:44:16,900 --> 00:44:21,620
and we have implemented it On a task of flying a plane

677
00:44:22,660 --> 00:44:23,930
with one degree of freedom,

678
00:44:24,010 --> 00:44:26,070
where the plane can only go up and down,

679
00:44:26,070 --> 00:44:28,350
but it has to hit these obstacles,

680
00:44:28,350 --> 00:44:32,565
which are at locations that the plane does not know

681
00:44:32,565 --> 00:44:35,000
and the plane also does not know what the environment looks like,

682
00:44:36,190 --> 00:44:38,330
and so in particular when you have,

683
00:44:38,590 --> 00:44:44,270
we have when, when you have the, when you implement the task with one degree of freedom control for the plane,

684
00:44:44,470 --> 00:44:48,320
all you need is eleven liquid neurons,

685
00:44:48,880 --> 00:44:51,950
if you want to control all the degrees of freedom of the plane,

686
00:44:52,480 --> 00:44:54,195
then you need 24 neurons,

687
00:44:54,195 --> 00:44:58,220
It's still much smaller than the huge models that we're talking about today.

688
00:44:59,140 --> 00:45:02,580
Here's another task, we call Drone Dodgeball,

689
00:45:02,580 --> 00:45:08,190
where the objective is to keep a drone at specified location

690
00:45:08,720 --> 00:45:14,100
and the drone has to protect itself, when when balls come its way

691
00:45:14,630 --> 00:45:18,580
and you can see a two degree of freedom solution to Drone Dodgeball

692
00:45:18,580 --> 00:45:19,470
and that's the network,

693
00:45:19,610 --> 00:45:23,610
you can see how it, how all the neurons fire

694
00:45:24,140 --> 00:45:28,890
and you can really associate the the function of

695
00:45:29,240 --> 00:45:35,250
of this controller of this learning based control with activation patterns of the neurons,

696
00:45:36,070 --> 00:45:37,100
and so very excited,

697
00:45:37,300 --> 00:45:42,470
because in fact, we're able to extract decision trees from these kinds of solutions,

698
00:45:42,730 --> 00:45:49,790
and these decision trees provide human understandable, human, understandable explanations,

699
00:45:50,730 --> 00:45:54,460
and so this is really important for safety critical systems.

700
00:45:56,310 --> 00:45:59,140
All right, so let's see,

701
00:45:59,370 --> 00:46:01,510
Ramin told you that,

702
00:46:01,890 --> 00:46:04,660
these liquid networks are dynamic causal models

703
00:46:04,740 --> 00:46:07,480
and I want to show you some more examples,

704
00:46:07,830 --> 00:46:12,610
that, that explain how these models are dynamic causal models.

705
00:46:13,110 --> 00:46:14,710
So here you can see the,

706
00:46:14,880 --> 00:46:16,715
so, so here we're studying the task

707
00:46:16,715 --> 00:46:21,070
of finding an object inside a wooded environment

708
00:46:21,810 --> 00:46:26,170
and the object, and here here are some examples of the data

709
00:46:26,580 --> 00:46:28,685
that we have used to train this task,

710
00:46:28,685 --> 00:46:35,740
so essentially, we've had a human pilot drive a drone to, to, to accomplish the task.

711
00:46:36,350 --> 00:46:37,920
Now, this is the data,

712
00:46:39,320 --> 00:46:40,200
so check this out,

713
00:46:40,820 --> 00:46:43,980
we have then used a standard deep neural network

714
00:46:45,110 --> 00:46:48,930
and we have asked this network to solve this problem,

715
00:46:49,460 --> 00:46:53,310
and the attention map of the network is really all over the place,

716
00:46:53,540 --> 00:46:58,950
you can see that the network, the deep neural network solution, is very confused,

717
00:46:59,450 --> 00:47:00,990
but check out something else,

718
00:47:01,550 --> 00:47:05,250
the data that we collected was summertime data,

719
00:47:05,510 --> 00:47:06,510
and now it's Fall,

720
00:47:07,070 --> 00:47:09,060
so the background is no longer green,

721
00:47:09,530 --> 00:47:13,050
we have, we don't have as many leaves on trees

722
00:47:13,550 --> 00:47:17,970
and so the context for this task has completely changed.

723
00:47:21,090 --> 00:47:27,010
By comparison, the the liquid network is able to focus on the task,

724
00:47:27,540 --> 00:47:28,600
is not confused

725
00:47:29,010 --> 00:47:33,815
and is able to go directly to the object that it needs to find,

726
00:47:33,815 --> 00:47:38,620
and look how clean the attention map of this of this example is,

727
00:47:39,000 --> 00:47:40,390
so this is very exciting,

728
00:47:40,920 --> 00:47:43,360
because it, it really shows that,

729
00:47:43,410 --> 00:47:44,470
with liquid networks,

730
00:47:44,970 --> 00:47:45,910
we have a solution,

731
00:47:46,290 --> 00:47:53,080
that is, that is able to in some sense abstract out the context of training

732
00:47:53,430 --> 00:47:58,570
and that means we can get zero shot transfers from one environment to another.

733
00:47:59,430 --> 00:48:04,510
And so moreover, we actually have done the same task in the middle of the Winter,

734
00:48:04,920 --> 00:48:06,290
when we no longer have leaves,

735
00:48:06,290 --> 00:48:08,140
we have black tree lines,

736
00:48:08,610 --> 00:48:15,730
and the environment looks much, much different than the environment where we trained,

737
00:48:16,290 --> 00:48:24,310
and this kind of this kind of ability to transfer from one set of training data to completely different environments

738
00:48:24,660 --> 00:48:28,810
is truly transformational for the capabilities of machine learning.

739
00:48:29,700 --> 00:48:31,450
Well, we've done more than that,

740
00:48:31,470 --> 00:48:34,330
so we've taken our train solution

741
00:48:34,560 --> 00:48:36,280
and we deployed it in the lab,

742
00:48:37,860 --> 00:48:42,030
so here is, here is Markram,

743
00:48:42,530 --> 00:48:43,710
who worked on this problem

744
00:48:43,760 --> 00:48:44,940
and he is,

745
00:48:45,620 --> 00:48:47,275
and and look at the attention map,

746
00:48:47,275 --> 00:48:50,580
I mean, he, the environment is not not even the woods,

747
00:48:50,870 --> 00:48:53,020
it's, it's an office,

748
00:48:53,020 --> 00:48:54,090
it's an indoor environment.

749
00:48:54,650 --> 00:48:57,030
And we see other examples,

750
00:48:57,230 --> 00:48:59,670
where we take our solution

751
00:48:59,900 --> 00:49:03,840
and we deploy it to find the same object, the chair,

752
00:49:04,130 --> 00:49:06,000
just outside of this data building

753
00:49:06,200 --> 00:49:09,390
and this is the the deep neural network solution,

754
00:49:09,560 --> 00:49:11,670
that gets completely confused

755
00:49:11,690 --> 00:49:13,890
and here is the liquid network solution,

756
00:49:14,750 --> 00:49:16,465
that has the exact same input

757
00:49:16,465 --> 00:49:21,990
and has no problem going to to the the going to the robot.

758
00:49:22,530 --> 00:49:24,545
Let's see a few more examples here,

759
00:49:24,545 --> 00:49:27,790
where we are doing hop by hop,

760
00:49:28,050 --> 00:49:30,500
we're actually searching the object

761
00:49:30,500 --> 00:49:32,860
and doing multi step solutions

762
00:49:33,060 --> 00:49:35,240
and in fact, in fact we can,

763
00:49:35,240 --> 00:49:37,780
if I can get to my next video,

764
00:49:39,010 --> 00:49:42,170
sorry, so, I,

765
00:49:42,430 --> 00:49:43,550
the next one is,

766
00:49:43,810 --> 00:49:46,280
it shows you that we can actually do this forever,

767
00:49:46,300 --> 00:49:48,890
so here is an infinite hop demo,

768
00:49:49,420 --> 00:49:52,700
that was done just outside on the baseball field,

769
00:49:52,990 --> 00:49:58,160
and we, we placed three of the same objects that we trained on

770
00:49:58,480 --> 00:50:01,850
and we placed them at unknown locations

771
00:50:02,050 --> 00:50:07,160
and we added the the search feature to our machine learning solution,

772
00:50:07,750 --> 00:50:11,480
and so we can, the system can go on and on and on

773
00:50:11,740 --> 00:50:13,370
hopping from one to the other.

774
00:50:14,380 --> 00:50:16,430
The final example I will show you is,

775
00:50:16,840 --> 00:50:19,940
in is on the patio of the state building,

776
00:50:20,470 --> 00:50:23,720
where we have put a number of chairs,

777
00:50:24,700 --> 00:50:26,310
we have put our favorite chair,

778
00:50:26,310 --> 00:50:30,110
but also we have put a lot of other other similar chairs

779
00:50:30,220 --> 00:50:32,565
and we can see that,

780
00:50:32,565 --> 00:50:35,330
liquid networks generalize very well,

781
00:50:35,380 --> 00:50:38,630
whereas if we take LSTM solution,

782
00:50:39,100 --> 00:50:42,440
it gets confused and goes to the wrong object.

783
00:50:43,830 --> 00:50:47,350
So, all of these ideas come together

784
00:50:47,790 --> 00:50:51,550
to really point to a new type of machine learning,

785
00:50:52,110 --> 00:50:57,280
that yields models that generalize to unseen scenarios,

786
00:50:57,360 --> 00:51:01,420
essentially addressing a challenge with today's neural networks,

787
00:51:01,500 --> 00:51:04,900
that do not generalize well to unseen test scenarios,

788
00:51:06,150 --> 00:51:09,040
because the models are so fast and compact,

789
00:51:09,300 --> 00:51:10,570
you can train them online,

790
00:51:10,890 --> 00:51:12,760
you can train them on edge devices,

791
00:51:13,320 --> 00:51:18,680
and you can really see that they are beginning to understand the task that they're given,

792
00:51:18,680 --> 00:51:19,835
so you can see that,

793
00:51:19,835 --> 00:51:24,460
we're really beginning to get at the semantics of what these systems,

794
00:51:25,860 --> 00:51:27,275
what these systems have to do.

795
00:51:27,275 --> 00:51:30,540
So, what does this have to do with the future?

796
00:51:32,790 --> 00:51:36,970
I think it is so exciting to use machine learning to study nature

797
00:51:37,500 --> 00:51:41,140
and to begin to understand the nature of intelligence

798
00:51:42,150 --> 00:51:45,305
and in our lab here, at csail,

799
00:51:45,305 --> 00:51:47,080
we have one project,

800
00:51:47,310 --> 00:51:51,370
that is looking at whether we can understand the lives of whales

801
00:51:52,230 --> 00:51:53,825
and so what do I mean by this.

802
00:51:53,825 --> 00:51:55,240
So here is an example,

803
00:51:55,350 --> 00:52:01,600
where we have used a robotic drone to, to go very,

804
00:52:01,620 --> 00:52:03,460
sorry this is, this is very loud,

805
00:52:07,140 --> 00:52:10,600
we have used a robotic drone to find whales

806
00:52:10,710 --> 00:52:12,370
and look at what they do

807
00:52:12,750 --> 00:52:13,630
and track them

808
00:52:14,280 --> 00:52:16,150
and here is some imaging

809
00:52:17,220 --> 00:52:22,385
and here some clips from what this what this system is able to do,

810
00:52:22,385 --> 00:52:26,390
we have used machine learning to identify the whales

811
00:52:26,390 --> 00:52:28,565
and then once you have identify the whale,

812
00:52:28,565 --> 00:52:32,470
we can actually [servo] to the center of the whale,

813
00:52:32,610 --> 00:52:35,675
essentially tracking the whale along the way,

814
00:52:35,675 --> 00:52:38,615
and here is how, how the system works,

815
00:52:38,615 --> 00:52:43,780
you can you can see a bunch of a group of whales,

816
00:52:44,730 --> 00:52:51,400
and you can see our robot serving and following the whales as they move along.

817
00:52:52,170 --> 00:52:56,920
Now, we, this is a very exciting project,

818
00:52:57,270 --> 00:53:01,570
because whales are such majestic, intelligent and mysterious creatures,

819
00:53:02,360 --> 00:53:07,390
and so if we can use our technologies to get better insights into their lives,

820
00:53:08,040 --> 00:53:16,630
we will be able to to understand more about about other animals and other other creatures,

821
00:53:16,710 --> 00:53:18,640
we share this beautiful planet with.

822
00:53:19,290 --> 00:53:22,475
So we can study these whales from above, from the air,

823
00:53:22,475 --> 00:53:28,970
we can also study the whales from from, within from, from inside the the ocean,

824
00:53:28,970 --> 00:53:32,290
and here's here's sophie, our soft robotic fish,

825
00:53:32,730 --> 00:53:39,500
with Joseph, who is with us today has participated in in building,

826
00:53:39,500 --> 00:53:46,180
and here's this beautiful, beautiful, very natural, moving robot,

827
00:53:46,380 --> 00:53:49,270
that can get close to aquatic creatures,

828
00:53:49,380 --> 00:53:52,600
that can move in the same way aquatic creatures do,

829
00:53:52,860 --> 00:53:55,910
without without disturbing them,

830
00:53:55,910 --> 00:53:59,660
when you put [] based robots in ocean environments,

831
00:53:59,660 --> 00:54:02,870
they behave differently than than the fish do

832
00:54:02,870 --> 00:54:04,840
and they tend to scare the fish.

833
00:54:05,220 --> 00:54:05,980
So if you're curious,

834
00:54:07,260 --> 00:54:10,355
the tail is made out of silicone

835
00:54:10,355 --> 00:54:17,090
and there is a pump, that can, that can pump water in two of its chambers,

836
00:54:17,090 --> 00:54:19,280
so you see it has two ripped chambers

837
00:54:19,280 --> 00:54:22,295
and you can move water from one chamber to the other,

838
00:54:22,295 --> 00:54:25,810
and depending on how much water we move and in what proportions,

839
00:54:25,980 --> 00:54:29,320
you can get the fish to move forward, to turn left or to turn right.

840
00:54:30,180 --> 00:54:34,690
So we can observe the motion of animals using robotic technologies,

841
00:54:34,920 --> 00:54:35,920
but we can do more,

842
00:54:36,180 --> 00:54:38,765
we can also listen in on what,

843
00:54:38,765 --> 00:54:40,850
oops, actually I need sound here,

844
00:54:40,850 --> 00:54:41,800
I forgot about this,

845
00:54:42,210 --> 00:54:46,685
we can observe the, the way we can observe what they say to each other,

846
00:54:46,685 --> 00:54:47,260
can you hear?

847
00:55:21,550 --> 00:55:23,780
So this is a sperm whale

848
00:55:24,820 --> 00:55:27,800
and you have heard the vocalization of sperm whales,

849
00:55:28,240 --> 00:55:32,990
we believe that, they're talking, that the sperm whale is talking to its family and friends,

850
00:55:33,760 --> 00:55:35,190
and we would like to know what it's saying,

851
00:55:37,630 --> 00:55:38,660
we have no idea,

852
00:55:38,740 --> 00:55:41,180
but we can use machine learning to make progress.

853
00:55:42,140 --> 00:55:45,930
And the way we can do that is by, is by

854
00:55:48,560 --> 00:55:54,330
using data, using the kind of data you have just heard,

855
00:55:55,220 --> 00:55:59,100
to look for, to look for the presence of language,

856
00:55:59,300 --> 00:56:01,350
which is a major sign of intelligence,

857
00:56:03,280 --> 00:56:05,805
we can look at whether we have discrete compounds,

858
00:56:05,805 --> 00:56:07,970
we can look at whether there might be grammar,

859
00:56:08,140 --> 00:56:11,120
we can look at whether we have long range dependencies,

860
00:56:11,380 --> 00:56:15,530
we can look at whether we have other properties that human language has.

861
00:56:16,940 --> 00:56:22,345
And so basically, our project is very much work in progress,

862
00:56:22,345 --> 00:56:25,260
I can't tell you today what the whales are saying to each other,

863
00:56:25,490 --> 00:56:26,785
but I can tell you that,

864
00:56:26,785 --> 00:56:27,900
we have made progress,

865
00:56:28,730 --> 00:56:29,970
I can tell you that,

866
00:56:30,140 --> 00:56:35,490
we are beginning to find which parts of their calls carry information,

867
00:56:36,360 --> 00:56:39,620
we can use machine learning to differentiate the clicks,

868
00:56:39,620 --> 00:56:42,340
that allow the whales to echolocate

869
00:56:42,750 --> 00:56:47,320
from the clicks that seem to be vocalization and information carrying clicks,

870
00:56:48,550 --> 00:56:52,550
we can begin to look at what the protocols for information exchange are,

871
00:56:52,630 --> 00:56:54,170
how do they engage in dialogue,

872
00:56:54,960 --> 00:56:56,810
and we can begin to ask,

873
00:56:57,040 --> 00:56:59,990
what is the information that they say to one another,

874
00:57:00,880 --> 00:57:04,845
so with our project, we are trying to understand

875
00:57:04,845 --> 00:57:10,020
the phonetics, the semantics and the syntax and the discourse for whales.

876
00:57:11,570 --> 00:57:16,410
So we have a big dataset consisting of about 22,000 clicks,

877
00:57:17,240 --> 00:57:20,400
the clicks get grouped into [codas],

878
00:57:20,420 --> 00:57:22,350
the codas are like the phonemes,

879
00:57:23,640 --> 00:57:25,750
and using machine learning,

880
00:57:25,830 --> 00:57:27,670
we can identify coda types,

881
00:57:28,260 --> 00:57:31,690
we can identify patterns for coda exchanges,

882
00:57:32,160 --> 00:57:37,090
and we can begin to really ask ourselves,

883
00:57:38,580 --> 00:57:43,450
how is it that that whales exchange information.

884
00:57:44,080 --> 00:57:45,570
And if you're interested in this problem,

885
00:57:45,710 --> 00:57:47,220
please come see us,

886
00:57:47,420 --> 00:57:52,380
because we have a lot of projects that are very, very exciting

887
00:57:52,790 --> 00:57:55,200
and important towards reverse engineering,

888
00:57:55,730 --> 00:58:02,730
what this really extraordinary and majestic animal is capable of doing.

889
00:58:04,120 --> 00:58:06,980
So let me close by saying that.

890
00:58:08,130 --> 00:58:14,080
In this class, you have looked at a number of really exciting machine learning algorithms,

891
00:58:14,660 --> 00:58:20,290
but you have also looked at what some of the technical challenges with the machine learning algorithms are,

892
00:58:21,460 --> 00:58:22,850
including data availability,

893
00:58:23,290 --> 00:58:24,350
including data quality,

894
00:58:25,030 --> 00:58:28,980
including the amount of computation required, the model size

895
00:58:28,980 --> 00:58:34,340
and the ability of that model to run on edge devices or on huge devices,

896
00:58:35,550 --> 00:58:39,760
you have seen that many of our solutions are black box solutions

897
00:58:39,870 --> 00:58:41,890
and sometimes we have brittle function,

898
00:58:42,600 --> 00:58:46,120
we have, we have easily attackable models,

899
00:58:47,960 --> 00:58:50,370
you have also seen some alternative models,

900
00:58:50,450 --> 00:58:51,390
like liquid networks,

901
00:58:51,950 --> 00:58:54,930
which attempt to address some of these questions,

902
00:58:56,030 --> 00:59:01,740
there is so much opportunity for developing improved machine learning,

903
00:59:01,760 --> 00:59:05,160
using existing models and inventing new models,

904
00:59:05,970 --> 00:59:07,180
and if we can do this,

905
00:59:08,070 --> 00:59:10,750
we can create an exciting world,

906
00:59:11,130 --> 00:59:13,880
where machines will really empower us,

907
00:59:13,880 --> 00:59:21,100
will really augment us and enhance us in our cognitive abilities and in our physical abilities.

908
00:59:22,080 --> 00:59:22,960
So, just imagine,

909
00:59:23,740 --> 00:59:26,430
waking up enabled by your personal assistant,

910
00:59:27,340 --> 00:59:29,180
that figures out the optimal time

911
00:59:29,530 --> 00:59:33,980
and helps you organize all the items that you need for the day

912
00:59:34,570 --> 00:59:36,825
and then brings them to you,

913
00:59:36,825 --> 00:59:39,650
so you don't have to think about whether your outfit matches,

914
00:59:41,090 --> 00:59:43,140
and as you walk by a store,

915
00:59:43,370 --> 00:59:49,410
the image in the store window displays your picture with the latest fashion on your body,

916
00:59:51,350 --> 00:59:53,020
and inside the store,

917
00:59:53,020 --> 00:59:55,270
maybe you want to buy a certain shoe,

918
00:59:55,270 --> 00:59:58,050
AI system can analyze how you walk,

919
00:59:58,070 --> 01:00:01,620
can analyze your your dimensions

920
01:00:02,000 --> 01:00:06,600
and can create a bespoke shoe, a bespoke model just for you,

921
01:00:07,540 --> 01:00:09,350
and then all the, all the clothing,

922
01:00:09,430 --> 01:00:12,560
all the items in our environment can kind of awaken,

923
01:00:12,700 --> 01:00:15,380
our clothing could become robots,

924
01:00:15,700 --> 01:00:20,810
and so our clothing could become monitoring devices,

925
01:00:20,950 --> 01:00:23,730
but they could also become programmable,

926
01:00:23,730 --> 01:00:24,945
for instance, in this case,

927
01:00:24,945 --> 01:00:29,720
you can see the ability of a sweater to change color,

928
01:00:30,070 --> 01:00:32,540
so that the girl can match her friend.

929
01:00:33,040 --> 01:00:35,565
Now, this is actually not far fetched,

930
01:00:35,565 --> 01:00:37,040
we have a group on campus,

931
01:00:37,690 --> 01:00:41,295
that is delivering these programmable [fibers],

932
01:00:41,295 --> 01:00:44,540
that can change color and that they can do some computation.

933
01:00:46,210 --> 01:00:49,050
At work, inside the intelligent boardroom,

934
01:00:49,050 --> 01:00:55,940
the temperature could get adjusted automatically by monitoring people's comfort and gesture,

935
01:00:55,960 --> 01:00:57,105
and just in time,

936
01:00:57,105 --> 01:01:03,710
holograms could be used to make the virtual world much more, much more realistic,

937
01:01:04,270 --> 01:01:05,480
much more connected,

938
01:01:06,460 --> 01:01:12,500
and so here they're discussing the the design of a new flying cars,

939
01:01:13,030 --> 01:01:15,560
and let's say we have these flying cars

940
01:01:16,120 --> 01:01:19,910
and then we can integrate these cars with the IT infrastructure,

941
01:01:20,520 --> 01:01:21,965
the cars will know your needs,

942
01:01:21,965 --> 01:01:23,180
so that they can tell you,

943
01:01:23,180 --> 01:01:26,740
for instance, that you can buy the plants you have been wanting nearby

944
01:01:27,060 --> 01:01:28,690
by computing a small detour,

945
01:01:29,340 --> 01:01:33,070
and back at home, you can take a first ride on a bike

946
01:01:33,600 --> 01:01:35,690
and the bike itself becomes a robot,

947
01:01:35,690 --> 01:01:40,000
with adaptable wheels that appear and disappear according to your skill level,

948
01:01:40,620 --> 01:01:43,040
you can have robots that help with planting,

949
01:01:43,040 --> 01:01:44,855
you have, you can have delivery robots

950
01:01:44,855 --> 01:01:48,370
and there's the garbage bin, the garbage bin that takes itself out.

951
01:01:50,350 --> 01:01:51,410
And after a good day,

952
01:01:51,460 --> 01:01:53,330
when it's time for a bedtime story,

953
01:01:54,190 --> 01:01:56,270
you can begin to enter the story

954
01:01:56,320 --> 01:01:58,430
and control the flow

955
01:01:58,600 --> 01:02:02,030
and begin to interact with the characters in the story.

956
01:02:03,580 --> 01:02:07,040
These are some possibilities for the kind of future,

957
01:02:07,480 --> 01:02:12,110
that machine learning, artificial intelligence and robots are enabling,

958
01:02:12,650 --> 01:02:15,570
and I'm personally very excited about this future,

959
01:02:15,770 --> 01:02:19,800
with robots helping us with cognitive and physical work.

960
01:02:20,830 --> 01:02:25,730
But this future is really dependent on very important new advancements,

961
01:02:26,020 --> 01:02:27,830
that will come from all of you,

962
01:02:28,240 --> 01:02:33,260
and so I'm so excited to see what you will be doing in the next years in the years ahead.

963
01:02:33,430 --> 01:02:34,490
So thank you very much

964
01:02:34,750 --> 01:02:36,560
and come work with us.

