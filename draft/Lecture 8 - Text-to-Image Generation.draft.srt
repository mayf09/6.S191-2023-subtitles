1
00:00:09,150 --> 00:00:12,455
0,1480 2010,2270 2270,2495 2495,2860 3030,3305
Thanks for the invite,| so,
谢谢邀请，|我是街对面 Google Research 的一名研究科学家，

2
00:00:12,455 --> 00:00:14,045
0,210 210,690 690,1005 1005,1230 1230,1590
yeah, I'm, I'm a research

3
00:00:14,045 --> 00:00:15,880
0,365 475,840 840,1125 1125,1440 1440,1835
scientist at Google Research just

4
00:00:15,930 --> 00:00:19,310
0,305 305,500 500,790 2070,2470 3120,3380
across the street,| and I
|今天我想谈一谈我们刚刚放在存档的一篇论文，

5
00:00:19,310 --> 00:00:20,255
0,105 105,225 225,420 420,675 675,945
want to talk today about

6
00:00:20,255 --> 00:00:21,380
0,270 270,555 555,780 780,930 930,1125
a paper that we just

7
00:00:21,380 --> 00:00:23,285
0,165 165,330 330,585 585,1130 1630,1905
put up on archives,| which
|这是一种新的模型，

8
00:00:23,285 --> 00:00:24,610
0,210 210,450 450,660 660,945 945,1325
is a new model for|
|

9
00:00:24,630 --> 00:00:26,800
0,365 365,650 650,920 920,1270 1770,2170
{Text-to-image - -} generation via
通过掩码生成转换器文本到图像生成，

10
00:00:26,940 --> 00:00:30,080
0,400 930,1570 1590,2105 2105,2470 2850,3140
Masked Generative Transformers,| and as
|正如你从这些图中可以猜到的那样，

11
00:00:30,080 --> 00:00:31,240
0,150 150,285 285,540 540,840 840,1160
you can guess from these

12
00:00:31,710 --> 00:00:33,200
0,400 810,1070 1070,1220 1220,1370 1370,1490
figures,| the name of the
|这个模型的名字是 Muse 。

13
00:00:33,200 --> 00:00:34,900
0,210 210,510 510,860
model is Muse.|
|

14
00:00:36,090 --> 00:00:37,670
0,260 260,395 395,670 720,1120 1230,1580
So this is a work
这是我和谷歌研究中心的一群了不起的同事一起做的工作，

15
00:00:37,670 --> 00:00:39,850
0,315 315,630 630,860 910,1310 1780,2180
that I've done with awesome

16
00:00:40,110 --> 00:00:41,510
0,275 275,440 440,730 750,1115 1115,1400
set of colleagues at Google

17
00:00:41,510 --> 00:00:44,500
0,320 1600,1860 1860,2120 2260,2625 2625,2990
research,| the paper and a
|一篇论文和一个在线网页。

18
00:00:44,730 --> 00:00:46,760
0,320 320,640 810,1210
web pages online.|
|

19
00:00:48,960 --> 00:00:50,990
0,380 380,760 1350,1640 1640,1820 1820,2030
So, {} many of you
所以，你们中的许多人一定很熟悉文本图像生成，

20
00:00:50,990 --> 00:00:52,310
0,210 210,405 405,710 790,1095 1095,1320
must be familiar with {text-to-image

21
00:00:52,310 --> 00:00:54,040
0,165 165,345 345,680 1120,1425 1425,1730
- -} generation,| which has
|它在过去一两年里确实取得了很大的进步，

22
00:00:54,090 --> 00:00:56,540
0,400 1110,1510 1590,1990 2070,2330 2330,2450
{} really advanced in the

23
00:00:56,540 --> 00:00:59,290
0,260 310,710 730,1095 1095,1460 2350,2750
last year or two| and
|一个问题可能是为什么要文本图像生成，

24
00:00:59,490 --> 00:01:00,670
0,275 275,500 500,740 740,905 905,1180
a question might be why

25
00:01:00,690 --> 00:01:02,720
0,305 305,470 470,605 605,880 1740,2030
{text-to-image - -} generation,| I
|我认为这是因为文本是一种非常自然的生成控制机制，

26
00:01:02,720 --> 00:01:04,040
0,225 225,570 570,840 840,1125 1125,1320
think it's because text is

27
00:01:04,040 --> 00:01:06,040
0,105 105,350 400,800 1330,1665 1665,2000
a very natural control mechanism

28
00:01:06,270 --> 00:01:08,060
0,260 260,520 1290,1535 1535,1625 1625,1790
for generation,| we are able
|我们能够通过使用文本来表达我们的想法和创造性的想法，

29
00:01:08,060 --> 00:01:10,330
0,285 285,615 615,960 960,1340 1870,2270
to express our thoughts, creative

30
00:01:10,470 --> 00:01:11,720
0,400 450,785 785,965 965,1085 1085,1250
ideas through the use of

31
00:01:11,720 --> 00:01:13,510
0,290 670,945 945,1125 1125,1410 1410,1790
text| and then that allows
|允许非专家、非艺术家生成引人注目的图像，

32
00:01:13,650 --> 00:01:16,220
0,395 395,770 770,1070 1070,1390 2310,2570
{non-experts -}, {non-artists -} to

33
00:01:16,220 --> 00:01:17,810
0,260 340,750 750,1010 1150,1425 1425,1590
generate compelling images,| and then
|然后能够通过编辑工具重复它们，

34
00:01:17,810 --> 00:01:18,980
0,150 150,315 315,480 480,885 885,1170
be able to iterate them

35
00:01:18,980 --> 00:01:21,380
0,285 285,750 750,1070 1870,2160 2160,2400
through editing tools| to create
|从而创造出你自己的个人艺术和想法。

36
00:01:21,380 --> 00:01:23,920
0,240 240,480 480,830 1150,1550 2140,2540
your own personal art and

37
00:01:23,940 --> 00:01:25,300
0,400
ideas.|
|

38
00:01:25,700 --> 00:01:27,870
0,400 510,860 860,1130 1130,1450 1770,2170
Also, {} very importantly, {}|
另外，非常重要的是，|

39
00:01:28,430 --> 00:01:29,800
0,320 320,640 690,1025 1025,1235 1235,1370
deep learning requires lots of
深度学习需要大量的数据，

40
00:01:29,800 --> 00:01:31,135
0,260 370,660 660,825 825,1065 1065,1335
data| and it is much
|收集大规模的配对图文数据要可行得多，

41
00:01:31,135 --> 00:01:32,935
0,210 210,615 615,795 795,1115 1435,1800
more feasible to collect large

42
00:01:32,935 --> 00:01:34,740
0,365 385,885 885,1185 1185,1485 1485,1805
scale paired image text data,|
|

43
00:01:35,390 --> 00:01:36,655
0,335 335,650 650,935 935,1100 1100,1265
an example is the {LAION-5B
一个例子是 LAION-5B 数据集，

44
00:01:36,655 --> 00:01:39,025
0,305 625,945 945,1265 1735,2085 2085,2370
- - -} {dataset -}|
|

45
00:01:39,025 --> 00:01:40,735
0,255 255,575 955,1320 1320,1545 1545,1710
on which models such as
在这些数据集上已经训练了稳定扩散等模型。

46
00:01:40,735 --> 00:01:42,150
0,240 240,705 705,960 960,1125 1125,1415
stable diffusion have been trained.|
|

47
00:01:43,060 --> 00:01:44,540
0,395 395,790 870,1130 1130,1235 1235,1480
{} So we should recognize
所以我们应该认识到这些数据集中存在着各种各样的偏差，

48
00:01:44,620 --> 00:01:46,365
0,290 290,580 630,1295 1295,1580 1580,1745
that various biases exist in

49
00:01:46,365 --> 00:01:47,670
0,165 165,405 405,690 690,960 960,1305
these {datasets -},| and bias
|而减轻偏差是一个重要的研究问题。

50
00:01:47,670 --> 00:01:49,820
0,480 480,735 735,945 945,1280 1750,2150
mitigation is an important research

51
00:01:49,900 --> 00:01:50,980
0,400
problem.|
|

52
00:01:51,650 --> 00:01:54,670
0,365 365,620 620,1150 2310,2675 2675,3020
{} And lastly, these models
最后，这些模型可以利用预先训练的大型语言模型，

53
00:01:54,670 --> 00:01:56,980
0,375 375,900 900,1280 1750,2055 2055,2310
can exploit {} {pre-trained -}

54
00:01:56,980 --> 00:01:59,185
0,350 520,900 900,1280 1810,2070 2070,2205
large language models,| which are
|这些模型非常强大，

55
00:01:59,185 --> 00:02:00,820
0,165 165,455 1015,1305 1305,1470 1470,1635
very powerful| and they allow
|它们允许对文本进行极其细粒度的理解，

56
00:02:00,820 --> 00:02:02,880
0,225 225,525 525,840 840,1220 1660,2060
for extremely {fine-grained -} understanding

57
00:02:02,990 --> 00:02:05,220
0,320 320,640 1380,1700 1700,1925 1925,2230
of text,| parts of speech,
|词性、名词、动词、形容词，

58
00:02:05,450 --> 00:02:07,480
0,260 260,395 395,905 905,1415 1415,2030
you know, nouns, verbs, adjectives,|
|

59
00:02:07,480 --> 00:02:08,520
0,255 255,420 420,555 555,720 720,1040
and then be able to
然后能够将这些语义概念转换为输出图像，

60
00:02:08,600 --> 00:02:11,430
0,320 320,640 1260,1865 1865,2495 2495,2830
translate those semantic concepts to

61
00:02:12,500 --> 00:02:14,160
0,335 335,670
output images,|
|

62
00:02:14,730 --> 00:02:16,180
0,275 275,455 455,665 665,970 1050,1450
and these {LLM -} importantly
这些 LLM 可以在各种具有数量级更大的文本数据的文本任务上进行预训练，

63
00:02:16,200 --> 00:02:17,225
0,260 260,410 410,590 590,800 800,1025
can be {pre-trained -} on

64
00:02:17,225 --> 00:02:18,905
0,300 300,645 645,995 1075,1395 1395,1680
various text tasks with orders

65
00:02:18,905 --> 00:02:21,100
0,240 240,810 810,1145 1525,1860 1860,2195
of magnitude larger text data,|
|

66
00:02:21,150 --> 00:02:22,250
0,365 365,605 605,770 770,935 935,1100
so you can {pre-trained -}
所以你可以用纯文本数据进行预训练，

67
00:02:22,250 --> 00:02:23,465
0,165 165,345 345,570 570,890 940,1215
with text only data| and
|然后使用配对数据来进行文本图像转换训练。

68
00:02:23,465 --> 00:02:24,820
0,150 150,360 360,630 630,905 955,1355
then use paired data to

69
00:02:25,050 --> 00:02:26,080
0,275 275,410 410,590 590,770 770,1030
do the {text-to-image - -}

70
00:02:26,250 --> 00:02:28,080
0,400 840,1240
translation training.|
|

71
00:02:29,600 --> 00:02:30,385
0,245 245,425 425,530 530,650 650,785
So what's the state of
那么最先进的技术是什么，

72
00:02:30,385 --> 00:02:31,735
0,120 120,365 715,990 990,1155 1155,1350
the art,| you must be
|你一定对其中的许多都很熟悉，

73
00:02:31,735 --> 00:02:33,150
0,305 355,720 720,960 960,1125 1125,1415
familiar with many of these,|
|

74
00:02:34,190 --> 00:02:35,710
0,320 320,680 680,905 905,1220 1220,1520
so {Dall-E2 -} from {OpenAI
来自 OpenAI 的 Dall-E2 是最早的模型之一，

75
00:02:35,710 --> 00:02:37,345
0,270 270,620 1120,1380 1380,1500 1500,1635
-} was one of the

76
00:02:37,345 --> 00:02:39,745
0,275 655,1055 1735,1995 1995,2160 2160,2400
first models,| which is a
|这是一个扩散模型，

77
00:02:39,745 --> 00:02:42,580
0,510 510,905 1735,2025 2025,2315
diffusion model,| that was,
|这是建立在预先训练的 CLIP 表示法基础上的，

78
00:02:43,230 --> 00:02:44,300
0,260 260,395 395,575 575,815 815,1070
that was built on {pre-trained

79
00:02:44,300 --> 00:02:47,390
0,320 340,735 735,1490 2650,2910 2910,3090
-} CLIP representations,| I won't
|我没有时间逐一介绍这些术语，

80
00:02:47,390 --> 00:02:48,035
0,105 105,270 270,405 405,495 495,645
have time to go into

81
00:02:48,035 --> 00:02:49,430
0,180 180,345 345,540 540,1230 1230,1395
each of these terminology,| so
|所以很抱歉，对于你们中不熟悉这些的人。

82
00:02:49,430 --> 00:02:50,420
0,180 180,440 460,705 705,825 825,990
I'm sorry, some of you

83
00:02:50,420 --> 00:02:52,220
0,165 165,360 360,680
is not familiar.|
|

84
00:02:52,680 --> 00:02:54,785
0,400 600,995 995,1390 1470,1865 1865,2105
Imagen from Google is also
Google 的 Imagen 也是一个扩散模型，

85
00:02:54,785 --> 00:02:56,590
0,120 120,510 510,875 1105,1455 1455,1805
a diffusion model,| that was
|建立在预先训练好的大语言模型上。

86
00:02:57,150 --> 00:02:58,130
0,305 305,470 470,605 605,770 770,980
built on a {pre-trained -}

87
00:02:58,130 --> 00:03:01,130
0,270 270,600 600,980 2170,2570 2740,3000
large language model.| Parti, which
|Parti 是 Google 的另一个大规模模型，

88
00:03:01,130 --> 00:03:02,300
0,180 180,435 435,690 690,915 915,1170
is another large scale model

89
00:03:02,300 --> 00:03:03,860
0,270 270,590 910,1185 1185,1335 1335,1560
from Google,| is an {autoregressive
|是一个基于潜在符号空间的自回归模型。

90
00:03:03,860 --> 00:03:06,440
0,615 615,890 1030,1430 1720,2205 2205,2580
-} model on latent token

91
00:03:06,440 --> 00:03:07,540
0,320
space.|
|

92
00:03:07,540 --> 00:03:09,540
0,350 430,830 1030,1320 1320,1620 1620,2000
And {Stable/Latent - -} diffusion
Stable/Latent 扩散是 Stability AI 发布的模型，

93
00:03:09,710 --> 00:03:12,145
0,365 365,730 1530,1895 1895,2180 2180,2435
is a model release from

94
00:03:12,145 --> 00:03:14,020
0,335 595,995 1165,1515 1515,1740 1740,1875
Stability AI,| which is a
|它是关于潜在嵌入的扩散模型。

95
00:03:14,020 --> 00:03:15,780
0,330 330,615 615,855 855,1260 1260,1760
diffusion model on latent embeddings.|
|

96
00:03:17,000 --> 00:03:18,870
0,400 960,1220 1220,1400 1400,1550 1550,1870
So, so here's an example,|
这里有一个例子，|

97
00:03:19,220 --> 00:03:22,495
0,400 1350,1655 1655,1960 2280,2830 2850,3275
that is showing comparing DALL·E
显示了在这个特定的文本提示下 DALL·E 2, Imagen 和 Muse 模型的比较，

98
00:03:22,495 --> 00:03:23,845
0,225 225,765 765,1005 1005,1155 1155,1350
2, Imagen and the Muse

99
00:03:23,845 --> 00:03:25,915
0,335 685,1020 1020,1320 1320,1685 1735,2070
model on this particular text

100
00:03:25,915 --> 00:03:29,830
0,335 1705,2105 2665,3015 3015,3275 3385,3915
prompt.| and I'll go revisit
|稍后我将重温这个例子，

101
00:03:29,830 --> 00:03:30,955
0,195 195,465 465,705 705,900 900,1125
this example| and point out
|并指出不同模型的一些优缺点。

102
00:03:30,955 --> 00:03:32,710
0,335 835,1125 1125,1305 1305,1530 1530,1755
some pros and cons of

103
00:03:32,710 --> 00:03:34,300
0,135 135,345 345,705 705,1100
the different models later.|
|

104
00:03:36,290 --> 00:03:37,975
0,290 290,500 500,970 990,1390 1410,1685
Some image editing applications that
一些图像编辑应用程序基于这些模型构建的，

105
00:03:37,975 --> 00:03:39,805
0,150 150,330 330,615 615,995 1525,1830
have been built on these

106
00:03:39,805 --> 00:03:43,800
0,305 775,1125 1125,1475 1525,1925 3115,3995
models,| one example is Personalization
|一个例子是 Personalization 通过一个名为 Dreambooth 的工具，

107
00:03:44,120 --> 00:03:45,895
0,400 660,965 965,1220 1220,1520 1520,1775
via a tool called {Dreambooth

108
00:03:45,895 --> 00:03:47,185
0,425 625,885 885,1005 1005,1110 1110,1290
-},| which was a paper
|这是我的一些同事写的一篇论文，

109
00:03:47,185 --> 00:03:48,430
0,285 285,635 655,900 900,1050 1050,1245
written by some of my

110
00:03:48,430 --> 00:03:50,185
0,290 880,1155 1155,1320 1320,1545 1545,1755
colleagues,| so here the idea
|所以这里的想法是，

111
00:03:50,185 --> 00:03:52,560
0,150 150,425 505,795 795,1085 1855,2375
is that,| instead of generating,
|不是生成例如一只猫的图像，

112
00:03:53,030 --> 00:03:54,595
0,305 305,610 690,995 995,1265 1265,1565
for example, an image of

113
00:03:54,595 --> 00:03:56,095
0,255 255,575 865,1125 1125,1275 1275,1500
a cat,| you can generate
|而是生成一张你的猫的图片，

114
00:03:56,095 --> 00:03:57,480
0,180 180,360 360,645 645,990 990,1385
an image of your cat|
|

115
00:03:58,040 --> 00:03:59,290
0,350 350,620 620,920 920,1040 1040,1250
by fine tuning the model
通过在你自己的猫的一些图像上微调模型，

116
00:03:59,290 --> 00:04:00,790
0,270 270,495 495,800 970,1290 1290,1500
on some images of your

117
00:04:00,790 --> 00:04:02,230
0,210 210,530 790,1065 1065,1215 1215,1440
own cat| and then doing
|然后在文本指导下生成你的猫的图像。

118
00:04:02,230 --> 00:04:05,110
0,285 285,675 675,950 1930,2330 2560,2880
text guided generation of your

119
00:04:05,110 --> 00:04:05,780
0,320
cat.|
|

120
00:04:06,000 --> 00:04:08,225
0,305 305,760 1290,1595 1595,1895 1895,2225
So that's actually proven, extremely
这实际上是被证明非常受欢迎，

121
00:04:08,225 --> 00:04:09,790
0,335 685,945 945,1095 1095,1275 1275,1565
popular,| a couple of apps
|几款基于 Dreambooth 工具构建的应用程序已经登上了 App Store 的榜首，

122
00:04:09,840 --> 00:04:11,330
0,350 350,700 810,1085 1085,1250 1250,1490
built on the {Dreambooth -}

123
00:04:11,330 --> 00:04:12,680
0,320 430,795 795,1065 1065,1215 1215,1350
tool have gone to the

124
00:04:12,680 --> 00:04:13,750
0,210 210,405 405,540 540,735 735,1070
top of the App Store,|
|

125
00:04:14,190 --> 00:04:15,215
0,290 290,485 485,680 680,845 845,1025
and many people have used
许多人用它生成社交媒体上自己的头像。

126
00:04:15,215 --> 00:04:17,210
0,165 165,300 300,575 1015,1415 1645,1995
it to generate their own

127
00:04:17,210 --> 00:04:20,225
0,590 1300,1700 2320,2610 2610,2790 2790,3015
avatars for you on social

128
00:04:20,225 --> 00:04:21,300
0,335
media.|
|

129
00:04:22,700 --> 00:04:26,905
0,400 420,820 930,1330 3510,3830 3830,4205
Another example is {fine-tuning -}
另一个例子是对指令跟随进行微调，

130
00:04:26,905 --> 00:04:29,185
0,305 445,1085 1105,1505 1885,2145 2145,2280
for instruction following,| so that
|以便可以进行各种灵活的[自由掩码]编辑，

131
00:04:29,185 --> 00:04:30,295
0,180 180,345 345,525 525,825 825,1110
one can do various kinds

132
00:04:30,295 --> 00:04:32,160
0,225 225,545 655,1035 1035,1305 1305,1865
of flexible mask free editing,|
|

133
00:04:32,780 --> 00:04:35,755
0,290 290,580 1260,1660 2340,2705 2705,2975
for example, here on this
例如，在这张右上角的图像上，

134
00:04:35,755 --> 00:04:36,955
0,210 210,420 420,725 745,1020 1020,1200
top right image,| we say
|我们说向天空添加烟花，

135
00:04:36,955 --> 00:04:38,340
0,240 240,765 765,945 945,1095 1095,1385
add fireworks to the sky|
|

136
00:04:38,690 --> 00:04:39,925
0,290 290,515 515,695 695,890 890,1235
and then the model has,
然后模型经过微调，

137
00:04:39,925 --> 00:04:41,050
0,330 330,555 555,780 780,885 885,1125
after {fine-tuning -},| has an
|对天空和烟花等概念有了理解，

138
00:04:41,050 --> 00:04:43,075
0,315 315,650 1210,1755 1755,1860 1860,2025
understanding of concepts such as

139
00:04:43,075 --> 00:04:44,965
0,210 210,515 535,840 840,1385 1555,1890
the sky and fireworks,| and
|它能够很好地遵循指令。

140
00:04:44,965 --> 00:04:45,835
0,255 255,420 420,585 585,705 705,870
it's able to do a

141
00:04:45,835 --> 00:04:47,755
0,450 450,645 645,965 1345,1650 1650,1920
reasonably good job of following

142
00:04:47,755 --> 00:04:48,980
0,210 210,665
the instruction.|
|

143
00:04:52,070 --> 00:04:53,160
0,400
So,
所以， Muse 和我之前列出的模型有什么不同。

144
00:04:53,370 --> 00:04:55,640
0,395 395,605 605,940 1050,1450 1890,2270
how {is,Muse} different from the

145
00:04:55,640 --> 00:04:57,470
0,315 315,600 600,950 1420,1680 1680,1830
the prior models that I

146
00:04:57,470 --> 00:04:59,315
0,290 880,1125 1125,1410 1410,1620 1620,1845
listed.| So firstly, it's neither
|首先，它既不是扩散模型，也不是自回归模型，

147
00:04:59,315 --> 00:05:00,770
0,225 225,600 600,965 985,1290 1290,1455
a diffusion model nor is

148
00:05:00,770 --> 00:05:02,360
0,150 150,375 375,1040 1240,1485 1485,1590
it {autoregressive -},| although it
|虽然它与扩散和自回归模型族都有一些联系。

149
00:05:02,360 --> 00:05:04,610
0,165 165,435 435,800 970,1370 1930,2250
has some connections to both

150
00:05:04,610 --> 00:05:06,845
0,390 390,630 630,870 870,1520 1900,2235
diffusion and {autoregressive -} family

151
00:05:06,845 --> 00:05:09,350
0,210 210,485 1615,1875 1875,2130 2130,2505
of models.| It is extremely
|它的速度非常快，

152
00:05:09,350 --> 00:05:11,765
0,380 1270,1575 1575,1770 1770,2060 2080,2415
fast,| so for example a
|例如，一个 512x512 的图像在 1.3 秒内生成，

153
00:05:11,765 --> 00:05:12,680
0,225 225,405 405,585 585,765 765,915
{512x512 - - - -

154
00:05:12,680 --> 00:05:14,320
0,135 135,270 270,530 1090,1365 1365,1640
- -} image is generated

155
00:05:14,340 --> 00:05:16,160
0,275 275,920 920,1210 1440,1685 1685,1820
in 1.3 seconds,| instead of
|而不是 Imagen 或 Parti 的 10 秒，

156
00:05:16,160 --> 00:05:18,650
0,195 195,500 520,920 1780,2325 2325,2490
10s seconds for Imagen or

157
00:05:18,650 --> 00:05:20,375
0,290 340,740 940,1260 1260,1470 1470,1725
Parti| and about 4 seconds
|或者 Stable Diffusion 的大约 4 秒，

158
00:05:20,375 --> 00:05:21,665
0,255 255,450 450,885 885,1140 1140,1290
for Stable Diffusion| on the
|在相同的硬件上。

159
00:05:21,665 --> 00:05:22,700
0,210 210,545
same hardware.|
|

160
00:05:23,370 --> 00:05:25,685
0,320 320,560 560,880 1200,1955 1955,2315
{} And on Quantitative Eval,
还有在定量评估上，比如 SLIP score 和 FID ，

161
00:05:25,685 --> 00:05:26,915
0,150 150,375 375,675 675,945 945,1230
such as CLIP score and

162
00:05:27,140 --> 00:05:28,780
0,290 340,615 615,795 795,1100 1240,1640
FID,| which are measures of
|它们是衡量文本提示和图像彼此对齐程度的指标，

163
00:05:29,160 --> 00:05:30,845
0,305 305,545 545,880 990,1340 1340,1685
how well the text prompt

164
00:05:30,845 --> 00:05:31,955
0,285 285,435 435,690 690,960 960,1110
and the image line up

165
00:05:31,955 --> 00:05:32,885
0,135 135,255 255,495 495,810 810,930
with each other,| that's the
|这就是 CLIP score ，

166
00:05:32,885 --> 00:05:34,490
0,210 210,515 1110,1365 1365,1500 1500,1605
CLIP score,| and FID is
|而 FID 是衡量图像质量本身，多样性和保真度，

167
00:05:34,490 --> 00:05:35,530
0,120 120,345 345,615 615,780 780,1040
a measure of the image

168
00:05:35,550 --> 00:05:37,310
0,400 480,845 845,1070 1070,1330 1470,1760
quality itself, the diversity and

169
00:05:37,310 --> 00:05:39,880
0,680 880,1280 1330,1650 1650,1970 1990,2570
fidelity,| {} the model performs
|这个模型表现得很好。

170
00:05:40,050 --> 00:05:41,260
0,320 320,640
very well.|
|

171
00:05:42,030 --> 00:05:43,160
0,400 480,785 785,920 920,995 995,1130
{} So, so it has
所以它的语义性能和质量与这些更大的模型相似，

172
00:05:43,160 --> 00:05:44,860
0,270 270,840 840,1140 1140,1395 1395,1700
similar semantic performance and quality

173
00:05:45,180 --> 00:05:46,900
0,400 690,995 995,1190 1190,1400 1400,1720
as these much larger models,|
|

174
00:05:46,950 --> 00:05:49,490
0,320 320,640 870,1235 1235,1780 2190,2540
but significantly faster inference,| and
但推理速度要快得多，|而且它的性能明显好于 Stable Diffusion 。

175
00:05:49,490 --> 00:05:51,220
0,210 210,390 390,710 910,1310 1330,1730
it has significantly better performance

176
00:05:51,300 --> 00:05:52,960
0,335 335,605 605,1060
than Stable Diffusion.|
|

177
00:05:52,960 --> 00:05:53,940
0,180 180,360 360,495 495,675 675,980
{} All of these statements
所有这些说法在这个时间点上是正确的，

178
00:05:53,990 --> 00:05:55,000
0,290 290,500 500,680 680,815 815,1010
just hold true at this

179
00:05:55,000 --> 00:05:55,900
0,210 210,375 375,585 585,780 780,900
point in time,| all of
|所有这些模式都在不断改进，

180
00:05:55,900 --> 00:05:57,900
0,165 165,450 450,830 910,1310 1600,2000
these models keep improving,| So
|所以，下周可能会有一款表现更好的新模型。

181
00:05:59,300 --> 00:05:59,995
0,260 260,395 395,500 500,575 575,695
there could be a new

182
00:05:59,995 --> 00:06:01,260
0,240 240,480 480,660 660,915 915,1265
model that does even better,

183
00:06:01,520 --> 00:06:03,060
0,245 245,350 350,545 545,880
you know, next week.|
|

184
00:06:03,350 --> 00:06:05,065
0,380 380,760 840,1240 1290,1565 1565,1715
And some applications that are
我们训练模型的方式支持的一些应用程序是 Zero-shot 编辑，

185
00:06:05,065 --> 00:06:06,205
0,390 390,675 675,810 810,945 945,1140
enabled by the way we

186
00:06:06,205 --> 00:06:08,260
0,210 210,360 360,570 570,935 1675,2055
train the model are {Zero-shot

187
00:06:08,260 --> 00:06:12,595
0,380 1180,1850 3460,3860 3880,4200 4200,4335
-} editing,| so I'll show
|我将展示一些绘画中无掩码编辑的例子，

188
00:06:12,595 --> 00:06:14,125
0,180 180,420 420,615 615,875 1105,1530
some examples of that {Mask-free

189
00:06:14,125 --> 00:06:15,685
0,150 150,660 660,930 930,1235 1255,1560
-} editing in painting| to
|以移除对象和裁剪扩展图像边界。

190
00:06:15,685 --> 00:06:17,305
0,305 325,690 690,975 975,1365 1365,1620
remove objects and cropping to

191
00:06:17,305 --> 00:06:19,195
0,365 985,1260 1260,1380 1380,1725 1725,1890
expand beyond the boundaries of

192
00:06:19,195 --> 00:06:20,260
0,120 120,365
an image.|
|

193
00:06:22,280 --> 00:06:23,020
0,230 230,335 335,455 455,575 575,740
So I give a quick
我在这里先简要概述一下体系结构，

194
00:06:23,020 --> 00:06:25,060
0,465 465,690 690,1010 1120,1520 1720,2040
overview of the architecture here|
|

195
00:06:25,060 --> 00:06:26,095
0,195 195,345 345,480 480,705 705,1035
and then go into the
然后再一个接一个地介绍各个组件。

196
00:06:26,095 --> 00:06:27,750
0,365 535,930 930,1200 1200,1365 1365,1655
individual components one by one.|
|

197
00:06:28,850 --> 00:06:30,985
0,365 365,665 665,1000 1050,1450 1710,2135
Muse is mostly a transformer
Muse 主要是一个基于转换器的架构，

198
00:06:30,985 --> 00:06:33,700
0,275 1195,1595
based architecture,|
|

199
00:06:34,230 --> 00:06:35,450
0,275 275,485 485,710 710,950 950,1220
for both the text and
对于网络的文本和图像部分，

200
00:06:35,450 --> 00:06:37,060
0,320 550,945 945,1215 1215,1350 1350,1610
image parts of the network,|
|

201
00:06:37,710 --> 00:06:39,550
0,400 570,935 935,1160 1160,1340 1595,1840
but we also use CNNs,|
但我们也使用 CNN ，|

202
00:06:40,320 --> 00:06:41,460
0,400
and
我们也用矢量量化，

203
00:06:41,460 --> 00:06:42,945
0,350 550,900 900,1095 1095,1230 1230,1485
{} we also use vector

204
00:06:42,945 --> 00:06:44,415
0,630 630,855 855,1065 1065,1290 1290,1470
quantization| and we also use
|我们也使用 GAN ，

205
00:06:44,415 --> 00:06:45,690
0,285 285,480 480,630 630,845 1015,1275
GANs,| so we're using a
|所以我们使用了现代 CNN 的很多工具包，

206
00:06:45,690 --> 00:06:46,830
0,135 135,270 270,420 420,885 885,1140
lot of the toolkits in

207
00:06:46,830 --> 00:06:50,300
0,320 520,915 915,1520 1960,2360 3070,3470
the modern CNN,| {} modern
|现代深度网络，使用了很多工具包。

208
00:06:50,650 --> 00:06:52,950
0,290 290,580 1200,1460 1460,1720 2010,2300
deep network, you know, from

209
00:06:52,950 --> 00:06:55,260
0,150 150,590 1630,1890 1890,2055 2055,2310
the toolbox.| We use image
|我们使用了图像标记，在 CNN-VQGAN 的量化潜在空间中。

210
00:06:55,260 --> 00:06:56,355
0,405 405,585 585,750 750,915 915,1095
tokens, that are in the

211
00:06:56,355 --> 00:06:58,260
0,495 495,975 975,1325 1375,1680 1680,1905
quantized latent space of a

212
00:06:58,260 --> 00:06:59,960
0,420 420,1010
{CNN-VQGAN -}.|
|

213
00:07:00,120 --> 00:07:01,130
0,290 290,500 500,695 695,860 860,1010
And we train it with
我们用掩蔽损失来训练它，

214
00:07:01,130 --> 00:07:02,930
0,120 120,480 480,770 1360,1635 1635,1800
a masking loss,| which is
|这类似于大型语言模型中使用的掩蔽损失，

215
00:07:02,930 --> 00:07:03,965
0,255 255,465 465,555 555,855 855,1035
similar to the masking loss

216
00:07:03,965 --> 00:07:05,285
0,225 225,420 420,630 630,945 945,1320
used in large language model,|
|

217
00:07:05,285 --> 00:07:06,850
0,395 685,930 930,1080 1080,1275 1275,1565
so the model is basically
所以，模型基本上是在学习如何重建掩码 token ，

218
00:07:06,870 --> 00:07:09,070
0,335 335,560 560,710 710,1330 1800,2200
learning how to reconstruct mask

219
00:07:09,120 --> 00:07:10,640
0,580 900,1160 1160,1310 1310,1430 1430,1520
tokens,| you pass in a
|你传入一组 token ，

220
00:07:10,640 --> 00:07:12,005
0,105 105,225 225,650 880,1200 1200,1365
set of tokens,| mask a
|随机掩蔽其中的一组，

221
00:07:12,005 --> 00:07:12,815
0,135 135,285 285,405 405,555 555,810
bunch of them at random,|
|

222
00:07:12,815 --> 00:07:13,730
0,225 225,360 360,525 525,690 690,915
and then learn to predict
然后学习预测，

223
00:07:13,730 --> 00:07:14,480
0,350
that,|
|

224
00:07:14,480 --> 00:07:15,470
0,255 255,405 405,540 540,720 720,990
and just by doing that
通过使用可变的掩蔽率来实现，

225
00:07:15,470 --> 00:07:17,450
0,270 270,590 610,1095 1095,1560 1560,1980
with variable masking ratios| enables
|它就可以获得非常高质量的生成。

226
00:07:17,450 --> 00:07:18,365
0,225 225,345 345,480 480,675 675,915
it to get very good

227
00:07:18,365 --> 00:07:20,460
0,335 445,845
quality generation.|
|

228
00:07:21,230 --> 00:07:22,350
0,335 335,530 530,680 680,845 845,1120
{} We have two models,|
我们有两个模型，|

229
00:07:22,370 --> 00:07:23,910
0,305 305,560 560,860 860,1085 1085,1540
a base model that generates
一个基本模型可以生成 256x256 大小的图像，

230
00:07:25,250 --> 00:07:26,485
0,335 335,605 605,830 830,995 995,1235
{256x256 - - - -

231
00:07:26,485 --> 00:07:28,105
0,270 270,480 480,785 1255,1515 1515,1620
-} size images| and then
|然后一个超分辨率模型将其放大到 512x512 。

232
00:07:28,105 --> 00:07:29,620
0,150 150,375 375,695 895,1245 1245,1515
a super resolution model that

233
00:07:29,620 --> 00:07:31,570
0,540 540,825 825,1160 1480,1770 1770,1950
upscales that to {512x512 -

234
00:07:31,570 --> 00:07:32,920
0,225 225,555 555,840 840,1130
- - - -}.|
|

235
00:07:35,040 --> 00:07:36,040
0,400
Okay,
好的，所以第一个主要的组成部分是，

236
00:07:36,840 --> 00:07:38,710
0,260 260,380 380,620 620,1000 1470,1870
so the first {} major

237
00:07:38,760 --> 00:07:40,160
0,380 380,665 665,905 905,1145 1145,1400
component is| this {pre-trained -}
|这个预先训练好的大语言模型。

238
00:07:40,160 --> 00:07:41,930
0,285 285,600 600,980 1300,1605 1605,1770
large language model.| So we
|所以我们使用了谷歌的一个语言模型训练叫做 T5-XXL 模型，

239
00:07:41,930 --> 00:07:45,050
0,195 195,530 2140,2490 2490,2805 2805,3120
use a language model train

240
00:07:45,050 --> 00:07:46,820
0,240 240,530 1120,1425 1425,1605 1605,1770
at Google called the {T5-XXL

241
00:07:46,820 --> 00:07:48,130
0,255 255,510 510,705 705,960 960,1310
- - - -} model,|
|

242
00:07:48,630 --> 00:07:50,045
0,260 260,515 515,845 845,1100 1100,1415
which has about 5 billion
它有大约 50 亿个参数，

243
00:07:50,045 --> 00:07:52,580
0,605 1645,1920 1920,2115 2115,2340 2340,2535
parameters,| which was trained on
|在许多文本任务上进行了训练，

244
00:07:52,580 --> 00:07:54,515
0,270 270,650 880,1280 1480,1770 1770,1935
many text tasks,| {text-to-text -
|文本到文本的任务，

245
00:07:54,515 --> 00:07:55,720
0,195 195,465 465,675 675,870 870,1205
-} tasks,| such as translation,
|比如翻译，问题回答和分类。

246
00:07:56,730 --> 00:07:59,860
0,320 320,800 800,1025 1025,1600 2730,3130
question answering and classification.| And
|当提供文本提示时，

247
00:08:00,570 --> 00:08:01,640
0,260 260,380 380,575 575,815 815,1070
when a text prompt is

248
00:08:01,640 --> 00:08:04,340
0,350 1750,2100 2100,2265 2265,2505 2505,2700
provided,| it's passed in through
|它通过文本编码器传递进来，

249
00:08:04,340 --> 00:08:05,360
0,120 120,270 270,765 765,900 900,1020
the text encoder| and we
|我们得到一个维度为 4096 的向量序列，

250
00:08:05,360 --> 00:08:07,210
0,165 165,470 640,1020 1020,1305 1305,1850
get a sequence of vectors

251
00:08:07,530 --> 00:08:08,900
0,275 275,700 720,995 995,1160 1160,1370
of dimens {4096 - -

252
00:08:08,900 --> 00:08:10,340
0,320 370,645 645,795 795,1050 1050,1440
-},| which are then projected
|然后向下投影到另一个较低维度的序列，

253
00:08:10,340 --> 00:08:12,610
0,195 195,530 670,1070 1300,1590 1590,2270
down to another lower dimensional

254
00:08:12,870 --> 00:08:14,600
0,400 600,935 935,1235 1235,1475 1475,1730
sequence| and those set of
|然后将这些文本 token 传递到网络的其余部分，

255
00:08:14,600 --> 00:08:15,995
0,300 300,750 750,960 960,1125 1125,1395
text tokens are then passed

256
00:08:15,995 --> 00:08:17,645
0,365 385,785 805,1140 1140,1365 1365,1650
into the rest of the

257
00:08:17,645 --> 00:08:18,560
0,395
network,|
|

258
00:08:20,060 --> 00:08:21,625
0,260 260,395 395,515 515,760 1200,1565
and then we use {cross-attention
然后，我们使用从文本到图像 token 的 cross-attention 来指导生成过程。

259
00:08:21,625 --> 00:08:23,470
0,365 625,945 945,1155 1155,1445 1585,1845
-} from the text to

260
00:08:23,470 --> 00:08:24,960
0,120 120,330 330,840 840,1155 1155,1490
the image tokens to guide

261
00:08:25,310 --> 00:08:27,460
0,260 260,520 630,1030
the generation process.|
|

262
00:08:30,220 --> 00:08:31,730
0,245 245,485 485,860 860,1175 1175,1510
The next component is {}
下一个组件是向量量化潜在空间，

263
00:08:31,990 --> 00:08:33,740
0,320 320,590 590,910 960,1430 1430,1750
the vector quantized latent space,|
|

264
00:08:34,240 --> 00:08:35,280
0,260 260,380 380,575 575,800 800,1040
so for this, we built
为此，我们首先建立了一个 VQ GAN ，

265
00:08:35,280 --> 00:08:37,995
0,315 315,680 970,1395 1395,1670 2440,2715
first a VQ GAN,| which
|这只是一种形式的自编码器，

266
00:08:37,995 --> 00:08:41,175
0,275 325,725 745,1145 2665,2955 2955,3180
is simply a form of

267
00:08:41,175 --> 00:08:42,960
0,255 255,900 900,1185 1185,1455 1455,1785
{autoencoder -}| with some vector
|在潜在空间中内置了一些向量量化。

268
00:08:42,960 --> 00:08:45,255
0,660 660,975 975,1275 1275,1640 1840,2295
quantization built into the latent

269
00:08:45,255 --> 00:08:48,945
0,335 895,1295 3085,3330 3330,3510 3510,3690
space.| And the reason we
|我们之所以使用一组量化的 token ，

270
00:08:48,945 --> 00:08:50,535
0,165 165,405 405,905 1195,1455 1455,1590
use a quantized set of

271
00:08:50,535 --> 00:08:52,635
0,345 345,665 1495,1785 1785,1965 1965,2100
tokens about,| in most of
|在大多数模型中，我们使用 8000 个 token ， 8192 ，

272
00:08:52,635 --> 00:08:53,895
0,105 105,285 285,465 465,660 660,1260
the models we use 8000

273
00:08:53,895 --> 00:08:55,250
0,495 495,705 705,855 855,1035 1035,1355
tokens, {8192 - - -},|
|

274
00:08:55,570 --> 00:08:56,760
0,260 260,410 410,575 575,710 710,1190
is that this is amenable
这是服从于一个交叉熵分类型损失，

275
00:08:56,760 --> 00:08:58,460
0,210 210,450 450,675 675,1140 1140,1700
to a {cross-entropy -} classification

276
00:08:58,510 --> 00:08:59,775
0,320 320,640 810,1055 1055,1160 1160,1265
type loss,| so when you
|所以，当你想要预测丢失哪个 token ，

277
00:08:59,775 --> 00:09:01,470
0,135 135,315 315,605 1075,1380 1380,1695
want to predict what token

278
00:09:01,470 --> 00:09:02,900
0,180 180,500 640,930 930,1125 1125,1430
is missing,| you just say
|你只需说需要是 8192 个 token 的哪一个，

279
00:09:03,310 --> 00:09:04,395
0,335 335,590 590,770 770,920 920,1085
which of the {8192 -

280
00:09:04,395 --> 00:09:05,685
0,195 195,390 390,815 895,1155 1155,1290
- -} tokens does it

281
00:09:05,685 --> 00:09:06,570
0,150 150,270 270,465 465,690 690,885
need to be| and that's
|这就是分类损失，

282
00:09:06,570 --> 00:09:08,550
0,105 105,675 675,1070 1510,1800 1800,1980
a classification loss,| and we
|我们发现，这比回归损失要有效得多，

283
00:09:08,550 --> 00:09:09,825
0,240 240,465 465,675 675,975 975,1275
find that this works much

284
00:09:09,825 --> 00:09:12,120
0,335 745,1145 1465,1740 1740,1890 1890,2295
more effectively than a regression

285
00:09:12,120 --> 00:09:13,095
0,330 330,540 540,660 660,810 810,975
loss,| where you might try
|在那里，你可能尝试预测丢失 token 的准确像素值。

286
00:09:13,095 --> 00:09:14,700
0,135 135,330 330,665 895,1215 1215,1605
to predict the exact pixel

287
00:09:14,700 --> 00:09:16,400
0,290 430,765 765,945 945,1155 1155,1700
values of the missing token.|
|

288
00:09:17,740 --> 00:09:20,130
0,400 720,980 980,1510 1830,2135 2135,2390
{} The VQGAN has an
VQGAN 具有完全卷积结构和编解码器结构，

289
00:09:20,130 --> 00:09:22,500
0,350 400,1215 1215,1520 1570,1890 1890,2370
entirely convolutional structure and encoder

290
00:09:22,500 --> 00:09:24,500
0,375 375,650 1210,1515 1515,1710 1710,2000
decoder structure,| which we found
|我们发现比基于 transformer 的 VQGAN 性能更好，

291
00:09:24,820 --> 00:09:26,580
0,380 380,620 620,890 890,1210 1350,1760
performs better than either transformer

292
00:09:26,580 --> 00:09:30,480
0,260 1300,1970 2050,2450 3100,3600 3600,3900
based VQGANs| or hybrid approaches
|或者 transformer 和 CNN 的混合方法。

293
00:09:30,480 --> 00:09:32,360
0,300 300,620 730,1200 1200,1395 1395,1880
that mix transformers and CNN.|
|

294
00:09:34,150 --> 00:09:35,445
0,245 245,365 365,575 575,845 845,1295
We use a down sampling
我们使用 16 的下采样率，

295
00:09:35,445 --> 00:09:37,500
0,210 210,420 420,695 1375,1665 1665,2055
ratio of 16,| which generates
|这会产生潜在，

296
00:09:37,500 --> 00:09:39,270
0,530 760,1160 1360,1605 1605,1680 1680,1770
latent,| so, so when you
|所以，当你到 CNN 的编码层时，

297
00:09:39,270 --> 00:09:40,950
0,165 165,345 345,620 730,1365 1365,1680
go through the encoder layers

298
00:09:40,950 --> 00:09:42,840
0,165 165,390 690,950 1450,1710 1710,1890
of the CNN,| you get
|你会得到一个大小为 16x16 的潜在（空间），

299
00:09:42,840 --> 00:09:44,220
0,320 460,960 960,1140 1140,1245 1245,1380
a latents that are of

300
00:09:44,220 --> 00:09:46,290
0,225 225,540 540,825 825,1130 1750,2070
size 16 by 16| from
|从 256x256 的输入。

301
00:09:46,290 --> 00:09:47,430
0,210 210,480 480,765 765,945 945,1140
a {256 -} by {256

302
00:09:47,430 --> 00:09:48,760
0,315 315,710
-} input.|
|

303
00:09:48,760 --> 00:09:50,035
0,150 150,300 300,480 480,800 910,1275
And the super resolution model
而超分辨率模型使用了另一个 VQGAN ，

304
00:09:50,035 --> 00:09:52,720
0,365 445,765 765,1385 2035,2340 2340,2685
uses another VQGAN,| with latents
|潜在空间是 64x64 ，

305
00:09:52,720 --> 00:09:54,630
0,210 210,510 510,1140 1140,1335 1335,1910
of size 64 by 64,|
|

306
00:09:54,890 --> 00:09:56,400
0,335 335,545 545,710 710,965 965,1510
so those are larger latents,|
所以这些是更大的潜在空间，|

307
00:09:56,660 --> 00:09:58,740
0,400 450,710 710,970 1020,1420 1680,2080
because we want more information
因为我们想要在每个超分辨率潜在空间中获得更多的信息。

308
00:09:58,880 --> 00:10:00,505
0,305 305,485 485,635 635,910 1320,1625
in each of those super

309
00:10:00,505 --> 00:10:04,770
0,305 445,1025 1735,2135 2305,2705 3865,4265
resolution latents.| And this, the
|VQGAN 是建立在这篇论文 MaskGIT 之上的，

310
00:10:06,110 --> 00:10:07,530
0,530 530,710 710,920 920,1130 1130,1420
VQGAN is built on work

311
00:10:08,360 --> 00:10:11,605
0,400 1620,2020 2610,2915 2915,3095 3095,3245
from some, from, from this

312
00:10:11,605 --> 00:10:13,080
0,225 225,465 465,975 975,1200 1200,1475
paper called MaskGIT,| which is
|也是来自我们的团队。

313
00:10:13,130 --> 00:10:15,060
0,400 810,1100 1100,1325 1325,1660
also from our group.|
|

314
00:10:18,950 --> 00:10:20,230
0,275 275,440 440,680 680,995 995,1280
So a key component of
所以，我们训练一个关键组成部分是可变比率掩码的想法。

315
00:10:20,230 --> 00:10:23,760
0,320 1210,1880 2680,2955 2955,3180 3180,3530
our masking of our training

316
00:10:23,810 --> 00:10:25,020
0,290 290,545 545,755 755,905 905,1210
is this idea of variable

317
00:10:25,040 --> 00:10:27,310
0,400 450,1090 1530,1805 1805,2030 2030,2270
ratio masking.| So given a
|所以，给定一组 token ，

318
00:10:27,310 --> 00:10:29,050
0,165 165,360 360,860 1360,1620 1620,1740
set of tokens,| you have
|你就有了一个图像，

319
00:10:29,050 --> 00:10:30,565
0,105 105,350 850,1140 1140,1350 1350,1515
an image,| it goes through
|它经过 VQ tokenizer ，

320
00:10:30,565 --> 00:10:32,490
0,105 105,300 300,1055 1345,1635 1635,1925
the VQ tokenizer| and then
|然后你得到一个序列，比方说 196 个 token ，

321
00:10:32,660 --> 00:10:33,870
0,260 260,410 410,590 590,845 845,1210
you get a sequence of,

322
00:10:34,640 --> 00:10:36,000
0,275 275,425 425,650 650,890 890,1360
say {196 - -} tokens,|
|

323
00:10:36,470 --> 00:10:37,240
0,275 275,395 395,485 485,605 605,770
what we do is to
我们所做的是丢弃这些 token 中的一部分，

324
00:10:37,240 --> 00:10:38,680
0,225 225,465 465,770 850,1305 1305,1440
drop a variable fraction of

325
00:10:38,680 --> 00:10:40,240
0,195 195,650 730,975 975,1215 1215,1560
those tokens {},| then pass
|然后传递到网络进行学习预测。

326
00:10:40,240 --> 00:10:41,395
0,300 300,495 495,735 735,990 990,1155
into the network to learn

327
00:10:41,395 --> 00:10:42,400
0,180 180,455
to predict.|
|

328
00:10:42,570 --> 00:10:44,780
0,400 1170,1520 1520,1760 1760,1970 1970,2210
{} So unlike {LLMs -},|
所以与 LLM 不同，|

329
00:10:44,780 --> 00:10:45,935
0,210 210,495 495,750 750,900 900,1155
where usually have a fixed
它通常有固定的 token 投放比例，

330
00:10:45,935 --> 00:10:47,410
0,330 330,585 585,930 930,1200 1200,1475
ratio of tokens that's dropped,|
|

331
00:10:47,490 --> 00:10:49,745
0,335 335,670 720,1120 1560,1925 1925,2255
here we use {a -}
我们使用一个变量分布，

332
00:10:49,745 --> 00:10:51,875
0,365 685,1085 1345,1605 1605,1755 1755,2130
variable distribution,| which is biased
|它偏向于非常高的值，

333
00:10:51,875 --> 00:10:53,560
0,225 225,360 360,540 540,875 1285,1685
towards a very high {}

334
00:10:54,570 --> 00:10:56,570
0,350 350,605 605,910 930,1775 1775,2000
value of| about 64% of
|大约 64% 的 token 投放，

335
00:10:56,570 --> 00:10:58,450
0,120 120,465 465,750 750,1100 1480,1880
the tokens being dropped,| and
|我们发现，这使得该网络更容易接受编辑应用程序，

336
00:10:59,120 --> 00:11:00,200
0,255 255,450 450,660 660,855 855,1080
we find that this makes

337
00:11:00,200 --> 00:11:02,555
0,180 180,440 490,810 810,1130 1660,2355
the network much more amenable

338
00:11:02,555 --> 00:11:04,790
0,255 255,815 835,1235 1735,2025 2025,2235
to editing applications,| like in
|比如绘画和裁剪，

339
00:11:04,790 --> 00:11:06,470
0,285 285,510 510,690 690,1190 1360,1680
painting and on cropping,| and
|因为它在推理时间上是可变的，

340
00:11:06,470 --> 00:11:08,440
0,210 210,450 450,710 1300,1635 1635,1970
since it's variable at, at

341
00:11:09,380 --> 00:11:10,645
0,530 530,770 770,950 950,1070 1070,1265
inference time,| you can then
|你可以传递不同大小的掩码，

342
00:11:10,645 --> 00:11:11,800
0,225 225,405 405,780 780,930 930,1155
pass in masks of different

343
00:11:11,800 --> 00:11:13,285
0,380 520,885 885,1140 1140,1290 1290,1485
sizes,| and since it has
|因为它在训练期间这样做，

344
00:11:13,285 --> 00:11:15,010
0,240 240,450 450,735 735,1115 1375,1725
done that during training,| it's
|所以它能够在推理时间这样做。

345
00:11:15,010 --> 00:11:15,930
0,165 165,345 345,450 450,615 615,920
able to do that during

346
00:11:15,950 --> 00:11:17,400
0,530 530,850
inference time.|
|

347
00:11:20,260 --> 00:11:21,630
0,400 510,830 830,1085 1085,1190 1190,1370
Okay, so here's the base
好的，这是基本模型，

348
00:11:21,630 --> 00:11:24,915
0,320 1270,1650 1650,1905 1905,2180 2980,3285
model,| {} which has, which
|它产生 256x256 大小的图像。

349
00:11:24,915 --> 00:11:26,745
0,285 285,665 1105,1365 1365,1575 1575,1830
is producing {256x256 - -

350
00:11:26,745 --> 00:11:28,040
0,255 255,555 555,795 795,990 990,1295
- - -} size images.|
|

351
00:11:29,150 --> 00:11:30,330
0,400 450,695 695,800 800,920 920,1180
{} So this is a
这是一个基于 transformer 的模型，

352
00:11:30,980 --> 00:11:32,440
0,410 410,605 605,940 990,1295 1295,1460
transformer based model, the the

353
00:11:32,440 --> 00:11:34,165
0,260 370,920 1090,1365 1365,1545 1545,1725
base transformer,| that gets in
|获取掩码 token ，以及文本 token ，

354
00:11:34,165 --> 00:11:36,130
0,135 135,345 345,845 1225,1625 1675,1965
the mask tokens and also

355
00:11:36,130 --> 00:11:37,660
0,255 255,720 720,915 915,1125 1125,1530
the token, the text tokens,|
|

356
00:11:37,660 --> 00:11:39,010
0,180 180,390 390,645 645,915 915,1350
so these these mask tokens
这些掩码令牌是图像 token ，

357
00:11:39,010 --> 00:11:41,185
0,195 195,345 345,585 585,1130 1900,2175
are the image tokens| and
|然后是文本 token ，

358
00:11:41,185 --> 00:11:42,115
0,240 240,450 450,585 585,735 735,930
then we have the text

359
00:11:42,115 --> 00:11:44,110
0,485 1195,1440 1440,1530 1530,1710 1710,1995
tokens,| so we have cross
|所以，我们有从文本 token 到图像符号的交叉注意，

360
00:11:44,110 --> 00:11:45,460
0,345 345,615 615,750 750,945 945,1350
attention from the text tokens

361
00:11:45,460 --> 00:11:47,065
0,180 180,440 730,1080 1080,1335 1335,1605
to image| and also self
|也有图像 token 之间的自我注意，

362
00:11:47,065 --> 00:11:48,720
0,365 505,795 795,930 930,1140 1140,1655
attention between the image tokens,|
|

363
00:11:49,730 --> 00:11:51,220
0,320 320,620 620,935 935,1130 1130,1490
during training, all the tokens
在训练期间，与交叉熵损失并行地预测所有 token ，

364
00:11:51,220 --> 00:11:54,685
0,350 970,1670 2290,2610 2610,2930 3190,3465
are predicted in parallel with

365
00:11:54,685 --> 00:11:56,785
0,150 150,315 315,705 705,965 1795,2100
the {cross-entropy -} loss,| but
|但在推断期间，我们发现更好的做法是进行迭代调度，

366
00:11:56,785 --> 00:11:58,285
0,270 270,840 840,1065 1065,1275 1275,1500
during inference, we find that

367
00:11:58,285 --> 00:11:59,965
0,180 180,455 835,1235 1255,1515 1515,1680
it is better to do

368
00:11:59,965 --> 00:12:02,395
0,210 210,780 780,1085 1705,2105 2125,2430
an iterative schedule,| where we
|其中我们首先预测 token 的子集，

369
00:12:02,395 --> 00:12:04,570
0,305 505,905 1465,1920 1920,2055 2055,2175
predict a subset of the

370
00:12:04,570 --> 00:12:07,180
0,315 315,620 1270,1575 1575,1880 2350,2610
tokens first,| we choose, we
|我们选择具有最高置信度的 token ，

371
00:12:07,180 --> 00:12:08,005
0,150 150,405 405,525 525,630 630,825
choose tokens with the highest

372
00:12:08,005 --> 00:12:09,990
0,335 865,1200 1200,1440 1440,1665 1665,1985
confidence,| pass those back in
|将那些作为未屏蔽的 token 传回，

373
00:12:10,610 --> 00:12:12,370
0,320 320,860 860,1250 1250,1535 1535,1760
as unmasked tokens,| and repeat
|并且重复该过程约 12 24 次，

374
00:12:12,370 --> 00:12:14,380
0,195 195,500 1120,1500 1500,1770 1770,2010
this process ah for about

375
00:12:14,380 --> 00:12:16,840
0,1335 1335,1670 1870,2160 2160,2325 2325,2460
{12,24} steps| until all of
|直到所有 token 都被解除屏蔽。

376
00:12:16,840 --> 00:12:18,420
0,120 120,420 420,615 615,1340
the tokens are unmasked.|
|

377
00:12:18,730 --> 00:12:19,850
0,275 275,440 440,605 605,800 800,1120
We find that this significantly
我们发现，这显著提高了结果的质量。

378
00:12:20,290 --> 00:12:22,350
0,400 870,1160 1160,1445 1445,1775 1775,2060
increases the quality of the

379
00:12:22,350 --> 00:12:24,280
0,350
result.|
|

380
00:12:24,920 --> 00:12:26,360
0,400

381
00:12:26,360 --> 00:12:27,670
0,195 195,330 330,620 670,990 990,1310
And then the super resolution
然后超分辨率模型，

382
00:12:27,840 --> 00:12:31,295
0,400 1140,1460 1460,2020 3030,3305 3305,3455
model,| up samples the {256x256
|向上采样 256x256 到 512x512 的图像，

383
00:12:31,295 --> 00:12:32,210
0,195 195,435 435,630 630,765 765,915
- - - - -

384
00:12:32,210 --> 00:12:33,185
0,180 180,435 435,660 660,810 810,975
-} image to {512x512 -

385
00:12:33,185 --> 00:12:34,090
0,150 150,300 300,450 450,615 615,905
- - - - -},|
|

386
00:12:35,040 --> 00:12:36,575
0,400 480,785 785,1010 1010,1265 1265,1535
importantly, this does this in
重要的是，这是在 token 空间中完成的，

387
00:12:36,575 --> 00:12:38,555
0,450 450,785 925,1325 1435,1830 1830,1980
token space| by transforming the
|通过将隐空间从 16x16 转换到 64x64 。

388
00:12:38,555 --> 00:12:40,235
0,455 565,870 870,1140 1140,1395 1395,1680
latent from {16x16 - -}

389
00:12:40,235 --> 00:12:42,400
0,255 255,660 660,825 825,1475
to {64x64 - -}.|
|

390
00:12:42,710 --> 00:12:44,250
0,400 510,755 755,920 920,1190 1190,1540
And we use {cross-attention -}
我们使用来自文本嵌入的交叉注意以及低分辨率 token ，

391
00:12:44,360 --> 00:12:45,985
0,305 305,515 515,820 840,1340 1340,1625
from the text embedding as

392
00:12:45,985 --> 00:12:46,975
0,165 165,315 315,510 510,735 735,990
well as the {low-res -}

393
00:12:46,975 --> 00:12:49,170
0,515 1075,1410 1410,1665 1665,1890 1890,2195
tokens| and pass it into
|并将其传递到高分辨率 transformer 。

394
00:12:49,940 --> 00:12:52,960
0,275 275,470 470,790 1890,2500 2760,3020
the {high-res -} transformer.| So
|这是我们掩蔽高分辨率 token ，

395
00:12:52,960 --> 00:12:54,610
0,410 700,975 975,1230 1230,1485 1485,1650
that's we mask the {high-res

396
00:12:54,610 --> 00:12:56,520
0,195 195,650 1270,1545 1545,1665 1665,1910
-} tokens,| but the low-res
|但低分辨率 token 不被掩蔽，

397
00:12:56,600 --> 00:12:58,315
0,260 260,425 425,1150 1170,1460 1460,1715
are left unmasked,| the text
|文本嵌入不被掩蔽，

398
00:12:58,315 --> 00:13:00,120
0,345 345,495 495,705 705,1425 1425,1805
embedding is left unmasked,| and
|超分辨率模型被训练来预测被掩蔽的高 token 。

399
00:13:00,230 --> 00:13:01,690
0,275 275,550 570,920 920,1205 1205,1460
the super-res model is trained

400
00:13:01,690 --> 00:13:03,130
0,180 180,390 390,675 675,1125 1125,1440
to predict the masked higher

401
00:13:03,130 --> 00:13:04,540
0,530 790,1035 1035,1125 1125,1260 1260,1410
tokens.| So you can see
|所以你可以在这里看到效果，特别是在文本上，

402
00:13:04,540 --> 00:13:06,025
0,195 195,465 465,800 880,1245 1245,1485
the effect here, especially on

403
00:13:06,025 --> 00:13:07,120
0,165 165,455
the text,|
|

404
00:13:07,120 --> 00:13:08,700
0,270 270,585 585,980 1030,1305 1305,1580
{} the the the low-res
低分辨率输出是 256x256 ，

405
00:13:08,900 --> 00:13:10,210
0,335 335,635 635,875 875,1085 1085,1310
output of {256x256 - -

406
00:13:10,210 --> 00:13:11,140
0,165 165,315 315,525 525,765 765,930
- - - -},| it
|很难看到文本，

407
00:13:11,140 --> 00:13:11,995
0,150 150,345 345,525 525,690 690,855
is hard to see the

408
00:13:11,995 --> 00:13:14,185
0,275 745,990 990,1125 1125,1415 1765,2190
text| and then once it's
|然后一旦在 token 空间中[更正]，

409
00:13:14,185 --> 00:13:17,200
0,405 405,600 600,975 975,1325 2755,3015
corrected in token space,| you
|你就可以阅读文本，

410
00:13:17,200 --> 00:13:18,400
0,260 520,780 780,900 900,1050 1050,1200
can you can read the

411
00:13:18,400 --> 00:13:20,125
0,240 240,495 495,870 870,1190 1450,1725
text,| the facial features of
|仓鼠的面部特征要清晰得多，

412
00:13:20,125 --> 00:13:22,120
0,135 135,660 660,1025 1495,1800 1800,1995
the hamster are much more

413
00:13:22,120 --> 00:13:23,260
0,290
clear,|
|

414
00:13:23,770 --> 00:13:27,930
0,395 395,790 1230,1630 3120,3965 3965,4160
many details are reconstructed in
许多细节都在 token 空间中重构。

415
00:13:27,930 --> 00:13:29,720
0,345 345,650
token space.|
|

416
00:13:29,940 --> 00:13:31,280
0,320 320,530 530,710 710,1000 1050,1340
{} What we found is
我们发现的是，

417
00:13:31,280 --> 00:13:32,600
0,285 285,585 585,990 990,1110 1110,1320
that,| a cascade of models
|一连串的模式其实是非常重要的，

418
00:13:32,600 --> 00:13:34,085
0,285 285,525 525,795 795,1160 1210,1485
is actually very important,| if
|如果你直接尝试训练一个 512x512 的模型，

419
00:13:34,085 --> 00:13:35,345
0,195 195,465 465,705 705,930 930,1260
you directly try to train

420
00:13:35,345 --> 00:13:36,320
0,300 300,495 495,675 675,825 825,975
a {512 - - -

421
00:13:36,320 --> 00:13:37,840
0,180 180,360 360,525 525,800
- - -} model,|
|

422
00:13:37,840 --> 00:13:39,145
0,195 195,420 420,675 675,950 1060,1305
{} it tends to, the
模型倾向于过于关注细节，

423
00:13:39,145 --> 00:13:40,240
0,180 180,465 465,585 585,825 825,1095
model tends to focus too

424
00:13:40,240 --> 00:13:42,790
0,270 270,525 525,750 750,1100 2230,2550
much on the details,| whereas
|而如果你首先训练一个低分辨率的模型，

425
00:13:42,790 --> 00:13:44,050
0,210 210,465 465,780 780,1050 1050,1260
if you first train a

426
00:13:44,050 --> 00:13:45,505
0,180 180,375 375,680 1060,1320 1320,1455
{low-res -} model,| you get
|你会得到场景的整体语义或布局，

427
00:13:45,505 --> 00:13:47,245
0,105 105,335 355,1085 1105,1470 1470,1740
the overall semantics or the

428
00:13:47,245 --> 00:13:49,170
0,405 405,645 645,825 825,1115 1525,1925
layout of the scene, right,|
|

429
00:13:49,190 --> 00:13:50,320
0,350 350,590 590,740 740,875 875,1130
somewhat like what an artist
有点像一个艺术家可能会做的事情，

430
00:13:50,320 --> 00:13:52,165
0,300 300,620 850,1125 1125,1400 1570,1845
might do,| where you you
|你首先把场景弄对，

431
00:13:52,165 --> 00:13:53,260
0,150 150,315 315,525 525,795 795,1095
get the scene right first|
|

432
00:13:53,260 --> 00:13:54,190
0,210 210,375 375,585 585,825 825,930
and then start filling in
然后开始填充细节。

433
00:13:54,190 --> 00:13:56,800
0,225 225,590 1300,1700 2230,2490 2490,2610
the details.| So one of
|那么我们研究的事情之一就是如何，

434
00:13:56,800 --> 00:13:57,490
0,120 120,300 300,450 450,525 525,690
the things we are looking

435
00:13:57,490 --> 00:13:59,130
0,180 180,330 330,525 525,830 1240,1640
at is how to, {}|
|

436
00:13:59,420 --> 00:14:00,655
0,400 450,740 740,890 890,1055 1055,1235
maybe increase the depth of
可能增加这个[瀑布]的深度，

437
00:14:00,655 --> 00:14:01,900
0,180 180,660 660,900 900,1080 1080,1245
this cascade,| go from {128
|从 128 到 256 到 512 ，

438
00:14:01,900 --> 00:14:03,025
0,210 210,435 435,615 615,855 855,1125
- -} to {256 -}

439
00:14:03,025 --> 00:14:04,260
0,165 165,300 300,450 450,725
to {512 - -},|
|

440
00:14:04,260 --> 00:14:05,670
0,165 165,270 270,530 550,1080 1080,1410
see if that improves further
看看这是否会进一步提高质量。

441
00:14:05,670 --> 00:14:06,900
0,225 225,500
the quality.|
|

442
00:14:10,100 --> 00:14:11,770
0,380 380,635 635,910 1230,1490 1490,1670
Um, so here you can
嗯，所以你可以在这里看到。所以，这是我们基于令牌的超分辨率与基于扩散的像素超分辨率的比较，所以。

443
00:14:11,770 --> 00:14:14,440
0,320 1840,2115 2115,2280 2280,2505 2505,2670
see. So, so here's a

444
00:14:14,440 --> 00:14:16,165
0,320 370,765 765,1110 1110,1500 1500,1725
comparison of our token based

445
00:14:16,165 --> 00:14:18,670
0,255 255,575 1495,2040 2040,2325 2325,2505
super resolution comparing to a

446
00:14:18,670 --> 00:14:20,635
0,470 520,920 1060,1545 1545,1725 1725,1965
diffusion based pixel based super

447
00:14:20,635 --> 00:14:23,380
0,335 1165,1565
resolution, so.|
|

448
00:14:23,380 --> 00:14:25,110
0,240 240,390 390,495 495,740 1330,1730
Here on the left, the,
在左边，文本提示符是一只正在拉小提琴的兔子，这是两个，这是两个样本，来自2，5，6乘以2，5，6分辨率的模型。右上角是基于令牌的超分辨率输出的结果。所以你可以看到音符，小提琴上的细节，所有这些都要清晰得多。

449
00:14:25,280 --> 00:14:26,305
0,245 245,395 395,620 620,830 830,1025
the text prompt here is

450
00:14:26,305 --> 00:14:27,690
0,180 180,455 475,780 780,945 945,1385
a rabbit playing the violin

451
00:14:28,190 --> 00:14:29,455
0,380 380,650 650,815 815,980 980,1265
and this is the two,

452
00:14:29,455 --> 00:14:30,625
0,255 255,405 405,600 600,1005 1005,1170
these are two samples from

453
00:14:30,625 --> 00:14:33,970
0,135 135,395 1975,2375 2905,3180 3180,3345
the model of two, five,

454
00:14:33,970 --> 00:14:34,885
0,195 195,390 390,555 555,720 720,915
six by two, five, six

455
00:14:34,885 --> 00:14:37,255
0,305 985,1385 1765,2040 2040,2175 2175,2370
resolution. And on the top

456
00:14:37,255 --> 00:14:38,440
0,270 270,495 495,690 690,930 930,1185
right is the result of

457
00:14:38,440 --> 00:14:40,470
0,320 760,1200 1200,1425 1425,1695 1695,2030
our token based super resolution

458
00:14:40,700 --> 00:14:41,995
0,400 690,920 920,1010 1010,1145 1145,1295
output. So you can see

459
00:14:41,995 --> 00:14:43,885
0,135 135,255 255,515 565,965 1555,1890
that the musical notes, the

460
00:14:43,885 --> 00:14:45,430
0,255 255,435 435,540 540,995 1285,1545
details on the violin, all

461
00:14:45,430 --> 00:14:46,270
0,120 120,270 270,420 420,600 600,840
of these are much more

462
00:14:46,270 --> 00:14:47,220
0,320
clear.|
|

463
00:14:47,220 --> 00:14:47,925
0,135 135,240 240,360 360,510 510,705
But if we just took
但如果我们只是用这个来做一个基于扩散的超分辨率，这只是像素幻觉，看起来就不是那么好了。

464
00:14:47,925 --> 00:14:50,175
0,305 895,1230 1230,1485 1485,1740 1740,2250
this and did a diffusion

465
00:14:50,175 --> 00:14:51,645
0,330 330,570 570,875 1045,1320 1320,1470
based super resolution, which is

466
00:14:51,645 --> 00:14:54,945
0,210 210,755 1285,2045 2725,3030 3030,3300
just pixel hallucination, that doesn't

467
00:14:54,945 --> 00:14:56,060
0,165 165,345 345,635
look so good.|
|

468
00:14:56,130 --> 00:14:59,320
0,400 1470,1870 2130,2645 2645,2885 2885,3190
So our token super resolution
因此，我们的令牌超分辨率与基于令牌的方法配合得很好。

469
00:14:59,610 --> 00:15:01,055
0,365 365,665 665,860 860,1070 1070,1445
plays well with our token

470
00:15:01,055 --> 00:15:02,480
0,270 270,665
based approach.|
|

471
00:15:08,870 --> 00:15:11,440
0,335 335,635 635,1000 2070,2330 2330,2570
So another important kind of
因此，另一种重要的启发式或技巧是所谓的分类器自由指导，我们发现这在推理时至关重要，以权衡多样性和质量。

472
00:15:11,440 --> 00:15:13,885
0,660 660,840 840,1005 1005,1310 2140,2445
heuristic or a trick is

473
00:15:13,885 --> 00:15:15,570
0,270 270,555 555,1140 1140,1395 1395,1685
something called classifier free guidance,

474
00:15:16,400 --> 00:15:17,545
0,290 290,455 455,650 650,875 875,1145
which we found is crucial

475
00:15:17,545 --> 00:15:19,795
0,270 270,705 705,960 960,1295 1975,2250
at inference time to trade

476
00:15:19,795 --> 00:15:22,720
0,180 180,485 655,990 990,1325
off diversity and quality.|
|

477
00:15:23,980 --> 00:15:27,465
0,335 335,670 2700,3005 3005,3215 3215,3485
What this this trick does
这个技巧所做的是简单地权衡，抱歉，将日志从无条件生成推向文本条件生成，这样您就可以将比例视为推离无条件生成的量。下面是一个对比。左侧的图像，左侧的两个图像是由生成的，制导比例尺为0.5。所以猫的姿势和背景有更多的多样性，但并不是因为有更多的错误，例如在猫的嘴巴上。

478
00:15:27,465 --> 00:15:29,750
0,255 255,420 420,695 1585,1935 1935,2285
is to simply trade off,

479
00:15:30,040 --> 00:15:32,445
0,400 1050,1445 1445,1685 1685,1930 2070,2405
sorry, push the log away

480
00:15:32,445 --> 00:15:34,410
0,225 225,930 930,1205 1315,1665 1665,1965
from unconditional generation and towards

481
00:15:34,410 --> 00:15:36,630
0,345 345,840 840,1130 1870,2115 2115,2220
text conditional generation so you

482
00:15:36,630 --> 00:15:37,605
0,150 150,315 315,465 465,675 675,975
can think of the scale

483
00:15:37,605 --> 00:15:39,480
0,365 835,1170 1170,1455 1455,1695 1695,1875
as the amount by which

484
00:15:39,480 --> 00:15:40,470
0,105 105,195 195,465 465,780 780,990
you are pushing away from

485
00:15:40,470 --> 00:15:43,290
0,690 690,980 1660,2060 2320,2670 2670,2820
unconditional generation. And here's a

486
00:15:43,290 --> 00:15:44,940
0,320 850,1110 1110,1305 1305,1515 1515,1650
comparison. The image on the

487
00:15:44,940 --> 00:15:46,335
0,260 550,825 825,945 945,1155 1155,1395
left, the two images on

488
00:15:46,335 --> 00:15:47,990
0,120 120,365 535,825 825,1115 1255,1655
the left were generated by

489
00:15:48,640 --> 00:15:49,830
0,260 260,380 380,640 660,980 980,1190
with the guidance scale of

490
00:15:49,830 --> 00:15:51,950
0,800 1180,1440 1440,1665 1665,1830 1830,2120
0.5. So there's more diversity

491
00:15:52,090 --> 00:15:53,280
0,275 275,440 440,860 860,1040 1040,1190
in the poses of the

492
00:15:53,280 --> 00:15:55,665
0,260 580,840 840,960 960,1610 2050,2385
cat and the backgrounds, but

493
00:15:55,665 --> 00:15:57,165
0,360 360,645 645,795 795,1085 1135,1500
it's, it's not as there

494
00:15:57,165 --> 00:15:58,965
0,300 300,555 555,1085 1255,1530 1530,1800
are more errors, for example

495
00:15:58,965 --> 00:16:00,240
0,395 505,765 765,885 885,1080 1080,1275
here on the mouth of

496
00:16:00,240 --> 00:16:01,000
0,120 120,380
the cat.|
|

497
00:16:01,160 --> 00:16:03,160
0,365 365,730 1380,1655 1655,1775 1775,2000
Compared to the two images
与右侧的两幅图像相比，这两幅图像的制导比例要大得多。

498
00:16:03,160 --> 00:16:04,405
0,240 240,375 375,650 670,960 960,1245
on the right, which are

499
00:16:04,405 --> 00:16:06,840
0,395 415,690 690,930 930,1295 2035,2435
generated with a much larger

500
00:16:07,190 --> 00:16:08,600
0,400 420,820
guidance scale.|
|

501
00:16:09,110 --> 00:16:11,520
0,260 260,485 485,850 1860,2135 2135,2410
So during training we just
所以在训练期间，我们只是以10%的概率掉下了一些代币。很抱歉，我们以10%的概率放弃了条件反射，但在推断时，我们选择了一个特定的指导标度。

502
00:16:11,870 --> 00:16:13,285
0,335 335,575 575,1040 1040,1280 1280,1415
dropped some tokens with a

503
00:16:13,285 --> 00:16:16,090
0,420 420,645 645,2280 2280,2520 2520,2805
probability of 10%. {I'm,sorry,} we

504
00:16:16,090 --> 00:16:18,070
0,270 270,950 1060,1350 1350,1530 1530,1980
drop conditioning with a probability

505
00:16:18,070 --> 00:16:20,005
0,195 195,680 1450,1710 1710,1815 1815,1935
of 10% but then at

506
00:16:20,005 --> 00:16:21,130
0,390 390,585 585,765 765,930 930,1125
inference time we choose a

507
00:16:21,130 --> 00:16:22,780
0,270 270,620 670,1070
particular guidance scale.|
|

508
00:16:25,970 --> 00:16:28,630
0,400 900,1250 1250,1600 2010,2375 2375,2660
Um, another trick, so many
嗯，另一个技巧，这里有很多让图像看起来很好的技巧是一种叫做负面提示的东西。

509
00:16:28,630 --> 00:16:29,875
0,300 300,480 480,660 660,920 1000,1245
tricks here to make the

510
00:16:29,875 --> 00:16:30,985
0,225 225,480 480,675 675,870 870,1110
images look good is something

511
00:16:30,985 --> 00:16:32,860
0,285 285,605 625,1295
called negative prompting.|
|

512
00:16:33,170 --> 00:16:34,390
0,245 245,440 440,710 710,980 980,1220
So the idea here is
所以这里的想法是，有一些概念是我们不能在文本提示本身中说出来的。例如，如果您想要生成一个图像，但其中没有树，那么在文本提示符中这样说就有点麻烦了。

513
00:16:34,390 --> 00:16:36,100
0,290 640,870 870,990 990,1500 1500,1710
that there are concepts which

514
00:16:36,100 --> 00:16:38,500
0,315 315,710 1180,1580 1930,2220 2220,2400
one cannot say in the

515
00:16:38,500 --> 00:16:39,970
0,255 255,620 640,1005 1005,1260 1260,1470
text prompt itself. For example,

516
00:16:39,970 --> 00:16:40,705
0,195 195,315 315,420 420,570 570,735
if you would like to

517
00:16:40,705 --> 00:16:43,105
0,275 745,1035 1035,1325 1825,2145 2145,2400
generate an image but not

518
00:16:43,105 --> 00:16:44,740
0,270 270,510 510,660 660,905 1285,1635
have trees in it, it's

519
00:16:44,740 --> 00:16:46,150
0,195 195,720 720,930 930,1155 1155,1410
somewhat cumbersome to say that

520
00:16:46,150 --> 00:16:47,620
0,225 225,375 375,570 570,890
in the text prompt.|
|

521
00:16:47,660 --> 00:16:50,515
0,400 480,880 1080,1480 1950,2585 2585,2855
So, so the classifier free
所以，所以分类器免费指南允许我们说生成这个场景，但其中没有树。我们通过推开负面提示来做到这一点。这里有一个例子。短信提示说萨尔瓦多·达利的赛伯朋克森林。所以这就产生了。

522
00:16:50,515 --> 00:16:51,870
0,305 415,720 720,915 915,1080 1080,1355
guidance allows us to say

523
00:16:52,160 --> 00:16:54,055
0,400 510,845 845,1180 1260,1580 1580,1895
generate this scene but don't

524
00:16:54,055 --> 00:16:55,150
0,225 225,480 480,645 645,855 855,1095
have trees in it. And

525
00:16:55,150 --> 00:16:56,125
0,135 135,255 255,435 435,660 660,975
we do that by pushing

526
00:16:56,125 --> 00:16:57,960
0,395 775,1050 1050,1185 1185,1440 1440,1835
away from the negative prompt.

527
00:16:58,370 --> 00:17:00,535
0,260 260,470 470,620 620,940 1860,2165
So here's an example. It

528
00:17:00,535 --> 00:17:01,810
0,305 385,645 645,825 825,1050 1050,1275
text the text prompt says

529
00:17:01,810 --> 00:17:03,790
0,360 360,650 880,1245 1245,1515 1515,1980
cyber punk forest by Salvador

530
00:17:03,790 --> 00:17:06,120
0,440 670,915 915,1050 1050,1340
dali. So that generated.|
|

531
00:17:06,220 --> 00:17:08,520
0,275 275,425 425,620 620,940 2010,2300
These are two examples that
这是使用该提示生成的两个示例，然后我添加了一个负面提示，我说我不想要树，我不想要绿色和模糊的文本提示，然后它会生成其他类型的样式，这些样式似乎尊重负面提示。

532
00:17:08,520 --> 00:17:10,010
0,150 150,410 550,885 885,1155 1155,1490
were generated using that prompt,

533
00:17:10,540 --> 00:17:11,775
0,245 245,365 365,590 590,940 960,1235
and then I added a

534
00:17:11,775 --> 00:17:13,110
0,275 295,660 660,960 960,1185 1185,1335
negative prompt where I said

535
00:17:13,110 --> 00:17:14,160
0,120 120,270 270,435 435,770 790,1050
I don't want trees, I

536
00:17:14,160 --> 00:17:15,650
0,150 150,330 330,675 675,990 990,1490
don't want green and blurry

537
00:17:15,880 --> 00:17:17,205
0,350 350,575 575,800 800,1115 1115,1325
that the text prompt that

538
00:17:17,205 --> 00:17:18,750
0,195 195,545 1045,1305 1305,1410 1410,1545
I provided and then it

539
00:17:18,750 --> 00:17:20,145
0,360 360,600 600,885 885,1170 1170,1395
generates these other kinds of

540
00:17:20,145 --> 00:17:22,410
0,335 1255,1545 1545,1740 1740,1995 1995,2265
styles which seem to respect

541
00:17:22,410 --> 00:17:23,680
0,180 180,435 435,830
the negative prompt.|
|

542
00:17:24,830 --> 00:17:26,005
0,245 245,490 630,905 905,1025 1025,1175
So that is a very
所以这是一个非常有用的把戏。

543
00:17:26,005 --> 00:17:28,380
0,305 445,845
useful trick.|
|

544
00:17:28,960 --> 00:17:31,125
0,380 380,910 1170,1570 1590,1955 1955,2165
For generating images closer to
用来生成更接近你脑海中所思所想的图像。

545
00:17:31,125 --> 00:17:32,115
0,120 120,330 330,570 570,750 750,990
the things you're thinking of

546
00:17:32,115 --> 00:17:33,300
0,150 150,255 255,515
in your head.|
|

547
00:17:38,430 --> 00:17:40,145
0,275 275,550 810,1210 1350,1595 1595,1715
This is, yeah. So this
这是，是的。这就是我之前在推理时提到的迭代解码。而且你可以看到，多步骤解码令牌对于高质量的生成非常有帮助。因此，这里有一系列的去屏蔽步骤，它们生成高达18个步骤的令牌，并且输出质量有了稳步的提高。

548
00:17:40,145 --> 00:17:41,585
0,165 165,330 330,855 855,1305 1305,1440
is the iterative decoding I

549
00:17:41,585 --> 00:17:43,270
0,240 240,605 655,945 945,1365 1365,1685
mentioned earlier at inference time.

550
00:17:43,830 --> 00:17:45,070
0,320 320,515 515,710 710,935 935,1240
And you can see that

551
00:17:45,750 --> 00:17:48,850
0,730 750,1330 2010,2285 2285,2560 2700,3100
decoding tokens in multiple steps

552
00:17:49,950 --> 00:17:51,440
0,290 290,485 485,790 840,1205 1205,1490
is very helpful for high

553
00:17:51,440 --> 00:17:53,705
0,300 300,680 1390,1710 1710,2085 2085,2265
quality generation. So here's a

554
00:17:53,705 --> 00:17:56,615
0,305 325,725 1435,2250 2250,2585 2635,2910
sequence of unmasking steps which

555
00:17:56,615 --> 00:18:00,080
0,150 150,540 540,1085 3055,3330 3330,3465
are generating tokens up to

556
00:18:00,080 --> 00:18:03,310
0,260 280,680 820,1220 1780,2300 2830,3230
eighteen steps and there's steady

557
00:18:03,330 --> 00:18:04,520
0,365 365,590 590,725 725,965 965,1190
improvement in the quality of

558
00:18:04,520 --> 00:18:05,420
0,195 195,530
the output.|
|

559
00:18:05,420 --> 00:18:07,250
0,315 315,585 585,855 855,1185 1185,1830
Our base model uses 24
我们的基本模式使用24个步骤，超分辨率模式使用8个步骤，但这些步骤的数量明显低于扩散模型所需的50或1000个步骤。这实际上是我们方法速度最重要的原因之一。所以我们需要的向前道具数量减少了十分之一，我们需要通过模型。

560
00:18:07,250 --> 00:18:09,010
0,320 700,1020 1020,1230 1230,1440 1440,1760
steps and the super resolution

561
00:18:09,150 --> 00:18:12,335
0,350 350,700 720,1070 1070,1420 2910,3185
model uses eight steps, but

562
00:18:12,335 --> 00:18:13,400
0,255 255,540 540,720 720,885 885,1065
these number of steps are

563
00:18:13,400 --> 00:18:15,430
0,290 610,1010 1300,1590 1590,1755 1755,2030
significantly lower than the fifty

564
00:18:15,450 --> 00:18:16,790
0,305 305,605 605,935 935,1130 1130,1340
or thousand steps that are

565
00:18:16,790 --> 00:18:18,830
0,285 285,510 510,915 915,1280 1750,2040
required by diffusion models. And

566
00:18:18,830 --> 00:18:19,865
0,150 150,345 345,675 675,915 915,1035
this is actually one of

567
00:18:19,865 --> 00:18:21,245
0,135 135,395 445,795 795,1140 1140,1380
the most important reasons for

568
00:18:21,245 --> 00:18:22,570
0,165 165,435 435,690 690,960 960,1325
the speed of our approach.

569
00:18:23,220 --> 00:18:24,110
0,260 260,380 380,560 560,740 740,890
So we get like a

570
00:18:24,110 --> 00:18:25,865
0,150 150,410 910,1275 1275,1545 1545,1755
ten X lower number of

571
00:18:25,865 --> 00:18:26,990
0,255 255,675 675,855 855,990 990,1125
forward props that we need

572
00:18:26,990 --> 00:18:28,330
0,240 240,590 700,975 975,1095 1095,1340
to push through the model.|
|

573
00:18:29,530 --> 00:18:31,440
0,380 380,760 930,1220 1220,1460 1460,1910
Ah, however these days there's
啊，然而，现在有一些渐进蒸馏的想法，它可能会减少扩散模型的步骤，我们希望利用它来进一步减少我们的。

574
00:18:31,440 --> 00:18:33,980
0,350 820,1170 1170,1425 1425,1730 1870,2540
these ideas of progressive distillation

575
00:18:34,900 --> 00:18:36,615
0,275 275,470 470,790 1260,1565 1565,1715
which can potentially reduce the

576
00:18:36,615 --> 00:18:38,430
0,120 120,300 300,570 570,935 1285,1815
number of steps for diffusion

577
00:18:38,430 --> 00:18:40,590
0,380 580,980 1450,1725 1725,1905 1905,2160
models, and we hope to

578
00:18:40,590 --> 00:18:42,435
0,300 300,560 940,1215 1215,1485 1485,1845
exploit that to further reduce

579
00:18:42,435 --> 00:18:44,760
0,365
our.|
|

580
00:18:44,830 --> 00:18:46,400
0,485 485,790
Sampling steps.|
采样步骤。|

581
00:18:48,830 --> 00:18:50,040
0,380 380,650 650,800 800,935 935,1210
Okay, so we did some,
好的，我们做了一些，所以现在来评估一下，这是对整个模型的快速浏览。因此，我们做了一些定性的EV VAL，我们从党报中提取了1,650条提示，从我们的模型和稳定扩散模型生成了图像，并将其发送给评分员来回答问题，这两张图像中，哪一张来自我们的模型，一张来自稳定扩散，哪一张更符合提示？

582
00:18:50,840 --> 00:18:51,805
0,245 245,365 365,530 530,665 665,965
so now on to evals

583
00:18:51,805 --> 00:18:53,575
0,255 255,450 450,600 600,875 1375,1770
that that was a quick

584
00:18:53,575 --> 00:18:56,100
0,360 360,615 615,840 840,1175 2125,2525
tour of the entire model.

585
00:18:57,470 --> 00:18:58,840
0,350 350,560 560,695 695,860 860,1370
So we did some qualitative

586
00:18:58,840 --> 00:19:00,610
0,135 135,390 390,660 660,950 1510,1770
ev val where we took

587
00:19:00,610 --> 00:19:02,770
0,165 165,390 390,710 1360,1760 1840,2160
a set of sixteen hundred

588
00:19:02,770 --> 00:19:04,720
0,210 210,495 495,1100 1510,1800 1800,1950
and fifty prompts from the

589
00:19:04,720 --> 00:19:07,110
0,240 240,620 730,1130 1570,1970 1990,2390
party paper and generated images

590
00:19:07,550 --> 00:19:09,130
0,275 275,485 485,820 1080,1385 1385,1580
from our model and the

591
00:19:09,130 --> 00:19:10,510
0,210 210,630 630,930 930,1185 1185,1380
stable diffusion model and sent

592
00:19:10,510 --> 00:19:13,810
0,165 165,440 1000,1610 1690,2090 2950,3300
it to raters to answer

593
00:19:13,810 --> 00:19:15,400
0,210 210,470 1030,1305 1305,1455 1455,1590
the question, which of the

594
00:19:15,400 --> 00:19:17,350
0,135 135,410 1270,1605 1605,1785 1785,1950
two images, one from our

595
00:19:17,350 --> 00:19:18,760
0,285 285,570 570,780 780,975 975,1410
model, one from stable diffusion,

596
00:19:18,760 --> 00:19:19,960
0,270 270,420 420,585 585,890 910,1200
which of them matches the

597
00:19:19,960 --> 00:19:21,100
0,255 255,620
prompt better?|
|

598
00:19:21,520 --> 00:19:23,055
0,305 305,455 455,880 960,1280 1280,1535
So the raters preferred our
因此，评分者在70%的时间里喜欢我们的模型，相比之下，25%的时间是稳定扩散的，剩下的几个百分点他们不感兴趣。所以我们把它们从这块地块上移走了。

599
00:19:23,055 --> 00:19:24,920
0,335 595,1275 1275,1485 1485,1605 1605,1865
model 70% of the time,

600
00:19:25,960 --> 00:19:27,990
0,320 320,590 590,1715 1715,1910 1910,2030
compared to 25% of the

601
00:19:27,990 --> 00:19:29,720
0,195 195,420 420,615 615,1070 1330,1730
time for stable diffusion, and

602
00:19:30,310 --> 00:19:32,840
0,350 350,605 605,910 1770,2150 2150,2530
for the remaining few percent

603
00:19:32,920 --> 00:19:34,640
0,380 380,650 650,1250 1250,1460 1460,1720
they were indifferent. So we

604
00:19:34,720 --> 00:19:36,290
0,290 290,545 545,910 930,1250 1250,1570
removed those from this plot.|
|

605
00:19:37,750 --> 00:19:38,840
0,400
Um.|
恩。|

606
00:19:39,020 --> 00:19:40,420
0,320 320,500 500,650 650,830 830,1400
We also did some qualitative
我们还对模型的各种性质做了一些定性的评价。它在多大程度上尊重基数之类的东西？

607
00:19:40,420 --> 00:19:42,535
0,320 340,645 645,950 1030,1430 1840,2115
val on various properties of

608
00:19:42,535 --> 00:19:43,495
0,135 135,345 345,585 585,780 780,960
the model. How well does

609
00:19:43,495 --> 00:19:45,420
0,255 255,570 570,855 855,1110 1110,1925
it respect things like cardinality?|
|

610
00:19:45,960 --> 00:19:47,840
0,290 400,690 690,980 1000,1515 1515,1880
So, so here it's about,
所以，所以这里是关于，你知道的，计数。所以我们有三头大象站在一起。我们有三只大象，四个酒瓶，一，二，三，四，三个黄色网球前的一个小足球，所以。

611
00:19:47,980 --> 00:19:49,530
0,245 245,380 380,880 1140,1415 1415,1550
you know, counting. So we

612
00:19:49,530 --> 00:19:51,030
0,195 195,405 405,885 885,1230 1230,1500
have three elephants standing on

613
00:19:51,030 --> 00:19:52,455
0,165 165,300 300,450 450,740 1180,1425
top of each other. We

614
00:19:52,455 --> 00:19:55,290
0,150 150,300 300,755 2215,2550 2550,2835
get three elephants, four wine

615
00:19:55,290 --> 00:19:57,200
0,530 730,1035 1035,1275 1275,1560 1560,1910
bottles, one, two, three, four,

616
00:19:57,940 --> 00:19:59,520
0,290 290,580 720,1115 1115,1400 1400,1580
a tiny football in front

617
00:19:59,520 --> 00:20:01,250
0,225 225,560 700,1065 1065,1380 1380,1730
of three yellow tennis balls,

618
00:20:01,510 --> 00:20:02,480
0,400
so.|
|

619
00:20:02,490 --> 00:20:03,770
0,335 335,620 620,920 920,1130 1130,1280
Seems to respect all of
似乎尊重所有这些。然而，当计数超过6或7个时，该模型往往会开始出错。

620
00:20:03,770 --> 00:20:07,025
0,290 1240,1605 1605,1970 2770,3045 3045,3255
those. However, when the count

621
00:20:07,025 --> 00:20:08,735
0,225 225,515 805,1200 1200,1500 1500,1710
goes up beyond six or

622
00:20:08,735 --> 00:20:10,445
0,305 745,990 990,1155 1155,1470 1470,1710
seven, the model tends to

623
00:20:10,445 --> 00:20:12,540
0,345 345,725 775,1175
start making mistakes.|
|

624
00:20:13,860 --> 00:20:15,305
0,400 420,800 800,1100 1100,1280 1280,1445
Ah, here we are looking
啊，这里我们在看不同的风格，所以这里有一幅伦勃朗风格的衣着考究的浣熊油画的肖像。

625
00:20:15,305 --> 00:20:17,435
0,165 165,420 420,845 1195,1595 1645,2130
at different styles, so here's

626
00:20:17,435 --> 00:20:18,575
0,270 270,690 690,810 810,960 960,1140
a portrait of a well

627
00:20:18,575 --> 00:20:20,045
0,195 195,690 690,1020 1020,1290 1290,1470
dressed raccoon oil painting in

628
00:20:20,045 --> 00:20:21,500
0,135 135,285 285,420 420,1025
the style of rembrandt.|
|

629
00:20:22,390 --> 00:20:24,900
0,350 350,695 695,1090 1710,2110 2130,2510
Pop art style and a
波普艺术风格和中国水墨画风格。

630
00:20:24,900 --> 00:20:26,445
0,380 400,780 780,945 945,1200 1200,1545
Chinese ink and wash painting

631
00:20:26,445 --> 00:20:27,580
0,395
style.|
|

632
00:20:30,320 --> 00:20:32,370
0,365 365,650 650,970 1140,1540 1650,2050
Ah, other ev are about
啊，其他的电动汽车是关于构图，几何构图，三个小黄盒在一个大蓝盒上，和。

633
00:20:32,480 --> 00:20:35,050
0,380 380,1060 1110,1510 1950,2300 2300,2570
composition, geometric composition, three small

634
00:20:35,050 --> 00:20:37,390
0,285 285,650 940,1340 1780,2085 2085,2340
yellow boxes on a large

635
00:20:37,390 --> 00:20:40,400
0,240 240,530 1810,2210
blue box, and.|
|

636
00:20:40,650 --> 00:20:41,705
0,245 245,425 425,725 725,950 950,1055
A large present with a
圣诞树等左边系有红丝带的大礼物。

637
00:20:41,705 --> 00:20:42,620
0,150 150,510 510,660 660,765 765,915
red ribbon to the left

638
00:20:42,620 --> 00:20:44,620
0,165 165,315 315,590 640,1040 1420,2000
of a Christmas tree, etc.|
|

639
00:20:46,130 --> 00:20:47,545
0,275 275,470 470,695 695,1000 1080,1415
We find that it if
我们发现，如果你想要渲染文本，那么它就做得很好。如果一个词或两个词的文本中没有太多的词，则模型能够很好地呈现它们。

640
00:20:47,545 --> 00:20:49,740
0,255 255,575 1465,1755 1755,1920 1920,2195
you have want to render

641
00:20:49,760 --> 00:20:51,265
0,400 930,1175 1175,1265 1265,1385 1385,1505
text, then it does a

642
00:20:51,265 --> 00:20:52,540
0,245 295,645 645,930 930,1125 1125,1275
reasonable job. If there is

643
00:20:52,540 --> 00:20:54,265
0,225 225,420 420,615 615,950 1450,1725
not too many words in

644
00:20:54,265 --> 00:20:55,210
0,135 135,330 330,525 525,705 705,945
the text of one word

645
00:20:55,210 --> 00:20:57,190
0,210 210,360 360,620 1570,1815 1815,1980
or two words, the model

646
00:20:57,190 --> 00:20:58,210
0,195 195,405 405,585 585,780 780,1020
is able to render them

647
00:20:58,210 --> 00:20:59,160
0,290
well.|
|

648
00:20:59,840 --> 00:21:00,700
0,260 260,350 350,515 515,695 695,860
And we are also able
我们还可以提供非常长的、详细的提示。

649
00:21:00,700 --> 00:21:04,890
0,270 270,620 1720,2120 2560,2960 3790,4190
to provide very long, detailed

650
00:21:06,230 --> 00:21:07,540
0,670
prompts.|
|

651
00:21:07,700 --> 00:21:09,490
0,400 690,1085 1085,1250 1250,1520 1520,1790
So here's an example, an
这里有一个例子，一个展示莫奈画作的美术馆。美术馆被淹了。机器人正在使用划桨板在美术馆里转悠。因此，这种力量很大程度上来自语言模型本身，它非常非常强大，能够在嵌入空间中表示这些概念中的每一个。我们所能做的就是将其映射到像素。

652
00:21:09,490 --> 00:21:11,640
0,240 240,770 910,1335 1335,1650 1650,2150
art gallery displaying monet paintings.

653
00:21:11,660 --> 00:21:13,110
0,260 260,425 425,770 770,965 965,1450
The art gallery is flooded.

654
00:21:13,310 --> 00:21:14,605
0,515 515,725 725,935 935,1160 1160,1295
Robots are going around the

655
00:21:14,605 --> 00:21:16,260
0,150 150,555 555,810 810,1200 1200,1655
art gallery using paddle boards.

656
00:21:16,310 --> 00:21:18,460
0,400 1440,1700 1700,1835 1835,1970 1970,2150
So a lot of this

657
00:21:18,460 --> 00:21:19,930
0,320 340,735 735,1050 1050,1230 1230,1470
power comes from the language

658
00:21:19,930 --> 00:21:21,925
0,375 375,770 1000,1305 1305,1610 1630,1995
model itself, which is really,

659
00:21:21,925 --> 00:21:24,180
0,285 285,605 1165,1485 1485,1805 1855,2255
really powerful and able to

660
00:21:24,200 --> 00:21:26,130
0,400 570,860 860,1025 1025,1220 1220,1930
represent each of those concepts

661
00:21:26,180 --> 00:21:29,400
0,400 1170,1570 1590,2030 2030,2350 2820,3220
in the embedding space. And

662
00:21:29,600 --> 00:21:30,355
0,275 275,380 380,440 440,590 590,755
what we are able to

663
00:21:30,355 --> 00:21:31,180
0,75 75,180 180,330 330,540 540,825
do is to map that

664
00:21:31,180 --> 00:21:32,840
0,350 430,1040
to pixels.|
|

665
00:21:34,150 --> 00:21:35,310
0,335 335,470 470,665 665,965 965,1160
It's still mind blowing, though
这仍然令人惊叹，尽管我们仍然可以从文本提示中获得这样的输出，这仍然是令人惊讶的。

666
00:21:35,310 --> 00:21:36,795
0,255 255,405 405,705 705,1100 1210,1485
it's still amazing that we

667
00:21:36,795 --> 00:21:37,935
0,150 150,345 345,585 585,825 825,1140
can get these kinds of

668
00:21:37,935 --> 00:21:40,220
0,450 450,755 805,1140 1140,1655
outputs from text prompts.|
|

669
00:21:40,860 --> 00:21:42,040
0,245 245,365 365,545 545,815 815,1180
Here are some failure cases,
这里有一些失败的案例，就像我提到的那样。

670
00:21:42,390 --> 00:21:44,940
0,275 275,410 410,670
like I mentioned.|
|

671
00:21:45,670 --> 00:21:47,010
0,380 380,620 620,880 900,1160 1160,1340
Here we asked the model
在这里，我们要求模型呈现很长的单词数，但它做得不是很好。

672
00:21:47,010 --> 00:21:49,215
0,165 165,410 700,990 990,1280 1900,2205
to render a long number

673
00:21:49,215 --> 00:21:50,850
0,210 210,495 495,875 1225,1485 1485,1635
of words and it did

674
00:21:50,850 --> 00:21:51,690
0,210 210,420 420,585 585,705 705,840
not do such a good

675
00:21:51,690 --> 00:21:53,740
0,290
job.|
|

676
00:21:53,740 --> 00:21:55,765
0,180 180,420 420,920 940,1340 1690,2025
Ten wine bottles and it
十个酒瓶，它停在一，二，三，四，五，六，七。

677
00:21:55,765 --> 00:21:57,250
0,270 270,570 570,870 870,1185 1185,1485
stopped at one, two, three,

678
00:21:57,250 --> 00:21:59,360
0,285 285,600 600,855 855,1160
four, five, six, seven.|
|

679
00:21:59,370 --> 00:22:00,680
0,305 305,470 470,730
And so on.|
诸若此类。|

680
00:22:04,250 --> 00:22:06,430
0,455 455,725 725,1370 1370,1750 1890,2180
Here's a subjective comparison to
这是与其他最先进模型的主观比较，所以可以说一件事是。

681
00:22:06,430 --> 00:22:07,690
0,290 550,825 825,960 960,1080 1080,1260
other state of the art

682
00:22:07,690 --> 00:22:10,780
0,320 1300,1700 2500,2790 2790,2955 2955,3090
models, so one thing to

683
00:22:10,780 --> 00:22:12,180
0,120 120,270 270,560
say is that.|
|

684
00:22:12,340 --> 00:22:13,800
0,290 290,485 485,710 710,950 950,1460
In this case, the evaluations
在这种情况下，在我看来，评估并不是很可靠，因为根据定义，我们要求的文本提示和样式通常不是自然的。我们想要各种款式的混合搭配。因此，在我看来，一个重要的开放研究问题是，除了只看一些结果，你如何评价模型a比模型b更好。

685
00:22:13,800 --> 00:22:15,375
0,380 610,885 885,1080 1080,1335 1335,1575
are, in my opinion, not

686
00:22:15,375 --> 00:22:18,740
0,305 415,815 1885,2285 2845,3105 3105,3365
very robust because by definition,

687
00:22:18,880 --> 00:22:20,685
0,400 450,740 740,980 980,1520 1520,1805
often the text prompts and

688
00:22:20,685 --> 00:22:22,440
0,335 595,1020 1020,1260 1260,1470 1470,1755
the styles we ask for

689
00:22:22,440 --> 00:22:23,870
0,195 195,470 550,915 915,1155 1155,1430
are not natural. We want

690
00:22:23,980 --> 00:22:25,380
0,400 450,770 770,980 980,1190 1190,1400
various mix and match of

691
00:22:25,380 --> 00:22:28,310
0,320 970,1260 1260,1550 2110,2510 2530,2930
styles. And so an important

692
00:22:28,330 --> 00:22:29,580
0,275 275,550 570,920 920,1130 1130,1250
open research question in my

693
00:22:29,580 --> 00:22:30,345
0,150 150,345 345,525 525,630 630,765
mind is how do you

694
00:22:30,345 --> 00:22:32,025
0,300 300,695 985,1290 1290,1500 1500,1680
evaluate that model a is

695
00:22:32,025 --> 00:22:33,165
0,195 195,390 390,600 600,885 885,1140
better than model b, other

696
00:22:33,165 --> 00:22:34,460
0,180 180,390 390,675 675,960 960,1295
than just looking at some

697
00:22:34,870 --> 00:22:37,400
0,400 1320,1595 1595,1870
results, I think.|
|

698
00:22:37,500 --> 00:22:38,920
0,400 570,890 890,1115 1115,1175 1175,1420
Ah, that. That's a very
啊，那个。这是一个非常有趣和开放的。

699
00:22:39,030 --> 00:22:41,020
0,260 260,455 455,790
interesting and open.|
|

700
00:22:41,020 --> 00:22:43,585
0,320 1180,1440 1440,1700 2050,2310 2310,2565
Question. So here. So here
问题来了。所以在这里。所以在这里，我将通过底部的例子指出一些事情，这是一只彩虹色的企鹅。达利也在制作企鹅，但在颜色方面做得不是很好。而Imagen和MUSE模型似乎能够同时尊重这两种模型，我们认为这可能是因为。

701
00:22:43,585 --> 00:22:45,370
0,330 330,575 1135,1440 1440,1635 1635,1785
I'll just point out a

702
00:22:45,370 --> 00:22:46,440
0,135 135,300 300,540 540,780 780,1070
couple of things with the

703
00:22:47,150 --> 00:22:48,670
0,320 320,500 500,605 605,850 1260,1520
example at the bottom, which

704
00:22:48,670 --> 00:22:50,220
0,150 150,330 330,735 735,945 945,1550
is a rainbow colored penguin.

705
00:22:50,360 --> 00:22:52,330
0,305 305,650 650,970 1290,1580 1580,1970
And dali too is generating

706
00:22:52,330 --> 00:22:54,040
0,555 555,920 1000,1410 1410,1560 1560,1710
penguins but doesn't do such

707
00:22:54,040 --> 00:22:54,805
0,120 120,270 270,465 465,630 630,765
a good job with the

708
00:22:54,805 --> 00:22:58,200
0,275 1465,1830 1830,2130 2130,2645 2995,3395
colors. Whereas the imagen and

709
00:22:58,580 --> 00:22:59,910
0,320 320,635 635,935 935,1085 1085,1330
muse models seem to be

710
00:23:00,140 --> 00:23:02,215
0,350 350,635 635,905 905,1240 1800,2075
able to respect both and

711
00:23:02,215 --> 00:23:03,300
0,165 165,360 360,540 540,750 750,1085
we think this is probably

712
00:23:03,500 --> 00:23:04,740
0,320 320,640
because the.|
|

713
00:23:05,210 --> 00:23:06,880
0,400 570,1010 1010,1175 1175,1415 1415,1670
Ah, dali two model is
啊，大理二号机型靠的是夹子嵌入，这可能会丢失一些细节。

714
00:23:06,880 --> 00:23:08,460
0,285 285,465 465,690 690,1050 1050,1580
relying on a clip embedding,

715
00:23:08,570 --> 00:23:09,760
0,305 305,530 530,815 815,1040 1040,1190
which might lose some of

716
00:23:09,760 --> 00:23:11,200
0,285 285,680
these details.|
|

717
00:23:15,800 --> 00:23:17,020
0,400
Um.|
恩。|

718
00:23:17,120 --> 00:23:18,985
0,260 260,410 410,605 605,1300 1530,1865
We did some quantitative on
我们对我们已有的指标做了一些量化，这是在一个名为CC 3M的数据集上进行剪辑的。

719
00:23:18,985 --> 00:23:20,020
0,180 180,420 420,690 690,825 825,1035
the metrics that we have,

720
00:23:20,020 --> 00:23:21,025
0,225 225,360 360,510 675,810 810,1005
which is f I'd and

721
00:23:21,025 --> 00:23:23,245
0,365 775,1175 1465,1740 1740,1965 1965,2220
clip on a data set

722
00:23:23,245 --> 00:23:24,940
0,240 465,660 660,825 825,1085
called CC three m.|
|

723
00:23:25,100 --> 00:23:27,220
0,365 365,730 1020,1280 1280,1540 1830,2120
And compared to a number
并与其他一些模型进行了比较，包括扩散模型和自回归模型。

724
00:23:27,220 --> 00:23:28,825
0,150 150,330 330,650 1120,1425 1425,1605
of other models, both in

725
00:23:28,825 --> 00:23:30,835
0,425 715,1035 1035,1275 1275,1815 1815,2010
diffusion and auto regressive type

726
00:23:30,835 --> 00:23:31,960
0,305
models.|
|

727
00:23:32,100 --> 00:23:34,475
0,400 960,1340 1340,1655 1655,1990 2040,2375
Um, here for I score.
嗯，在这里等我得分。越低越好。对于CLIP，越高越好。

728
00:23:34,475 --> 00:23:36,185
0,285 285,525 525,815 1195,1500 1500,1710
Lower is better. And for

729
00:23:36,185 --> 00:23:37,980
0,225 225,510 510,810 810,1115
clip, higher is better.|
|

730
00:23:38,260 --> 00:23:39,780
0,260 260,520
So, overall.|
所以，总体而言。|

731
00:23:40,050 --> 00:23:42,400
0,275 275,455 455,605 605,850 1920,2350
We seem to be scoring
我们似乎在这两个指标上都取得了最好的成绩。

732
00:23:42,690 --> 00:23:43,835
0,260 260,470 470,770 770,995 995,1145
the best on both of

733
00:23:43,835 --> 00:23:46,420
0,275 415,815
these metrics.|
|

734
00:23:47,420 --> 00:23:49,255
0,290 290,730 1050,1310 1310,1595 1595,1835
And here's the yal on
这是对可可的雅尔，与许多最先进的模式，如达利，达利，两，想象党。

735
00:23:49,255 --> 00:23:51,505
0,450 450,930 930,1325 1825,2100 2100,2250
cocoa, comparing to many of

736
00:23:51,505 --> 00:23:52,210
0,165 165,330 330,465 465,570 570,705
the state of the art

737
00:23:52,210 --> 00:23:54,300
0,270 270,555 555,1040 1420,1815 1815,2090
models like dali, dali, two,

738
00:23:54,710 --> 00:23:58,060
0,400 420,820
imagine party.|
|

739
00:23:58,160 --> 00:23:59,620
0,245 245,490 810,1085 1085,1265 1265,1460
We are almost as good
我们几乎是一样好的。

740
00:23:59,620 --> 00:24:00,740
0,290
as.|
|

741
00:24:00,740 --> 00:24:02,795
0,320 550,950 970,1370 1390,1740 1740,2055
Um, the party 20 billion
嗯，派对200亿的模型，只是在f I‘s得分上略差一些，但在剪辑得分上要好得多，所以。

742
00:24:02,795 --> 00:24:07,090
0,365 2755,3090 3090,3425 3565,3930 3930,4295
model, just slightly worse in

743
00:24:07,500 --> 00:24:09,815
0,275 425,620 620,940 1560,1960 1980,2315
f I'd score, but doing

744
00:24:09,815 --> 00:24:11,680
0,335 505,870 870,1200 1200,1530 1530,1865
significantly better on clip score,

745
00:24:13,500 --> 00:24:14,540
0,400
so.|
|

746
00:24:14,710 --> 00:24:16,605
0,245 245,455 455,700 1380,1670 1670,1895
So that's good, which means
这很好，这意味着我们能够尊重文本提示符的语义。啊，比那些模特要好，如果你相信剪辑的代码的话。

747
00:24:16,605 --> 00:24:18,285
0,335 895,1125 1125,1200 1200,1395 1395,1680
that we are able to

748
00:24:18,285 --> 00:24:19,545
0,240 240,420 420,915 915,1110 1110,1260
respect the semantics of the

749
00:24:19,545 --> 00:24:21,120
0,225 225,575 625,990 990,1320 1320,1575
text prompt. Ah, better than

750
00:24:21,120 --> 00:24:22,620
0,195 195,480 480,860 1060,1335 1335,1500
those models, if one was

751
00:24:22,620 --> 00:24:23,660
0,180 180,360 360,525 525,735 735,1040
to believe the clips code.|
|

752
00:24:25,930 --> 00:24:28,605
0,290 290,580 1170,1805 1805,2200 2310,2675
And finally, runtime on tp
最后，在硬件之前，tp u上的运行时间在哪里？

753
00:24:28,605 --> 00:24:32,460
0,245 1045,1445 1765,2165 2845,3245
u before hardware where?|
|

754
00:24:33,040 --> 00:24:34,970
0,410 410,530 530,790 1410,1670 1670,1930
Here's the model, the resolution
这是模型，生成的分辨率和所用的时钟时间。因此，大部分计算都是在超分辨率下进行的。基础模型取点，5秒，然后它需要另一个点，8秒来做超分辨率，总共1.3秒。

755
00:24:35,170 --> 00:24:36,315
0,260 260,395 395,670 720,995 995,1145
that is generated and the

756
00:24:36,315 --> 00:24:38,280
0,255 255,570 570,810 810,1115 1705,1965
time wall clock time that

757
00:24:38,280 --> 00:24:40,050
0,150 150,440 790,1190 1270,1575 1575,1770
it took. So most of

758
00:24:40,050 --> 00:24:42,465
0,290 1090,1605 1605,1875 1875,2175 2175,2415
the compute goes into the

759
00:24:42,465 --> 00:24:44,445
0,195 195,515 1255,1530 1530,1725 1725,1980
super resolution. The base model

760
00:24:44,445 --> 00:24:46,680
0,335 415,735 735,990 990,1325 1945,2235
takes point, five seconds and

761
00:24:46,680 --> 00:24:47,820
0,210 210,405 405,615 615,885 885,1140
then it takes another point,

762
00:24:47,820 --> 00:24:49,020
0,240 240,560 670,930 930,1065 1065,1200
eight seconds to do the

763
00:24:49,020 --> 00:24:50,295
0,180 180,500 730,975 975,1095 1095,1275
super resolution for a total

764
00:24:50,295 --> 00:24:53,360
0,305 445,1230 1230,1535
of 1.3 seconds.|
|

765
00:24:54,750 --> 00:24:56,555
0,400 810,1085 1085,1325 1325,1580 1580,1805
So now here are some
现在，这里有一些由模型启用的编辑示例。左边是一张真实的输入图像，我们在图像的一部分绘制了一个蒙版，然后要求模型使用文本指导提示来填充它。一只有趣的充气黄色大鸭子。

766
00:24:56,555 --> 00:24:59,230
0,335 415,815 1315,1590 1590,2135 2275,2675
examples of the editing that

767
00:24:59,250 --> 00:25:01,625
0,485 485,740 740,875 875,1120 2100,2375
enabled by the model. On

768
00:25:01,625 --> 00:25:02,470
0,105 105,255 255,450 450,585 585,845
the left is a real

769
00:25:02,580 --> 00:25:04,925
0,275 275,550 990,1390 1890,2225 2225,2345
input image, and we've drawn

770
00:25:04,925 --> 00:25:06,530
0,150 150,420 420,815 1105,1410 1410,1605
a mask over one part

771
00:25:06,530 --> 00:25:07,985
0,165 165,270 270,500 1090,1335 1335,1455
of the image and then

772
00:25:07,985 --> 00:25:09,005
0,210 210,390 390,585 585,825 825,1020
ask the model to fill

773
00:25:09,005 --> 00:25:10,085
0,225 225,480 480,675 675,825 825,1080
that in with a text

774
00:25:10,085 --> 00:25:12,005
0,390 390,695 1225,1530 1530,1710 1710,1920
guided prompt. So a funny

775
00:25:12,005 --> 00:25:14,080
0,240 240,915 915,1155 1155,1505
big inflatable yellow duck.|
|

776
00:25:15,030 --> 00:25:18,940
0,320 320,530 530,940 2520,2920 3030,3910
Hot air balloons and futuristic,
热气球和未来主义、流线型的现代建筑。

777
00:25:18,990 --> 00:25:21,900
0,610 750,1150 1710,2110
streamlined modern building.|
|

778
00:25:22,760 --> 00:25:23,910
0,260 260,410 410,605 605,830 830,1150
So all that came just
因此，所有这些都是零命中。我们没有对模型进行任何微调，但我们使用变量掩码进行训练的事实允许它开箱即用地进行调整。

779
00:25:23,990 --> 00:25:25,330
0,335 335,670 810,1070 1070,1235 1235,1340
zero shot. We didn't do

780
00:25:25,330 --> 00:25:26,365
0,195 195,450 450,780 780,915 915,1035
any fine tuning of the

781
00:25:26,365 --> 00:25:27,910
0,245 745,1005 1005,1125 1125,1290 1290,1545
model, but the fact that

782
00:25:27,910 --> 00:25:30,220
0,350 640,1005 1005,1290 1290,1610 1690,2310
we trained with variable masking

783
00:25:30,220 --> 00:25:32,530
0,285 285,495 495,800 1780,2070 2070,2310
allows it to do this

784
00:25:32,530 --> 00:25:34,020
0,225 225,345 345,435 435,680
out of the box.|
|

785
00:25:35,900 --> 00:25:38,290
0,320 320,590 590,940 1650,2050 2100,2390
So another example where we
这是另一个我们画画的例子，所以这里的面具是剩下的。我们只想保留这个，啊，这个人，把形象的其余部分替换掉。

786
00:25:38,290 --> 00:25:40,300
0,290 700,1065 1065,1430 1660,1905 1905,2010
do out painting, so the

787
00:25:40,300 --> 00:25:42,660
0,240 240,555 555,890 1840,2100 2100,2360
mask here is the rest.

788
00:25:43,250 --> 00:25:44,200
0,260 260,425 425,605 605,755 755,950
We only want to keep

789
00:25:44,200 --> 00:25:47,370
0,320 1030,1430 2050,2355 2355,2660 2770,3170
the, ah, this person and

790
00:25:48,170 --> 00:25:49,620
0,320 320,500 500,755 755,1100 1100,1450
replace the rest of the

791
00:25:49,850 --> 00:25:50,980
0,400
image.|
|

792
00:25:50,980 --> 00:25:52,530
0,380 430,705 705,885 885,1245 1245,1550
And the text guided prompt
并提供了文本引导提示，以填充该区域的其余部分，因此伦敦的天际线。

793
00:25:53,630 --> 00:25:54,960
0,350 350,650 650,905 905,1070 1070,1330
was provided to fill in

794
00:25:55,250 --> 00:25:56,190
0,245 245,395 395,575 575,695 695,940
the rest of the region,

795
00:25:56,420 --> 00:25:58,500
0,305 305,470 470,730 750,1330
so the London skyline.|
|

796
00:25:58,540 --> 00:26:00,840
0,275 275,850 1140,1520 1520,1870 1980,2300
A wildflower bloom at mount
一朵野花盛开在土星环上的雨花山。

797
00:26:00,840 --> 00:26:02,640
0,530 1000,1290 1290,1440 1440,1590 1590,1800
rainania on the ring of

798
00:26:02,640 --> 00:26:03,760
0,500
Saturn.|
|

799
00:26:06,180 --> 00:26:07,360
0,335 335,530 530,725 725,920 920,1180
Ah, we can also do
啊，我们还可以做其他的编辑。啊，我们在那里拍了一张真实的照片。有一个消极的提示，使用一个穿着T恤的人，然后这些是积极的提示。啊，要，要做各种款式的转换。

800
00:26:07,890 --> 00:26:09,790
0,350 350,620 620,800 800,1330 1500,1900
other kinds of editing. Ah,

801
00:26:09,810 --> 00:26:11,255
0,400 750,995 995,1115 1115,1250 1250,1445
where we took a real

802
00:26:11,255 --> 00:26:14,720
0,335 1945,2345 3025,3270 3270,3375 3375,3465
image with. There was a

803
00:26:14,720 --> 00:26:15,950
0,210 210,525 525,810 810,1020 1020,1230
negative prompt, use a man

804
00:26:15,950 --> 00:26:17,480
0,300 300,540 540,705 705,980 1270,1530
wearing A T shirt and

805
00:26:17,480 --> 00:26:18,370
0,135 135,330 330,510 510,630 630,890
then these were the positive

806
00:26:18,450 --> 00:26:20,255
0,610 690,1010 1010,1330 1410,1670 1670,1805
prompts. Ah, to, to do

807
00:26:20,255 --> 00:26:22,420
0,275 325,645 645,965 1435,1800 1800,2165
various kinds of style transfer.|
|

808
00:26:23,550 --> 00:26:25,235
0,320 320,640 900,1190 1190,1445 1445,1685
On the on the on
在T恤上的。

809
00:26:25,235 --> 00:26:26,360
0,135 135,285 285,575
the t shirt.|
|

810
00:26:28,720 --> 00:26:29,840
0,260
The.|
这个。|

811
00:26:31,050 --> 00:26:32,140
0,290 290,440 440,575 575,770 770,1090
And here are some examples
这里有一些自由蒙版编辑的例子，顶行是输入图像，底部是转换后的输出，它只依赖于文本和文本与图像之间的注意力来进行各种更改。

812
00:26:32,160 --> 00:26:34,210
0,400 540,920 920,1205 1205,1700 1700,2050
of mask free editing, where

813
00:26:34,890 --> 00:26:36,250
0,290 290,455 455,680 680,995 995,1360
on the top row are

814
00:26:36,300 --> 00:26:38,150
0,290 290,580 1320,1595 1595,1730 1730,1850
input images, and on the

815
00:26:38,150 --> 00:26:41,260
0,260 640,1040 1090,1490 1750,2300 2710,3110
bottom, the transformed outputs where

816
00:26:41,730 --> 00:26:42,830
0,290 290,470 470,755 755,935 935,1100
it just relies on the

817
00:26:42,830 --> 00:26:44,525
0,290 370,735 735,1100 1270,1545 1545,1695
text and attention between the

818
00:26:44,525 --> 00:26:45,755
0,195 195,375 375,635 805,1065 1065,1230
text and images to make

819
00:26:45,755 --> 00:26:47,420
0,300 300,695
various changes.|
|

820
00:26:47,420 --> 00:26:49,420
0,260 310,660 660,975 975,1340 1600,2000
Eh, so for eh, eh,
嗯，所以呢，比如说，我们这里说aibbau，那个，呃，这个模型就是把猫变成狗狗。

821
00:26:49,800 --> 00:26:51,940
0,275 275,530 530,910 990,1390 1740,2140
for example, here we say

822
00:26:52,110 --> 00:26:54,920
0,950 950,1250 1250,1600 1860,2260 2550,2810
aibbau and that, eh, the

823
00:26:54,920 --> 00:26:56,540
0,260 280,630 630,870 870,1250 1270,1620
model converts the cat to

824
00:26:56,540 --> 00:26:58,580
0,315 315,1005 1005,1400
a shbau dog.|
|

825
00:26:59,300 --> 00:27:00,640
0,260 260,455 455,740 740,1070 1070,1340
A dog holding a football
一只狗嘴里叼着一只足球。所以在这里，狗本身发生了变化。然后。

826
00:27:00,640 --> 00:27:02,215
0,150 150,330 330,650 1090,1350 1350,1575
in his mouth. So here

827
00:27:02,215 --> 00:27:04,150
0,225 225,485 565,885 885,1205 1615,1935
the dog itself changes. And

828
00:27:04,150 --> 00:27:05,140
0,320
then.|
|

829
00:27:05,240 --> 00:27:06,685
0,320 320,635 635,1010 1010,1220 1220,1445
This ball changes to a
这个球变成了足球。

830
00:27:06,685 --> 00:27:07,720
0,395
football.|
|

831
00:27:07,820 --> 00:27:09,990
0,275 275,500 500,740 740,1330 1770,2170
A basket of oranges where
一篮子橙子，模型能够保持场景的大体构图，只需将苹果更改为橙子。篮子的质地也有了一些变化，但大部分成分都保持不变。

832
00:27:10,580 --> 00:27:11,500
0,245 245,380 380,545 545,740 740,920
the model is able to

833
00:27:11,500 --> 00:27:12,985
0,165 165,345 345,620 940,1275 1275,1485
keep the general composition of

834
00:27:12,985 --> 00:27:14,125
0,165 165,420 420,690 690,915 915,1140
the scene and just change

835
00:27:14,125 --> 00:27:17,005
0,165 165,425 535,935 1285,1985 2635,2880
the apples to oranges. The

836
00:27:17,005 --> 00:27:18,610
0,245 325,840 840,1110 1110,1380 1380,1605
basket texture has also changed

837
00:27:18,610 --> 00:27:19,960
0,210 210,360 360,650 760,1065 1065,1350
a little bit, but mostly

838
00:27:19,960 --> 00:27:21,990
0,360 360,675 675,1010 1330,1680 1680,2030
the composition is kept similar.|
|

839
00:27:23,840 --> 00:27:25,615
0,400 630,1025 1025,1265 1265,1505 1505,1775
Here, it's able to change
在这里，它能够改变这一点，只需让猫打哈欠，而不改变场景的组成。

840
00:27:25,615 --> 00:27:28,615
0,395 445,845 2335,2625 2625,2775 2775,3000
this just make the cat

841
00:27:28,615 --> 00:27:30,720
0,425 535,935 1015,1365 1365,1710 1710,2105
yawn without changing the composition

842
00:27:30,770 --> 00:27:32,260
0,290 290,455 455,730
of the scene.|
|

843
00:27:32,330 --> 00:27:33,190
0,305 305,455 455,575 575,710 710,860
And one of the things
我们正在探索的一件事是如何进一步控制模型，你知道，真正能够在不影响其余部分的情况下调整图像的特定部分。

844
00:27:33,190 --> 00:27:34,120
0,120 120,255 255,510 510,720 720,930
we are exploring is how

845
00:27:34,120 --> 00:27:35,370
0,165 165,285 285,480 480,800 850,1250
we can have further control

846
00:27:35,870 --> 00:27:37,510
0,290 290,440 440,700 1200,1445 1445,1640
of the model, you know,

847
00:27:37,510 --> 00:27:39,270
0,315 315,525 525,705 705,1040 1360,1760
really be able to adjust

848
00:27:39,650 --> 00:27:41,070
0,400 450,785 785,995 995,1145 1145,1420
specific parts of the image

849
00:27:41,180 --> 00:27:43,360
0,400 540,1010 1010,1160 1160,1420
without affecting the rest.|
|

850
00:27:45,530 --> 00:27:48,550
0,320 320,640 1080,1630 2400,2735 2735,3020
Yeah, and here's an example
是的，这是一个迭代编辑的例子，上面提供了图像，右上角提供了图像。然后在右下角是一个牛角面包的输出，旁边是一杯带有花朵拿铁艺术的拿铁咖啡。

851
00:27:48,550 --> 00:27:53,095
0,285 285,855 855,1370 1900,2300 4210,4545
of iterative editing where the,

852
00:27:53,095 --> 00:27:54,685
0,335 805,1110 1110,1275 1275,1410 1410,1590
the image at the top

853
00:27:54,685 --> 00:27:56,305
0,255 255,605 1015,1305 1305,1455 1455,1620
was provided and the top

854
00:27:56,305 --> 00:27:57,310
0,305 355,630 630,765 765,885 885,1005
right. And then on the

855
00:27:57,310 --> 00:27:58,770
0,225 225,555 555,810 810,1080 1080,1460
bottom right is the output

856
00:27:59,930 --> 00:28:01,555
0,400 510,815 815,1220 1220,1475 1475,1625
for a croissant next to

857
00:28:01,555 --> 00:28:02,815
0,90 90,420 420,660 660,960 960,1260
a latte with a flower

858
00:28:02,815 --> 00:28:04,160
0,405 405,695
latte art.|
|

859
00:28:04,160 --> 00:28:07,160
0,225 225,560 820,1220 2380,2760 2760,3000
And these are we. We
这就是我们。我们进行了一百个步骤的编辑，逐步调整令牌。这些是推理序列的不同迭代的结果。所以你可以看到，它从蛋糕和拿铁开始，然后逐渐将蛋糕转变为。

860
00:28:07,160 --> 00:28:08,135
0,165 165,300 300,675 675,840 840,975
ran the editing for a

861
00:28:08,135 --> 00:28:11,690
0,275 295,695 1585,2495 2935,3420 3420,3555
hundred steps, progressively adjusting the

862
00:28:11,690 --> 00:28:13,205
0,440 700,1005 1005,1185 1185,1335 1335,1515
tokens. And these are the

863
00:28:13,205 --> 00:28:15,190
0,255 255,495 495,645 645,905 1375,1985
results of the different iterations

864
00:28:15,330 --> 00:28:17,320
0,380 380,695 695,1030 1110,1445 1445,1990
of, of train of inference.

865
00:28:18,300 --> 00:28:19,205
0,335 335,530 530,665 665,800 800,905
So you can see that

866
00:28:19,205 --> 00:28:20,015
0,120 120,315 315,480 480,615 615,810
it starts with the cake

867
00:28:20,015 --> 00:28:21,050
0,180 180,300 300,660 660,870 870,1035
and the latte and then

868
00:28:21,050 --> 00:28:23,020
0,800 910,1305 1305,1440 1440,1635 1635,1970
progressively transforms the cake to.|
|

869
00:28:23,540 --> 00:28:24,845
0,345 345,585 585,765 765,1035 1035,1305
Something that looks in between
介于蛋糕和牛角面包之间的东西。最后，它看起来像羊角面包。同样，拿铁的艺术也从一颗心变成了一朵花，你知道，有点。

870
00:28:24,845 --> 00:28:25,960
0,180 180,345 345,510 510,660 660,1115
the cake and the croissant.

871
00:28:26,040 --> 00:28:27,665
0,260 260,410 410,700 1110,1415 1415,1625
And then finally, it looks

872
00:28:27,665 --> 00:28:29,900
0,195 195,345 345,755 1255,1635 1635,2235
like the croissant. And similarly,

873
00:28:29,900 --> 00:28:31,730
0,135 135,435 435,710 1150,1550 1570,1830
the latte art changes from

874
00:28:31,730 --> 00:28:33,250
0,135 135,410 490,840 840,1155 1155,1520
a heart to a flower,

875
00:28:34,380 --> 00:28:36,120
0,260 260,520 570,830 830,1090
you know, kind of.|
|

876
00:28:37,630 --> 00:28:38,900
0,290 290,455 455,575 575,665 665,1270
Some kind of an interpolation
某种潜在空间中的一种插补。

877
00:28:39,100 --> 00:28:41,160
0,305 305,485 485,860 860,1180
in some latent space.|
|

878
00:28:43,150 --> 00:28:44,655
0,400 720,995 995,1130 1130,1310 1310,1505
So because of the speed
因此，由于模型的速度，存在一些交互编辑的可能性。所以我将只播放这个短片，它展示了我们如何能够与模型进行交互工作。

879
00:28:44,655 --> 00:28:46,095
0,135 135,255 255,515 925,1290 1290,1440
of the model, there's some

880
00:28:46,095 --> 00:28:48,660
0,305 475,875 1255,1695 1695,2195 2335,2565
possibility of interactive editing. So

881
00:28:48,660 --> 00:28:49,890
0,150 150,330 330,570 570,855 855,1230
I'll just play this short

882
00:28:49,890 --> 00:28:52,455
0,330 330,570 570,890 1960,2325 2325,2565
clip, which shows how we

883
00:28:52,455 --> 00:28:53,630
0,165 165,300 300,495 495,810 810,1175
might be able to do

884
00:28:53,650 --> 00:28:55,640
0,610 690,1055 1055,1420 1470,1730 1730,1990
interactive work with the model.|
|

885
00:29:25,040 --> 00:29:26,310
0,350 350,665 665,770 770,950 950,1270
So that's a real time.
所以这是一个真实的时间。也就是说，它没有加速或减速。

886
00:29:26,570 --> 00:29:28,255
0,290 290,530 530,1295 1295,1460 1460,1685
As in it's not sped

887
00:29:28,255 --> 00:29:29,820
0,195 195,465 465,735 735,995
up or slowed down.|
|

888
00:29:39,275 --> 00:29:41,030
0,60 60,210 210,515 565,965 1495,1755
It's not perfect, but you
这不是完美的，但你可以看到这个想法。

889
00:29:41,030 --> 00:29:42,940
0,180 180,375 375,585 585,920
can see the idea.|
|

890
00:29:51,750 --> 00:29:53,340
0,395 395,790
So, um.|
那么，嗯。|

891
00:29:53,340 --> 00:29:54,590
0,285 285,525 525,720 720,915 915,1250
Next steps for us are
对于我们来说，下一步是提高分辨率、质量，处理细节，如渲染文本，探测文本和图像之间的交叉关注以实现更多控制，探索应用程序。所以是的，报纸和。

892
00:29:54,700 --> 00:29:56,940
0,350 350,545 545,790 1080,1480 1800,2240
improving the resolution, quality, handling

893
00:29:56,940 --> 00:29:58,350
0,240 240,585 585,825 825,1005 1005,1410
of details such as rendered

894
00:29:58,350 --> 00:30:01,460
0,320 1930,2355 2355,2505 2505,2745 2745,3110
text, probing the cross attention

895
00:30:01,630 --> 00:30:03,120
0,290 290,500 500,695 695,970 1200,1490
between text and images to

896
00:30:03,120 --> 00:30:05,720
0,255 255,620 640,1040 1630,2060 2200,2600
enable more control, exploring applications.

897
00:30:06,850 --> 00:30:09,230
0,400 540,940 1020,1280 1280,1540 1980,2380
So yeah, the paper and.|
|

898
00:30:10,620 --> 00:30:11,980
0,305 305,545 545,770 770,1010 1010,1360
Web page are listed here
网页在这里列出，我很乐意回答问题。太好了，非常感谢。也许我有一个问题，只是为了让事情开始。我很好奇，在你看来，我特别为实现令人印象深刻的加速结果做出了哪些最重要的贡献？因为与过去的方法相比，这真的是一个巨大的差距。所以我很好奇，在你看来，在众多的贡献中，这是什么样子的？是的，主要是并行译码，所以在自动回归模型中。

899
00:30:12,060 --> 00:30:13,595
0,400 660,1025 1025,1190 1190,1370 1370,1535
and I'm happy to take

900
00:30:13,595 --> 00:30:22,400
0,305 7975,8295 8295,8490 8490,8640 8640,8805
questions. Excellent, thank you so

901
00:30:22,400 --> 00:30:23,825
0,290 520,915 915,1155 1155,1260 1260,1425
much. Maybe I have one

902
00:30:23,825 --> 00:30:24,815
0,270 270,525 525,660 660,780 780,990
question just to get things

903
00:30:24,815 --> 00:30:26,800
0,335 865,1230 1230,1455 1455,1695 1695,1985
started. I'm curious in your

904
00:30:26,850 --> 00:30:28,300
0,365 365,605 605,800 800,1085 1085,1450
opinion, what are the most

905
00:30:28,410 --> 00:30:30,250
0,400 480,880 900,1205 1205,1475 1475,1840
important contributions that m made

906
00:30:30,900 --> 00:30:32,300
0,400 540,860 860,1085 1085,1235 1235,1400
specifically to achieve the very

907
00:30:32,300 --> 00:30:34,265
0,315 315,570 570,780 780,1130 1630,1965
impressive speed up results? Because

908
00:30:34,265 --> 00:30:35,510
0,285 285,600 600,810 810,990 990,1245
in comparison to the past

909
00:30:35,510 --> 00:30:36,650
0,285 285,555 555,780 780,1020 1020,1140
methods, it's really like a

910
00:30:36,650 --> 00:30:37,715
0,210 210,525 525,720 720,885 885,1065
huge gap. So I'm curious

911
00:30:37,715 --> 00:30:39,335
0,335 595,945 945,1215 1215,1455 1455,1620
what like what across the

912
00:30:39,335 --> 00:30:41,045
0,245 355,755 1015,1335 1335,1515 1515,1710
many contributions LED to that

913
00:30:41,045 --> 00:30:42,275
0,195 195,345 345,635 805,1080 1080,1230
in your opinion? Yeah, it

914
00:30:42,275 --> 00:30:44,320
0,225 225,575 715,1050 1050,1365 1365,2045
was primarily the parallel decoding,

915
00:30:44,430 --> 00:30:45,970
0,320 320,545 545,770 770,1175 1175,1540
so in auto regress models.|
|

916
00:30:46,780 --> 00:30:48,945
0,275 275,550 780,1180 1380,1940 1940,2165
By definition, you decode one
根据定义，您一次只能解码一个令牌，所以您必须离开。让我们假设图像由196个令牌组成。然后，您将执行196个步骤，因为您要取消屏蔽一个令牌，将其传递回第二个令牌，依此类推。在扩散模型中，你要做的是一步一步地消除噪声。是的，你从纯噪音开始，通过网络传递，得到一些东西，然后把它传递回来，重复这个过程。

917
00:30:48,945 --> 00:30:50,430
0,375 375,495 495,630 630,905 1255,1485
token at a time, so

918
00:30:50,430 --> 00:30:51,195
0,75 75,165 165,255 255,465 465,765
you have to go. Let's

919
00:30:51,195 --> 00:30:52,905
0,245 505,780 780,1055 1225,1515 1515,1710
say the image consists of

920
00:30:52,905 --> 00:30:54,015
0,240 240,495 495,675 675,870 870,1110
a hundred and ninety six

921
00:30:54,015 --> 00:30:55,125
0,375 375,555 555,705 705,885 885,1110
tokens. Then you're doing a

922
00:30:55,125 --> 00:30:57,150
0,225 225,525 525,810 810,1145 1705,2025
hundred ninety six steps because

923
00:30:57,150 --> 00:30:58,680
0,210 210,615 615,855 855,1290 1290,1530
you unmask one token, pass

924
00:30:58,680 --> 00:31:00,675
0,165 165,360 360,680 1360,1710 1710,1995
it back in number two,

925
00:31:00,675 --> 00:31:02,280
0,225 225,360 360,605 955,1230 1230,1605
and so on. In diffusion

926
00:31:02,280 --> 00:31:03,585
0,350 490,795 795,960 960,1125 1125,1305
models, what you have to

927
00:31:03,585 --> 00:31:04,760
0,210 210,480 480,705 705,885 885,1175
do is to den noise

928
00:31:05,200 --> 00:31:06,780
0,275 275,470 470,785 785,1180 1290,1580
step by step. Yeah, you

929
00:31:06,780 --> 00:31:08,910
0,195 195,495 495,810 810,1130 1840,2130
start with pure noise, pass

930
00:31:08,910 --> 00:31:10,110
0,180 180,345 345,465 465,710 880,1200
it through the network, get

931
00:31:10,110 --> 00:31:11,520
0,315 315,710 820,1095 1095,1260 1260,1410
something out, then pass it

932
00:31:11,520 --> 00:31:12,615
0,150 150,300 300,560 580,885 885,1095
back in and repeat this

933
00:31:12,615 --> 00:31:13,580
0,305
process.|
|

934
00:31:14,020 --> 00:31:15,825
0,305 305,530 530,850 1260,1610 1610,1805
And that process needs to
而这一过程需要相当缓慢。

935
00:31:15,825 --> 00:31:17,440
0,150 150,455 565,965
be fairly slow.|
|

936
00:31:17,610 --> 00:31:20,120
0,400 1020,1310 1310,1535 1535,1870
Otherwise, it breaks down.|
否则，它就会崩溃。|

937
00:31:20,280 --> 00:31:21,060
0,180 180,375 375,525 525,660 660,780
And a lot of the
很多研究都是关于如何加快这一进程的。

938
00:31:21,060 --> 00:31:22,095
0,240 240,540 540,735 735,885 885,1035
research is about how to

939
00:31:22,095 --> 00:31:23,240
0,165 165,315 315,575
speed that up.|
|

940
00:31:23,340 --> 00:31:24,455
0,245 245,380 380,590 590,860 860,1115
So that takes thousands of
因此，这需要成千上万的步骤。因此，如果你有一个类似的模型，只需要做一些基本的向前道具。因此，我们所做的是进行并行解码。

941
00:31:24,455 --> 00:31:25,040
0,210 210,345 345,435 435,525 525,585
steps. So if you have

942
00:31:25,040 --> 00:31:27,005
0,165 165,600 600,890 1540,1860 1860,1965
a comparable model, there's just

943
00:31:27,005 --> 00:31:28,385
0,150 150,425 565,885 885,1095 1095,1380
a fundamental number of forward

944
00:31:28,385 --> 00:31:29,375
0,420 420,570 570,735 735,870 870,990
props that need to be

945
00:31:29,375 --> 00:31:30,710
0,275 535,795 795,930 930,1080 1080,1335
done. So what we did

946
00:31:30,710 --> 00:31:31,850
0,270 270,465 465,630 630,855 855,1140
instead was to go for

947
00:31:31,850 --> 00:31:33,300
0,300 300,980
parallel decoding.|
|

948
00:31:33,300 --> 00:31:35,430
0,380 550,950 1420,1725 1725,2025 2025,2130
And the one token at
一次一个令牌，您一次只做n个令牌，然后如果。

949
00:31:35,430 --> 00:31:36,320
0,120 120,270 270,435 435,600 600,890
a time, you just do

950
00:31:36,820 --> 00:31:38,090
0,365 365,755 755,890 890,1010 1010,1270
n tokens at a time

951
00:31:38,260 --> 00:31:39,520
0,275 275,550 690,1090
and then if.|
|

952
00:31:39,520 --> 00:31:40,705
0,255 255,450 450,645 645,915 915,1185
It's fixed and you just
它是固定的，你只需要196xn步就可以了。

953
00:31:40,705 --> 00:31:41,830
0,180 180,360 360,585 585,870 870,1125
need one ninety six by

954
00:31:41,830 --> 00:31:43,240
0,285 285,650
n steps.|
|

955
00:31:44,300 --> 00:31:46,080
0,350 350,620 620,830 830,1120
The idea is that.|
我们的想法是。|

956
00:31:46,080 --> 00:31:47,000
0,120 120,225 225,390 390,615 615,920
If you use high confidence
如果你使用高置信度令牌，那么它们在每一步都可能是有条件的相互独立的，我们可以预测它们中的每一个，而不会影响另一个。

957
00:31:47,050 --> 00:31:48,950
0,470 470,680 680,845 845,1120 1500,1900
tokens, then they are potentially

958
00:31:49,090 --> 00:31:50,610
0,635 635,950 950,1145 1145,1295 1295,1520
conditionally independent of each other

959
00:31:50,610 --> 00:31:52,605
0,225 225,435 435,770 1390,1755 1755,1995
at each step, and we

960
00:31:52,605 --> 00:31:53,910
0,275 565,855 855,1005 1005,1155 1155,1305
can each of them can

961
00:31:53,910 --> 00:31:55,845
0,135 135,555 555,830 910,1490 1690,1935
be predicted without affecting the

962
00:31:55,845 --> 00:31:56,560
0,245
other.|
|

963
00:31:56,720 --> 00:31:57,550
0,230 230,365 365,575 575,710 710,830
So that seems to hold
因此，这似乎是站得住脚的。

964
00:31:57,550 --> 00:31:58,240
0,290
up.|
|

965
00:32:00,430 --> 00:32:01,420
0,400
Interesting.|
有意思的。|

966
00:32:09,900 --> 00:32:11,890
0,275 275,455 455,710 710,1060 1590,1990
So the prompt is allowing
因此，该提示允许您导航图像本身的潜在空间。你有没有做过任何分析或看过提示与方向、速度或任何其他你可以在潜在表征中看到的指标之间的关系？探险。

967
00:32:11,910 --> 00:32:13,760
0,380 380,635 635,1190 1190,1385 1385,1850
you to navigate the latent

968
00:32:13,760 --> 00:32:15,760
0,380 640,990 990,1200 1200,1460 1600,2000
space of the image itself.

969
00:32:16,140 --> 00:32:18,130
0,350 350,700 1050,1340 1340,1610 1610,1990
Have you done any analysis

970
00:32:18,330 --> 00:32:19,970
0,305 305,560 560,910 1200,1460 1460,1640
or looked at what is

971
00:32:19,970 --> 00:32:21,905
0,240 240,560 1000,1275 1275,1455 1455,1935
the relationship between the prompting

972
00:32:21,905 --> 00:32:24,965
0,165 165,455 685,1085 2005,2370 2370,3060
and the directions or velocity

973
00:32:24,965 --> 00:32:25,835
0,120 120,270 270,495 495,705 705,870
or any other sort of

974
00:32:25,835 --> 00:32:27,035
0,515 535,780 780,900 900,1050 1050,1200
metric that you can look

975
00:32:27,035 --> 00:32:30,520
0,275 1225,1530 1530,1835 2005,2585 2635,3485
at in that latent representation?

976
00:32:30,780 --> 00:32:32,120
0,580
Exploration.|
|

977
00:32:32,400 --> 00:32:33,365
0,290 290,485 485,545 545,680 680,965
Yeah, it's a good question.
是啊，这是个好问题。我们还没有进行非常彻底的调查。我们看到的是，我这里没有，但我们看到了文本嵌入和之间的交叉注意图。

978
00:32:33,365 --> 00:32:34,700
0,270 270,495 495,645 645,935 1045,1335
We haven't yet done a

979
00:32:34,700 --> 00:32:37,460
0,285 285,680 820,1220 2350,2625 2625,2760
very thorough investigation. What we

980
00:32:37,460 --> 00:32:39,065
0,135 135,315 315,620 1240,1485 1485,1605
looked at was, and I

981
00:32:39,065 --> 00:32:40,040
0,210 210,330 330,495 495,705 705,975
don't have it here, but

982
00:32:40,040 --> 00:32:41,285
0,195 195,315 315,480 480,770 880,1245
we looked at some cross

983
00:32:41,285 --> 00:32:43,720
0,365 385,785 1615,1935 1935,2145 2145,2435
attention maps between the text

984
00:32:43,860 --> 00:32:45,720
0,530 530,905 905,1300
embeddings and the.|
|

985
00:32:45,720 --> 00:32:47,595
0,285 285,510 510,830 1360,1620 1620,1875
And the image, the generated
以及图像，生成的图像。你可以看到的是，嗯，你知道，嗯，它做的是你可能会想到的，那就是不同的名词和图像中的物体之间存在交叉紧张。当你有一个动词连接两个宾语时，它似乎会突出显示这两个宾语，有点像是在注意这一点。

986
00:32:47,595 --> 00:32:48,960
0,395 415,780 780,1035 1035,1185 1185,1365
image. And what you can

987
00:32:48,960 --> 00:32:50,955
0,240 240,450 450,740 1180,1580 1750,1995
see is that, um, you

988
00:32:50,955 --> 00:32:53,685
0,210 210,575 835,1235 2125,2475 2475,2730
know, um, the, it does

989
00:32:53,685 --> 00:32:54,840
0,195 195,330 330,555 555,915 915,1155
what you might expect, which

990
00:32:54,840 --> 00:32:56,720
0,150 150,440 700,1095 1095,1370 1450,1880
is that there's cross tension

991
00:32:56,890 --> 00:32:58,275
0,320 320,500 500,710 710,1220 1220,1385
between the different nouns and

992
00:32:58,275 --> 00:32:59,270
0,225 225,465 465,615 615,735 735,995
the objects in the image.

993
00:32:59,740 --> 00:33:01,095
0,260 260,395 395,635 635,950 950,1355
When you have a verb

994
00:33:01,095 --> 00:33:02,775
0,240 240,720 720,1085 1105,1440 1440,1680
that connects two objects, it

995
00:33:02,775 --> 00:33:04,335
0,270 270,495 495,660 660,965 1225,1560
seems to then highlight both

996
00:33:04,335 --> 00:33:06,390
0,335 385,785 1285,1560 1560,1770 1770,2055
those objects, sort of paying

997
00:33:06,390 --> 00:33:07,580
0,330 330,555 555,800
attention to that.|
|

998
00:33:08,120 --> 00:33:09,930
0,260 260,620 620,970 1170,1490 1490,1810
So that's one level of
这只是一个层面的探索，但我认为我们需要做更多的工作。

999
00:33:10,370 --> 00:33:13,105
0,550 930,1220 1220,1385 1385,1660 2490,2735
exploration, but I think we

1000
00:33:13,105 --> 00:33:14,190
0,105 105,210 210,345 345,635 685,1085
need to do more about.|
|

1001
00:33:14,800 --> 00:33:16,660
0,225 225,480 480,830 1240,1605 1605,1860
Like as we walk around
就像我们在潜在空间里走来走去，结果是什么图像？如果我们在两个文本提示之间移动，会发生什么？我们还不知道。潜伏空间平坦或有突变。

1002
00:33:16,660 --> 00:33:18,010
0,165 165,525 525,840 840,1200 1200,1350
in latent space, what's the

1003
00:33:18,010 --> 00:33:20,185
0,240 240,590 730,1020 1020,1310 1810,2175
resulting image? If we move

1004
00:33:20,185 --> 00:33:21,385
0,255 255,435 435,660 660,1050 1050,1200
between two text prompts, what

1005
00:33:21,385 --> 00:33:23,335
0,275 565,965 1225,1560 1560,1695 1695,1950
happens? We don't know yet.

1006
00:33:23,335 --> 00:33:25,270
0,210 210,525 525,845 865,1265 1645,1935
The latent space smooth or

1007
00:33:25,270 --> 00:33:27,880
0,165 165,315 315,675 675,1220
it has abrupt jumps.|
|

1008
00:33:27,880 --> 00:33:30,390
0,180 180,690 690,1070 1420,1820 1930,2510
The editing suggests some smoothness
编辑表明这里有一些流畅。

1009
00:33:31,160 --> 00:33:32,240
0,400
here.|
|

1010
00:33:32,240 --> 00:33:33,890
0,105 105,240 240,740 1270,1530 1530,1650
As you iterate, but this
但这是一个固定的提示，而不是不断变化的提示，所以我认为我们需要做更多的工作。

1011
00:33:33,890 --> 00:33:34,760
0,120 120,240 240,360 360,570 570,870
is with a fixed prompt,

1012
00:33:34,760 --> 00:33:35,980
0,255 255,405 405,525 525,800 820,1220
not with the changing prompt,

1013
00:33:37,350 --> 00:33:38,075
0,230 230,320 320,455 455,575 575,725
so I think we need

1014
00:33:38,075 --> 00:33:40,560
0,150 150,255 255,515
to do more.|
|

1015
00:33:42,230 --> 00:33:44,580
0,275 275,470 470,790
Any other questions?|
还有其他问题吗？|

1016
00:33:56,490 --> 00:33:57,290
0,305 305,500 500,635 635,710 710,800
Yeah, um, I had a
是的，呃，我对结果的基数部分有个问题。嗯，LIKE其中一个失败案例表明，LIKE模型如果给它超过六到七件相同的物品，就不能真正处理。但有时，当您没有在提示符中指定任何内容时，它会自动生成比项目数更多的内容。你知道为什么当你给它喜欢的时候它就会分解吗？

1017
00:33:57,290 --> 00:33:58,850
0,195 195,450 450,675 675,885 885,1560
question about like the cardinality

1018
00:33:58,850 --> 00:34:00,910
0,290 400,720 720,960 960,1280 1660,2060
portion of the results. Um,

1019
00:34:01,050 --> 00:34:01,955
0,305 305,455 455,545 545,665 665,905
like one of the failure

1020
00:34:01,955 --> 00:34:03,305
0,365 385,675 675,930 930,1200 1200,1350
cases show that like the

1021
00:34:03,305 --> 00:34:04,415
0,210 210,495 495,645 645,900 900,1110
model can't really handle if

1022
00:34:04,415 --> 00:34:05,120
0,135 135,240 240,360 360,540 540,705
you give it more than

1023
00:34:05,120 --> 00:34:06,035
0,195 195,405 405,555 555,750 750,915
like six or seven of

1024
00:34:06,035 --> 00:34:07,145
0,105 105,285 285,585 585,855 855,1110
the same item. But sometimes

1025
00:34:07,145 --> 00:34:08,705
0,225 225,390 390,795 795,1380 1380,1560
when you don't specify anything

1026
00:34:08,705 --> 00:34:09,845
0,90 90,210 210,435 435,765 765,1140
in the prompt, it automatically

1027
00:34:09,845 --> 00:34:12,095
0,480 480,785 1465,1800 1800,2010 2010,2250
generates like more than that

1028
00:34:12,095 --> 00:34:12,970
0,225 225,330 330,480 480,630 630,875
of the number of items.

1029
00:34:12,990 --> 00:34:13,940
0,230 230,335 335,530 530,755 755,950
Do you know why it

1030
00:34:13,940 --> 00:34:14,840
0,225 225,480 480,645 645,765 765,900
breaks down when you give

1031
00:34:14,840 --> 00:34:15,900
0,195 195,530
it like?|
|

1032
00:34:15,900 --> 00:34:17,000
0,300 300,495 495,660 660,825 825,1100
Six or seven or eight,
六个、七个或八个，我想这是我的感觉。再说一次，我们没有分析，这只是因为数据中更常见的小数字更少。啊，明白了。你知道，大多数图像只会有几个，两个，三个，四个，然后模型就从来没有见过。

1033
00:34:17,500 --> 00:34:19,130
0,275 275,470 470,880 990,1310 1310,1630
I think it's my feeling.

1034
00:34:19,270 --> 00:34:20,625
0,400 450,740 740,860 860,1040 1040,1355
And again, we haven't analyzed

1035
00:34:20,625 --> 00:34:21,590
0,180 180,330 330,495 495,720 720,965
this is that it's just

1036
00:34:21,730 --> 00:34:24,300
0,400 510,910 1770,2060 2060,2315 2315,2570
that fewer small numbers are

1037
00:34:24,300 --> 00:34:25,310
0,210 210,450 450,630 630,750 750,1010
more common in the data.

1038
00:34:25,450 --> 00:34:27,770
0,350 350,880 1560,1820 1820,2000 2000,2320
Ah, gotcha. You know, most

1039
00:34:27,910 --> 00:34:29,160
0,400 480,755 755,920 920,1085 1085,1250
images would just have a

1040
00:34:29,160 --> 00:34:31,230
0,210 210,480 480,750 750,1070 1780,2070
few, two, three, four, and

1041
00:34:31,230 --> 00:34:32,115
0,225 225,405 405,555 555,735 735,885
then the model has just

1042
00:34:32,115 --> 00:34:33,480
0,240 240,605
never seen.|
|

1043
00:34:33,490 --> 00:34:36,300
0,400 600,1000 1350,1685 1685,2020 2550,2810
Ah, ten or twenty. And
啊，十个或二十个。还有，我们训练它的方式也不一样。不存在优雅地从十个人退化到许多人的概念，那里可能只会说，好吧，一群人，然后你就会产生。

1044
00:34:36,300 --> 00:34:38,340
0,260 280,690 690,1010 1090,1490 1780,2040
then there's also not the

1045
00:34:38,340 --> 00:34:39,225
0,150 150,330 330,495 495,660 660,885
way we train it. There's

1046
00:34:39,225 --> 00:34:40,515
0,135 135,315 315,585 585,870 870,1290
not this concept of graceful

1047
00:34:40,515 --> 00:34:42,140
0,515 535,870 870,1140 1140,1350 1350,1625
degradation from ten to many,

1048
00:34:43,000 --> 00:34:44,130
0,335 335,560 560,725 725,950 950,1130
where might just say, okay,

1049
00:34:44,130 --> 00:34:45,300
0,120 120,285 285,450 450,740 910,1170
a crowd of people and

1050
00:34:45,300 --> 00:34:46,420
0,120 120,255 255,530
then you generate.|
|

1051
00:34:46,860 --> 00:34:48,700
0,305 305,545 545,800 800,1120 1440,1840
What looks like thirty people?
三十个人看起来像什么？我想我们需要更多。已经有一些报纸试图解决这个问题。我认为，仅仅通过丰富数据就可以了。

1052
00:34:50,400 --> 00:34:51,370
0,275 275,410 410,515 515,665 665,970
I think we need more.

1053
00:34:51,570 --> 00:34:52,790
0,350 350,560 560,695 695,965 965,1220
There have been papers that

1054
00:34:52,790 --> 00:34:53,840
0,150 150,360 360,525 525,765 765,1050
are trying to overcome this

1055
00:34:53,840 --> 00:34:56,735
0,290 1510,1770 1770,2030 2080,2685 2685,2895
problem. I think explicitly through

1056
00:34:56,735 --> 00:34:59,020
0,305 475,1095 1095,1230 1230,1475
just enriching the data.|
|

1057
00:34:59,040 --> 00:35:00,530
0,180 180,390 390,710 820,1155 1155,1490
I think more clever solutions
我认为还需要更聪明的解决方案。

1058
00:35:00,610 --> 00:35:02,540
0,260 260,520
are needed.|
|

1059
00:35:06,380 --> 00:35:09,540
0,350 350,700 1740,2045 2045,2345 2345,3160
When you do not specify
当您未指定请求的背景时。

1060
00:35:09,860 --> 00:35:11,880
0,290 290,580 1200,1460 1460,1670 1670,2020
a background of a request.|
|

1061
00:35:13,110 --> 00:35:15,010
0,400 780,1055 1055,1205 1205,1480 1500,1900
How does it happen that
为什么它必须随机选择有限数量的背景？

1062
00:35:15,210 --> 00:35:17,585
0,290 290,455 455,605 605,880 2010,2375
is there a limited amount

1063
00:35:17,585 --> 00:35:19,475
0,365 565,1425 1425,1665 1665,1755 1755,1890
of backgrounds that it has

1064
00:35:19,475 --> 00:35:21,340
0,195 195,405 405,615 615,1295
to choose from randomly?|
|

1065
00:35:21,380 --> 00:35:23,380
0,400 570,970 1200,1490 1490,1745 1745,2000
Or ah, is there some
或者啊，有没有一种生成器，当你不指定背景时，例如，它是如何工作的？是啊，这是个很好的问题。所以我们做的一件事就是在文本提示符中输入无稽之谈，然后看看会发生什么，它似乎只会生成随机的场景。

1066
00:35:23,380 --> 00:35:25,110
0,150 150,270 270,435 435,950 1330,1730
sort of a generator that

1067
00:35:25,310 --> 00:35:27,010
0,400 870,1145 1145,1310 1310,1490 1490,1700
just how does it work

1068
00:35:27,010 --> 00:35:28,180
0,180 180,315 315,450 450,645 645,1170
when you do not specify

1069
00:35:28,180 --> 00:35:29,545
0,180 180,440 580,900 900,1155 1155,1365
the background, for example at

1070
00:35:29,545 --> 00:35:30,715
0,275 475,765 765,945 945,1020 1020,1170
all? Yeah, it's a great

1071
00:35:30,715 --> 00:35:31,990
0,305 595,855 855,1005 1005,1155 1155,1275
question. So one thing we

1072
00:35:31,990 --> 00:35:32,890
0,150 150,330 330,525 525,720 720,900
did was just type in

1073
00:35:32,890 --> 00:35:34,360
0,525 525,855 855,1050 1050,1230 1230,1470
nonsense into the text prompt

1074
00:35:34,360 --> 00:35:35,890
0,225 225,405 405,555 555,830 1270,1530
and see what happens, and

1075
00:35:35,890 --> 00:35:37,090
0,195 195,450 450,645 645,900 900,1200
it seems to generate just

1076
00:35:37,090 --> 00:35:38,700
0,320 400,800
random scenes.|
|

1077
00:35:38,700 --> 00:35:40,830
0,300 300,570 570,890 1240,1640 1870,2130
Ah, like of of, you
啊，就像，你知道的，山和海滩等等。它不会产生无稽之谈。因此，我们认为潜在空间中的码本是由这类背景主导的。

1078
00:35:40,830 --> 00:35:42,120
0,150 150,440 610,900 900,1050 1050,1290
know, mountains and the beach

1079
00:35:42,120 --> 00:35:43,215
0,285 285,435 435,585 585,795 795,1095
and so on. It doesn't

1080
00:35:43,215 --> 00:35:44,955
0,240 240,845 1225,1485 1485,1605 1605,1740
generate nonsense. So we think

1081
00:35:44,955 --> 00:35:46,050
0,135 135,285 285,480 480,785 835,1095
that the code book in

1082
00:35:46,050 --> 00:35:47,385
0,105 105,405 405,615 615,825 825,1335
the latent space is dominated

1083
00:35:47,385 --> 00:35:48,980
0,300 300,525 525,765 765,945 945,1595
by these kinds of backgrounds.|
|

1084
00:35:49,520 --> 00:35:52,670
0,320 1150,1550 2200,2610 2610,2850 2850,3150
And somehow that what gets
不知何故，当你通过解码器时，这些东西就会被输入。

1085
00:35:52,670 --> 00:35:53,750
0,240 240,480 480,705 705,870 870,1080
Fed in when you go

1086
00:35:53,750 --> 00:35:55,120
0,180 180,315 315,920
through the decoder.|
|

1087
00:35:55,360 --> 00:35:58,290
0,400 2160,2495 2495,2675 2675,2825 2825,2930
So, yeah, I don't have
所以，是的，我没有比这更好的答案了。

1088
00:35:58,290 --> 00:35:59,360
0,120 120,285 285,555 555,795 795,1070
a better answer than that.|
|

1089
00:36:02,910 --> 00:36:04,150
0,395 395,695 695,860 860,980 980,1240
Hi, thanks for the talk.
嗨，谢谢你的谈话。我把面具画得有点儿自由。非常有趣。你展示的很多提示都有。

1090
00:36:05,370 --> 00:36:06,515
0,290 290,425 425,530 530,770 770,1145
I kind of the mask

1091
00:36:06,515 --> 00:36:08,410
0,300 300,495 495,785 1045,1445 1495,1895
free in painting. Super interesting.

1092
00:36:09,270 --> 00:36:10,055
0,260 260,365 365,440 440,545 545,785
A lot of the prompts

1093
00:36:10,055 --> 00:36:11,940
0,150 150,455 595,995
you showed had.|
|

1094
00:36:12,070 --> 00:36:14,040
0,305 305,605 605,1000 1410,1775 1775,1970
Like the the correction was
就像上一次的调整一样，这次调整可能与下一次下滑相比变化不大。我想还有一个抱歉，我在找那个嘴里叼着足球的狗。

1095
00:36:14,040 --> 00:36:15,465
0,255 255,630 630,960 960,1215 1215,1425
a small change from maybe

1096
00:36:15,465 --> 00:36:16,370
0,150 150,300 300,495 495,645 645,905
the next slide. I think

1097
00:36:18,010 --> 00:36:19,140
0,275 275,485 485,770 770,1010 1010,1130
one more sorry, I'm looking

1098
00:36:19,140 --> 00:36:19,680
0,135 135,210 210,300 300,420 420,540
for the one with the

1099
00:36:19,680 --> 00:36:20,385
0,150 150,285 285,450 450,615 615,705
dog with the football in

1100
00:36:20,385 --> 00:36:21,400
0,120 120,395
his mouth.|
|

1101
00:36:21,770 --> 00:36:23,590
0,400 1230,1490 1490,1595 1595,1700 1700,1820
Yeah, many of these are
是的，其中许多都是与里面的东西不同的小变化，对吗？你想把它作为一个装着苹果的篮子来输入，而你说的是橘子？你试过了吗？如果喜欢编辑文本是完全不同的，就像你从一只带着篮球的狗变成了一只猫保龄球或类似的东西？是的，甚至有一些疯狂的东西，比如一座城市。

1102
00:36:23,590 --> 00:36:24,550
0,105 105,285 285,510 510,750 750,960
kind of small changes from

1103
00:36:24,550 --> 00:36:25,375
0,195 195,285 285,435 435,585 585,825
what's in there, right? You're

1104
00:36:25,375 --> 00:36:26,695
0,210 210,545 775,1020 1020,1125 1125,1320
going like give it the

1105
00:36:26,695 --> 00:36:27,580
0,195 195,300 300,405 405,660 660,885
input as a basket with

1106
00:36:27,580 --> 00:36:28,225
0,300 300,375 375,450 450,510 510,645
apples in it and you're

1107
00:36:28,225 --> 00:36:29,520
0,135 135,615 615,795 795,975 975,1295
saying oranges? Have you tried?

1108
00:36:30,020 --> 00:36:32,395
0,400 600,1000 1020,1420 1470,2060 2060,2375
If the like editing text

1109
00:36:32,395 --> 00:36:34,360
0,365 805,1155 1155,1505 1525,1800 1800,1965
is completely different, like you

1110
00:36:34,360 --> 00:36:35,605
0,225 225,560 730,1035 1035,1170 1170,1245
go from dog with a

1111
00:36:35,605 --> 00:36:37,480
0,245 385,785 985,1275 1275,1485 1485,1875
basketball to a cat bowling

1112
00:36:37,480 --> 00:36:38,455
0,135 135,345 345,555 555,720 720,975
or something like that? Yep,

1113
00:36:38,455 --> 00:36:39,910
0,300 300,525 525,870 870,1185 1185,1455
there even something crazy like

1114
00:36:39,910 --> 00:36:41,260
0,240 240,560
a city.|
|

1115
00:36:41,330 --> 00:36:43,000
0,260 260,515 515,790 1230,1490 1490,1670
That doesn't work. So we've
这不管用。所以我们已经试过了。啊，不是的。所以我觉得就像指示挑人挑菜啊，它是管用的，因为他们明确地用大编辑来训练它。

1116
00:36:43,000 --> 00:36:44,850
0,150 150,440 730,1095 1095,1350 1350,1850
tried that. Ah, it doesn't.

1117
00:36:45,560 --> 00:36:46,675
0,260 260,380 380,560 560,845 845,1115
So I think something like

1118
00:36:46,675 --> 00:36:48,090
0,285 285,585 585,855 855,990 990,1415
the instruct pickics to pickics

1119
00:36:48,560 --> 00:36:50,110
0,400 420,800 800,1070 1070,1325 1325,1550
ah, that it works for

1120
00:36:50,110 --> 00:36:52,000
0,260 370,690 690,1010 1030,1680 1680,1890
that because they explicitly train

1121
00:36:52,000 --> 00:36:54,520
0,290 370,770 880,1275 1275,1820
it with large edits.|
|

1122
00:36:54,950 --> 00:36:55,945
0,275 275,410 410,530 530,755 755,995
Part of the problem could
部分问题可能是我们进行编辑的方式，这是基于这些小的背景步骤，只允许您进行本地更改。所以我们不知道这是模型的限制还是梯度的限制。我们在编辑时所做的S的G D步骤。因此，编辑过程是从原始图像开始，然后接受文本提示，然后一直保持支撑，直到。

1123
00:36:55,945 --> 00:36:56,725
0,150 150,285 285,435 435,615 615,780
be the way we do

1124
00:36:56,725 --> 00:36:57,655
0,120 120,480 480,615 615,750 750,930
the editing, which is based

1125
00:36:57,655 --> 00:36:59,160
0,180 180,390 390,645 645,1170 1170,1505
on these small backdrop steps

1126
00:36:59,450 --> 00:37:00,415
0,275 275,500 500,725 725,860 860,965
which just allow you to

1127
00:37:00,415 --> 00:37:02,305
0,120 120,375 375,755 1555,1800 1800,1890
do local changes. So we

1128
00:37:02,305 --> 00:37:03,420
0,180 180,345 345,555 555,840 840,1115
don't know if it's a

1129
00:37:04,550 --> 00:37:06,190
0,515 515,710 710,845 845,1120 1350,1640
limitation of the model or

1130
00:37:06,190 --> 00:37:08,070
0,150 150,615 615,945 945,1305 1305,1880
a limitation of the gradient.

1131
00:37:08,390 --> 00:37:10,345
0,400 780,1085 1085,1250 1250,1510 1620,1955
The S G D steps

1132
00:37:10,345 --> 00:37:11,695
0,225 225,450 450,785 895,1215 1215,1350
we do when we're doing

1133
00:37:11,695 --> 00:37:12,670
0,135 135,480 480,630 630,750 750,975
the editing. So what happens

1134
00:37:12,670 --> 00:37:14,065
0,195 195,270 270,630 630,890 1090,1395
with the editing is start

1135
00:37:14,065 --> 00:37:15,745
0,150 150,240 240,485 505,905 1405,1680
with the original image, then

1136
00:37:15,745 --> 00:37:16,705
0,150 150,285 285,480 480,750 750,960
take the text prompt and

1137
00:37:16,705 --> 00:37:18,360
0,165 165,345 345,555 555,1055 1255,1655
just keep back propping till.|
|

1138
00:37:18,900 --> 00:37:20,000
0,90 90,195 195,405 405,795 795,1100
You know, it settles down,
你知道，它会稳定下来，会聚在一起。

1139
00:37:20,140 --> 00:37:21,440
0,580
converges.|
|

1140
00:37:21,440 --> 00:37:23,260
0,350
Um.|
恩。|

1141
00:37:23,440 --> 00:37:25,620
0,320 320,620 620,1000 1620,1955 1955,2180
Say hundred steps. And so
比方说一百步。所以像我在这里展示的每一个步骤都是很小的变化。如果你想要像你描述的那样，你需要有在一百步中跳得更快的能力。也许如果你走了一百万步，你就会有这样的改变。我们还没有看到这一点。是的，几乎很难想象这种中间立场会是什么样子。是的，比如，你必须穿过一些不切实际的图像谷才能到达这里的巨大变化吗？这些步骤中的每一个看起来都是可信的。

1142
00:37:25,620 --> 00:37:27,135
0,165 165,315 315,495 495,800 1240,1515
each of those steps like

1143
00:37:27,135 --> 00:37:29,025
0,240 240,555 555,905 1015,1415 1525,1890
I showed here is small

1144
00:37:29,025 --> 00:37:30,780
0,365 1135,1380 1380,1485 1485,1605 1605,1755
changes. And if you want

1145
00:37:30,780 --> 00:37:31,920
0,240 240,480 480,645 645,870 870,1140
something like what you describe,

1146
00:37:31,920 --> 00:37:33,030
0,180 180,345 345,630 630,915 915,1110
you need to have the

1147
00:37:33,030 --> 00:37:34,070
0,240 240,435 435,540 540,720 720,1040
ability to kind of jump

1148
00:37:34,360 --> 00:37:36,690
0,400 1140,1415 1415,1670 1670,2030 2030,2330
faster through hundred steps. Maybe

1149
00:37:36,690 --> 00:37:37,320
0,180 180,300 300,405 405,525 525,630
if you did it for

1150
00:37:37,320 --> 00:37:38,385
0,90 90,345 345,690 690,915 915,1065
a million steps, you could

1151
00:37:38,385 --> 00:37:39,645
0,165 165,360 360,665 715,990 990,1260
have that change. We haven't

1152
00:37:39,645 --> 00:37:41,325
0,255 255,525 525,690 690,965 1345,1680
yet looked at that. Yeah,

1153
00:37:41,325 --> 00:37:42,615
0,285 285,575 625,930 930,1095 1095,1290
it's almost hard to imagine

1154
00:37:42,615 --> 00:37:43,980
0,255 255,465 465,750 750,1095 1095,1365
even what that middle ground

1155
00:37:43,980 --> 00:37:45,615
0,195 195,360 360,630 630,1010 1240,1635
would look like. Yes, like

1156
00:37:45,615 --> 00:37:46,485
0,240 240,345 345,510 510,675 675,870
do you have to pass

1157
00:37:46,485 --> 00:37:48,780
0,210 210,485 925,1290 1290,1655 1705,2295
through some valley of unrealistic

1158
00:37:48,780 --> 00:37:49,830
0,260 340,585 585,750 750,915 915,1050
images to get to this

1159
00:37:49,830 --> 00:37:51,585
0,210 210,465 465,800 1180,1530 1530,1755
very large change here? Each

1160
00:37:51,585 --> 00:37:52,755
0,150 150,330 330,570 570,885 885,1170
of those sub steps look

1161
00:37:52,755 --> 00:37:54,620
0,305 325,930 930,1265
like believable things.|
|

1162
00:37:55,510 --> 00:37:56,880
0,400 720,995 995,1130 1130,1220 1220,1370
So it could be an
所以这可能是一个最优化问题。谢谢。

1163
00:37:56,880 --> 00:37:59,220
0,570 570,950 1270,1545 1545,1820
optimization problem. Thank you.|
|

1164
00:38:00,660 --> 00:38:01,970
0,395 395,725 725,935 935,1100 1100,1310
Maybe extension on that same
也许可以延伸到同样的问题上。我很好奇这种情况，不是整个图像都在变化，但可能是在图像层面上，就像风格一样。例如，喜欢的主要内容保持不变，但风格正在改变和修改。这是你以前尝试过这种类型的东西吗？

1165
00:38:01,970 --> 00:38:04,130
0,320 370,750 750,1010 1420,1820 1900,2160
question. I'm curious for the

1166
00:38:04,130 --> 00:38:05,465
0,165 165,345 345,620 880,1155 1155,1335
case where the not the

1167
00:38:05,465 --> 00:38:06,665
0,195 195,420 420,660 660,945 945,1200
entire image is changing, but

1168
00:38:06,665 --> 00:38:08,630
0,275 835,1155 1155,1335 1335,1595 1615,1965
maybe at the image level

1169
00:38:08,630 --> 00:38:09,820
0,300 300,510 510,735 735,900 900,1190
you're changing, like the style.

1170
00:38:09,870 --> 00:38:11,720
0,305 305,610 1230,1520 1520,1670 1670,1850
For example, like the main

1171
00:38:11,720 --> 00:38:13,580
0,320 340,740 880,1280 1420,1695 1695,1860
content is maintained, but the

1172
00:38:13,580 --> 00:38:15,260
0,195 195,405 405,710 1150,1485 1485,1680
style is being changed and

1173
00:38:15,260 --> 00:38:17,030
0,530 550,825 825,1100 1420,1665 1665,1770
modified. Is that have you

1174
00:38:17,030 --> 00:38:17,870
0,150 150,330 330,495 495,645 645,840
tried these type of things

1175
00:38:17,870 --> 00:38:19,860
0,320
before?|
|

1176
00:38:21,830 --> 00:38:23,605
0,400 630,1010 1010,1280 1280,1490 1490,1775
Ah, yes, I think so.
啊，是的，我想是的。比如，我没有这样的例子，也许是这样。

1177
00:38:23,605 --> 00:38:25,110
0,365 625,870 870,1005 1005,1170 1170,1505
Like, I don't have example

1178
00:38:25,160 --> 00:38:27,420
0,380 380,710 710,950 950,1240
maybe something like this.|
|

1179
00:38:27,910 --> 00:38:31,395
0,350 350,635 635,860 860,1150 3210,3485
Maybe something like this, I
我想，大概是这样吧。

1180
00:38:31,395 --> 00:38:32,300
0,275
think.|
|

1181
00:38:33,510 --> 00:38:34,775
0,305 305,545 545,785 785,1055 1055,1265
It seems much harder to
像这样做现实的停顿改变似乎要困难得多。

1182
00:38:34,775 --> 00:38:36,785
0,135 135,815 955,1350 1350,1740 1740,2010
do realistic pause changes like

1183
00:38:36,785 --> 00:38:38,340
0,150 150,425
the kinds.|
|

1184
00:38:39,020 --> 00:38:40,855
0,275 275,410 410,635 635,1000 1530,1835
He was asking about compared
他问的是与这些全球风格的变化相比，因为我认为全球风格可能由代码簿的一两个元素或类似的东西控制。

1185
00:38:40,855 --> 00:38:42,240
0,165 165,360 360,690 690,1035 1035,1385
to these global style changes,

1186
00:38:42,380 --> 00:38:43,255
0,245 245,365 365,515 515,650 650,875
because I think the global

1187
00:38:43,255 --> 00:38:45,205
0,255 255,545 565,930 930,1295 1585,1950
style is controlled by maybe

1188
00:38:45,205 --> 00:38:47,050
0,240 240,515 1045,1305 1305,1565 1585,1845
one or two elements of

1189
00:38:47,050 --> 00:38:48,010
0,135 135,315 315,555 555,765 765,960
the code book or something

1190
00:38:48,010 --> 00:38:49,080
0,195 195,470
like that.|
|

1191
00:38:49,300 --> 00:38:52,040
0,400 450,850 1620,1925 1925,2230 2280,2740
Whereas to to change pose
然而，要改变姿势或进行剧烈的几何变化，可能需要不同令牌之间更多的交互。

1192
00:38:52,120 --> 00:38:55,155
0,400 720,1120 1230,1660 2160,2750 2750,3035
or make drastic geometry changes

1193
00:38:55,155 --> 00:38:56,780
0,375 375,690 690,945 945,1245 1245,1625
might require much more interaction

1194
00:38:56,860 --> 00:38:59,100
0,275 275,395 395,605 605,1150
among the different tokens.|
|

1195
00:39:04,180 --> 00:39:05,320
0,260
The.|
这个。|

1196
00:39:08,710 --> 00:39:09,705
0,350 350,575 575,725 725,860 860,995
Hi, thank you for that
嗨，谢谢你的精彩演讲。

1197
00:39:09,705 --> 00:39:10,840
0,180 180,485
great talk.|
|

1198
00:39:10,840 --> 00:39:11,980
0,210 210,345 345,620 640,945 945,1140
I was wondering, how does
我想知道，模型如何确定哪张图片更符合描述，或者哪张图片的分辨率更高？模型的一部分决定了这一点，而不需要人类的监督。

1199
00:39:11,980 --> 00:39:13,915
0,165 165,440 490,825 825,1160 1570,1935
the model determine which picture

1200
00:39:13,915 --> 00:39:15,940
0,315 315,665 955,1355 1555,1800 1800,2025
is more accurate to the

1201
00:39:15,940 --> 00:39:18,880
0,380 490,890 1000,1400 1540,1940 2650,2940
description or which is which

1202
00:39:18,880 --> 00:39:21,250
0,180 180,330 330,525 525,860 2110,2370
has a better resolution? Part

1203
00:39:21,250 --> 00:39:22,500
0,105 105,195 195,440 610,990 990,1250
of the model determines that

1204
00:39:22,940 --> 00:39:25,440
0,400 720,1175 1175,1510 1800,2210 2210,2500
without needing to needing the

1205
00:39:25,610 --> 00:39:27,820
0,400 450,1180
human oversight.|
|

1206
00:39:28,370 --> 00:39:30,450
0,260 260,410 410,850 1440,1760 1760,2080
So it doesn't, in practice,
所以在实践中并不是这样，你是在说训练还是推理？

1207
00:39:31,310 --> 00:39:32,530
0,400 540,785 785,890 890,1025 1025,1220
well, are you talking about

1208
00:39:32,530 --> 00:39:34,920
0,320 340,735 735,1340
training or inference?|
|

1209
00:39:35,330 --> 00:39:37,200
0,320 320,515 515,620 620,1030 1470,1870
Ah, with the infants. So
啊，和婴儿在一起。所以我们所做的就是使用随机的种子，生成一大堆图像，然后我们只需挑选出我们喜欢的一个。经常发生的情况是，如果你有8张或16张图片，其中3或4张会很好，但有几张看起来会很糟糕。

1210
00:39:37,280 --> 00:39:38,440
0,335 335,545 545,680 680,890 890,1160
we what we do is

1211
00:39:38,440 --> 00:39:40,000
0,320 370,690 690,1005 1005,1395 1395,1560
just use random seeds and

1212
00:39:40,000 --> 00:39:40,900
0,225 225,405 405,555 555,750 750,900
generate a whole bunch of

1213
00:39:40,900 --> 00:39:42,415
0,260 820,1080 1080,1215 1215,1350 1350,1515
images and then we just

1214
00:39:42,415 --> 00:39:43,240
0,180 180,360 360,525 525,675 675,825
pick out the one we

1215
00:39:43,240 --> 00:39:44,815
0,260 430,735 735,1040 1060,1335 1335,1575
like. So often what happens

1216
00:39:44,815 --> 00:39:45,490
0,225 225,345 345,465 465,555 555,675
is that if you have

1217
00:39:45,490 --> 00:39:47,605
0,270 270,630 630,900 900,1190 1870,2115
eight images or sixteen, three

1218
00:39:47,605 --> 00:39:48,235
0,105 105,225 225,345 345,480 480,630
or four of them would

1219
00:39:48,235 --> 00:39:49,495
0,135 135,315 315,635 835,1125 1125,1260
be really nice and then

1220
00:39:49,495 --> 00:39:50,095
0,120 120,225 225,315 315,450 450,600
a few of them would

1221
00:39:50,095 --> 00:39:51,320
0,150 150,345 345,665
look really bad.|
|

1222
00:39:51,330 --> 00:39:52,955
0,400 750,1040 1040,1205 1205,1445 1445,1625
And we still don't have
我们仍然没有一种自我纠正的方式或自动的方式来说这张图片更符合提示。

1223
00:39:52,955 --> 00:39:55,055
0,335 715,1110 1110,1605 1605,1815 1815,2100
a self correcting way or

1224
00:39:55,055 --> 00:39:56,615
0,555 555,795 795,990 990,1260 1260,1560
automated way to say this

1225
00:39:56,615 --> 00:39:58,090
0,315 315,660 660,900 900,1125 1125,1475
image matches the prompt better.|
|

1226
00:39:58,950 --> 00:40:00,900
0,400
Um.|
恩。|

1227
00:40:00,940 --> 00:40:04,000
0,400 870,1250 1250,1580 1580,1930
So yeah, so so.|
所以是啊，一般吧。|

1228
00:40:04,190 --> 00:40:05,215
0,275 275,440 440,665 665,860 860,1025
So in general, the hope
因此，总的来说，人们希望潜在的空间已经以一种合理的方式进行了训练，这样它就会产生可信的图像。但是，例如，你可能会产生一只猫，你知道，腿都在错误的位置，然后。

1229
00:40:05,215 --> 00:40:07,135
0,195 195,485 565,965 1225,1680 1680,1920
is that the latent space

1230
00:40:07,135 --> 00:40:07,900
0,210 210,360 360,510 510,645 645,765
has been trained in a

1231
00:40:07,900 --> 00:40:09,085
0,390 390,645 645,855 855,990 990,1185
sensible way so that it

1232
00:40:09,085 --> 00:40:12,630
0,195 195,485 565,1125 1125,1415 3145,3545
will generate plausible images. But,

1233
00:40:12,680 --> 00:40:14,670
0,320 320,640 960,1220 1220,1480 1590,1990
for example, you might generate

1234
00:40:14,690 --> 00:40:15,760
0,290 290,545 545,815 815,965 965,1070
a cat with, you know,

1235
00:40:15,760 --> 00:40:16,690
0,120 120,330 330,615 615,810 810,930
the legs all in the

1236
00:40:16,690 --> 00:40:18,820
0,225 225,590 1060,1320 1320,1580
wrong position and then.|
|

1237
00:40:18,820 --> 00:40:19,900
0,300 300,495 495,750 750,930 930,1080
It's still using the right
它仍在使用代码簿中正确的元素，但排列错误。因此，我们确实看到了这类错误。好吧?那么，对于你提高分辨率的下一步，你想象你会以同样的方式去做吗？嗯。

1238
00:40:19,900 --> 00:40:20,935
0,285 285,525 525,690 690,870 870,1035
elements of the code book,

1239
00:40:20,935 --> 00:40:22,290
0,135 135,330 330,615 615,825 825,1355
but it arranged them wrongly.

1240
00:40:22,370 --> 00:40:23,575
0,400 420,680 680,830 830,1010 1010,1205
So we do see those

1241
00:40:23,575 --> 00:40:25,710
0,225 225,525 525,905 1375,1755 1755,2135
kinds of mistakes. Okay? So

1242
00:40:25,760 --> 00:40:26,940
0,305 305,485 485,680 680,890 890,1180
for your next step in

1243
00:40:27,050 --> 00:40:28,500
0,320 320,485 485,730 930,1190 1190,1450
improving the resolution, you imagine

1244
00:40:29,090 --> 00:40:29,890
0,305 305,425 425,560 560,680 680,800
you'd go about it the

1245
00:40:29,890 --> 00:40:32,300
0,135 135,410 640,1040
same way? Yeah.|
|

1246
00:40:32,300 --> 00:40:33,140
0,210 210,360 360,435 435,570 570,840
Yeah, we d have to
是的，我们必须设法解决这些问题。

1247
00:40:33,140 --> 00:40:35,600
0,380 400,800 970,1260 1260,1550
somehow fix those issues.|
|

1248
00:40:36,630 --> 00:40:39,360
0,320 320,530 530,695 695,970
Okay, one more question.|
好了，还有一个问题。|

1249
00:40:46,480 --> 00:40:47,460
0,400 420,665 665,785 785,905 905,980
EM, so I kind of
嗯，所以我有两个问题，一个问题，所以我的第一个问题是，我不确定文本和图像语料库的大小限制是什么，但可以说，在接下来的两个月里，有一个新的或只是还没有出来的或类似遗嘱的东西。如果我在提示符中问，我想让你用这个还没有发明的东西的风格来画这个，这个模型会对这种变化有什么反应？而EM，一个单独的问题是关于这些的，特别是带有蒙版EM图像的示例。

1250
00:40:47,460 --> 00:40:48,650
0,150 150,345 345,615 615,900 900,1190
have two questions in one

1251
00:40:48,760 --> 00:40:50,210
0,350 350,590 590,785 785,1070 1070,1450
so my first question is

1252
00:40:50,890 --> 00:40:52,380
0,290 290,470 470,820 1050,1325 1325,1490
I'm not sure what are

1253
00:40:52,380 --> 00:40:54,225
0,135 135,650 760,1095 1095,1430 1510,1845
the limitations on the size

1254
00:40:54,225 --> 00:40:55,580
0,210 210,360 360,795 795,1050 1050,1355
of the corpus for the

1255
00:40:56,080 --> 00:40:58,340
0,400 510,860 860,1130 1130,1450 1860,2260
text and for the images

1256
00:41:00,010 --> 00:41:02,730
0,335 335,670 1620,2045 2045,2350 2400,2720
but say there's like a

1257
00:41:02,730 --> 00:41:04,725
0,320 670,1070 1090,1395 1395,1590 1590,1995
new or just that hasn't

1258
00:41:04,725 --> 00:41:05,895
0,210 210,420 420,720 720,960 960,1170
come out yet or like

1259
00:41:05,895 --> 00:41:06,945
0,270 270,480 480,615 615,825 825,1050
will in the next two

1260
00:41:06,945 --> 00:41:09,195
0,275 1435,1710 1710,1875 1875,2055 2055,2250
months. If I were to

1261
00:41:09,195 --> 00:41:11,570
0,305 925,1215 1215,1410 1410,1715 1975,2375
ask in the prompt say

1262
00:41:11,740 --> 00:41:12,720
0,275 275,425 425,575 575,740 740,980
I want you to draw

1263
00:41:12,720 --> 00:41:13,995
0,300 300,510 510,675 675,945 945,1275
this in the style of

1264
00:41:13,995 --> 00:41:15,980
0,285 285,555 555,905 1045,1605 1605,1985
this thing that isn't invented

1265
00:41:16,000 --> 00:41:17,720
0,400 810,1130 1130,1310 1310,1445 1445,1720
yet, how would the model

1266
00:41:17,950 --> 00:41:19,730
0,365 365,575 575,800 800,1180 1380,1780
react to that change? And

1267
00:41:20,200 --> 00:41:22,190
0,400 720,1010 1010,1265 1265,1610 1610,1990
EM, a separate question is

1268
00:41:22,450 --> 00:41:25,040
0,305 305,610 1620,1985 1985,2270 2270,2590
with these, especially the example

1269
00:41:25,060 --> 00:41:27,140
0,275 275,425 425,1000 1020,1420 1680,2080
with the masked EM images

1270
00:41:27,850 --> 00:41:29,120
0,400
the.|
|

1271
00:41:29,220 --> 00:41:30,940
0,400 720,1025 1025,1235 1235,1430 1430,1720
Top row in the bottom
顶行在底行我们实际上从这些模型吐出的图像中获得了多少新信息？

1272
00:41:30,990 --> 00:41:32,860
0,400 630,920 920,1145 1145,1475 1475,1870
row how much new information

1273
00:41:33,120 --> 00:41:35,255
0,275 275,550 570,970 1290,1690 1800,2135
are we actually getting from

1274
00:41:35,255 --> 00:41:37,880
0,335 1345,1745 1915,2220 2220,2385 2385,2625
these images that the model

1275
00:41:37,880 --> 00:41:39,520
0,300 300,590
spits out?|
|

1276
00:41:40,290 --> 00:41:42,070
0,290 290,640 1140,1460 1460,1535 1535,1780
So it's, it's a great,
所以，这是一个非常非常好的问题。所以对于，对于数据来说。

1277
00:41:43,110 --> 00:41:44,780
0,305 305,610 870,1205 1205,1415 1415,1670
great question. So for the,

1278
00:41:44,780 --> 00:41:46,260
0,225 225,345 345,620
for the data.|
|

1279
00:41:46,260 --> 00:41:48,180
0,290 700,1080 1080,1320 1320,1580 1600,1920
Um, clearly the data is
嗯，很明显，数据偏向于著名艺术家，这就是为什么我们可以说伦勃朗或莫奈的风格。它已经看到了许多这样的例子，因为这是从网络上收集的数据。

1280
00:41:48,180 --> 00:41:50,570
0,435 435,765 765,1130 1270,1670 1990,2390
biased towards famous artists and

1281
00:41:50,770 --> 00:41:51,810
0,365 365,500 500,650 650,815 815,1040
that's why we can say

1282
00:41:51,810 --> 00:41:52,890
0,255 255,540 540,750 750,900 900,1080
things like, you know, in

1283
00:41:52,890 --> 00:41:54,015
0,165 165,315 315,465 465,975 975,1125
the style of rembrandt or

1284
00:41:54,015 --> 00:41:55,215
0,405 405,690 690,870 870,1035 1035,1200
monet. And it has seen

1285
00:41:55,215 --> 00:41:56,850
0,225 225,540 540,780 780,1055 1345,1635
many examples of those because

1286
00:41:56,850 --> 00:41:58,320
0,150 150,270 270,530 610,1100 1180,1470
this is data scraped from

1287
00:41:58,320 --> 00:41:59,180
0,135 135,380
the web.|
|

1288
00:41:59,880 --> 00:42:01,060
0,290 290,440 440,635 635,875 875,1180
If you had a new.|
如果你有一个新的。|

1289
00:42:02,690 --> 00:42:03,655
0,290 290,500 500,695 695,830 830,965
The style of a new
一个新艺术家的风格，目前唯一的方法是通过微调我们将这些人的图像。

1290
00:42:03,655 --> 00:42:05,350
0,275 475,870 870,1215 1215,1500 1500,1695
artist, the only current way

1291
00:42:05,350 --> 00:42:05,950
0,105 105,195 195,300 300,435 435,600
to do it would be

1292
00:42:05,950 --> 00:42:07,195
0,165 165,375 375,855 855,1110 1110,1245
through fine tuning where we

1293
00:42:07,195 --> 00:42:08,770
0,195 195,465 465,815 1075,1395 1395,1575
take those images of the

1294
00:42:08,770 --> 00:42:10,260
0,260
person.|
|

1295
00:42:10,300 --> 00:42:11,360
0,275 275,440 440,590 590,755 755,1060
Pair it with some text
将其与一些文本配对，然后对模型进行训练以生成该文本。这就是梦想展位的方法试图做到的，尽管这是特定的，你知道，对象，而不是风格。但你可以想象用这样的东西来塑造一位新艺术家的风格。

1296
00:42:11,710 --> 00:42:12,750
0,260 260,395 395,545 545,785 785,1040
and then kind of train

1297
00:42:12,750 --> 00:42:13,940
0,150 150,345 345,555 555,810 810,1190
the model to generate that.

1298
00:42:14,350 --> 00:42:15,165
0,260 260,380 380,515 515,650 650,815
This is kind of what

1299
00:42:15,165 --> 00:42:17,265
0,195 195,405 405,815 1345,1740 1740,2100
the dream booth approach tries

1300
00:42:17,265 --> 00:42:18,710
0,210 210,455 565,825 825,1140 1140,1445
to do, although that's specific

1301
00:42:18,850 --> 00:42:21,750
0,400 1200,1445 1445,1655 1655,2020 2580,2900
to, you know, objects rather

1302
00:42:21,750 --> 00:42:23,100
0,195 195,390 390,710 880,1170 1170,1350
than the style. But you

1303
00:42:23,100 --> 00:42:24,660
0,255 255,585 585,950 970,1320 1320,1560
could imagine using something like

1304
00:42:24,660 --> 00:42:26,385
0,240 240,465 465,630 630,920 1450,1725
this for the style of

1305
00:42:26,385 --> 00:42:27,500
0,135 135,255 255,515
a new artist.|
|

1306
00:42:27,930 --> 00:42:29,040
0,400
Um.|
恩。|

1307
00:42:29,320 --> 00:42:30,615
0,350 350,500 500,710 710,980 980,1295
It's not yet zero shot
这还不是零，你只是展示这些例子，然后说，你能根据这个文本提示生成一些东西，但以那种风格吗？

1308
00:42:30,615 --> 00:42:32,055
0,300 300,510 510,735 735,1080 1080,1440
where you just present these

1309
00:42:32,055 --> 00:42:33,240
0,360 360,660 660,870 870,1035 1035,1185
examples and say, can you

1310
00:42:33,240 --> 00:42:34,920
0,290 370,770 1000,1305 1305,1500 1500,1680
generate something based on this

1311
00:42:34,920 --> 00:42:35,970
0,225 225,480 480,660 660,810 810,1050
text prompt, but in that

1312
00:42:35,970 --> 00:42:36,880
0,350
style?|
|

1313
00:42:37,610 --> 00:42:38,410
0,230 230,305 305,515 515,710 710,800
So that's something we would
因此，这是我们愿意努力的方向。

1314
00:42:38,410 --> 00:42:39,980
0,150 150,300 300,510 510,860
like to work towards.|
|

1315
00:42:40,800 --> 00:42:41,980
0,290 290,425 425,725 725,830 830,1180
Regarding the masking, I didn't
关于遮罩，我没有完全理解问题，所以你说了一些关于顶排和底排的事情。

1316
00:42:42,300 --> 00:42:44,045
0,380 380,665 665,845 845,1120 1500,1745
fully follow the question, so

1317
00:42:44,045 --> 00:42:45,125
0,105 105,240 240,480 480,825 825,1080
you said something about the

1318
00:42:45,125 --> 00:42:47,700
0,165 165,330 330,585 585,1175
top and bottom rows.|
|

1319
00:42:48,370 --> 00:42:49,460
0,275 275,455 455,650 650,815 815,1090
Yeah, I think it was
是的，我认为这是其中一张幻灯片，但这是一个非常普遍的问题，我们实际上从这些图片中获得了多少新信息，我想还没有。

1320
00:42:49,780 --> 00:42:51,390
0,260 260,395 395,605 605,940 1260,1610
one of the slides way

1321
00:42:51,390 --> 00:42:53,355
0,330 330,675 675,1040 1630,1875 1875,1965
past this one, but it

1322
00:42:53,355 --> 00:42:54,860
0,105 105,255 255,545 625,1025 1105,1505
was a pretty general question

1323
00:42:55,690 --> 00:42:57,470
0,260 260,520 690,995 995,1300 1380,1780
it was, how much new

1324
00:42:57,730 --> 00:43:01,070
0,400 900,1205 1205,1510 1890,2290 2910,3340
information are we actually gaining

1325
00:43:01,390 --> 00:43:03,740
0,320 320,640 1200,1475 1475,1750 1950,2350
from these, I guess pictures

1326
00:43:03,910 --> 00:43:05,540
0,400 480,1030
that haven't.|
|

1327
00:43:05,640 --> 00:43:06,755
0,380 380,635 635,770 770,920 920,1115
Like no one has seen
就像没人见过的那样。就像骑自行车的熊一样。哦，你是说你是在说记忆和喜欢吗？真的是这样吗？你的意思是，如果这已经在数据集中，训练数据集中？

1328
00:43:06,755 --> 00:43:08,135
0,305 565,855 855,1020 1020,1170 1170,1380
before. Like with the bear

1329
00:43:08,135 --> 00:43:10,655
0,210 210,345 345,605 1525,1925 2275,2520
on the bicycle. Oh, you

1330
00:43:10,655 --> 00:43:12,320
0,245 865,1140 1140,1275 1275,1410 1410,1665
mean are you talking about

1331
00:43:12,320 --> 00:43:14,920
0,770 850,1250 1810,2130 2130,2325 2325,2600
memorization versus like? Is it

1332
00:43:15,120 --> 00:43:16,580
0,400 630,995 995,1175 1175,1265 1265,1460
actually is? Do you mean

1333
00:43:16,580 --> 00:43:17,660
0,195 195,345 345,570 570,840 840,1080
if this is already in

1334
00:43:17,660 --> 00:43:18,755
0,180 180,390 390,690 690,900 900,1095
the data set, the training

1335
00:43:18,755 --> 00:43:19,740
0,285 285,635
data set?|
|

1336
00:43:21,110 --> 00:43:22,920
0,400 660,1060
Um, yeah.|
嗯，是的。|

1337
00:43:23,250 --> 00:43:24,640
0,320 320,485 485,590 590,850 990,1390
Yeah. So this is actually
嗯。因此，这实际上是一个很好的问题，我认为，我认为我们对这个问题仍然没有很好的答案。

1338
00:43:24,780 --> 00:43:26,270
0,335 335,575 575,860 860,1220 1220,1490
a great question, and I

1339
00:43:26,270 --> 00:43:27,485
0,290 370,615 615,825 825,1005 1005,1215
think, I don't think we

1340
00:43:27,485 --> 00:43:28,450
0,180 180,345 345,525 525,690 690,965
still have very good answers

1341
00:43:28,530 --> 00:43:29,820
0,245 245,490
to this.|
|

1342
00:43:29,820 --> 00:43:31,540
0,350
Um.|
恩。|

1343
00:43:31,620 --> 00:43:33,140
0,245 245,395 395,575 575,850 1170,1520
You know, is it is
你知道，这是一个大型语言模型吗？只是出现了以前见过的幻觉吗？只要混合搭配就行了。

1344
00:43:33,140 --> 00:43:34,445
0,225 225,420 420,705 705,1005 1005,1305
a large language model? Just

1345
00:43:34,445 --> 00:43:36,545
0,705 705,1085 1285,1590 1590,1845 1845,2100
hallucinating something seen before? Just

1346
00:43:36,545 --> 00:43:38,360
0,210 210,390 390,665
mix and match.|
|

1347
00:43:38,520 --> 00:43:41,075
0,400 1020,1385 1385,1660 2040,2315 2315,2555
Probably, it's probably this is
可能，它可能也在做类似的事情，它以前见过熊和自行车。

1348
00:43:41,075 --> 00:43:42,245
0,225 225,420 420,705 705,945 945,1170
also doing something like that,

1349
00:43:42,245 --> 00:43:44,470
0,330 330,875 925,1325 1435,1935 1935,2225
where it's seen bears before

1350
00:43:44,490 --> 00:43:46,080
0,305 305,695 695,1000
and bikes and.|
|

1351
00:43:46,080 --> 00:43:47,145
0,260 340,600 600,765 765,930 930,1065
Has been able to put
已经能够以一种可信的方式将它们结合在一起。数据集中不太可能有完全相同的图像，但同样，我们没有。

1352
00:43:47,145 --> 00:43:48,285
0,210 210,435 435,570 570,705 705,1140
them together in a plausible

1353
00:43:48,285 --> 00:43:50,220
0,275 835,1230 1230,1605 1605,1740 1740,1935
way. It's unlikely that the

1354
00:43:50,220 --> 00:43:51,480
0,255 255,525 525,840 840,1110 1110,1260
exact same image was in

1355
00:43:51,480 --> 00:43:52,845
0,105 105,315 315,680 790,1125 1125,1365
the data set, but again,

1356
00:43:52,845 --> 00:43:54,060
0,150 150,360 360,635
we don't have.|
|

1357
00:43:54,510 --> 00:43:56,080
0,335 335,590 590,875 875,1205 1205,1570
Really good tools yet to.|
真正好的工具还没有出现。|

1358
00:43:57,310 --> 00:43:58,665
0,400 540,845 845,1040 1040,1190 1190,1355
Go like like we could
就像我们可以尝试基于某种嵌入、剪辑嵌入或其他方式进行搜索，以寻找相似的图像。啊，但是在我们训练数以亿计的图像的规模上，我们实际上还没有进去看。

1359
00:43:58,665 --> 00:43:59,925
0,225 225,435 435,720 720,1035 1035,1260
try to search based on

1360
00:43:59,925 --> 00:44:01,155
0,210 210,390 390,600 600,960 960,1230
some kind of embedding clip

1361
00:44:01,155 --> 00:44:02,790
0,345 345,480 480,755 1195,1455 1455,1635
embedding or something to look

1362
00:44:02,790 --> 00:44:06,240
0,320 790,1140 1140,1490 2260,2660 3100,3450
for similar images. Ah, but

1363
00:44:06,240 --> 00:44:07,215
0,240 240,435 435,615 615,795 795,975
at the scale at which

1364
00:44:07,215 --> 00:44:08,160
0,105 105,210 210,450 450,750 750,945
we are training hundreds of

1365
00:44:08,160 --> 00:44:09,770
0,210 210,420 420,680 1000,1245 1245,1610
millions of images, we haven't

1366
00:44:09,940 --> 00:44:11,300
0,335 335,560 560,725 725,980 980,1360
actually gone in and look.|
|

1367
00:44:11,770 --> 00:44:12,540
0,260 260,380 380,485 485,605 605,770
To see how much his
看看他的记忆力有多强。

1368
00:44:12,540 --> 00:44:15,220
0,600 600,980
memorization was.|
|

1369
00:44:15,290 --> 00:44:16,760
0,400
New.|
新的。|

1370
00:44:17,000 --> 00:44:18,820
0,400 510,815 815,1445 1445,1700 1700,1820
Combination of concepts. I do
概念的结合。我确实认为是后者，因为这似乎不太可能。

1371
00:44:18,820 --> 00:44:19,990
0,135 135,345 345,450 450,710 790,1170
think it's the latter because

1372
00:44:19,990 --> 00:44:21,780
0,300 300,585 585,1020 1020,1280
it seems unlikely that.|
|

1373
00:44:21,820 --> 00:44:23,235
0,260 260,520 750,1055 1055,1265 1265,1415
You know, these kinds of
你知道，这些类型的图像会在训练数据集中。

1374
00:44:23,235 --> 00:44:24,585
0,245 325,630 630,915 915,1185 1185,1350
images would be in the

1375
00:44:24,585 --> 00:44:26,240
0,210 210,495 495,845
training data set.|
|

1376
00:44:27,530 --> 00:44:29,965
0,275 275,440 440,730 2040,2315 2315,2435
Okay, thank you. Thank you
好的谢谢。非常感谢。让我们再给民主党一次掌声。

1377
00:44:29,965 --> 00:44:31,060
0,120 120,345 345,645 645,795 795,1095
very much. Let's give dp

1378
00:44:31,060 --> 00:44:32,070
0,180 180,360 360,525 525,690 690,1010
one more round of applause.|
|
