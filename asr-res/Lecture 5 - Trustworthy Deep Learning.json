{"Data": {"TaskId": 5710902483, "Status": 2, "StatusStr": "success", "Result": "[0:9.620,1:5.520]  I'm really excited especially for this lecture, which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course, themeis AI. And as you'll see today, themeis AI is a startup actually locally based here in Cambridge. Our mission is to design, advance and deploy the future of AI and trustworthy. I specifically, I'm especially excited about today's lecture because I Co founded themis right here at MIT right here in this very building. In fact, this all stemmed from really the incredible scientific innovation and advances that we created right here just a few floors higher than where you're sitting today. And because of our background in really cutting edge scientific innovation stemming from MIT, themus is very rooted deeply in science and like I said, innovation.\n[1:5.520,2:5.800]  We really aim to advance the future of deep learning and AI and much of our technology has already grown from published research that we've published at top tier peer review conferences in the AI venues around the world and our work has been covered by high profile international media outlets. This scientific innovation with this scientific innovation themeis we are tackling some of the biggest challenges in safety critical AI that exists today. And really that stems from the fact that we want to take all of these amazing advances that you're learning as part of this course and actually achieve them in reality as part of our daily lives and we're working together with leading global industry partners across many different disciplines ranging from robotics, autonomy, health care and more to develop a line of products that will guarantee safe and trustworthy AI and we drive this really eh deeply with our technical engineering and machine learning team.\n[2:5.800,3:5.840]  And our focus is very much on the engineering, very flexible and very modular platforms to scale algorithms towards robust and trustworthy AI. This really enables this deployment towards grand challenges that our society faces with AI today. Specifically the ability for AI solutions today are not very trustworthy at all, even if they may be very high performance on some of the tasks that we study as part of this course. So it's an incredibly exciting time for themis and specific right now where VC backed we're located. Our offices are right here in Cambridge so we're local and we have just closed around the funding so we're actively hiring the best and the brightest engineers like all of you to realize the future of safe and trustworthy AI and we hope that really today's lecture inspires you to join us on this mission to build the future of AI. And with that it's my great pleasure to introduce SOA. SOA is a machine learning scientist.\n[3:5.840,3:44.860]  At themis, she's also the lead ta of this course, intro to deep learning at MIT. Her research at themis focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy AI. And today she'll be teaching us more about specifically the bias and the uncertainty realms of AI algorithms, which are really two key or critical components towards achieving this mission or this vision of safe and trustworthy deployment of AI all around us. So thank you and please give a big, warm round of applause for s.\n[3:46.140,4:4.740]  Thank you so much, Alexander, for the introduction. Hi everyone. I'm salina, I'm a machine learning scientist here at themis AI and the lead ta of the course this year. And today I'm super excited to talk to you all about robust and trustworthy deep learning on behalf of themis.\n[4:6.040,4:51.020]  So over the past decade, we've seen some tremendous growth in artificial intelligence, across safety, critical domains, in the spheres of autonomy and robotics. We now have models that can make critical decisions about things like self driving at a second's notice, and these are paving the way for fully autonomous vehicles and robots, and that's not where this stops in the spheres of medicine and healthcare. Robots are now equipped to conduct life saving surgery. We have algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable. And we have models that can automatically diagnose diseases without intervention from any health care professionals at all. These advances are revolutionary, and they have the potential to change life as we know it today.\n[4:51.480,5:7.480]  But there's another question that we need to ask, which is where are these models in real life? A lot of these technologies were innovated five, ten years ago, but you and I don't see them in our daily lives. So what is, what's the gap here between innovation and deployment?\n[5:9.200,5:41.000]  The reason why you and I can't go by self driving cars or robots don't typically assist in operating rooms is this. These are some headlines about the failures of AI from the last few years alone. In addition to these incredible advances, we've also seen catastrophic failures in every single one of the safety critical domains I just mentioned. These problems range from crashing autonomous vehicles to health care algorithms that don't actually work for everyone, even though they're deployed out in the real world so everyone can use them.\n[5:41.420,6:14.500]  Now, at a first glance, this seems really demoralizing. If these are all of the things wrong with artificial intelligence, how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives in terms of safety, critical deployment? But at themis, this is exactly the type of problem that we solve. We want to bring these advances to the real world, and the way we do this is by innovating in the spheres of safe and trustworthy artificial intelligence in order to bring the things that were developed in research labs around the world to customers like you and me.\n[6:16.260,6:27.100]  And we do this by our core ideology, is that we believe that all of the problems on this slide are underlaid by two key notions. The first is bias.\n[6:27.100,6:50.740]  Bias is what happens when machine learning models do better on some demographics than others. This results in things like facial detection systems, not being able to detect certain faces with high accuracy, Siri not being able to recognize voices with accents, or algorithms that are trained on imbalanced data sets. So what the algorithm believes is a good solution doesn't actually work for everyone in the real world.\n[6:51.960,7:19.460]  And the second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty. This is when models don't know when they can or can't be trusted. And this results in scenarios such as self driving cars continuing to operate in environments when they're not 100% confident instead of giving control to users, or robots being moving around in environments that they've never been in before and have high unfamiliarity with.\n[7:21.060,7:29.200]  And a lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty.\n[7:30.820,7:49.980]  So today in this lecture, we're going to focus on investigating the root causes of all of these problems, these two big challenges to robust deep learning. We'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone. And we'll start by talking about bias.\n[7:50.180,7:59.060]  Bias is a word that we've all heard outside the context of deep learning, but in the context of machine learning, it can be quantified and mathematically defined.\n[7:59.060,8:12.200]  Today we'll talk about how to do this and methods for mitigation of this bias algorithmically and how themus is innovating in these areas in order to bring new algorithms in this space to industries around the world.\n[8:12.200,8:24.780]  Afterwards, we'll talk about uncertainty, which is can we teach a model when it does or doesn't know the answer to to its given task? And we'll talk about the ramifications for this for real world AI.\n[8:27.740,8:34.860]  So what exactly does bias mean, and where is it present in the artificial intelligence lifecycle?\n[8:34.860,9:2.040]  The most intuitive form of bias comes from data. We have two different two main types of bias here. The first is sampling bias, which is when we over sample from some regions of our input, data distribution and under sample from others. A good example of this is a lot of clinical data sets, where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts.\n[9:2.540,9:21.260]  In addition, we also have selection bias at the data portion of the AI life cycle. Think about Apple's series voice recognition algorithm. This model is trained largely on flawless American English, but it's deployed across the real world to be able to recognize voices with accents from all over the world.\n[9:21.780,9:33.900]  The distribution of the model's training data doesn't match the distribution of this type of language in the real world, because American English is highly overrepresented, as opposed to other demographics.\n[9:35.000,9:47.140]  But that's not where, that's not where bias and data stops. These biases can be propagated towards models, training cycles themselves, which is what we'll focus on in the second half of this lecture.\n[9:48.060,10:18.900]  And then once the model is actually deployed, which means it's actually put out into the real world and customers or users can actually get the predictions from it, we may see further biases perpetuated that we haven't seen before. The first of these is distribution shifts. Let's say I have a model that I trained on the past twenty years of data, and then I deploy it into the real world in 2023. This model will probably do fine because the data input distribution is quite similar to data in the training distribution.\n[10:18.900,10:37.480]  But what would happen to this model in 2033 it, it probably would not work as well because the distribution that the data is coming from would shift significantly across this decade. And if we don't continue to update our models with this input stream of data, we're going to have obsolete and incorrect predictions.\n[10:38.440,11:20.340]  And finally, after deployment there, the evaluation aspect. So think back to the Apple Siri example that we've been talking about. EM, if the evaluation metric or the evaluation data set that Siri was evaluated on was also mostly comprised of American English, then to anybody this model will look like it does extremely well, right? It can detect, it, can recognize American English voices with extremely high accuracy and therefore is deployed into the real world. But what about its accuracy on subgroups, on accented voices, on people who for whom English is not their first language? If we don't also test on subgroups in our evaluation metrics, we're going to faceation bias.\n[11:20.540,11:28.720]  So now let's talk about another example in the real world of how bias can perpetuate throughout the course of this artificial intelligence life cycle.\n[11:30.120,11:40.520]  Commercial facial detection systems are everywhere. You actually played around with some of them in lab two when you trained your v on a facial detection data set.\n[11:40.520,11:54.080]  In addition to the lock screens on your cell phones, facial detection systems are also present in the automatic filters that your phone cameras apply whenever you try to take a picture, and they're also used in criminal investigations.\n[11:54.080,12:4.940]  These are three commercial facial detection systems that were deployed, and we'll analyze the biases that might have been present in all of them for in the next few minutes.\n[12:5.600,12:14.520]  So the first thing you may notice is that there is a huge accuracy gap between two different demographics in this plot.\n[12:14.520,12:34.860]  This accuracy gap can get up to 34%. Keep in mind that this facial detection is a binary classification task. Everything is either a face or it's not a face. This means that a randomly initialized model would be expected to have an accuracy of 50% because it's going to randomly assign whether or not something is a face or not.\n[12:35.020,12:46.520]  Some of these facial detection classifiers do only barely better than random on these underrepresented data on these underrepresented samples in this population.\n[12:46.660,13:0.700]  So how did this happen? Why is there such a blatant gap in accuracy between these different demographic groups? And how did these models ever get deployed in the first place? What types of biases were present in these models?\n[13:1.180,13:49.960]  So a lot of facial detection systems exhibit very clear selection bias. This model was likely trained mostly on lighter skin faces and therefore EM learned those much more effectively than it learned to classify darker skin faces. But that's not the only bias that was present. The second bias that's often EM very present in facial detection systems is evaluation bias because originally this data set that you see on the screen is not the data set that these models were evaluated on. They were evaluated on one big bulk dataas set without any classification into subgroups at all. And therefore you can imagine if the dataset was also comprised mostly of lighter skinned faces, these accuracy metrics would be incredibly inflated and therefore would cause unnecessary confidence and we could deploy them into the real world.\n[13:50.560,14:4.200]  In fact, the biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender.\n[14:4.620,14:10.080]  However, there are other ways that data sets can be biased that we haven't yet talked about.\n[14:10.900,14:30.860]  So, so far we've assumed a pretty key assumption in our data set, which is that the number of faces in our data is the exact same as the number of non faces in our data. But you can imagine, especially if you're looking at things like security feeds, this might not always be the case. You might be faced with many more negative samples than positive samples in your data set.\n[14:31.400,14:50.680]  In the most so, what's the problem here? In the most extreme case, we may assign the label non face to every item in the data set, because the model sees items that are labeled as faces so infrequently that it isn't able to learn an accurate class boundary between the two Sam between the two classes.\n[14:52.360,15:31.400]  So how can we mitigate this? This is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets. And the first way that we can EM try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set at a rate EM, we instead sample at a rate that is inversely proportional to the incidence of a class in our data set. So in the previous example, if EM the likelihood if faces were much, if the number of faces was much lower than the number of non faces in our data set, we would sample the faces with a higher probability than the negatives so that the model sees both classes equally.\n[15:33.060,16:5.520]  The second example, the second way we can mitigate class imbalance is through loss re-rating, which is when instead of having every single mistake that the model makes contribute equally to our total loss function, we're weight the samples such that samples from underrepresented classes contribute more to the loss function. So instead of the model assigning every single input face to a as a negative, it'll be highly penalized if it does so because the loss of the faces would contribute more to the total loss function than the loss of the negatives.\n[16:7.140,16:19.780]  And the final way that we can mitigate class imbalance is through batch selection, which is when we choose randomly from classes so that every single batch has an equal number of data points per class.\n[16:22.140,16:25.080]  So is everything solved?\n[16:25.320,16:37.220]  Clearly, there are other forms of bias that exist even when the classes are completely balanced, because the thing that we haven't thought about yet is latent features.\n[16:37.260,17:11.960]  So if you remember from lab two and the last lecture, latent features are the actual represent is the actual representation of this image according to the model. And so far we've mitigated the problem of when we know that we have underrepresented classes, but we haven't mitigated the problem of when we have a lot of variability within the same class. Let's say we have an equal number of faces and negative examples in our data set. What happens if the majority of the faces are from a certain demographic or they have a certain set of features? Can we still apply the techniques that we just learned about?\n[17:11.960,17:19.380]  The answer is that we cannot do this, and the problem is that the bias present right now is in our latent features.\n[17:19.500,17:42.180]  All of these images are labeled with the exact same label, so according to as the model, all we know is that they're all faces. So we have no information about any of these features, only from the label. Therefore, we can't apply any of the previous approaches that we used to mitigate class imbalanced because our classes are balanced, but we have feature imbalance now.\n[17:42.240,17:50.520]  However, we can adapt the previous methods to account for bias in latent features, which we'll do in just a few slides.\n[17:51.900,18:12.020]  So let's unpack this a little bit further. We have our potentially biased data set, and we're trying to build and deploy a model that classifies the faces in a traditional training pipeline. This is what that pipeline would look like. We would train our classifier and we would deploy it into the real world. But this training pipeline doesn't de bias our inputs in any way.\n[18:13.280,18:39.480]  So one thing we could do is label our biased features and then apply resampling. So let's say in reality that this data set was biased on hair color. Most of the data set is made up of people with blonde hair, with faces with black hair and red hair underrepresented. If we knew this information, we could label the hair color of every single person in this data set, and we could apply either sample weighting or loss re-weighting, just as we did previously.\n[18:39.620,18:43.160]  But does anyone want to tell me what the problem is here?\n[18:45.920,18:48.800]  You go through eat samples, it takes a lot of time.\n[18:48.840,19:16.680]  Yeah, so there are a couple problems here, and that's definitely one of them. The first is how do we know that hair color is a biased feature in this data set? Unless we visually inspect every single sample in this data set, we're not going to know what the biased features are. And the second thing is exactly what you said, which is once we have our biased features, going through and annotating every image with this feature is an extremely labor intensive task that is infeasible in the real world.\n[19:17.180,19:27.620]  So now the question is, what if we had a way to automatically learn latent features and use this learn feature representation to de bi a model?\n[19:29.360,20:8.160]  So what we want is a way to learn the features of this data set and then automatically determine the EM samples with the highest feature bias and the samples with the lowest feature bias. We've already learned a method of doing this in the generative modeling lecture. You all learned about variational auto encoders, which are models that learn the latent features of a data set as a recap variational auto encoders work by probabilistically sampling from a learn latent space, and then they decode this new latent vector into back into the original input space, measure the reconstruction loss between the inputs and the outputs, and continue to update their representation of the latent space.\n[20:8.460,20:30.120]  And the reason why we care so much about this latent space is that we want samples that are similar to each other in the input to decode to latent vectors that are very close to each other in this latent space. And samples that are far from each other or samples that are dissimilar to each other in the input should decode encode to latent vectors that are far from each other in the latent space.\n[20:31.960,21:6.440]  So now we'll walk through step by step, a deb biasing algorithm that automatically uses the latent features learned by a variational auto encoder EM to under sample and over sample from regions in our data set EM. Before I start, I want to point out that this deb biasing model is actually the foundation of themos's work. This work comes out of a paper that we published a few years ago that has been demonstrated to deias commercial facial detection algorithms. And it was so impactful that we decided to make it available and work with companies and industries. And that's how themis was started.\n[21:6.540,21:20.800]  So let's first start by training a vae on this data set. The z shown here in this diagram ends up being our latent space, and the latent space automatically captures features that were important for classification.\n[21:20.800,21:47.740]  So here's an example, latent feature that this model captured EM. This is the facial position of an input face. And something that's really crucial here is that we never told the model to calculate, to encode the feature vector of the facial position of a given face. It learned this automatically, because this feature is important for the model to develop a good representation of what a face actually is.\n[21:48.500,22:13.640]  So now that we have our latent structure, we can use it to calculate a distribution of the inputs across every latent variable, and we can estimate a probability distribution depending on that's based on the features of every item in this data set. Essentially, what this means is that we can calculate the probability that a certain combination of features appears in our data set based on the latent space that we just learned.\n[22:13.640,22:20.640]  And then we can over sample denser, sparser areas of this data set and under sample from denser areas of this data set.\n[22:20.640,22:43.020]  So let's say our distribution looks something like this. This is an oversimplification, but for visualization purposes and the denser portions of this data set, we would expect to have a homogeneous skin color and pose in hair color and very good lighting. And then in the sparser portions of this data set, we would expect to see diverse skin color, pose and illumination.\n[22:46.180,23:22.000]  So now that we have this distribution and we know what areas of our distribution are dense and which areas are sparse, we want to under sample areas from the under sample samples that fall in the denser areas of this distribution and over sample data points that fall in the sparser areas of this distribution. So for example, we would probably under sample points with the very common skin colors, hair colors and good lighting that is extremely present in this data set and over sample the diverse images that we saw on the last slide. And this allows us to train in a fair and unbiased manner.\n[23:24.040,23:48.920]  To dig in a little bit more into the math behind how this resampling works, this approach basically approximates the latent space VIA a joint histogram over the individual latent variables. So we have a histogram for every latent variable z sub I, and what the histogram essentially does is it discretizes the continuous distribution so that we can calculate probabilities more easily.\n[23:49.240,24:1.820]  Then we multiply the probabilities together across all of the latent distributions, and then after that we can develop an understanding of the joint distribution of all of the samples in our latent space.\n[24:3.100,24:31.080]  Based on this, we can define the adjusted probability for sampling for a particular data point as follows. The probability of selecting a sample data point X will be based on the latent space of X, such that it is the inverse of the joint approximated distribution. We have a parameter Alpha here, which is a biasing parameter, and as Alpha increases, this probability will tend to the uniform distribution, and if Alpha increases, we tend to de bias more strongly.\n[24:32.500,24:42.620]  And this gives us the final weight of the sample in our data set that we can calculate on the fly and use it to adaptively resample while training.\n[24:43.700,24:57.340]  And so once we apply this biasing, we have pretty remarkable results. This is the original EM graph that shows the accuracy gap between EM, the darker Mills and the lighter Mills in this dataset.\n[24:57.640,25:16.340]  Once we apply the device algorithm, where as Alpha gets smaller, we're devising more and more, as we just talked about, this accuracy gap decreases significantly and that's because we tend to over sample samples with darker skin color and therefore the model learns them better and tends to do better on them.\n[25:16.340,25:23.560]  Keep this algorithm in mind because you're going to need it for the lab three competition, which I'll talk more about towards the end of this lecture.\n[25:25.260,25:54.960]  So, so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias. However, bias is actually far more widespread in machine learning. Consider the example of autonomous driving. Many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. And this is because the data for these cars for these algorithms is actually just collected by cars driving down roads.\n[25:55.000,26:11.900]  However, in some specific cases, you're going to face adverse weather, bad, bad visibility, near collision scenarios. And these are actually the samples that are the most important for the model to learn because they're the hardest samples and they're the samples where the model is most likely to fail.\n[26:11.900,26:33.480]  But in a traditional EM autonomous driving pipeline, these samples are often extremely low, have extremely low representation. So this is an example where using the unsupervised latent biasing that we just talked about, we would be able to up sample these EM important data points and under sample the data points of driving down straight and sunny roads.\n[26:35.460,27:10.020]  Similarly, consider the example of large language models. EM, an extremely famous paper a couple years ago showed that if you put terms that imply female or woman into a large language model powered job search engine, you're going to get roles such as artist or things in the humanities. But if you input similar things but of the male counterpart, you put things like mail into the the search engine. You'll end up with roles for scientists and engineers, so this type of bias also occurs regardless of the task at hand for a specific model.\n[27:11.120,27:37.640]  And finally, let's talk about health care recommendation algorithms. These recommendation algorithms tend to amplify racial biases. A paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care, and that's because of inherent bias in the data set of this model. And so in all of these examples, we can use the above algorithmic bias mitigation method to try and solve these problems and more.\n[27:40.200,28:0.560]  So we just went through how to mitigate some forms of bias in artificial intelligence and where these solutions may be applied. And we talked about a foundational algorithm that themis uses that you all will also be developing today. And for the next part of the lecture, we'll focus on uncertainty or when a model does not know the answer.\n[28:0.560,28:8.320]  We'll talk about why uncertainty is important and how we can estimate it, and also the applications of uncertainty estimation.\n[28:8.440,28:13.720]  So, to start with, what is uncertainty and why is it necessary to compute?\n[28:13.720,28:26.380]  Let's look at the following example. This is a binary classifier that is trained on images of cats and dogs. For every single input, it will output a probability distribution over these two classes.\n[28:27.480,28:42.040]  Now, let's say I give this model an image of a horse. It's never seen a horse before. The horse is clearly neither a cat nor a dog. However, the model has no choice but to output a probability distribution, because that's how this model is structured.\n[28:42.620,29:3.080]  However, what if in addition to this prediction, we also achieved a confidence estimate, in this case the model which should be able to say, I've never seen anything like this before and I have very low confidence in this prediction. So you as the user should not trust my prediction on this model, and that's the core idea behind uncertainty estimation.\n[29:3.540,29:30.460]  So in the real world, uncertainty estimation is useful for scenarios like this. This is an example of a Tesla car driving behind a horse drawn buggy, which are very common in some parts of the United States. It has no idea what this horse drawn buggy is. It first thinks it's a truck and then a car and then a person, and it continues to output predictions, even though it is very clear that the model does not know what this image is.\n[29:32.060,29:49.260]  And now you might be asking, okay, so what's the big deal? It didn't recognize the horse drawn buggy, but it seemed to drive successfully anyway. However, the exact same problem that resulted in that video has also resulted in numerous autonomous car crashes.\n[29:51.080,30:8.040]  So let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks, which may cause incidents like the ones that we just saw. We'll go through a simple example that illustrates the two main types of uncertainty that we'll focus on in this lecture.\n[30:9.820,30:24.920]  So let's say I'm trying to estimate the curve y equals X cubed as part of a regression task. The input here, X is some real number, and we want it to output f of X, which should be ideally X cubed.\n[30:24.920,30:33.780]  So right away you might notice that there are some issues in this data set. Assume the red points in this image are your training samples.\n[30:36.880,31:6.380]  So the box area of this image shows data points in our data set where we have really high noise. These points do not follow the curve. Why it equals X cubed. In fact, they don't really seem to follow any distribution at all. And the model won't be able to compute outputs for points in this region accurately, because very similar inputs have extremely different outputs, which is the definition of data uncertainty.\n[31:9.620,31:35.860]  We also have regions in this data set where we have no data. So if we queried the model for a prediction in this part of in this region of the data set, we should not really expect to see an accurate result because the model's never seen anything like this before. And this is what is called model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to output a correct prediction.\n[31:37.160,31:50.840]  So what would happen if I added the following blue training points to the areas of the data set with high model uncertainty? Do you think the model uncertainty would decrease? Raise your hand.\n[31:51.920,31:54.760]  Does anyone think it would not change?\n[31:55.360,32:5.980]  Okay, so yeah, most of you were correct. Model uncertainty can typically be reduced by adding in data into any region, but specifically regions with high model uncertainty.\n[32:5.980,32:17.660]  And now, what happens if we add these blue data points into this data set? Would anyone expect the data uncertainty to decrease? You raise your hand.\n[32:18.820,32:46.880]  That's correct. So data uncertainty is irreducible. In the real world. The blue points and the noisy red points on this image correspond to things like robot sensors. Let's say I I have a robot that's trained to has a sensor that is making measurements of depth. If the sensor has noise in it, there's no way that I can add any more data into the system to reduce that noise, unless I replace my sensor entirely.\n[32:49.640,33:7.760]  So now let's assign some names to the types of uncertainty that we just talked about. The blue area, or the area of high data uncertainty is known as aloric uncertainty. It is irreducible, as we just mentioned, and it can be directly learned from data, which we'll talk about in a little bit.\n[33:8.580,33:24.680]  The green areas of the green boxes that we talked about, which were model uncertainty, are known as epistemic uncertainty, and this cannot be learned directly from the data. However, it can be reduced by adding more data into our systems into these regions.\n[33:30.220,33:42.480]  Okay, so first let's go through allatoric uncertainty. So the goal of estimating allatoric uncertainty is to learn a set of variances that correspond to the input.\n[33:42.480,33:59.860]  Keep in mind that we are not looking at a data distribution and we are, as humans are not estimating the variance. We're training the model to do this task. And so what that means is typically when we train a model, we give it an input X and we expect an output y hat, which is the prediction of the model.\n[33:59.940,34:9.940]  Now we also predict an additional sigma squared, so we add another layer to our model. We have the same output size that predicts a variance for every output.\n[34:11.100,34:19.640]  So the reason why we do this is that we expect that areas in our data set with high data uncertainty are going to have higher variance.\n[34:20.580,34:46.220]  And the crucial thing to remember here is that this variance is not constant. It depends on the value of X, we typically tend to think of variance as a single number that parameterizes an entire distribution. However, in this case, we may have areas of our input distribution with really high variance, and we may have areas with very low variance. So our variance cannot be independent of the input, and it depends on our input X.\n[34:47.120,34:57.220]  So now that we have this model, we have an extra layer attached to it. In addition to predicting y hat, we also predict a sigma squared. How do we train this model?\n[34:58.320,35:14.240]  Our current loss function does not take into account variance at any point. This is your typical mean squared error loss function that is used to train regression models. And there's no way training from this loss function that we can learn whether or not the variance that we're estimating is accurate.\n[35:15.020,35:23.080]  So in addition to adding another layer to estimate allatoric uncertainty correctly, we also have to change our loss function.\n[35:24.240,35:38.560]  So the mean squared error actually learns A A multivariate gaussian with a mean Y I and constant variance, and we want to generalize this loss function to when we don't have constant variance.\n[35:39.940,36:1.560]  And the way we do this is by changing the loss function to the negative log likelihood. We can think about this for now as a generalization of the mean squared error loss to non constant variances. So now that we have a sigma squared term in our loss function, we can determine how accurately the sigma and the y that we're predicting parameterize the distribution. That is our input.\n[36:4.320,36:25.120]  So now that we know how to estimate alloric uncertainty, let's look at a real world example. For this task, we'll focus on semantic segmentation, which is when we label every pixel of an image with its corresponding class. We do this for scene understanding and because it is more fine grained than a typical object detection algorithm.\n[36:25.140,36:55.700]  So the inputs of this to this data set are known as it's from a data set called citycapes, and the inputs are rgb images of scenes. The labels are pixel wise annotations of this entire image, of which label every pixel belongs to, and the outputs try to mimic the labels. There are also predicted pixel wise masks, so why would we expect that this data set has high natural allatoric uncertainty? And which parts of this data set do you think would have allatoric uncertainty?\n[36:58.660,37:27.620]  Because labeling every single pixel of an image is such a labor intensive task, and it's also very hard to do accurately, we would expect that the boundaries between EM between objects in this image have high allatoric uncertainty, and that's exactly what we see. If you train a model to predict allatoric uncertainty on this data set, corners and boundaries have the highest allator uncertainty, because even if your pixels are like one row off or one column off, that introduces noise into the model.\n[37:27.620,37:33.500]  The model can still learn in the face of this noise, but it does exist and it can't be reduced.\n[37:36.080,37:53.720]  So now that we know about data uncertainty or allatoric uncertainty, let's move on to learning about epistemic uncertainty. As a recap, epistemic uncertainty can best be described as uncertainty in the model itself, and it is reducible by adding data to the model.\n[37:56.200,38:22.800]  So with epistemic uncertainty, essentially what we're trying to ask is, is the model uncon confidentf about a prediction? So a really simple and very smart way to do this is let's say I train the same network multiple times with random initializations and I ask it to predict the exact I call it on the same input. So let's say I give model one the exact same input, and the blue X is the output of this model.\n[38:23.200,38:29.560]  And then I do the same thing again with model two, and then again with model three.\n[38:29.560,38:44.600]  And again, with model four, these models all have the exact same hyper parameters, the exact same architecture, and their train in the same way. The only difference between them is that their weights are all randomly initialized, so where they start from is different.\n[38:45.260,39:3.100]  And the reason why we can use this to determine epistemic uncertainty is because we would expect that with familiar inputs in our network, our networks should all converge to around the same answer, and we should see very little variance in the the log or the outputs that we're predicting.\n[39:3.100,39:16.640]  However, if a model has never seen a specific input before, or that input is very hard to learn, all of these models should predict slightly different answers, and the variance of them should be higher than if they were predicting a similar input.\n[39:18.360,39:32.540]  So creating an ensemble of networks is quite simple, quite simple. You start out with defining the number of ensembles you want, you create them all the exact same way, and then you fit them all on the same training data and training data.\n[39:33.020,40:13.200]  And then afterwards, when at inference time we call all of the models, every model in the ensemble on our specific input, and then we can treat our new prediction as the average of all of the ensembles. This results in a usually more robust and accurate prediction, and we can treat the uncertainty as the variance of all of these predictions. Again, remember that if we saw familiar inputs or inputs with low epistemic uncertainty, we should expect to have very little variance. And if we had a very unfamiliar input or something that was out of distribution or something the model hasn't seen before, we should have very high epistemic uncertainty or variance.\n[40:15.100,40:22.420]  So what's the problem with this? Can anyone raise their hand and tell me what a problem with training an ensemble of networks is?\n[40:25.220,40:43.660]  So training an ensemble of networks is really compute expensive. Even if your model is not very large, training five copies of it or ten copies of it tends to, it takes up compute and time, and that's just not really feasible when we're training on specific tasks.\n[40:43.660,41:15.060]  However, the key insight for ensembles is that by introducing some method of randomness or stochasticity into our networks, we're able to estimate epistemic uncertainty. So another way that we've seen about introducing stochasticity into networks is by using dropout layers. We've seen dropout layers as a method of reducing overfitting because we randomly drop out different nodes in our, in our layer, and then we continue to propagate information through them and it prevents models from memorizing data.\n[41:15.060,41:48.200]  However, in the EM case of epistemic uncertainty, we can add dropout layers after every single layer in our model, and in addition, we can keep these dropout layers enabled at test time. Usually we don't keep dropout layers enabled at test time because we don't want to lose any information about the the network's process or any weights when we're at inference time. However, when we're estimating epistemic uncertainty, we do want to keep dropout enabled at test time, because that's how we can introduce randomness, inference time as well.\n[41:48.200,42:6.800]  So what we do here is we have one model. It's the same model the entire way through. We add dropout layers with a specific probability, and then we run multiple forward passes and at every forward passes, different layers get different nodes in a layer, get dropped out. So we have that measure of randomness and stochasticity.\n[42:7.800,42:32.340]  So again, in order to implement this, what we have is a model with the exact one model. And then when we're running our forward passes, we can simply run t forward passes where t is usually a number like twenty EM. We keep dropout enabled at test time, and then we use the mean of these samples as the new prediction and the variance of these samples as a measure of epistemic uncertainty.\n[42:34.140,42:45.400]  So both of the methods we talked about just now involves sampling, and sampling is expensive. Ense sampling is very expensive, but even if you have a pretty large model.\n[42:45.400,43:5.680]  Having our introducing dropout layers and calling twenty forward passes might also be something that's pretty infeasible. And at themus, we're dedicated to developing innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more industries and people.\n[43:5.680,43:39.100]  So a method that we've developed to estimate a method that we've studied to estimate epistemic uncertainty is by using generative modeling. So we've talked about V A couple times now, but let's say I trained AV on the exact same data set we were talking about earlier, which is only dogs and cats. The latent space of this model would be comprised of features that relate to dogs and cats. And if I give it a prototypical dog, it should be able to generate a pretty good representation of this dog, and it should have pretty low reconstruction loss.\n[43:40.420,44:8.880]  Now, if I gave the same example of the horse to this V A, the latent vector that this horse would be decoded to would be incomprehensible to the decoder of this network. The decoder wouldn't be able to know how to project the latent vector back into the original input space, and therefore we should expect to see a much worse reconstruction here. And we should see that the reconstruction loss is much higher than if we gave the model a familiar input or something that it was used to seeing.\n[44:12.840,44:44.600]  So now let's move on to what I think is the most exciting method of estimating epistemic uncertainty that we'll talk about today. So in both of the examples before, EM sampling is compute intensive, but generative modeling can also be compute intensive. Let's say you don't actually need a variational auto encoder for your task, then you're training an entire decoder for no reason and other than to estimate the epistemic uncertainty, so what if we had a method that did not rely on generative modeling or sampling in order to estimate the epistemic uncertainty?\n[44:44.600,45:4.100]  That's exactly what a method that we've developed here at themis does. So we view learning as an evidence based process. So if you remember from earlier when we were training the ensemble and calling multiple ensembles on the same input, we received multiple predictions and we calculated that variance.\n[45:4.100,45:28.620]  Now, the way we frame evidential learning is what if we assume that those data points, those predictions, were actually drawn from a distribution themselves. If we could estimate the parameters of this higher order evidential distribution, we would be able to learn this variance or this measure of epistemic uncertainty automatically without doing any sampling or generative modeling. And that's exactly what evidential uncertainty does.\n[45:30.760,45:40.120]  So now that we have many methods in our toolbox for estimating epistemic uncertainty, let's go back to our real world example.\n[45:40.120,45:53.660]  Let's say again, the input is the same as before, it's an rgb image of some scene in a city, and the output again is a pixel level mask of what every pixel in this image belongs to, which class it belongs to.\n[45:53.660,46:14.220]  Which parts of the data set would you expect to have high epistemic uncertainty? In this example, take a look at the output of the model itself. The model does mostly well on semantic segmentation. However, it gets the sidewalk wrong. It assigns some of the sidewalk to the road, and other parts of the sidewalk are labeled incorrectly.\n[46:15.540,46:42.420]  And we can, using epistemic uncertainty, we can see why this is. The areas of the sidewalk that are discolored have high levels of epistemic uncertainty. Maybe this is because the model has never seen an example of a sidewalk with multiple different colors in it before, or maybe it hasn't been trained on examples with sidewalks generally. Either way, epistemic uncertainty has isolated this specific area of the image as an area of high uncertainty.\n[46:44.880,47:11.980]  So today, we've gone through two major challenges for robust deep learning. We've talked about bias, which is what happens when models are skewed by sensitive feature inputs and uncertainty, which is when we can measure a level of confidence of a certain model. Now we'll talk about how themus uses these concepts to build products that transform models to make them more risk aware, and how we're changing the AI landscape in terms of safe and trustworthy AI.\n[47:14.040,47:24.480]  So at themis, we believe that uncertainty and bias mitigation unlock a host of new solutions to solving these problems with safe and responsible AI.\n[47:24.480,47:45.220]  We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. Let's start with labeling data. Today, we talked about allatoric uncertainty, which is a method to detect mislabeled samples, to highlight label noise, and to generally maybe tell labelers to relabel images or samples that they've gotten. That may be wrong.\n[47:45.240,48:4.800]  In the second part of this cycle, we have analyzing the data before a model is even trained on any data. We can analyze the bias that is present in this data set and EM tell the creators whether or not they should add more samples, which demographics, which areas of the data set are underrepresented in the current data set before we even train a model on them.\n[48:5.060,48:17.160]  And then let's go to training the model. Once we're actually training a model, if it's already been trained on a biased data set, we can de bias it adaptively during training using the methods that we talked about today.\n[48:17.200,48:45.140]  And afterwards, we can also verify or certify deployed machine learning models, making sure that models that are actually out there are as safe and unbiased as they claim they are. And the way we can do this is by leveraging epistemic uncertainty or bias in order to calculate the samples or data points that the model will do. The worst on the model has the the most trouble learning or data set samples that are the most underrepresented in a model data set.\n[48:45.140,48:57.080]  If we can test the model on these samples, specifically the hardest samples for the model, and it does well, then we know that the model has probably been trained in a fair and unbiased manner that mitigates uncertainty.\n[48:57.240,49:25.080]  And lastly, we can think about, EM, we, we are developing a product at themis called AI guardian, and that's essentially a layer between the artificial intelligence algorithm and the user. And the way this works is this is a type of algorithm that if you're driving an autonomous vehicle, would say, hey, the model doesn't actually know what is happening in the world around it right now. As the user, you should take control of this autonomous vehicle and we can apply this to spears outside autonomy as well.\n[49:26.920,50:2.940]  So you'll notice that I skipped one, uh, part of the cycle. I skipped the part about building the model and that's because today we're going to focus a little bit on EM famous AI's product called capa, which is a model agnostic framework for risk estimation. So capa is an open source library. You all will actually use it in your lab today, EM, that transforms models so that they're risk aware. So this is a typical training pipeline. You've seen this many times in the course. By now we have our data, we have the model, and it's Fed into the training algorithm and we get a trained model at the end that outputs a prediction for every input.\n[50:4.220,50:40.060]  But with capa, what we can do is by adding a single line into any training workflow, we can turn this model into a risk aware variant that essentially calculates biases, uncertainty and label noise for you. Because today, as you've heard by now, there are so many methods of estimating uncertainty and bias, and sometimes certain methods are better than others. It's really hard to determine what kind of uncertainty you're trying to estimate and how to do so. So capa takes care of this for you. By inserting one line into your training workflow, you can achieve a risk aware model that you can then further analyze.\n[50:41.340,51:0.920]  And so this is the one line that I've been talking about. EM, after you build your model, you can just create a wrapper or you can call a wrapper that capa has an extensive library of. And then, in addition to achieving prediction, receiving predictions from your model, you can also receive whatever bias or uncertainty metric that you're trying to estimate.\n[51:3.080,51:28.580]  And the way capsule works is it does this by wrapping models for every uncertainty metric that we want to estimate, we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities. In the case of allatoric uncertainty, this could be adding a new layer. In the case of a variational auto encoder, this could be creating and training the decoder and calculating the reconstruction loss on the fly.\n[51:29.920,51:50.440]  And this is an example of capa working on one of the data sets that we talked about today, which was the cubic data set with added noise in it. And also another simple classification task. And the reason why I wanted to show this image is to show that using capsa, we can achieve all of these uncertainty estimates with very little additional added work.\n[51:53.680,52:20.380]  So using all of the products that I just talked about today and using capa themis is unlocking the key to deploy deep learning models safely across fields. We can now answer a lot of the questions that the headlines were raising earlier, which is when should a human take control of an autonomous vehicle? What types of data are underrepresented in commercial autonomous driving pipelines? We now have educated answers to these questions due to products that themis is developing.\n[52:21.340,52:39.980]  And in spheres such as medicine and health care, we can now answer questions such as when is a model uncertain about a life threatening diagnosis? When should this diagnosis be passed to a medical professional before this information is conveyed to a patient? Or what types of patients might drug discovery algorithms be biased against?\n[52:39.980,53:10.320]  And today, the, the application that you guys will focus on is on facial detection. You'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed in some ways for you so that you can discover them on your own. And we, we highly encourage you to compete in the competition, which the details are described in the lab. But basically it's about analyzing this data set, creating risk aware models that mitigate bias and uncertainty in the specific training pipeline.\n[53:11.340,53:46.540]  And so at themis our goal is to design, advance, and deploy trustworthy AI across industries and around the world. EM, we're passionate about scientific innovation. We release open source tools like the ones you'll use today, and our products transform AI workflows and make artificial intelligence safer for everyone. We partner with industries around the globe and we're hiring for the upcoming summer and for full time roles. So if you're interested, please send an email to careers at famousus I dot io or apply by submitting your resume to the deep learning resume drop and we'll see those resumes and get back to you. Thank you.\n", "ErrorMsg": "", "ResultDetail": [{"FinalSentence": "I'm really excited especially for this lecture, which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course, themeis AI. And as you'll see today, themeis AI is a startup actually locally based here in Cambridge. Our mission is to design, advance and deploy the future of AI and trustworthy. I specifically, I'm especially excited about today's lecture because I Co founded themis right here at MIT right here in this very building. In fact, this all stemmed from really the incredible scientific innovation and advances that we created right here just a few floors higher than where you're sitting today. And because of our background in really cutting edge scientific innovation stemming from MIT, themus is very rooted deeply in science and like I said, innovation.", "SliceSentence": "I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themeis AI . Andas you'll see today themeis AI is a startup actually locally based here in Cambridge . Ourmission is to design advance and deploy the future of AI and trustworthy . Ispecifically I'm especially excited about today's lecture because I Co founded themis right here at MIT right here in this very building . Infact this all stemmed from really the incredible scientific innovation and advances that we created right here just a few floors higher than where you're sitting today . Andbecause of our background in really cutting edge scientific innovation stemming from MIT themus is very rooted deeply in science and like I said innovation", "StartMs": 9620, "EndMs": 65520, "WordsNum": 137, "Words": [{"Word": "I'm", "OffsetStartMs": 40, "OffsetEndMs": 360}, {"Word": "really", "OffsetStartMs": 360, "OffsetEndMs": 555}, {"Word": "excited", "OffsetStartMs": 555, "OffsetEndMs": 920}, {"Word": "especially", "OffsetStartMs": 940, "OffsetEndMs": 1340}, {"Word": "for", "OffsetStartMs": 1360, "OffsetEndMs": 1620}, {"Word": "this", "OffsetStartMs": 1620, "OffsetEndMs": 1800}, {"Word": "lecture", "OffsetStartMs": 1800, "OffsetEndMs": 2120}, {"Word": "which", "OffsetStartMs": 2140, "OffsetEndMs": 2400}, {"Word": "is", "OffsetStartMs": 2400, "OffsetEndMs": 2535}, {"Word": "a", "OffsetStartMs": 2535, "OffsetEndMs": 2655}, {"Word": "very", "OffsetStartMs": 2655, "OffsetEndMs": 2900}, {"Word": "special", "OffsetStartMs": 3040, "OffsetEndMs": 3390}, {"Word": "lecture", "OffsetStartMs": 3390, "OffsetEndMs": 3720}, {"Word": "on", "OffsetStartMs": 3720, "OffsetEndMs": 4050}, {"Word": "robust", "OffsetStartMs": 4050, "OffsetEndMs": 4365}, {"Word": "and", "OffsetStartMs": 4365, "OffsetEndMs": 4620}, {"Word": "trustworthy", "OffsetStartMs": 4620, "OffsetEndMs": 5175}, {"Word": "deep", "OffsetStartMs": 5175, "OffsetEndMs": 5355}, {"Word": "learning", "OffsetStartMs": 5355, "OffsetEndMs": 5660}, {"Word": "by", "OffsetStartMs": 5800, "OffsetEndMs": 6135}, {"Word": "one", "OffsetStartMs": 6135, "OffsetEndMs": 6330}, {"Word": "of", "OffsetStartMs": 6330, "OffsetEndMs": 6450}, {"Word": "our", "OffsetStartMs": 6450, "OffsetEndMs": 6710}, {"Word": "sponsors", "OffsetStartMs": 7060, "OffsetEndMs": 7590}, {"Word": "of", "OffsetStartMs": 7590, "OffsetEndMs": 7785}, {"Word": "this", "OffsetStartMs": 7785, "OffsetEndMs": 7980}, {"Word": "amazing", "OffsetStartMs": 7980, "OffsetEndMs": 8280}, {"Word": "course", "OffsetStartMs": 8280, "OffsetEndMs": 8655}, {"Word": "themeis", "OffsetStartMs": 8655, "OffsetEndMs": 9165}, {"Word": "AI", "OffsetStartMs": 9165, "OffsetEndMs": 9530}, {"Word": ".", "OffsetStartMs": 10180, "OffsetEndMs": 10545}, {"Word": "Andas", "OffsetStartMs": 10545, "OffsetEndMs": 10800}, {"Word": "you'll", "OffsetStartMs": 10800, "OffsetEndMs": 11025}, {"Word": "see", "OffsetStartMs": 11025, "OffsetEndMs": 11160}, {"Word": "today", "OffsetStartMs": 11160, "OffsetEndMs": 11450}, {"Word": "themeis", "OffsetStartMs": 11620, "OffsetEndMs": 12135}, {"Word": "AI", "OffsetStartMs": 12135, "OffsetEndMs": 12500}, {"Word": "is", "OffsetStartMs": 12610, "OffsetEndMs": 13010}, {"Word": "a", "OffsetStartMs": 13510, "OffsetEndMs": 13910}, {"Word": "startup", "OffsetStartMs": 14080, "OffsetEndMs": 14690}, {"Word": "actually", "OffsetStartMs": 14890, "OffsetEndMs": 15165}, {"Word": "locally", "OffsetStartMs": 15165, "OffsetEndMs": 15600}, {"Word": "based", "OffsetStartMs": 15600, "OffsetEndMs": 15885}, {"Word": "here", "OffsetStartMs": 15885, "OffsetEndMs": 16155}, {"Word": "in", "OffsetStartMs": 16155, "OffsetEndMs": 16395}, {"Word": "Cambridge", "OffsetStartMs": 16395, "OffsetEndMs": 16970}, {"Word": ".", "OffsetStartMs": 17260, "OffsetEndMs": 17550}, {"Word": "Ourmission", "OffsetStartMs": 17550, "OffsetEndMs": 17775}, {"Word": "is", "OffsetStartMs": 17775, "OffsetEndMs": 18015}, {"Word": "to", "OffsetStartMs": 18015, "OffsetEndMs": 18270}, {"Word": "design", "OffsetStartMs": 18270, "OffsetEndMs": 18620}, {"Word": "advance", "OffsetStartMs": 18940, "OffsetEndMs": 19340}, {"Word": "and", "OffsetStartMs": 19450, "OffsetEndMs": 19830}, {"Word": "deploy", "OffsetStartMs": 19830, "OffsetEndMs": 20210}, {"Word": "the", "OffsetStartMs": 20590, "OffsetEndMs": 20880}, {"Word": "future", "OffsetStartMs": 20880, "OffsetEndMs": 21170}, {"Word": "of", "OffsetStartMs": 21220, "OffsetEndMs": 21620}, {"Word": "AI", "OffsetStartMs": 21820, "OffsetEndMs": 22220}, {"Word": "and", "OffsetStartMs": 22270, "OffsetEndMs": 22605}, {"Word": "trustworthy", "OffsetStartMs": 22605, "OffsetEndMs": 23250}, {"Word": ".", "OffsetStartMs": 23250, "OffsetEndMs": 23490}, {"Word": "Ispecifically", "OffsetStartMs": 23490, "OffsetEndMs": 23810}, {"Word": "I'm", "OffsetStartMs": 24430, "OffsetEndMs": 24930}, {"Word": "especially", "OffsetStartMs": 24930, "OffsetEndMs": 25305}, {"Word": "excited", "OffsetStartMs": 25305, "OffsetEndMs": 25700}, {"Word": "about", "OffsetStartMs": 25720, "OffsetEndMs": 26070}, {"Word": "today's", "OffsetStartMs": 26070, "OffsetEndMs": 26690}, {"Word": "lecture", "OffsetStartMs": 26770, "OffsetEndMs": 27170}, {"Word": "because", "OffsetStartMs": 27520, "OffsetEndMs": 27920}, {"Word": "I", "OffsetStartMs": 27940, "OffsetEndMs": 28245}, {"Word": "Co", "OffsetStartMs": 28245, "OffsetEndMs": 28440}, {"Word": "founded", "OffsetStartMs": 28440, "OffsetEndMs": 28730}, {"Word": "themis", "OffsetStartMs": 28810, "OffsetEndMs": 29390}, {"Word": "right", "OffsetStartMs": 29410, "OffsetEndMs": 29715}, {"Word": "here", "OffsetStartMs": 29715, "OffsetEndMs": 29955}, {"Word": "at", "OffsetStartMs": 29955, "OffsetEndMs": 30255}, {"Word": "MIT", "OffsetStartMs": 30255, "OffsetEndMs": 30620}, {"Word": "right", "OffsetStartMs": 31060, "OffsetEndMs": 31350}, {"Word": "here", "OffsetStartMs": 31350, "OffsetEndMs": 31485}, {"Word": "in", "OffsetStartMs": 31485, "OffsetEndMs": 31590}, {"Word": "this", "OffsetStartMs": 31590, "OffsetEndMs": 31755}, {"Word": "very", "OffsetStartMs": 31755, "OffsetEndMs": 31965}, {"Word": "building", "OffsetStartMs": 31965, "OffsetEndMs": 32205}, {"Word": ".", "OffsetStartMs": 32205, "OffsetEndMs": 32445}, {"Word": "Infact", "OffsetStartMs": 32445, "OffsetEndMs": 32750}, {"Word": "this", "OffsetStartMs": 33310, "OffsetEndMs": 33615}, {"Word": "all", "OffsetStartMs": 33615, "OffsetEndMs": 33870}, {"Word": "stemmed", "OffsetStartMs": 33870, "OffsetEndMs": 34245}, {"Word": "from", "OffsetStartMs": 34245, "OffsetEndMs": 34520}, {"Word": "really", "OffsetStartMs": 34540, "OffsetEndMs": 34905}, {"Word": "the", "OffsetStartMs": 34905, "OffsetEndMs": 35205}, {"Word": "incredible", "OffsetStartMs": 35205, "OffsetEndMs": 35540}, {"Word": "scientific", "OffsetStartMs": 35740, "OffsetEndMs": 36140}, {"Word": "innovation", "OffsetStartMs": 36640, "OffsetEndMs": 37040}, {"Word": "and", "OffsetStartMs": 37150, "OffsetEndMs": 37485}, {"Word": "advances", "OffsetStartMs": 37485, "OffsetEndMs": 38025}, {"Word": "that", "OffsetStartMs": 38025, "OffsetEndMs": 38250}, {"Word": "we", "OffsetStartMs": 38250, "OffsetEndMs": 38445}, {"Word": "created", "OffsetStartMs": 38445, "OffsetEndMs": 38750}, {"Word": "right", "OffsetStartMs": 38830, "OffsetEndMs": 39135}, {"Word": "here", "OffsetStartMs": 39135, "OffsetEndMs": 39360}, {"Word": "just", "OffsetStartMs": 39360, "OffsetEndMs": 39540}, {"Word": "a", "OffsetStartMs": 39540, "OffsetEndMs": 39675}, {"Word": "few", "OffsetStartMs": 39675, "OffsetEndMs": 39870}, {"Word": "floors", "OffsetStartMs": 39870, "OffsetEndMs": 40310}, {"Word": "higher", "OffsetStartMs": 40480, "OffsetEndMs": 40815}, {"Word": "than", "OffsetStartMs": 40815, "OffsetEndMs": 41010}, {"Word": "where", "OffsetStartMs": 41010, "OffsetEndMs": 41145}, {"Word": "you're", "OffsetStartMs": 41145, "OffsetEndMs": 41385}, {"Word": "sitting", "OffsetStartMs": 41385, "OffsetEndMs": 41625}, {"Word": "today", "OffsetStartMs": 41625, "OffsetEndMs": 41990}, {"Word": ".", "OffsetStartMs": 42760, "OffsetEndMs": 43160}, {"Word": "Andbecause", "OffsetStartMs": 43330, "OffsetEndMs": 43620}, {"Word": "of", "OffsetStartMs": 43620, "OffsetEndMs": 43755}, {"Word": "our", "OffsetStartMs": 43755, "OffsetEndMs": 43890}, {"Word": "background", "OffsetStartMs": 43890, "OffsetEndMs": 44180}, {"Word": "in", "OffsetStartMs": 44500, "OffsetEndMs": 44880}, {"Word": "really", "OffsetStartMs": 44880, "OffsetEndMs": 45255}, {"Word": "cutting", "OffsetStartMs": 45255, "OffsetEndMs": 45600}, {"Word": "edge", "OffsetStartMs": 45600, "OffsetEndMs": 45950}, {"Word": "scientific", "OffsetStartMs": 46150, "OffsetEndMs": 46550}, {"Word": "innovation", "OffsetStartMs": 47140, "OffsetEndMs": 47540}, {"Word": "stemming", "OffsetStartMs": 48160, "OffsetEndMs": 48630}, {"Word": "from", "OffsetStartMs": 48630, "OffsetEndMs": 48840}, {"Word": "MIT", "OffsetStartMs": 48840, "OffsetEndMs": 49160}, {"Word": "themus", "OffsetStartMs": 49210, "OffsetEndMs": 49635}, {"Word": "is", "OffsetStartMs": 49635, "OffsetEndMs": 49845}, {"Word": "very", "OffsetStartMs": 49845, "OffsetEndMs": 50180}, {"Word": "rooted", "OffsetStartMs": 50290, "OffsetEndMs": 50900}, {"Word": "deeply", "OffsetStartMs": 50920, "OffsetEndMs": 51320}, {"Word": "in", "OffsetStartMs": 51760, "OffsetEndMs": 52160}, {"Word": "science", "OffsetStartMs": 52210, "OffsetEndMs": 52610}, {"Word": "and", "OffsetStartMs": 52900, "OffsetEndMs": 53280}, {"Word": "like", "OffsetStartMs": 53280, "OffsetEndMs": 53535}, {"Word": "I", "OffsetStartMs": 53535, "OffsetEndMs": 53700}, {"Word": "said", "OffsetStartMs": 53700, "OffsetEndMs": 53990}, {"Word": "innovation", "OffsetStartMs": 54100, "OffsetEndMs": 54500}], "SpeechSpeed": 14.5}, {"FinalSentence": "We really aim to advance the future of deep learning and AI and much of our technology has already grown from published research that we've published at top tier peer review conferences in the AI venues around the world and our work has been covered by high profile international media outlets. This scientific innovation with this scientific innovation themeis we are tackling some of the biggest challenges in safety critical AI that exists today. And really that stems from the fact that we want to take all of these amazing advances that you're learning as part of this course and actually achieve them in reality as part of our daily lives and we're working together with leading global industry partners across many different disciplines ranging from robotics, autonomy, health care and more to develop a line of products that will guarantee safe and trustworthy AI and we drive this really eh deeply with our technical engineering and machine learning team.", "SliceSentence": "We really aim to advance the future of deep learning and AI and much of our technology has already grown from published research that we've published at top tier peer review conferences in the AI venues around the world and our work has been covered by high profile international media outlets . Thisscientific innovation with this scientific innovation themeis we are tackling some of the biggest challenges in safety critical AI that exists today . Andreally that stems from the fact that we want to take all of these amazing advances that you're learning as part of this course and actually achieve them in reality as part of our daily lives and we're working together with leading global industry partners across many different disciplines ranging from robotics autonomy health care and more to develop a line of products that will guarantee safe and trustworthy AI and we drive this really eh deeply with our technical engineering and machine learning team", "StartMs": 65520, "EndMs": 125800, "WordsNum": 160, "Words": [{"Word": "We", "OffsetStartMs": 0, "OffsetEndMs": 195}, {"Word": "really", "OffsetStartMs": 195, "OffsetEndMs": 470}, {"Word": "aim", "OffsetStartMs": 550, "OffsetEndMs": 950}, {"Word": "to", "OffsetStartMs": 1210, "OffsetEndMs": 1590}, {"Word": "advance", "OffsetStartMs": 1590, "OffsetEndMs": 1970}, {"Word": "the", "OffsetStartMs": 2020, "OffsetEndMs": 2295}, {"Word": "future", "OffsetStartMs": 2295, "OffsetEndMs": 2535}, {"Word": "of", "OffsetStartMs": 2535, "OffsetEndMs": 2790}, {"Word": "deep", "OffsetStartMs": 2790, "OffsetEndMs": 2985}, {"Word": "learning", "OffsetStartMs": 2985, "OffsetEndMs": 3290}, {"Word": "and", "OffsetStartMs": 3490, "OffsetEndMs": 3890}, {"Word": "AI", "OffsetStartMs": 3910, "OffsetEndMs": 4290}, {"Word": "and", "OffsetStartMs": 4290, "OffsetEndMs": 4560}, {"Word": "much", "OffsetStartMs": 4560, "OffsetEndMs": 4740}, {"Word": "of", "OffsetStartMs": 4740, "OffsetEndMs": 4875}, {"Word": "our", "OffsetStartMs": 4875, "OffsetEndMs": 5010}, {"Word": "technology", "OffsetStartMs": 5010, "OffsetEndMs": 5300}, {"Word": "has", "OffsetStartMs": 5500, "OffsetEndMs": 5900}, {"Word": "already", "OffsetStartMs": 5920, "OffsetEndMs": 6320}, {"Word": "grown", "OffsetStartMs": 6430, "OffsetEndMs": 6830}, {"Word": "from", "OffsetStartMs": 6880, "OffsetEndMs": 7200}, {"Word": "published", "OffsetStartMs": 7200, "OffsetEndMs": 7485}, {"Word": "research", "OffsetStartMs": 7485, "OffsetEndMs": 7850}, {"Word": "that", "OffsetStartMs": 8560, "OffsetEndMs": 8960}, {"Word": "we've", "OffsetStartMs": 9040, "OffsetEndMs": 9620}, {"Word": "published", "OffsetStartMs": 10060, "OffsetEndMs": 10460}, {"Word": "at", "OffsetStartMs": 10990, "OffsetEndMs": 11340}, {"Word": "top", "OffsetStartMs": 11340, "OffsetEndMs": 11655}, {"Word": "tier", "OffsetStartMs": 11655, "OffsetEndMs": 12080}, {"Word": "peer", "OffsetStartMs": 12280, "OffsetEndMs": 12675}, {"Word": "review", "OffsetStartMs": 12675, "OffsetEndMs": 12945}, {"Word": "conferences", "OffsetStartMs": 12945, "OffsetEndMs": 13700}, {"Word": "in", "OffsetStartMs": 13720, "OffsetEndMs": 13995}, {"Word": "the", "OffsetStartMs": 13995, "OffsetEndMs": 14205}, {"Word": "AI", "OffsetStartMs": 14205, "OffsetEndMs": 14540}, {"Word": "venues", "OffsetStartMs": 14860, "OffsetEndMs": 15440}, {"Word": "around", "OffsetStartMs": 15520, "OffsetEndMs": 15810}, {"Word": "the", "OffsetStartMs": 15810, "OffsetEndMs": 15960}, {"Word": "world", "OffsetStartMs": 15960, "OffsetEndMs": 16220}, {"Word": "and", "OffsetStartMs": 16300, "OffsetEndMs": 16545}, {"Word": "our", "OffsetStartMs": 16545, "OffsetEndMs": 16725}, {"Word": "work", "OffsetStartMs": 16725, "OffsetEndMs": 16965}, {"Word": "has", "OffsetStartMs": 16965, "OffsetEndMs": 17160}, {"Word": "been", "OffsetStartMs": 17160, "OffsetEndMs": 17340}, {"Word": "covered", "OffsetStartMs": 17340, "OffsetEndMs": 17595}, {"Word": "by", "OffsetStartMs": 17595, "OffsetEndMs": 17960}, {"Word": "high", "OffsetStartMs": 18040, "OffsetEndMs": 18440}, {"Word": "profile", "OffsetStartMs": 18490, "OffsetEndMs": 18890}, {"Word": "international", "OffsetStartMs": 19090, "OffsetEndMs": 19490}, {"Word": "media", "OffsetStartMs": 19600, "OffsetEndMs": 19920}, {"Word": "outlets", "OffsetStartMs": 19920, "OffsetEndMs": 20420}, {"Word": ".", "OffsetStartMs": 21400, "OffsetEndMs": 21800}, {"Word": "Thisscientific", "OffsetStartMs": 21820, "OffsetEndMs": 22220}, {"Word": "innovation", "OffsetStartMs": 22450, "OffsetEndMs": 22850}, {"Word": "with", "OffsetStartMs": 22900, "OffsetEndMs": 23205}, {"Word": "this", "OffsetStartMs": 23205, "OffsetEndMs": 23400}, {"Word": "scientific", "OffsetStartMs": 23400, "OffsetEndMs": 23690}, {"Word": "innovation", "OffsetStartMs": 23890, "OffsetEndMs": 24290}, {"Word": "themeis", "OffsetStartMs": 24520, "OffsetEndMs": 25100}, {"Word": "we", "OffsetStartMs": 25330, "OffsetEndMs": 25575}, {"Word": "are", "OffsetStartMs": 25575, "OffsetEndMs": 25740}, {"Word": "tackling", "OffsetStartMs": 25740, "OffsetEndMs": 26360}, {"Word": "some", "OffsetStartMs": 26860, "OffsetEndMs": 27135}, {"Word": "of", "OffsetStartMs": 27135, "OffsetEndMs": 27270}, {"Word": "the", "OffsetStartMs": 27270, "OffsetEndMs": 27495}, {"Word": "biggest", "OffsetStartMs": 27495, "OffsetEndMs": 27860}, {"Word": "challenges", "OffsetStartMs": 27910, "OffsetEndMs": 28310}, {"Word": "in", "OffsetStartMs": 28750, "OffsetEndMs": 29115}, {"Word": "safety", "OffsetStartMs": 29115, "OffsetEndMs": 29445}, {"Word": "critical", "OffsetStartMs": 29445, "OffsetEndMs": 29810}, {"Word": "AI", "OffsetStartMs": 29980, "OffsetEndMs": 30380}, {"Word": "that", "OffsetStartMs": 30400, "OffsetEndMs": 30735}, {"Word": "exists", "OffsetStartMs": 30735, "OffsetEndMs": 31020}, {"Word": "today", "OffsetStartMs": 31020, "OffsetEndMs": 31365}, {"Word": ".", "OffsetStartMs": 31365, "OffsetEndMs": 31740}, {"Word": "Andreally", "OffsetStartMs": 31740, "OffsetEndMs": 32040}, {"Word": "that", "OffsetStartMs": 32040, "OffsetEndMs": 32310}, {"Word": "stems", "OffsetStartMs": 32310, "OffsetEndMs": 32670}, {"Word": "from", "OffsetStartMs": 32670, "OffsetEndMs": 32930}, {"Word": "the", "OffsetStartMs": 33070, "OffsetEndMs": 33405}, {"Word": "fact", "OffsetStartMs": 33405, "OffsetEndMs": 33645}, {"Word": "that", "OffsetStartMs": 33645, "OffsetEndMs": 33810}, {"Word": "we", "OffsetStartMs": 33810, "OffsetEndMs": 33945}, {"Word": "want", "OffsetStartMs": 33945, "OffsetEndMs": 34110}, {"Word": "to", "OffsetStartMs": 34110, "OffsetEndMs": 34290}, {"Word": "take", "OffsetStartMs": 34290, "OffsetEndMs": 34500}, {"Word": "all", "OffsetStartMs": 34500, "OffsetEndMs": 34695}, {"Word": "of", "OffsetStartMs": 34695, "OffsetEndMs": 34830}, {"Word": "these", "OffsetStartMs": 34830, "OffsetEndMs": 35010}, {"Word": "amazing", "OffsetStartMs": 35010, "OffsetEndMs": 35330}, {"Word": "advances", "OffsetStartMs": 35350, "OffsetEndMs": 35820}, {"Word": "that", "OffsetStartMs": 35820, "OffsetEndMs": 35955}, {"Word": "you're", "OffsetStartMs": 35955, "OffsetEndMs": 36135}, {"Word": "learning", "OffsetStartMs": 36135, "OffsetEndMs": 36300}, {"Word": "as", "OffsetStartMs": 36300, "OffsetEndMs": 36495}, {"Word": "part", "OffsetStartMs": 36495, "OffsetEndMs": 36615}, {"Word": "of", "OffsetStartMs": 36615, "OffsetEndMs": 36720}, {"Word": "this", "OffsetStartMs": 36720, "OffsetEndMs": 36900}, {"Word": "course", "OffsetStartMs": 36900, "OffsetEndMs": 37220}, {"Word": "and", "OffsetStartMs": 37420, "OffsetEndMs": 37820}, {"Word": "actually", "OffsetStartMs": 37840, "OffsetEndMs": 38220}, {"Word": "achieve", "OffsetStartMs": 38220, "OffsetEndMs": 38505}, {"Word": "them", "OffsetStartMs": 38505, "OffsetEndMs": 38745}, {"Word": "in", "OffsetStartMs": 38745, "OffsetEndMs": 38970}, {"Word": "reality", "OffsetStartMs": 38970, "OffsetEndMs": 39260}, {"Word": "as", "OffsetStartMs": 39640, "OffsetEndMs": 39960}, {"Word": "part", "OffsetStartMs": 39960, "OffsetEndMs": 40140}, {"Word": "of", "OffsetStartMs": 40140, "OffsetEndMs": 40245}, {"Word": "our", "OffsetStartMs": 40245, "OffsetEndMs": 40380}, {"Word": "daily", "OffsetStartMs": 40380, "OffsetEndMs": 40620}, {"Word": "lives", "OffsetStartMs": 40620, "OffsetEndMs": 40970}, {"Word": "and", "OffsetStartMs": 41290, "OffsetEndMs": 41565}, {"Word": "we're", "OffsetStartMs": 41565, "OffsetEndMs": 41745}, {"Word": "working", "OffsetStartMs": 41745, "OffsetEndMs": 41985}, {"Word": "together", "OffsetStartMs": 41985, "OffsetEndMs": 42315}, {"Word": "with", "OffsetStartMs": 42315, "OffsetEndMs": 42600}, {"Word": "leading", "OffsetStartMs": 42600, "OffsetEndMs": 42930}, {"Word": "global", "OffsetStartMs": 42930, "OffsetEndMs": 43310}, {"Word": "industry", "OffsetStartMs": 43510, "OffsetEndMs": 43785}, {"Word": "partners", "OffsetStartMs": 43785, "OffsetEndMs": 44060}, {"Word": "across", "OffsetStartMs": 44170, "OffsetEndMs": 44570}, {"Word": "many", "OffsetStartMs": 44800, "OffsetEndMs": 45120}, {"Word": "different", "OffsetStartMs": 45120, "OffsetEndMs": 45375}, {"Word": "disciplines", "OffsetStartMs": 45375, "OffsetEndMs": 45930}, {"Word": "ranging", "OffsetStartMs": 45930, "OffsetEndMs": 46335}, {"Word": "from", "OffsetStartMs": 46335, "OffsetEndMs": 46575}, {"Word": "robotics", "OffsetStartMs": 46575, "OffsetEndMs": 47090}, {"Word": "autonomy", "OffsetStartMs": 47380, "OffsetEndMs": 48090}, {"Word": "health", "OffsetStartMs": 48090, "OffsetEndMs": 48345}, {"Word": "care", "OffsetStartMs": 48345, "OffsetEndMs": 48585}, {"Word": "and", "OffsetStartMs": 48585, "OffsetEndMs": 48765}, {"Word": "more", "OffsetStartMs": 48765, "OffsetEndMs": 49035}, {"Word": "to", "OffsetStartMs": 49035, "OffsetEndMs": 49350}, {"Word": "develop", "OffsetStartMs": 49350, "OffsetEndMs": 49575}, {"Word": "a", "OffsetStartMs": 49575, "OffsetEndMs": 49755}, {"Word": "line", "OffsetStartMs": 49755, "OffsetEndMs": 49965}, {"Word": "of", "OffsetStartMs": 49965, "OffsetEndMs": 50205}, {"Word": "products", "OffsetStartMs": 50205, "OffsetEndMs": 50510}, {"Word": "that", "OffsetStartMs": 51130, "OffsetEndMs": 51405}, {"Word": "will", "OffsetStartMs": 51405, "OffsetEndMs": 51570}, {"Word": "guarantee", "OffsetStartMs": 51570, "OffsetEndMs": 51860}, {"Word": "safe", "OffsetStartMs": 52030, "OffsetEndMs": 52380}, {"Word": "and", "OffsetStartMs": 52380, "OffsetEndMs": 52620}, {"Word": "trustworthy", "OffsetStartMs": 52620, "OffsetEndMs": 53220}, {"Word": "AI", "OffsetStartMs": 53220, "OffsetEndMs": 53540}, {"Word": "and", "OffsetStartMs": 53920, "OffsetEndMs": 54225}, {"Word": "we", "OffsetStartMs": 54225, "OffsetEndMs": 54465}, {"Word": "drive", "OffsetStartMs": 54465, "OffsetEndMs": 54780}, {"Word": "this", "OffsetStartMs": 54780, "OffsetEndMs": 55160}, {"Word": "really", "OffsetStartMs": 55510, "OffsetEndMs": 55910}, {"Word": "eh", "OffsetStartMs": 56110, "OffsetEndMs": 56510}, {"Word": "deeply", "OffsetStartMs": 56530, "OffsetEndMs": 56930}, {"Word": "with", "OffsetStartMs": 57220, "OffsetEndMs": 57495}, {"Word": "our", "OffsetStartMs": 57495, "OffsetEndMs": 57675}, {"Word": "technical", "OffsetStartMs": 57675, "OffsetEndMs": 57980}, {"Word": "engineering", "OffsetStartMs": 58150, "OffsetEndMs": 58550}, {"Word": "and", "OffsetStartMs": 58690, "OffsetEndMs": 59025}, {"Word": "machine", "OffsetStartMs": 59025, "OffsetEndMs": 59250}, {"Word": "learning", "OffsetStartMs": 59250, "OffsetEndMs": 59535}, {"Word": "team", "OffsetStartMs": 59535, "OffsetEndMs": 59930}], "SpeechSpeed": 15.9}, {"FinalSentence": "And our focus is very much on the engineering, very flexible and very modular platforms to scale algorithms towards robust and trustworthy AI. This really enables this deployment towards grand challenges that our society faces with AI today. Specifically the ability for AI solutions today are not very trustworthy at all, even if they may be very high performance on some of the tasks that we study as part of this course. So it's an incredibly exciting time for themis and specific right now where VC backed we're located. Our offices are right here in Cambridge so we're local and we have just closed around the funding so we're actively hiring the best and the brightest engineers like all of you to realize the future of safe and trustworthy AI and we hope that really today's lecture inspires you to join us on this mission to build the future of AI. And with that it's my great pleasure to introduce SOA. SOA is a machine learning scientist.", "SliceSentence": "And our focus is very much on the engineering very flexible and very modular platforms to scale algorithms towards robust and trustworthy AI . Thisreally enables this deployment towards grand challenges that our society faces with AI today . Specificallythe ability for AI solutions today are not very trustworthy at all even if they may be very high performance on some of the tasks that we study as part of this course . Soit's an incredibly exciting time for themis and specific right now where VC backed we're located . Ouroffices are right here in Cambridge so we're local and we have just closed around the funding so we're actively hiring the best and the brightest engineers like all of you to realize the future of safe and trustworthy AI and we hope that really today's lecture inspires you to join us on this mission to build the future of AI . Andwith that it's my great pleasure to introduce SOA . SOAis a machine learning scientist", "StartMs": 125800, "EndMs": 185840, "WordsNum": 167, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 500}, {"Word": "our", "OffsetStartMs": 580, "OffsetEndMs": 900}, {"Word": "focus", "OffsetStartMs": 900, "OffsetEndMs": 1220}, {"Word": "is", "OffsetStartMs": 1540, "OffsetEndMs": 1890}, {"Word": "very", "OffsetStartMs": 1890, "OffsetEndMs": 2205}, {"Word": "much", "OffsetStartMs": 2205, "OffsetEndMs": 2570}, {"Word": "on", "OffsetStartMs": 2800, "OffsetEndMs": 3075}, {"Word": "the", "OffsetStartMs": 3075, "OffsetEndMs": 3330}, {"Word": "engineering", "OffsetStartMs": 3330, "OffsetEndMs": 3710}, {"Word": "very", "OffsetStartMs": 3970, "OffsetEndMs": 4335}, {"Word": "flexible", "OffsetStartMs": 4335, "OffsetEndMs": 4700}, {"Word": "and", "OffsetStartMs": 4900, "OffsetEndMs": 5175}, {"Word": "very", "OffsetStartMs": 5175, "OffsetEndMs": 5400}, {"Word": "modular", "OffsetStartMs": 5400, "OffsetEndMs": 5990}, {"Word": "platforms", "OffsetStartMs": 6370, "OffsetEndMs": 6770}, {"Word": "to", "OffsetStartMs": 7210, "OffsetEndMs": 7515}, {"Word": "scale", "OffsetStartMs": 7515, "OffsetEndMs": 7820}, {"Word": "algorithms", "OffsetStartMs": 7870, "OffsetEndMs": 8295}, {"Word": "towards", "OffsetStartMs": 8295, "OffsetEndMs": 8630}, {"Word": "robust", "OffsetStartMs": 8740, "OffsetEndMs": 9140}, {"Word": "and", "OffsetStartMs": 9400, "OffsetEndMs": 9800}, {"Word": "trustworthy", "OffsetStartMs": 9850, "OffsetEndMs": 10590}, {"Word": "AI", "OffsetStartMs": 10590, "OffsetEndMs": 10940}, {"Word": ".", "OffsetStartMs": 11410, "OffsetEndMs": 11670}, {"Word": "Thisreally", "OffsetStartMs": 11670, "OffsetEndMs": 11850}, {"Word": "enables", "OffsetStartMs": 11850, "OffsetEndMs": 12290}, {"Word": "this", "OffsetStartMs": 12490, "OffsetEndMs": 12885}, {"Word": "deployment", "OffsetStartMs": 12885, "OffsetEndMs": 13490}, {"Word": "towards", "OffsetStartMs": 13990, "OffsetEndMs": 14390}, {"Word": "grand", "OffsetStartMs": 14440, "OffsetEndMs": 14775}, {"Word": "challenges", "OffsetStartMs": 14775, "OffsetEndMs": 15110}, {"Word": "that", "OffsetStartMs": 15190, "OffsetEndMs": 15495}, {"Word": "our", "OffsetStartMs": 15495, "OffsetEndMs": 15800}, {"Word": "society", "OffsetStartMs": 15880, "OffsetEndMs": 16260}, {"Word": "faces", "OffsetStartMs": 16260, "OffsetEndMs": 16640}, {"Word": "with", "OffsetStartMs": 16660, "OffsetEndMs": 17025}, {"Word": "AI", "OffsetStartMs": 17025, "OffsetEndMs": 17325}, {"Word": "today", "OffsetStartMs": 17325, "OffsetEndMs": 17660}, {"Word": ".", "OffsetStartMs": 18010, "OffsetEndMs": 18410}, {"Word": "Specificallythe", "OffsetStartMs": 18580, "OffsetEndMs": 18870}, {"Word": "ability", "OffsetStartMs": 18870, "OffsetEndMs": 19155}, {"Word": "for", "OffsetStartMs": 19155, "OffsetEndMs": 19485}, {"Word": "AI", "OffsetStartMs": 19485, "OffsetEndMs": 19740}, {"Word": "solutions", "OffsetStartMs": 19740, "OffsetEndMs": 20060}, {"Word": "today", "OffsetStartMs": 20170, "OffsetEndMs": 20505}, {"Word": "are", "OffsetStartMs": 20505, "OffsetEndMs": 20775}, {"Word": "not", "OffsetStartMs": 20775, "OffsetEndMs": 21030}, {"Word": "very", "OffsetStartMs": 21030, "OffsetEndMs": 21285}, {"Word": "trustworthy", "OffsetStartMs": 21285, "OffsetEndMs": 21855}, {"Word": "at", "OffsetStartMs": 21855, "OffsetEndMs": 21960}, {"Word": "all", "OffsetStartMs": 21960, "OffsetEndMs": 22220}, {"Word": "even", "OffsetStartMs": 22390, "OffsetEndMs": 22790}, {"Word": "if", "OffsetStartMs": 22840, "OffsetEndMs": 23130}, {"Word": "they", "OffsetStartMs": 23130, "OffsetEndMs": 23295}, {"Word": "may", "OffsetStartMs": 23295, "OffsetEndMs": 23445}, {"Word": "be", "OffsetStartMs": 23445, "OffsetEndMs": 23595}, {"Word": "very", "OffsetStartMs": 23595, "OffsetEndMs": 23775}, {"Word": "high", "OffsetStartMs": 23775, "OffsetEndMs": 24045}, {"Word": "performance", "OffsetStartMs": 24045, "OffsetEndMs": 24360}, {"Word": "on", "OffsetStartMs": 24360, "OffsetEndMs": 24600}, {"Word": "some", "OffsetStartMs": 24600, "OffsetEndMs": 24735}, {"Word": "of", "OffsetStartMs": 24735, "OffsetEndMs": 24825}, {"Word": "the", "OffsetStartMs": 24825, "OffsetEndMs": 24960}, {"Word": "tasks", "OffsetStartMs": 24960, "OffsetEndMs": 25230}, {"Word": "that", "OffsetStartMs": 25230, "OffsetEndMs": 25470}, {"Word": "we", "OffsetStartMs": 25470, "OffsetEndMs": 25635}, {"Word": "study", "OffsetStartMs": 25635, "OffsetEndMs": 25830}, {"Word": "as", "OffsetStartMs": 25830, "OffsetEndMs": 26010}, {"Word": "part", "OffsetStartMs": 26010, "OffsetEndMs": 26160}, {"Word": "of", "OffsetStartMs": 26160, "OffsetEndMs": 26280}, {"Word": "this", "OffsetStartMs": 26280, "OffsetEndMs": 26475}, {"Word": "course", "OffsetStartMs": 26475, "OffsetEndMs": 26810}, {"Word": ".", "OffsetStartMs": 27310, "OffsetEndMs": 27690}, {"Word": "Soit's", "OffsetStartMs": 27690, "OffsetEndMs": 27990}, {"Word": "an", "OffsetStartMs": 27990, "OffsetEndMs": 28200}, {"Word": "incredibly", "OffsetStartMs": 28200, "OffsetEndMs": 28580}, {"Word": "exciting", "OffsetStartMs": 28750, "OffsetEndMs": 29145}, {"Word": "time", "OffsetStartMs": 29145, "OffsetEndMs": 29540}, {"Word": "for", "OffsetStartMs": 29950, "OffsetEndMs": 30350}, {"Word": "themis", "OffsetStartMs": 30400, "OffsetEndMs": 30915}, {"Word": "and", "OffsetStartMs": 30915, "OffsetEndMs": 31200}, {"Word": "specific", "OffsetStartMs": 31200, "OffsetEndMs": 31515}, {"Word": "right", "OffsetStartMs": 31515, "OffsetEndMs": 31815}, {"Word": "now", "OffsetStartMs": 31815, "OffsetEndMs": 32120}, {"Word": "where", "OffsetStartMs": 32350, "OffsetEndMs": 32655}, {"Word": "VC", "OffsetStartMs": 32655, "OffsetEndMs": 33045}, {"Word": "backed", "OffsetStartMs": 33045, "OffsetEndMs": 33555}, {"Word": "we're", "OffsetStartMs": 33555, "OffsetEndMs": 33990}, {"Word": "located", "OffsetStartMs": 33990, "OffsetEndMs": 34275}, {"Word": ".", "OffsetStartMs": 34275, "OffsetEndMs": 34575}, {"Word": "Ouroffices", "OffsetStartMs": 34575, "OffsetEndMs": 34875}, {"Word": "are", "OffsetStartMs": 34875, "OffsetEndMs": 35115}, {"Word": "right", "OffsetStartMs": 35115, "OffsetEndMs": 35340}, {"Word": "here", "OffsetStartMs": 35340, "OffsetEndMs": 35520}, {"Word": "in", "OffsetStartMs": 35520, "OffsetEndMs": 35685}, {"Word": "Cambridge", "OffsetStartMs": 35685, "OffsetEndMs": 36195}, {"Word": "so", "OffsetStartMs": 36195, "OffsetEndMs": 36435}, {"Word": "we're", "OffsetStartMs": 36435, "OffsetEndMs": 36615}, {"Word": "local", "OffsetStartMs": 36615, "OffsetEndMs": 36860}, {"Word": "and", "OffsetStartMs": 37330, "OffsetEndMs": 37590}, {"Word": "we", "OffsetStartMs": 37590, "OffsetEndMs": 37710}, {"Word": "have", "OffsetStartMs": 37710, "OffsetEndMs": 37935}, {"Word": "just", "OffsetStartMs": 37935, "OffsetEndMs": 38250}, {"Word": "closed", "OffsetStartMs": 38250, "OffsetEndMs": 38600}, {"Word": "around", "OffsetStartMs": 38620, "OffsetEndMs": 38940}, {"Word": "the", "OffsetStartMs": 38940, "OffsetEndMs": 39135}, {"Word": "funding", "OffsetStartMs": 39135, "OffsetEndMs": 39410}, {"Word": "so", "OffsetStartMs": 40120, "OffsetEndMs": 40485}, {"Word": "we're", "OffsetStartMs": 40485, "OffsetEndMs": 40905}, {"Word": "actively", "OffsetStartMs": 40905, "OffsetEndMs": 41190}, {"Word": "hiring", "OffsetStartMs": 41190, "OffsetEndMs": 41510}, {"Word": "the", "OffsetStartMs": 41560, "OffsetEndMs": 41850}, {"Word": "best", "OffsetStartMs": 41850, "OffsetEndMs": 42105}, {"Word": "and", "OffsetStartMs": 42105, "OffsetEndMs": 42345}, {"Word": "the", "OffsetStartMs": 42345, "OffsetEndMs": 42510}, {"Word": "brightest", "OffsetStartMs": 42510, "OffsetEndMs": 42980}, {"Word": "engineers", "OffsetStartMs": 43030, "OffsetEndMs": 43430}, {"Word": "like", "OffsetStartMs": 43510, "OffsetEndMs": 43815}, {"Word": "all", "OffsetStartMs": 43815, "OffsetEndMs": 43995}, {"Word": "of", "OffsetStartMs": 43995, "OffsetEndMs": 44130}, {"Word": "you", "OffsetStartMs": 44130, "OffsetEndMs": 44390}, {"Word": "to", "OffsetStartMs": 45040, "OffsetEndMs": 45360}, {"Word": "realize", "OffsetStartMs": 45360, "OffsetEndMs": 45600}, {"Word": "the", "OffsetStartMs": 45600, "OffsetEndMs": 45780}, {"Word": "future", "OffsetStartMs": 45780, "OffsetEndMs": 46020}, {"Word": "of", "OffsetStartMs": 46020, "OffsetEndMs": 46290}, {"Word": "safe", "OffsetStartMs": 46290, "OffsetEndMs": 46530}, {"Word": "and", "OffsetStartMs": 46530, "OffsetEndMs": 46770}, {"Word": "trustworthy", "OffsetStartMs": 46770, "OffsetEndMs": 47355}, {"Word": "AI", "OffsetStartMs": 47355, "OffsetEndMs": 47685}, {"Word": "and", "OffsetStartMs": 47685, "OffsetEndMs": 47940}, {"Word": "we", "OffsetStartMs": 47940, "OffsetEndMs": 48060}, {"Word": "hope", "OffsetStartMs": 48060, "OffsetEndMs": 48255}, {"Word": "that", "OffsetStartMs": 48255, "OffsetEndMs": 48585}, {"Word": "really", "OffsetStartMs": 48585, "OffsetEndMs": 48930}, {"Word": "today's", "OffsetStartMs": 48930, "OffsetEndMs": 49365}, {"Word": "lecture", "OffsetStartMs": 49365, "OffsetEndMs": 49560}, {"Word": "inspires", "OffsetStartMs": 49560, "OffsetEndMs": 50145}, {"Word": "you", "OffsetStartMs": 50145, "OffsetEndMs": 50540}, {"Word": "to", "OffsetStartMs": 50770, "OffsetEndMs": 51170}, {"Word": "join", "OffsetStartMs": 51190, "OffsetEndMs": 51495}, {"Word": "us", "OffsetStartMs": 51495, "OffsetEndMs": 51780}, {"Word": "on", "OffsetStartMs": 51780, "OffsetEndMs": 52065}, {"Word": "this", "OffsetStartMs": 52065, "OffsetEndMs": 52320}, {"Word": "mission", "OffsetStartMs": 52320, "OffsetEndMs": 52670}, {"Word": "to", "OffsetStartMs": 52750, "OffsetEndMs": 53025}, {"Word": "build", "OffsetStartMs": 53025, "OffsetEndMs": 53235}, {"Word": "the", "OffsetStartMs": 53235, "OffsetEndMs": 53430}, {"Word": "future", "OffsetStartMs": 53430, "OffsetEndMs": 53610}, {"Word": "of", "OffsetStartMs": 53610, "OffsetEndMs": 53865}, {"Word": "AI", "OffsetStartMs": 53865, "OffsetEndMs": 54120}, {"Word": ".", "OffsetStartMs": 54120, "OffsetEndMs": 54300}, {"Word": "Andwith", "OffsetStartMs": 54300, "OffsetEndMs": 54450}, {"Word": "that", "OffsetStartMs": 54450, "OffsetEndMs": 54740}, {"Word": "it's", "OffsetStartMs": 55240, "OffsetEndMs": 55620}, {"Word": "my", "OffsetStartMs": 55620, "OffsetEndMs": 55770}, {"Word": "great", "OffsetStartMs": 55770, "OffsetEndMs": 55995}, {"Word": "pleasure", "OffsetStartMs": 55995, "OffsetEndMs": 56330}, {"Word": "to", "OffsetStartMs": 56530, "OffsetEndMs": 56895}, {"Word": "introduce", "OffsetStartMs": 56895, "OffsetEndMs": 57165}, {"Word": "SOA", "OffsetStartMs": 57165, "OffsetEndMs": 57680}, {"Word": ".", "OffsetStartMs": 58030, "OffsetEndMs": 58515}, {"Word": "SOAis", "OffsetStartMs": 58515, "OffsetEndMs": 58620}, {"Word": "a", "OffsetStartMs": 58620, "OffsetEndMs": 58800}, {"Word": "machine", "OffsetStartMs": 58800, "OffsetEndMs": 59010}, {"Word": "learning", "OffsetStartMs": 59010, "OffsetEndMs": 59250}, {"Word": "scientist", "OffsetStartMs": 59250, "OffsetEndMs": 59600}], "SpeechSpeed": 15.6}, {"FinalSentence": "At themis, she's also the lead ta of this course, intro to deep learning at MIT. Her research at themis focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy AI. And today she'll be teaching us more about specifically the bias and the uncertainty realms of AI algorithms, which are really two key or critical components towards achieving this mission or this vision of safe and trustworthy deployment of AI all around us. So thank you and please give a big, warm round of applause for s.", "SliceSentence": "At themis she's also the lead ta of this course intro to deep learning at MIT . Herresearch at themis focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy AI . Andtoday she'll be teaching us more about specifically the bias and the uncertainty realms of AI algorithms which are really two key or critical components towards achieving this mission or this vision of safe and trustworthy deployment of AI all around us . Sothank you and please give a big warm round of applause for s", "StartMs": 185840, "EndMs": 224860, "WordsNum": 101, "Words": [{"Word": "At", "OffsetStartMs": 0, "OffsetEndMs": 120}, {"Word": "themis", "OffsetStartMs": 120, "OffsetEndMs": 560}, {"Word": "she's", "OffsetStartMs": 1240, "OffsetEndMs": 1790}, {"Word": "also", "OffsetStartMs": 2020, "OffsetEndMs": 2420}, {"Word": "the", "OffsetStartMs": 2440, "OffsetEndMs": 2730}, {"Word": "lead", "OffsetStartMs": 2730, "OffsetEndMs": 2970}, {"Word": "ta", "OffsetStartMs": 2970, "OffsetEndMs": 3320}, {"Word": "of", "OffsetStartMs": 3400, "OffsetEndMs": 3675}, {"Word": "this", "OffsetStartMs": 3675, "OffsetEndMs": 3885}, {"Word": "course", "OffsetStartMs": 3885, "OffsetEndMs": 4220}, {"Word": "intro", "OffsetStartMs": 4330, "OffsetEndMs": 4740}, {"Word": "to", "OffsetStartMs": 4740, "OffsetEndMs": 4860}, {"Word": "deep", "OffsetStartMs": 4860, "OffsetEndMs": 5010}, {"Word": "learning", "OffsetStartMs": 5010, "OffsetEndMs": 5300}, {"Word": "at", "OffsetStartMs": 5320, "OffsetEndMs": 5670}, {"Word": "MIT", "OffsetStartMs": 5670, "OffsetEndMs": 6020}, {"Word": ".", "OffsetStartMs": 6370, "OffsetEndMs": 6645}, {"Word": "Herresearch", "OffsetStartMs": 6645, "OffsetEndMs": 6915}, {"Word": "at", "OffsetStartMs": 6915, "OffsetEndMs": 7215}, {"Word": "themis", "OffsetStartMs": 7215, "OffsetEndMs": 7590}, {"Word": "focuses", "OffsetStartMs": 7590, "OffsetEndMs": 8145}, {"Word": "specifically", "OffsetStartMs": 8145, "OffsetEndMs": 8480}, {"Word": "on", "OffsetStartMs": 8560, "OffsetEndMs": 8865}, {"Word": "how", "OffsetStartMs": 8865, "OffsetEndMs": 9170}, {"Word": "we", "OffsetStartMs": 9520, "OffsetEndMs": 9780}, {"Word": "can", "OffsetStartMs": 9780, "OffsetEndMs": 9915}, {"Word": "build", "OffsetStartMs": 9915, "OffsetEndMs": 10140}, {"Word": "very", "OffsetStartMs": 10140, "OffsetEndMs": 10470}, {"Word": "modular", "OffsetStartMs": 10470, "OffsetEndMs": 10995}, {"Word": "and", "OffsetStartMs": 10995, "OffsetEndMs": 11205}, {"Word": "flexible", "OffsetStartMs": 11205, "OffsetEndMs": 11510}, {"Word": "methods", "OffsetStartMs": 11590, "OffsetEndMs": 11990}, {"Word": "for", "OffsetStartMs": 12460, "OffsetEndMs": 12860}, {"Word": "AI", "OffsetStartMs": 13000, "OffsetEndMs": 13350}, {"Word": "and", "OffsetStartMs": 13350, "OffsetEndMs": 13590}, {"Word": "building", "OffsetStartMs": 13590, "OffsetEndMs": 13880}, {"Word": "what", "OffsetStartMs": 14200, "OffsetEndMs": 14460}, {"Word": "we", "OffsetStartMs": 14460, "OffsetEndMs": 14670}, {"Word": "call", "OffsetStartMs": 14670, "OffsetEndMs": 14895}, {"Word": "a", "OffsetStartMs": 14895, "OffsetEndMs": 15060}, {"Word": "safe", "OffsetStartMs": 15060, "OffsetEndMs": 15240}, {"Word": "and", "OffsetStartMs": 15240, "OffsetEndMs": 15420}, {"Word": "trustworthy", "OffsetStartMs": 15420, "OffsetEndMs": 16065}, {"Word": "AI", "OffsetStartMs": 16065, "OffsetEndMs": 16430}, {"Word": ".", "OffsetStartMs": 16750, "OffsetEndMs": 17040}, {"Word": "Andtoday", "OffsetStartMs": 17040, "OffsetEndMs": 17280}, {"Word": "she'll", "OffsetStartMs": 17280, "OffsetEndMs": 17520}, {"Word": "be", "OffsetStartMs": 17520, "OffsetEndMs": 17625}, {"Word": "teaching", "OffsetStartMs": 17625, "OffsetEndMs": 17820}, {"Word": "us", "OffsetStartMs": 17820, "OffsetEndMs": 18060}, {"Word": "more", "OffsetStartMs": 18060, "OffsetEndMs": 18315}, {"Word": "about", "OffsetStartMs": 18315, "OffsetEndMs": 18650}, {"Word": "specifically", "OffsetStartMs": 18790, "OffsetEndMs": 19190}, {"Word": "the", "OffsetStartMs": 19360, "OffsetEndMs": 19650}, {"Word": "bias", "OffsetStartMs": 19650, "OffsetEndMs": 20115}, {"Word": "and", "OffsetStartMs": 20115, "OffsetEndMs": 20355}, {"Word": "the", "OffsetStartMs": 20355, "OffsetEndMs": 20565}, {"Word": "uncertainty", "OffsetStartMs": 20565, "OffsetEndMs": 20900}, {"Word": "realms", "OffsetStartMs": 20920, "OffsetEndMs": 21560}, {"Word": "of", "OffsetStartMs": 21850, "OffsetEndMs": 22250}, {"Word": "AI", "OffsetStartMs": 22480, "OffsetEndMs": 22880}, {"Word": "algorithms", "OffsetStartMs": 22900, "OffsetEndMs": 23420}, {"Word": "which", "OffsetStartMs": 23950, "OffsetEndMs": 24210}, {"Word": "are", "OffsetStartMs": 24210, "OffsetEndMs": 24345}, {"Word": "really", "OffsetStartMs": 24345, "OffsetEndMs": 24570}, {"Word": "two", "OffsetStartMs": 24570, "OffsetEndMs": 24920}, {"Word": "key", "OffsetStartMs": 25060, "OffsetEndMs": 25455}, {"Word": "or", "OffsetStartMs": 25455, "OffsetEndMs": 25755}, {"Word": "critical", "OffsetStartMs": 25755, "OffsetEndMs": 26060}, {"Word": "components", "OffsetStartMs": 26260, "OffsetEndMs": 26660}, {"Word": "towards", "OffsetStartMs": 26710, "OffsetEndMs": 27110}, {"Word": "achieving", "OffsetStartMs": 27130, "OffsetEndMs": 27570}, {"Word": "this", "OffsetStartMs": 27570, "OffsetEndMs": 27795}, {"Word": "mission", "OffsetStartMs": 27795, "OffsetEndMs": 28080}, {"Word": "or", "OffsetStartMs": 28080, "OffsetEndMs": 28305}, {"Word": "this", "OffsetStartMs": 28305, "OffsetEndMs": 28485}, {"Word": "vision", "OffsetStartMs": 28485, "OffsetEndMs": 28790}, {"Word": "of", "OffsetStartMs": 29020, "OffsetEndMs": 29420}, {"Word": "safe", "OffsetStartMs": 29500, "OffsetEndMs": 29850}, {"Word": "and", "OffsetStartMs": 29850, "OffsetEndMs": 30090}, {"Word": "trustworthy", "OffsetStartMs": 30090, "OffsetEndMs": 30770}, {"Word": "deployment", "OffsetStartMs": 30850, "OffsetEndMs": 31430}, {"Word": "of", "OffsetStartMs": 31540, "OffsetEndMs": 31890}, {"Word": "AI", "OffsetStartMs": 31890, "OffsetEndMs": 32220}, {"Word": "all", "OffsetStartMs": 32220, "OffsetEndMs": 32520}, {"Word": "around", "OffsetStartMs": 32520, "OffsetEndMs": 32745}, {"Word": "us", "OffsetStartMs": 32745, "OffsetEndMs": 33050}, {"Word": ".", "OffsetStartMs": 33580, "OffsetEndMs": 33960}, {"Word": "Sothank", "OffsetStartMs": 33960, "OffsetEndMs": 34230}, {"Word": "you", "OffsetStartMs": 34230, "OffsetEndMs": 34380}, {"Word": "and", "OffsetStartMs": 34380, "OffsetEndMs": 34575}, {"Word": "please", "OffsetStartMs": 34575, "OffsetEndMs": 34910}, {"Word": "give", "OffsetStartMs": 35020, "OffsetEndMs": 35280}, {"Word": "a", "OffsetStartMs": 35280, "OffsetEndMs": 35540}, {"Word": "big", "OffsetStartMs": 35710, "OffsetEndMs": 36015}, {"Word": "warm", "OffsetStartMs": 36015, "OffsetEndMs": 36320}, {"Word": "round", "OffsetStartMs": 36550, "OffsetEndMs": 36825}, {"Word": "of", "OffsetStartMs": 36825, "OffsetEndMs": 36990}, {"Word": "applause", "OffsetStartMs": 36990, "OffsetEndMs": 37230}, {"Word": "for", "OffsetStartMs": 37230, "OffsetEndMs": 37530}, {"Word": "s", "OffsetStartMs": 37530, "OffsetEndMs": 37910}], "SpeechSpeed": 14.2}, {"FinalSentence": "Thank you so much, Alexander, for the introduction. Hi everyone. I'm salina, I'm a machine learning scientist here at themis AI and the lead ta of the course this year. And today I'm super excited to talk to you all about robust and trustworthy deep learning on behalf of themis.", "SliceSentence": "Thank you so much Alexander for the introduction . Hieveryone .I'm salina I'm a machine learning scientist here at themis AI and the lead ta of the course this year . Andtoday I'm super excited to talk to you all about robust and trustworthy deep learning on behalf of themis", "StartMs": 226140, "EndMs": 244740, "WordsNum": 50, "Words": [{"Word": "Thank", "OffsetStartMs": 1900, "OffsetEndMs": 2160}, {"Word": "you", "OffsetStartMs": 2160, "OffsetEndMs": 2295}, {"Word": "so", "OffsetStartMs": 2295, "OffsetEndMs": 2430}, {"Word": "much", "OffsetStartMs": 2430, "OffsetEndMs": 2685}, {"Word": "Alexander", "OffsetStartMs": 2685, "OffsetEndMs": 3000}, {"Word": "for", "OffsetStartMs": 3000, "OffsetEndMs": 3165}, {"Word": "the", "OffsetStartMs": 3165, "OffsetEndMs": 3240}, {"Word": "introduction", "OffsetStartMs": 3240, "OffsetEndMs": 3470}, {"Word": ".", "OffsetStartMs": 4660, "OffsetEndMs": 5040}, {"Word": "Hieveryone", "OffsetStartMs": 5040, "OffsetEndMs": 5400}, {"Word": ".I'm", "OffsetStartMs": 5400, "OffsetEndMs": 5775}, {"Word": "salina", "OffsetStartMs": 5775, "OffsetEndMs": 6240}, {"Word": "I'm", "OffsetStartMs": 6240, "OffsetEndMs": 6510}, {"Word": "a", "OffsetStartMs": 6510, "OffsetEndMs": 6675}, {"Word": "machine", "OffsetStartMs": 6675, "OffsetEndMs": 6885}, {"Word": "learning", "OffsetStartMs": 6885, "OffsetEndMs": 7110}, {"Word": "scientist", "OffsetStartMs": 7110, "OffsetEndMs": 7460}, {"Word": "here", "OffsetStartMs": 7750, "OffsetEndMs": 8070}, {"Word": "at", "OffsetStartMs": 8070, "OffsetEndMs": 8295}, {"Word": "themis", "OffsetStartMs": 8295, "OffsetEndMs": 8700}, {"Word": "AI", "OffsetStartMs": 8700, "OffsetEndMs": 9050}, {"Word": "and", "OffsetStartMs": 9430, "OffsetEndMs": 9830}, {"Word": "the", "OffsetStartMs": 9910, "OffsetEndMs": 10170}, {"Word": "lead", "OffsetStartMs": 10170, "OffsetEndMs": 10350}, {"Word": "ta", "OffsetStartMs": 10350, "OffsetEndMs": 10575}, {"Word": "of", "OffsetStartMs": 10575, "OffsetEndMs": 10740}, {"Word": "the", "OffsetStartMs": 10740, "OffsetEndMs": 10875}, {"Word": "course", "OffsetStartMs": 10875, "OffsetEndMs": 11085}, {"Word": "this", "OffsetStartMs": 11085, "OffsetEndMs": 11295}, {"Word": "year", "OffsetStartMs": 11295, "OffsetEndMs": 11570}, {"Word": ".", "OffsetStartMs": 11770, "OffsetEndMs": 12045}, {"Word": "Andtoday", "OffsetStartMs": 12045, "OffsetEndMs": 12315}, {"Word": "I'm", "OffsetStartMs": 12315, "OffsetEndMs": 12690}, {"Word": "super", "OffsetStartMs": 12690, "OffsetEndMs": 12930}, {"Word": "excited", "OffsetStartMs": 12930, "OffsetEndMs": 13245}, {"Word": "to", "OffsetStartMs": 13245, "OffsetEndMs": 13455}, {"Word": "talk", "OffsetStartMs": 13455, "OffsetEndMs": 13635}, {"Word": "to", "OffsetStartMs": 13635, "OffsetEndMs": 13785}, {"Word": "you", "OffsetStartMs": 13785, "OffsetEndMs": 13890}, {"Word": "all", "OffsetStartMs": 13890, "OffsetEndMs": 14150}, {"Word": "about", "OffsetStartMs": 14200, "OffsetEndMs": 14600}, {"Word": "robust", "OffsetStartMs": 14800, "OffsetEndMs": 15180}, {"Word": "and", "OffsetStartMs": 15180, "OffsetEndMs": 15465}, {"Word": "trustworthy", "OffsetStartMs": 15465, "OffsetEndMs": 16080}, {"Word": "deep", "OffsetStartMs": 16080, "OffsetEndMs": 16275}, {"Word": "learning", "OffsetStartMs": 16275, "OffsetEndMs": 16580}, {"Word": "on", "OffsetStartMs": 16630, "OffsetEndMs": 16965}, {"Word": "behalf", "OffsetStartMs": 16965, "OffsetEndMs": 17235}, {"Word": "of", "OffsetStartMs": 17235, "OffsetEndMs": 17445}, {"Word": "themis", "OffsetStartMs": 17445, "OffsetEndMs": 17870}], "SpeechSpeed": 14.6}, {"FinalSentence": "So over the past decade, we've seen some tremendous growth in artificial intelligence, across safety, critical domains, in the spheres of autonomy and robotics. We now have models that can make critical decisions about things like self driving at a second's notice, and these are paving the way for fully autonomous vehicles and robots, and that's not where this stops in the spheres of medicine and healthcare. Robots are now equipped to conduct life saving surgery. We have algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable. And we have models that can automatically diagnose diseases without intervention from any health care professionals at all. These advances are revolutionary, and they have the potential to change life as we know it today.", "SliceSentence": "So over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the spheres of autonomy and robotics . Wenow have models that can make critical decisions about things like self driving at a second's notice and these are paving the way for fully autonomous vehicles and robots and that's not where this stops in the spheres of medicine and healthcare . Robotsare now equipped to conduct life saving surgery . Wehave algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable . Andwe have models that can automatically diagnose diseases without intervention from any health care professionals at all . Theseadvances are revolutionary and they have the potential to change life as we know it today", "StartMs": 246040, "EndMs": 291020, "WordsNum": 130, "Words": [{"Word": "So", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "over", "OffsetStartMs": 670, "OffsetEndMs": 1005}, {"Word": "the", "OffsetStartMs": 1005, "OffsetEndMs": 1215}, {"Word": "past", "OffsetStartMs": 1215, "OffsetEndMs": 1440}, {"Word": "decade", "OffsetStartMs": 1440, "OffsetEndMs": 1790}, {"Word": "we've", "OffsetStartMs": 1810, "OffsetEndMs": 2205}, {"Word": "seen", "OffsetStartMs": 2205, "OffsetEndMs": 2460}, {"Word": "some", "OffsetStartMs": 2460, "OffsetEndMs": 2760}, {"Word": "tremendous", "OffsetStartMs": 2760, "OffsetEndMs": 3080}, {"Word": "growth", "OffsetStartMs": 3190, "OffsetEndMs": 3555}, {"Word": "in", "OffsetStartMs": 3555, "OffsetEndMs": 3900}, {"Word": "artificial", "OffsetStartMs": 3900, "OffsetEndMs": 4245}, {"Word": "intelligence", "OffsetStartMs": 4245, "OffsetEndMs": 4610}, {"Word": "across", "OffsetStartMs": 4870, "OffsetEndMs": 5250}, {"Word": "safety", "OffsetStartMs": 5250, "OffsetEndMs": 5595}, {"Word": "critical", "OffsetStartMs": 5595, "OffsetEndMs": 5910}, {"Word": "domains", "OffsetStartMs": 5910, "OffsetEndMs": 6380}, {"Word": "in", "OffsetStartMs": 6970, "OffsetEndMs": 7245}, {"Word": "the", "OffsetStartMs": 7245, "OffsetEndMs": 7410}, {"Word": "spheres", "OffsetStartMs": 7410, "OffsetEndMs": 7785}, {"Word": "of", "OffsetStartMs": 7785, "OffsetEndMs": 7965}, {"Word": "autonomy", "OffsetStartMs": 7965, "OffsetEndMs": 8565}, {"Word": "and", "OffsetStartMs": 8565, "OffsetEndMs": 8760}, {"Word": "robotics", "OffsetStartMs": 8760, "OffsetEndMs": 9230}, {"Word": ".", "OffsetStartMs": 9490, "OffsetEndMs": 9780}, {"Word": "Wenow", "OffsetStartMs": 9780, "OffsetEndMs": 10050}, {"Word": "have", "OffsetStartMs": 10050, "OffsetEndMs": 10430}, {"Word": "models", "OffsetStartMs": 10540, "OffsetEndMs": 10890}, {"Word": "that", "OffsetStartMs": 10890, "OffsetEndMs": 11100}, {"Word": "can", "OffsetStartMs": 11100, "OffsetEndMs": 11235}, {"Word": "make", "OffsetStartMs": 11235, "OffsetEndMs": 11430}, {"Word": "critical", "OffsetStartMs": 11430, "OffsetEndMs": 11750}, {"Word": "decisions", "OffsetStartMs": 11770, "OffsetEndMs": 12165}, {"Word": "about", "OffsetStartMs": 12165, "OffsetEndMs": 12450}, {"Word": "things", "OffsetStartMs": 12450, "OffsetEndMs": 12630}, {"Word": "like", "OffsetStartMs": 12630, "OffsetEndMs": 12825}, {"Word": "self", "OffsetStartMs": 12825, "OffsetEndMs": 13035}, {"Word": "driving", "OffsetStartMs": 13035, "OffsetEndMs": 13340}, {"Word": "at", "OffsetStartMs": 13480, "OffsetEndMs": 13740}, {"Word": "a", "OffsetStartMs": 13740, "OffsetEndMs": 13875}, {"Word": "second's", "OffsetStartMs": 13875, "OffsetEndMs": 14280}, {"Word": "notice", "OffsetStartMs": 14280, "OffsetEndMs": 14540}, {"Word": "and", "OffsetStartMs": 14980, "OffsetEndMs": 15255}, {"Word": "these", "OffsetStartMs": 15255, "OffsetEndMs": 15405}, {"Word": "are", "OffsetStartMs": 15405, "OffsetEndMs": 15570}, {"Word": "paving", "OffsetStartMs": 15570, "OffsetEndMs": 15900}, {"Word": "the", "OffsetStartMs": 15900, "OffsetEndMs": 16020}, {"Word": "way", "OffsetStartMs": 16020, "OffsetEndMs": 16230}, {"Word": "for", "OffsetStartMs": 16230, "OffsetEndMs": 16485}, {"Word": "fully", "OffsetStartMs": 16485, "OffsetEndMs": 16710}, {"Word": "autonomous", "OffsetStartMs": 16710, "OffsetEndMs": 17280}, {"Word": "vehicles", "OffsetStartMs": 17280, "OffsetEndMs": 17570}, {"Word": "and", "OffsetStartMs": 17620, "OffsetEndMs": 17910}, {"Word": "robots", "OffsetStartMs": 17910, "OffsetEndMs": 18320}, {"Word": "and", "OffsetStartMs": 18970, "OffsetEndMs": 19245}, {"Word": "that's", "OffsetStartMs": 19245, "OffsetEndMs": 19470}, {"Word": "not", "OffsetStartMs": 19470, "OffsetEndMs": 19605}, {"Word": "where", "OffsetStartMs": 19605, "OffsetEndMs": 19800}, {"Word": "this", "OffsetStartMs": 19800, "OffsetEndMs": 20010}, {"Word": "stops", "OffsetStartMs": 20010, "OffsetEndMs": 20330}, {"Word": "in", "OffsetStartMs": 20440, "OffsetEndMs": 20700}, {"Word": "the", "OffsetStartMs": 20700, "OffsetEndMs": 20850}, {"Word": "spheres", "OffsetStartMs": 20850, "OffsetEndMs": 21240}, {"Word": "of", "OffsetStartMs": 21240, "OffsetEndMs": 21450}, {"Word": "medicine", "OffsetStartMs": 21450, "OffsetEndMs": 21740}, {"Word": "and", "OffsetStartMs": 21790, "OffsetEndMs": 22065}, {"Word": "healthcare", "OffsetStartMs": 22065, "OffsetEndMs": 22340}, {"Word": ".", "OffsetStartMs": 22810, "OffsetEndMs": 23295}, {"Word": "Robotsare", "OffsetStartMs": 23295, "OffsetEndMs": 23475}, {"Word": "now", "OffsetStartMs": 23475, "OffsetEndMs": 23700}, {"Word": "equipped", "OffsetStartMs": 23700, "OffsetEndMs": 24030}, {"Word": "to", "OffsetStartMs": 24030, "OffsetEndMs": 24240}, {"Word": "conduct", "OffsetStartMs": 24240, "OffsetEndMs": 24500}, {"Word": "life", "OffsetStartMs": 24580, "OffsetEndMs": 24930}, {"Word": "saving", "OffsetStartMs": 24930, "OffsetEndMs": 25260}, {"Word": "surgery", "OffsetStartMs": 25260, "OffsetEndMs": 25640}, {"Word": ".", "OffsetStartMs": 26140, "OffsetEndMs": 26415}, {"Word": "Wehave", "OffsetStartMs": 26415, "OffsetEndMs": 26690}, {"Word": "algorithms", "OffsetStartMs": 26710, "OffsetEndMs": 27150}, {"Word": "that", "OffsetStartMs": 27150, "OffsetEndMs": 27360}, {"Word": "generate", "OffsetStartMs": 27360, "OffsetEndMs": 27630}, {"Word": "predictions", "OffsetStartMs": 27630, "OffsetEndMs": 28215}, {"Word": "for", "OffsetStartMs": 28215, "OffsetEndMs": 28410}, {"Word": "critical", "OffsetStartMs": 28410, "OffsetEndMs": 28695}, {"Word": "drugs", "OffsetStartMs": 28695, "OffsetEndMs": 29090}, {"Word": "that", "OffsetStartMs": 29110, "OffsetEndMs": 29385}, {"Word": "may", "OffsetStartMs": 29385, "OffsetEndMs": 29595}, {"Word": "cure", "OffsetStartMs": 29595, "OffsetEndMs": 29880}, {"Word": "diseases", "OffsetStartMs": 29880, "OffsetEndMs": 30225}, {"Word": "that", "OffsetStartMs": 30225, "OffsetEndMs": 30495}, {"Word": "we", "OffsetStartMs": 30495, "OffsetEndMs": 30675}, {"Word": "previously", "OffsetStartMs": 30675, "OffsetEndMs": 30980}, {"Word": "thought", "OffsetStartMs": 31060, "OffsetEndMs": 31365}, {"Word": "were", "OffsetStartMs": 31365, "OffsetEndMs": 31530}, {"Word": "incurable", "OffsetStartMs": 31530, "OffsetEndMs": 32210}, {"Word": ".", "OffsetStartMs": 32710, "OffsetEndMs": 33105}, {"Word": "Andwe", "OffsetStartMs": 33105, "OffsetEndMs": 33360}, {"Word": "have", "OffsetStartMs": 33360, "OffsetEndMs": 33510}, {"Word": "models", "OffsetStartMs": 33510, "OffsetEndMs": 33780}, {"Word": "that", "OffsetStartMs": 33780, "OffsetEndMs": 34035}, {"Word": "can", "OffsetStartMs": 34035, "OffsetEndMs": 34310}, {"Word": "automatically", "OffsetStartMs": 34360, "OffsetEndMs": 34760}, {"Word": "diagnose", "OffsetStartMs": 34780, "OffsetEndMs": 35460}, {"Word": "diseases", "OffsetStartMs": 35460, "OffsetEndMs": 35840}, {"Word": "without", "OffsetStartMs": 35890, "OffsetEndMs": 36255}, {"Word": "intervention", "OffsetStartMs": 36255, "OffsetEndMs": 36870}, {"Word": "from", "OffsetStartMs": 36870, "OffsetEndMs": 37125}, {"Word": "any", "OffsetStartMs": 37125, "OffsetEndMs": 37335}, {"Word": "health", "OffsetStartMs": 37335, "OffsetEndMs": 37605}, {"Word": "care", "OffsetStartMs": 37605, "OffsetEndMs": 37860}, {"Word": "professionals", "OffsetStartMs": 37860, "OffsetEndMs": 38370}, {"Word": "at", "OffsetStartMs": 38370, "OffsetEndMs": 38550}, {"Word": "all", "OffsetStartMs": 38550, "OffsetEndMs": 38810}, {"Word": ".", "OffsetStartMs": 39730, "OffsetEndMs": 40095}, {"Word": "Theseadvances", "OffsetStartMs": 40095, "OffsetEndMs": 40575}, {"Word": "are", "OffsetStartMs": 40575, "OffsetEndMs": 40755}, {"Word": "revolutionary", "OffsetStartMs": 40755, "OffsetEndMs": 41535}, {"Word": "and", "OffsetStartMs": 41535, "OffsetEndMs": 41805}, {"Word": "they", "OffsetStartMs": 41805, "OffsetEndMs": 41955}, {"Word": "have", "OffsetStartMs": 41955, "OffsetEndMs": 42120}, {"Word": "the", "OffsetStartMs": 42120, "OffsetEndMs": 42330}, {"Word": "potential", "OffsetStartMs": 42330, "OffsetEndMs": 42600}, {"Word": "to", "OffsetStartMs": 42600, "OffsetEndMs": 42810}, {"Word": "change", "OffsetStartMs": 42810, "OffsetEndMs": 42990}, {"Word": "life", "OffsetStartMs": 42990, "OffsetEndMs": 43275}, {"Word": "as", "OffsetStartMs": 43275, "OffsetEndMs": 43515}, {"Word": "we", "OffsetStartMs": 43515, "OffsetEndMs": 43650}, {"Word": "know", "OffsetStartMs": 43650, "OffsetEndMs": 43755}, {"Word": "it", "OffsetStartMs": 43755, "OffsetEndMs": 43875}, {"Word": "today", "OffsetStartMs": 43875, "OffsetEndMs": 44150}], "SpeechSpeed": 17.9}, {"FinalSentence": "But there's another question that we need to ask, which is where are these models in real life? A lot of these technologies were innovated five, ten years ago, but you and I don't see them in our daily lives. So what is, what's the gap here between innovation and deployment?", "SliceSentence": "But there's another question that we need to ask which is where are these models in real life A lot of these technologies were innovated five ten years ago but you and I don't see them in our daily lives . Sowhat is what's the gap here between innovation and deployment", "StartMs": 291480, "EndMs": 307480, "WordsNum": 51, "Words": [{"Word": "But", "OffsetStartMs": 70, "OffsetEndMs": 420}, {"Word": "there's", "OffsetStartMs": 420, "OffsetEndMs": 750}, {"Word": "another", "OffsetStartMs": 750, "OffsetEndMs": 930}, {"Word": "question", "OffsetStartMs": 930, "OffsetEndMs": 1200}, {"Word": "that", "OffsetStartMs": 1200, "OffsetEndMs": 1410}, {"Word": "we", "OffsetStartMs": 1410, "OffsetEndMs": 1530}, {"Word": "need", "OffsetStartMs": 1530, "OffsetEndMs": 1680}, {"Word": "to", "OffsetStartMs": 1680, "OffsetEndMs": 1815}, {"Word": "ask", "OffsetStartMs": 1815, "OffsetEndMs": 2060}, {"Word": "which", "OffsetStartMs": 2140, "OffsetEndMs": 2415}, {"Word": "is", "OffsetStartMs": 2415, "OffsetEndMs": 2690}, {"Word": "where", "OffsetStartMs": 2980, "OffsetEndMs": 3300}, {"Word": "are", "OffsetStartMs": 3300, "OffsetEndMs": 3540}, {"Word": "these", "OffsetStartMs": 3540, "OffsetEndMs": 3750}, {"Word": "models", "OffsetStartMs": 3750, "OffsetEndMs": 4005}, {"Word": "in", "OffsetStartMs": 4005, "OffsetEndMs": 4245}, {"Word": "real", "OffsetStartMs": 4245, "OffsetEndMs": 4425}, {"Word": "life", "OffsetStartMs": 4425, "OffsetEndMs": 4730}, {"Word": "A", "OffsetStartMs": 5140, "OffsetEndMs": 5415}, {"Word": "lot", "OffsetStartMs": 5415, "OffsetEndMs": 5565}, {"Word": "of", "OffsetStartMs": 5565, "OffsetEndMs": 5715}, {"Word": "these", "OffsetStartMs": 5715, "OffsetEndMs": 5895}, {"Word": "technologies", "OffsetStartMs": 5895, "OffsetEndMs": 6200}, {"Word": "were", "OffsetStartMs": 6430, "OffsetEndMs": 6675}, {"Word": "innovated", "OffsetStartMs": 6675, "OffsetEndMs": 7065}, {"Word": "five", "OffsetStartMs": 7065, "OffsetEndMs": 7425}, {"Word": "ten", "OffsetStartMs": 7425, "OffsetEndMs": 7725}, {"Word": "years", "OffsetStartMs": 7725, "OffsetEndMs": 8010}, {"Word": "ago", "OffsetStartMs": 8010, "OffsetEndMs": 8390}, {"Word": "but", "OffsetStartMs": 8470, "OffsetEndMs": 8745}, {"Word": "you", "OffsetStartMs": 8745, "OffsetEndMs": 8880}, {"Word": "and", "OffsetStartMs": 8880, "OffsetEndMs": 9030}, {"Word": "I", "OffsetStartMs": 9030, "OffsetEndMs": 9300}, {"Word": "don't", "OffsetStartMs": 9300, "OffsetEndMs": 9675}, {"Word": "see", "OffsetStartMs": 9675, "OffsetEndMs": 9825}, {"Word": "them", "OffsetStartMs": 9825, "OffsetEndMs": 9990}, {"Word": "in", "OffsetStartMs": 9990, "OffsetEndMs": 10125}, {"Word": "our", "OffsetStartMs": 10125, "OffsetEndMs": 10260}, {"Word": "daily", "OffsetStartMs": 10260, "OffsetEndMs": 10485}, {"Word": "lives", "OffsetStartMs": 10485, "OffsetEndMs": 10820}, {"Word": ".", "OffsetStartMs": 11470, "OffsetEndMs": 11870}, {"Word": "Sowhat", "OffsetStartMs": 12040, "OffsetEndMs": 12330}, {"Word": "is", "OffsetStartMs": 12330, "OffsetEndMs": 12570}, {"Word": "what's", "OffsetStartMs": 12570, "OffsetEndMs": 12900}, {"Word": "the", "OffsetStartMs": 12900, "OffsetEndMs": 13020}, {"Word": "gap", "OffsetStartMs": 13020, "OffsetEndMs": 13230}, {"Word": "here", "OffsetStartMs": 13230, "OffsetEndMs": 13560}, {"Word": "between", "OffsetStartMs": 13560, "OffsetEndMs": 13935}, {"Word": "innovation", "OffsetStartMs": 13935, "OffsetEndMs": 14330}, {"Word": "and", "OffsetStartMs": 14380, "OffsetEndMs": 14760}, {"Word": "deployment", "OffsetStartMs": 14760, "OffsetEndMs": 15260}], "SpeechSpeed": 16.8}, {"FinalSentence": "The reason why you and I can't go by self driving cars or robots don't typically assist in operating rooms is this. These are some headlines about the failures of AI from the last few years alone. In addition to these incredible advances, we've also seen catastrophic failures in every single one of the safety critical domains I just mentioned. These problems range from crashing autonomous vehicles to health care algorithms that don't actually work for everyone, even though they're deployed out in the real world so everyone can use them.", "SliceSentence": "The reason why you and I can't go by self driving cars or robots don't typically assist in operating rooms is this . Theseare some headlines about the failures of AI from the last few years alone . Inaddition to these incredible advances we've also seen catastrophic failures in every single one of the safety critical domains I just mentioned . Theseproblems range from crashing autonomous vehicles to health care algorithms that don't actually work for everyone even though they're deployed out in the real world so everyone can use them", "StartMs": 309200, "EndMs": 341000, "WordsNum": 91, "Words": [{"Word": "The", "OffsetStartMs": 70, "OffsetEndMs": 330}, {"Word": "reason", "OffsetStartMs": 330, "OffsetEndMs": 555}, {"Word": "why", "OffsetStartMs": 555, "OffsetEndMs": 855}, {"Word": "you", "OffsetStartMs": 855, "OffsetEndMs": 1035}, {"Word": "and", "OffsetStartMs": 1035, "OffsetEndMs": 1140}, {"Word": "I", "OffsetStartMs": 1140, "OffsetEndMs": 1335}, {"Word": "can't", "OffsetStartMs": 1335, "OffsetEndMs": 1620}, {"Word": "go", "OffsetStartMs": 1620, "OffsetEndMs": 1740}, {"Word": "by", "OffsetStartMs": 1740, "OffsetEndMs": 1950}, {"Word": "self", "OffsetStartMs": 1950, "OffsetEndMs": 2190}, {"Word": "driving", "OffsetStartMs": 2190, "OffsetEndMs": 2475}, {"Word": "cars", "OffsetStartMs": 2475, "OffsetEndMs": 2840}, {"Word": "or", "OffsetStartMs": 2920, "OffsetEndMs": 3210}, {"Word": "robots", "OffsetStartMs": 3210, "OffsetEndMs": 3570}, {"Word": "don't", "OffsetStartMs": 3570, "OffsetEndMs": 4010}, {"Word": "typically", "OffsetStartMs": 4270, "OffsetEndMs": 4670}, {"Word": "assist", "OffsetStartMs": 4810, "OffsetEndMs": 5145}, {"Word": "in", "OffsetStartMs": 5145, "OffsetEndMs": 5445}, {"Word": "operating", "OffsetStartMs": 5445, "OffsetEndMs": 5745}, {"Word": "rooms", "OffsetStartMs": 5745, "OffsetEndMs": 6080}, {"Word": "is", "OffsetStartMs": 6130, "OffsetEndMs": 6435}, {"Word": "this", "OffsetStartMs": 6435, "OffsetEndMs": 6740}, {"Word": ".", "OffsetStartMs": 7450, "OffsetEndMs": 7800}, {"Word": "Theseare", "OffsetStartMs": 7800, "OffsetEndMs": 8040}, {"Word": "some", "OffsetStartMs": 8040, "OffsetEndMs": 8235}, {"Word": "headlines", "OffsetStartMs": 8235, "OffsetEndMs": 8670}, {"Word": "about", "OffsetStartMs": 8670, "OffsetEndMs": 8925}, {"Word": "the", "OffsetStartMs": 8925, "OffsetEndMs": 9060}, {"Word": "failures", "OffsetStartMs": 9060, "OffsetEndMs": 9420}, {"Word": "of", "OffsetStartMs": 9420, "OffsetEndMs": 9630}, {"Word": "AI", "OffsetStartMs": 9630, "OffsetEndMs": 9945}, {"Word": "from", "OffsetStartMs": 9945, "OffsetEndMs": 10200}, {"Word": "the", "OffsetStartMs": 10200, "OffsetEndMs": 10305}, {"Word": "last", "OffsetStartMs": 10305, "OffsetEndMs": 10545}, {"Word": "few", "OffsetStartMs": 10545, "OffsetEndMs": 10815}, {"Word": "years", "OffsetStartMs": 10815, "OffsetEndMs": 11040}, {"Word": "alone", "OffsetStartMs": 11040, "OffsetEndMs": 11390}, {"Word": ".", "OffsetStartMs": 12250, "OffsetEndMs": 12570}, {"Word": "Inaddition", "OffsetStartMs": 12570, "OffsetEndMs": 12885}, {"Word": "to", "OffsetStartMs": 12885, "OffsetEndMs": 13140}, {"Word": "these", "OffsetStartMs": 13140, "OffsetEndMs": 13400}, {"Word": "incredible", "OffsetStartMs": 13540, "OffsetEndMs": 13940}, {"Word": "advances", "OffsetStartMs": 14650, "OffsetEndMs": 15270}, {"Word": "we've", "OffsetStartMs": 15270, "OffsetEndMs": 15705}, {"Word": "also", "OffsetStartMs": 15705, "OffsetEndMs": 15960}, {"Word": "seen", "OffsetStartMs": 15960, "OffsetEndMs": 16250}, {"Word": "catastrophic", "OffsetStartMs": 16270, "OffsetEndMs": 17085}, {"Word": "failures", "OffsetStartMs": 17085, "OffsetEndMs": 17600}, {"Word": "in", "OffsetStartMs": 17710, "OffsetEndMs": 18015}, {"Word": "every", "OffsetStartMs": 18015, "OffsetEndMs": 18320}, {"Word": "single", "OffsetStartMs": 18340, "OffsetEndMs": 18690}, {"Word": "one", "OffsetStartMs": 18690, "OffsetEndMs": 18975}, {"Word": "of", "OffsetStartMs": 18975, "OffsetEndMs": 19170}, {"Word": "the", "OffsetStartMs": 19170, "OffsetEndMs": 19305}, {"Word": "safety", "OffsetStartMs": 19305, "OffsetEndMs": 19530}, {"Word": "critical", "OffsetStartMs": 19530, "OffsetEndMs": 19815}, {"Word": "domains", "OffsetStartMs": 19815, "OffsetEndMs": 20220}, {"Word": "I", "OffsetStartMs": 20220, "OffsetEndMs": 20460}, {"Word": "just", "OffsetStartMs": 20460, "OffsetEndMs": 20655}, {"Word": "mentioned", "OffsetStartMs": 20655, "OffsetEndMs": 20960}, {"Word": ".", "OffsetStartMs": 21940, "OffsetEndMs": 22275}, {"Word": "Theseproblems", "OffsetStartMs": 22275, "OffsetEndMs": 22610}, {"Word": "range", "OffsetStartMs": 22660, "OffsetEndMs": 23040}, {"Word": "from", "OffsetStartMs": 23040, "OffsetEndMs": 23355}, {"Word": "crashing", "OffsetStartMs": 23355, "OffsetEndMs": 23775}, {"Word": "autonomous", "OffsetStartMs": 23775, "OffsetEndMs": 24240}, {"Word": "vehicles", "OffsetStartMs": 24240, "OffsetEndMs": 24530}, {"Word": "to", "OffsetStartMs": 24880, "OffsetEndMs": 25140}, {"Word": "health", "OffsetStartMs": 25140, "OffsetEndMs": 25335}, {"Word": "care", "OffsetStartMs": 25335, "OffsetEndMs": 25635}, {"Word": "algorithms", "OffsetStartMs": 25635, "OffsetEndMs": 26070}, {"Word": "that", "OffsetStartMs": 26070, "OffsetEndMs": 26250}, {"Word": "don't", "OffsetStartMs": 26250, "OffsetEndMs": 26625}, {"Word": "actually", "OffsetStartMs": 26625, "OffsetEndMs": 26940}, {"Word": "work", "OffsetStartMs": 26940, "OffsetEndMs": 27195}, {"Word": "for", "OffsetStartMs": 27195, "OffsetEndMs": 27530}, {"Word": "everyone", "OffsetStartMs": 27550, "OffsetEndMs": 27950}, {"Word": "even", "OffsetStartMs": 28030, "OffsetEndMs": 28365}, {"Word": "though", "OffsetStartMs": 28365, "OffsetEndMs": 28575}, {"Word": "they're", "OffsetStartMs": 28575, "OffsetEndMs": 28860}, {"Word": "deployed", "OffsetStartMs": 28860, "OffsetEndMs": 29175}, {"Word": "out", "OffsetStartMs": 29175, "OffsetEndMs": 29355}, {"Word": "in", "OffsetStartMs": 29355, "OffsetEndMs": 29520}, {"Word": "the", "OffsetStartMs": 29520, "OffsetEndMs": 29640}, {"Word": "real", "OffsetStartMs": 29640, "OffsetEndMs": 29805}, {"Word": "world", "OffsetStartMs": 29805, "OffsetEndMs": 30110}, {"Word": "so", "OffsetStartMs": 30160, "OffsetEndMs": 30525}, {"Word": "everyone", "OffsetStartMs": 30525, "OffsetEndMs": 30780}, {"Word": "can", "OffsetStartMs": 30780, "OffsetEndMs": 30930}, {"Word": "use", "OffsetStartMs": 30930, "OffsetEndMs": 31095}, {"Word": "them", "OffsetStartMs": 31095, "OffsetEndMs": 31400}], "SpeechSpeed": 16.9}, {"FinalSentence": "Now, at a first glance, this seems really demoralizing. If these are all of the things wrong with artificial intelligence, how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives in terms of safety, critical deployment? But at themis, this is exactly the type of problem that we solve. We want to bring these advances to the real world, and the way we do this is by innovating in the spheres of safe and trustworthy artificial intelligence in order to bring the things that were developed in research labs around the world to customers like you and me.", "SliceSentence": "Now at a first glance this seems really demoralizing . Ifthese are all of the things wrong with artificial intelligence how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives in terms of safety critical deployment But at themis this is exactly the type of problem that we solve . Wewant to bring these advances to the real world and the way we do this is by innovating in the spheres of safe and trustworthy artificial intelligence in order to bring the things that were developed in research labs around the world to customers like you and me", "StartMs": 341420, "EndMs": 374500, "WordsNum": 109, "Words": [{"Word": "Now", "OffsetStartMs": 370, "OffsetEndMs": 770}, {"Word": "at", "OffsetStartMs": 820, "OffsetEndMs": 1080}, {"Word": "a", "OffsetStartMs": 1080, "OffsetEndMs": 1200}, {"Word": "first", "OffsetStartMs": 1200, "OffsetEndMs": 1410}, {"Word": "glance", "OffsetStartMs": 1410, "OffsetEndMs": 1760}, {"Word": "this", "OffsetStartMs": 1960, "OffsetEndMs": 2250}, {"Word": "seems", "OffsetStartMs": 2250, "OffsetEndMs": 2520}, {"Word": "really", "OffsetStartMs": 2520, "OffsetEndMs": 2820}, {"Word": "demoralizing", "OffsetStartMs": 2820, "OffsetEndMs": 3440}, {"Word": ".", "OffsetStartMs": 3940, "OffsetEndMs": 4290}, {"Word": "Ifthese", "OffsetStartMs": 4290, "OffsetEndMs": 4515}, {"Word": "are", "OffsetStartMs": 4515, "OffsetEndMs": 4680}, {"Word": "all", "OffsetStartMs": 4680, "OffsetEndMs": 4860}, {"Word": "of", "OffsetStartMs": 4860, "OffsetEndMs": 4995}, {"Word": "the", "OffsetStartMs": 4995, "OffsetEndMs": 5100}, {"Word": "things", "OffsetStartMs": 5100, "OffsetEndMs": 5280}, {"Word": "wrong", "OffsetStartMs": 5280, "OffsetEndMs": 5535}, {"Word": "with", "OffsetStartMs": 5535, "OffsetEndMs": 5835}, {"Word": "artificial", "OffsetStartMs": 5835, "OffsetEndMs": 6165}, {"Word": "intelligence", "OffsetStartMs": 6165, "OffsetEndMs": 6530}, {"Word": "how", "OffsetStartMs": 6910, "OffsetEndMs": 7185}, {"Word": "are", "OffsetStartMs": 7185, "OffsetEndMs": 7320}, {"Word": "we", "OffsetStartMs": 7320, "OffsetEndMs": 7470}, {"Word": "ever", "OffsetStartMs": 7470, "OffsetEndMs": 7695}, {"Word": "going", "OffsetStartMs": 7695, "OffsetEndMs": 7920}, {"Word": "to", "OffsetStartMs": 7920, "OffsetEndMs": 8145}, {"Word": "achieve", "OffsetStartMs": 8145, "OffsetEndMs": 8385}, {"Word": "that", "OffsetStartMs": 8385, "OffsetEndMs": 8610}, {"Word": "vision", "OffsetStartMs": 8610, "OffsetEndMs": 8910}, {"Word": "of", "OffsetStartMs": 8910, "OffsetEndMs": 9180}, {"Word": "having", "OffsetStartMs": 9180, "OffsetEndMs": 9470}, {"Word": "our", "OffsetStartMs": 9490, "OffsetEndMs": 9870}, {"Word": "AI", "OffsetStartMs": 9870, "OffsetEndMs": 10250}, {"Word": "integrated", "OffsetStartMs": 10360, "OffsetEndMs": 10695}, {"Word": "into", "OffsetStartMs": 10695, "OffsetEndMs": 10950}, {"Word": "the", "OffsetStartMs": 10950, "OffsetEndMs": 11130}, {"Word": "fabric", "OffsetStartMs": 11130, "OffsetEndMs": 11370}, {"Word": "of", "OffsetStartMs": 11370, "OffsetEndMs": 11610}, {"Word": "our", "OffsetStartMs": 11610, "OffsetEndMs": 11775}, {"Word": "daily", "OffsetStartMs": 11775, "OffsetEndMs": 12045}, {"Word": "lives", "OffsetStartMs": 12045, "OffsetEndMs": 12410}, {"Word": "in", "OffsetStartMs": 12670, "OffsetEndMs": 12990}, {"Word": "terms", "OffsetStartMs": 12990, "OffsetEndMs": 13260}, {"Word": "of", "OffsetStartMs": 13260, "OffsetEndMs": 13500}, {"Word": "safety", "OffsetStartMs": 13500, "OffsetEndMs": 13725}, {"Word": "critical", "OffsetStartMs": 13725, "OffsetEndMs": 14060}, {"Word": "deployment", "OffsetStartMs": 14080, "OffsetEndMs": 14630}, {"Word": "But", "OffsetStartMs": 16030, "OffsetEndMs": 16380}, {"Word": "at", "OffsetStartMs": 16380, "OffsetEndMs": 16635}, {"Word": "themis", "OffsetStartMs": 16635, "OffsetEndMs": 17070}, {"Word": "this", "OffsetStartMs": 17070, "OffsetEndMs": 17310}, {"Word": "is", "OffsetStartMs": 17310, "OffsetEndMs": 17520}, {"Word": "exactly", "OffsetStartMs": 17520, "OffsetEndMs": 17835}, {"Word": "the", "OffsetStartMs": 17835, "OffsetEndMs": 18075}, {"Word": "type", "OffsetStartMs": 18075, "OffsetEndMs": 18225}, {"Word": "of", "OffsetStartMs": 18225, "OffsetEndMs": 18375}, {"Word": "problem", "OffsetStartMs": 18375, "OffsetEndMs": 18585}, {"Word": "that", "OffsetStartMs": 18585, "OffsetEndMs": 18795}, {"Word": "we", "OffsetStartMs": 18795, "OffsetEndMs": 18945}, {"Word": "solve", "OffsetStartMs": 18945, "OffsetEndMs": 19220}, {"Word": ".", "OffsetStartMs": 19780, "OffsetEndMs": 20160}, {"Word": "Wewant", "OffsetStartMs": 20160, "OffsetEndMs": 20490}, {"Word": "to", "OffsetStartMs": 20490, "OffsetEndMs": 20700}, {"Word": "bring", "OffsetStartMs": 20700, "OffsetEndMs": 20880}, {"Word": "these", "OffsetStartMs": 20880, "OffsetEndMs": 21120}, {"Word": "advances", "OffsetStartMs": 21120, "OffsetEndMs": 21600}, {"Word": "to", "OffsetStartMs": 21600, "OffsetEndMs": 21765}, {"Word": "the", "OffsetStartMs": 21765, "OffsetEndMs": 21885}, {"Word": "real", "OffsetStartMs": 21885, "OffsetEndMs": 22065}, {"Word": "world", "OffsetStartMs": 22065, "OffsetEndMs": 22350}, {"Word": "and", "OffsetStartMs": 22350, "OffsetEndMs": 22605}, {"Word": "the", "OffsetStartMs": 22605, "OffsetEndMs": 22725}, {"Word": "way", "OffsetStartMs": 22725, "OffsetEndMs": 22875}, {"Word": "we", "OffsetStartMs": 22875, "OffsetEndMs": 23055}, {"Word": "do", "OffsetStartMs": 23055, "OffsetEndMs": 23190}, {"Word": "this", "OffsetStartMs": 23190, "OffsetEndMs": 23355}, {"Word": "is", "OffsetStartMs": 23355, "OffsetEndMs": 23535}, {"Word": "by", "OffsetStartMs": 23535, "OffsetEndMs": 23700}, {"Word": "innovating", "OffsetStartMs": 23700, "OffsetEndMs": 24180}, {"Word": "in", "OffsetStartMs": 24180, "OffsetEndMs": 24390}, {"Word": "the", "OffsetStartMs": 24390, "OffsetEndMs": 24540}, {"Word": "spheres", "OffsetStartMs": 24540, "OffsetEndMs": 24915}, {"Word": "of", "OffsetStartMs": 24915, "OffsetEndMs": 25140}, {"Word": "safe", "OffsetStartMs": 25140, "OffsetEndMs": 25410}, {"Word": "and", "OffsetStartMs": 25410, "OffsetEndMs": 25665}, {"Word": "trustworthy", "OffsetStartMs": 25665, "OffsetEndMs": 26340}, {"Word": "artificial", "OffsetStartMs": 26340, "OffsetEndMs": 26700}, {"Word": "intelligence", "OffsetStartMs": 26700, "OffsetEndMs": 27080}, {"Word": "in", "OffsetStartMs": 27430, "OffsetEndMs": 27720}, {"Word": "order", "OffsetStartMs": 27720, "OffsetEndMs": 27960}, {"Word": "to", "OffsetStartMs": 27960, "OffsetEndMs": 28170}, {"Word": "bring", "OffsetStartMs": 28170, "OffsetEndMs": 28430}, {"Word": "the", "OffsetStartMs": 28600, "OffsetEndMs": 28860}, {"Word": "things", "OffsetStartMs": 28860, "OffsetEndMs": 29025}, {"Word": "that", "OffsetStartMs": 29025, "OffsetEndMs": 29205}, {"Word": "were", "OffsetStartMs": 29205, "OffsetEndMs": 29370}, {"Word": "developed", "OffsetStartMs": 29370, "OffsetEndMs": 29595}, {"Word": "in", "OffsetStartMs": 29595, "OffsetEndMs": 29790}, {"Word": "research", "OffsetStartMs": 29790, "OffsetEndMs": 30030}, {"Word": "labs", "OffsetStartMs": 30030, "OffsetEndMs": 30405}, {"Word": "around", "OffsetStartMs": 30405, "OffsetEndMs": 30585}, {"Word": "the", "OffsetStartMs": 30585, "OffsetEndMs": 30750}, {"Word": "world", "OffsetStartMs": 30750, "OffsetEndMs": 31010}, {"Word": "to", "OffsetStartMs": 31090, "OffsetEndMs": 31365}, {"Word": "customers", "OffsetStartMs": 31365, "OffsetEndMs": 31640}, {"Word": "like", "OffsetStartMs": 31930, "OffsetEndMs": 32235}, {"Word": "you", "OffsetStartMs": 32235, "OffsetEndMs": 32385}, {"Word": "and", "OffsetStartMs": 32385, "OffsetEndMs": 32505}, {"Word": "me", "OffsetStartMs": 32505, "OffsetEndMs": 32780}], "SpeechSpeed": 18.0}, {"FinalSentence": "And we do this by our core ideology, is that we believe that all of the problems on this slide are underlaid by two key notions. The first is bias.", "SliceSentence": "And we do this by our core ideology is that we believe that all of the problems on this slide are underlaid by two key notions . Thefirst is bias", "StartMs": 376260, "EndMs": 387100, "WordsNum": 30, "Words": [{"Word": "And", "OffsetStartMs": 190, "OffsetEndMs": 555}, {"Word": "we", "OffsetStartMs": 555, "OffsetEndMs": 795}, {"Word": "do", "OffsetStartMs": 795, "OffsetEndMs": 945}, {"Word": "this", "OffsetStartMs": 945, "OffsetEndMs": 1220}, {"Word": "by", "OffsetStartMs": 1270, "OffsetEndMs": 1670}, {"Word": "our", "OffsetStartMs": 1840, "OffsetEndMs": 2160}, {"Word": "core", "OffsetStartMs": 2160, "OffsetEndMs": 2370}, {"Word": "ideology", "OffsetStartMs": 2370, "OffsetEndMs": 2985}, {"Word": "is", "OffsetStartMs": 2985, "OffsetEndMs": 3135}, {"Word": "that", "OffsetStartMs": 3135, "OffsetEndMs": 3300}, {"Word": "we", "OffsetStartMs": 3300, "OffsetEndMs": 3525}, {"Word": "believe", "OffsetStartMs": 3525, "OffsetEndMs": 3780}, {"Word": "that", "OffsetStartMs": 3780, "OffsetEndMs": 4020}, {"Word": "all", "OffsetStartMs": 4020, "OffsetEndMs": 4245}, {"Word": "of", "OffsetStartMs": 4245, "OffsetEndMs": 4410}, {"Word": "the", "OffsetStartMs": 4410, "OffsetEndMs": 4545}, {"Word": "problems", "OffsetStartMs": 4545, "OffsetEndMs": 4800}, {"Word": "on", "OffsetStartMs": 4800, "OffsetEndMs": 5055}, {"Word": "this", "OffsetStartMs": 5055, "OffsetEndMs": 5235}, {"Word": "slide", "OffsetStartMs": 5235, "OffsetEndMs": 5540}, {"Word": "are", "OffsetStartMs": 5560, "OffsetEndMs": 5960}, {"Word": "underlaid", "OffsetStartMs": 6130, "OffsetEndMs": 6735}, {"Word": "by", "OffsetStartMs": 6735, "OffsetEndMs": 7005}, {"Word": "two", "OffsetStartMs": 7005, "OffsetEndMs": 7370}, {"Word": "key", "OffsetStartMs": 7390, "OffsetEndMs": 7725}, {"Word": "notions", "OffsetStartMs": 7725, "OffsetEndMs": 8270}, {"Word": ".", "OffsetStartMs": 8500, "OffsetEndMs": 8775}, {"Word": "Thefirst", "OffsetStartMs": 8775, "OffsetEndMs": 9050}, {"Word": "is", "OffsetStartMs": 9190, "OffsetEndMs": 9510}, {"Word": "bias", "OffsetStartMs": 9510, "OffsetEndMs": 10010}], "SpeechSpeed": 13.3}, {"FinalSentence": "Bias is what happens when machine learning models do better on some demographics than others. This results in things like facial detection systems, not being able to detect certain faces with high accuracy, Siri not being able to recognize voices with accents, or algorithms that are trained on imbalanced data sets. So what the algorithm believes is a good solution doesn't actually work for everyone in the real world.", "SliceSentence": "Bias is what happens when machine learning models do better on some demographics than others . Thisresults in things like facial detection systems not being able to detect certain faces with high accuracy Siri not being able to recognize voices with accents or algorithms that are trained on imbalanced data sets . Sowhat the algorithm believes is a good solution doesn't actually work for everyone in the real world", "StartMs": 387100, "EndMs": 410740, "WordsNum": 69, "Words": [{"Word": "Bias", "OffsetStartMs": 10, "OffsetEndMs": 540}, {"Word": "is", "OffsetStartMs": 540, "OffsetEndMs": 750}, {"Word": "what", "OffsetStartMs": 750, "OffsetEndMs": 900}, {"Word": "happens", "OffsetStartMs": 900, "OffsetEndMs": 1160}, {"Word": "when", "OffsetStartMs": 1240, "OffsetEndMs": 1635}, {"Word": "machine", "OffsetStartMs": 1635, "OffsetEndMs": 1890}, {"Word": "learning", "OffsetStartMs": 1890, "OffsetEndMs": 2070}, {"Word": "models", "OffsetStartMs": 2070, "OffsetEndMs": 2390}, {"Word": "do", "OffsetStartMs": 2410, "OffsetEndMs": 2685}, {"Word": "better", "OffsetStartMs": 2685, "OffsetEndMs": 2940}, {"Word": "on", "OffsetStartMs": 2940, "OffsetEndMs": 3240}, {"Word": "some", "OffsetStartMs": 3240, "OffsetEndMs": 3450}, {"Word": "demographics", "OffsetStartMs": 3450, "OffsetEndMs": 4095}, {"Word": "than", "OffsetStartMs": 4095, "OffsetEndMs": 4290}, {"Word": "others", "OffsetStartMs": 4290, "OffsetEndMs": 4550}, {"Word": ".", "OffsetStartMs": 5290, "OffsetEndMs": 5655}, {"Word": "Thisresults", "OffsetStartMs": 5655, "OffsetEndMs": 5925}, {"Word": "in", "OffsetStartMs": 5925, "OffsetEndMs": 6120}, {"Word": "things", "OffsetStartMs": 6120, "OffsetEndMs": 6360}, {"Word": "like", "OffsetStartMs": 6360, "OffsetEndMs": 6660}, {"Word": "facial", "OffsetStartMs": 6660, "OffsetEndMs": 7095}, {"Word": "detection", "OffsetStartMs": 7095, "OffsetEndMs": 7455}, {"Word": "systems", "OffsetStartMs": 7455, "OffsetEndMs": 7730}, {"Word": "not", "OffsetStartMs": 7840, "OffsetEndMs": 8145}, {"Word": "being", "OffsetStartMs": 8145, "OffsetEndMs": 8310}, {"Word": "able", "OffsetStartMs": 8310, "OffsetEndMs": 8520}, {"Word": "to", "OffsetStartMs": 8520, "OffsetEndMs": 8790}, {"Word": "detect", "OffsetStartMs": 8790, "OffsetEndMs": 9045}, {"Word": "certain", "OffsetStartMs": 9045, "OffsetEndMs": 9300}, {"Word": "faces", "OffsetStartMs": 9300, "OffsetEndMs": 9620}, {"Word": "with", "OffsetStartMs": 9790, "OffsetEndMs": 10170}, {"Word": "high", "OffsetStartMs": 10170, "OffsetEndMs": 10545}, {"Word": "accuracy", "OffsetStartMs": 10545, "OffsetEndMs": 11210}, {"Word": "Siri", "OffsetStartMs": 11590, "OffsetEndMs": 12045}, {"Word": "not", "OffsetStartMs": 12045, "OffsetEndMs": 12225}, {"Word": "being", "OffsetStartMs": 12225, "OffsetEndMs": 12390}, {"Word": "able", "OffsetStartMs": 12390, "OffsetEndMs": 12570}, {"Word": "to", "OffsetStartMs": 12570, "OffsetEndMs": 12750}, {"Word": "recognize", "OffsetStartMs": 12750, "OffsetEndMs": 13010}, {"Word": "voices", "OffsetStartMs": 13120, "OffsetEndMs": 13520}, {"Word": "with", "OffsetStartMs": 13540, "OffsetEndMs": 13815}, {"Word": "accents", "OffsetStartMs": 13815, "OffsetEndMs": 14360}, {"Word": "or", "OffsetStartMs": 14560, "OffsetEndMs": 14955}, {"Word": "algorithms", "OffsetStartMs": 14955, "OffsetEndMs": 15360}, {"Word": "that", "OffsetStartMs": 15360, "OffsetEndMs": 15495}, {"Word": "are", "OffsetStartMs": 15495, "OffsetEndMs": 15645}, {"Word": "trained", "OffsetStartMs": 15645, "OffsetEndMs": 15885}, {"Word": "on", "OffsetStartMs": 15885, "OffsetEndMs": 16125}, {"Word": "imbalanced", "OffsetStartMs": 16125, "OffsetEndMs": 16710}, {"Word": "data", "OffsetStartMs": 16710, "OffsetEndMs": 16965}, {"Word": "sets", "OffsetStartMs": 16965, "OffsetEndMs": 17300}, {"Word": ".", "OffsetStartMs": 17410, "OffsetEndMs": 17685}, {"Word": "Sowhat", "OffsetStartMs": 17685, "OffsetEndMs": 17850}, {"Word": "the", "OffsetStartMs": 17850, "OffsetEndMs": 18135}, {"Word": "algorithm", "OffsetStartMs": 18135, "OffsetEndMs": 18570}, {"Word": "believes", "OffsetStartMs": 18570, "OffsetEndMs": 18885}, {"Word": "is", "OffsetStartMs": 18885, "OffsetEndMs": 19140}, {"Word": "a", "OffsetStartMs": 19140, "OffsetEndMs": 19320}, {"Word": "good", "OffsetStartMs": 19320, "OffsetEndMs": 19575}, {"Word": "solution", "OffsetStartMs": 19575, "OffsetEndMs": 19910}, {"Word": "doesn't", "OffsetStartMs": 20350, "OffsetEndMs": 20880}, {"Word": "actually", "OffsetStartMs": 20880, "OffsetEndMs": 21165}, {"Word": "work", "OffsetStartMs": 21165, "OffsetEndMs": 21405}, {"Word": "for", "OffsetStartMs": 21405, "OffsetEndMs": 21720}, {"Word": "everyone", "OffsetStartMs": 21720, "OffsetEndMs": 22005}, {"Word": "in", "OffsetStartMs": 22005, "OffsetEndMs": 22170}, {"Word": "the", "OffsetStartMs": 22170, "OffsetEndMs": 22275}, {"Word": "real", "OffsetStartMs": 22275, "OffsetEndMs": 22440}, {"Word": "world", "OffsetStartMs": 22440, "OffsetEndMs": 22760}], "SpeechSpeed": 17.5}, {"FinalSentence": "And the second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty. This is when models don't know when they can or can't be trusted. And this results in scenarios such as self driving cars continuing to operate in environments when they're not 100% confident instead of giving control to users, or robots being moving around in environments that they've never been in before and have high unfamiliarity with.", "SliceSentence": "And the second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty . Thisis when models don't know when they can or can't be trusted . Andthis results in scenarios such as self driving cars continuing to operate in environments when they're not 10 0 %confident instead of giving control to users or robots being moving around in environments that they've never been in before and have high unfamiliarity with", "StartMs": 411960, "EndMs": 439460, "WordsNum": 75, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 345}, {"Word": "the", "OffsetStartMs": 345, "OffsetEndMs": 480}, {"Word": "second", "OffsetStartMs": 480, "OffsetEndMs": 675}, {"Word": "notion", "OffsetStartMs": 675, "OffsetEndMs": 975}, {"Word": "that", "OffsetStartMs": 975, "OffsetEndMs": 1200}, {"Word": "underlies", "OffsetStartMs": 1200, "OffsetEndMs": 1605}, {"Word": "a", "OffsetStartMs": 1605, "OffsetEndMs": 1755}, {"Word": "lot", "OffsetStartMs": 1755, "OffsetEndMs": 1875}, {"Word": "of", "OffsetStartMs": 1875, "OffsetEndMs": 2025}, {"Word": "these", "OffsetStartMs": 2025, "OffsetEndMs": 2205}, {"Word": "problems", "OffsetStartMs": 2205, "OffsetEndMs": 2490}, {"Word": "today", "OffsetStartMs": 2490, "OffsetEndMs": 2870}, {"Word": "is", "OffsetStartMs": 3010, "OffsetEndMs": 3405}, {"Word": "unmitigated", "OffsetStartMs": 3405, "OffsetEndMs": 4230}, {"Word": "and", "OffsetStartMs": 4230, "OffsetEndMs": 4500}, {"Word": "uncommunicated", "OffsetStartMs": 4500, "OffsetEndMs": 5360}, {"Word": "uncertainty", "OffsetStartMs": 5500, "OffsetEndMs": 5900}, {"Word": ".", "OffsetStartMs": 6790, "OffsetEndMs": 7065}, {"Word": "Thisis", "OffsetStartMs": 7065, "OffsetEndMs": 7215}, {"Word": "when", "OffsetStartMs": 7215, "OffsetEndMs": 7410}, {"Word": "models", "OffsetStartMs": 7410, "OffsetEndMs": 7730}, {"Word": "don't", "OffsetStartMs": 7810, "OffsetEndMs": 8235}, {"Word": "know", "OffsetStartMs": 8235, "OffsetEndMs": 8510}, {"Word": "when", "OffsetStartMs": 8530, "OffsetEndMs": 8820}, {"Word": "they", "OffsetStartMs": 8820, "OffsetEndMs": 9030}, {"Word": "can", "OffsetStartMs": 9030, "OffsetEndMs": 9330}, {"Word": "or", "OffsetStartMs": 9330, "OffsetEndMs": 9645}, {"Word": "can't", "OffsetStartMs": 9645, "OffsetEndMs": 10005}, {"Word": "be", "OffsetStartMs": 10005, "OffsetEndMs": 10185}, {"Word": "trusted", "OffsetStartMs": 10185, "OffsetEndMs": 10490}, {"Word": ".", "OffsetStartMs": 11200, "OffsetEndMs": 11490}, {"Word": "Andthis", "OffsetStartMs": 11490, "OffsetEndMs": 11745}, {"Word": "results", "OffsetStartMs": 11745, "OffsetEndMs": 12030}, {"Word": "in", "OffsetStartMs": 12030, "OffsetEndMs": 12240}, {"Word": "scenarios", "OffsetStartMs": 12240, "OffsetEndMs": 12825}, {"Word": "such", "OffsetStartMs": 12825, "OffsetEndMs": 13035}, {"Word": "as", "OffsetStartMs": 13035, "OffsetEndMs": 13170}, {"Word": "self", "OffsetStartMs": 13170, "OffsetEndMs": 13380}, {"Word": "driving", "OffsetStartMs": 13380, "OffsetEndMs": 13680}, {"Word": "cars", "OffsetStartMs": 13680, "OffsetEndMs": 14060}, {"Word": "continuing", "OffsetStartMs": 14230, "OffsetEndMs": 14595}, {"Word": "to", "OffsetStartMs": 14595, "OffsetEndMs": 14925}, {"Word": "operate", "OffsetStartMs": 14925, "OffsetEndMs": 15240}, {"Word": "in", "OffsetStartMs": 15240, "OffsetEndMs": 15590}, {"Word": "environments", "OffsetStartMs": 16180, "OffsetEndMs": 16740}, {"Word": "when", "OffsetStartMs": 16740, "OffsetEndMs": 16935}, {"Word": "they're", "OffsetStartMs": 16935, "OffsetEndMs": 17130}, {"Word": "not", "OffsetStartMs": 17130, "OffsetEndMs": 17295}, {"Word": "10", "OffsetStartMs": 17295, "OffsetEndMs": 17625}, {"Word": "0", "OffsetStartMs": 17625, "OffsetEndMs": 17910}, {"Word": "%confident", "OffsetStartMs": 17910, "OffsetEndMs": 18260}, {"Word": "instead", "OffsetStartMs": 18550, "OffsetEndMs": 18795}, {"Word": "of", "OffsetStartMs": 18795, "OffsetEndMs": 18915}, {"Word": "giving", "OffsetStartMs": 18915, "OffsetEndMs": 19190}, {"Word": "control", "OffsetStartMs": 19270, "OffsetEndMs": 19590}, {"Word": "to", "OffsetStartMs": 19590, "OffsetEndMs": 19770}, {"Word": "users", "OffsetStartMs": 19770, "OffsetEndMs": 20030}, {"Word": "or", "OffsetStartMs": 20860, "OffsetEndMs": 21260}, {"Word": "robots", "OffsetStartMs": 21340, "OffsetEndMs": 21870}, {"Word": "being", "OffsetStartMs": 21870, "OffsetEndMs": 22220}, {"Word": "moving", "OffsetStartMs": 22420, "OffsetEndMs": 22785}, {"Word": "around", "OffsetStartMs": 22785, "OffsetEndMs": 23055}, {"Word": "in", "OffsetStartMs": 23055, "OffsetEndMs": 23295}, {"Word": "environments", "OffsetStartMs": 23295, "OffsetEndMs": 23760}, {"Word": "that", "OffsetStartMs": 23760, "OffsetEndMs": 23910}, {"Word": "they've", "OffsetStartMs": 23910, "OffsetEndMs": 24090}, {"Word": "never", "OffsetStartMs": 24090, "OffsetEndMs": 24255}, {"Word": "been", "OffsetStartMs": 24255, "OffsetEndMs": 24450}, {"Word": "in", "OffsetStartMs": 24450, "OffsetEndMs": 24570}, {"Word": "before", "OffsetStartMs": 24570, "OffsetEndMs": 24830}, {"Word": "and", "OffsetStartMs": 24850, "OffsetEndMs": 25110}, {"Word": "have", "OffsetStartMs": 25110, "OffsetEndMs": 25305}, {"Word": "high", "OffsetStartMs": 25305, "OffsetEndMs": 25545}, {"Word": "unfamiliarity", "OffsetStartMs": 25545, "OffsetEndMs": 26370}, {"Word": "with", "OffsetStartMs": 26370, "OffsetEndMs": 26630}], "SpeechSpeed": 16.9}, {"FinalSentence": "And a lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty.", "SliceSentence": "And a lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty", "StartMs": 441060, "EndMs": 449200, "WordsNum": 21, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 450}, {"Word": "a", "OffsetStartMs": 450, "OffsetEndMs": 660}, {"Word": "lot", "OffsetStartMs": 660, "OffsetEndMs": 795}, {"Word": "of", "OffsetStartMs": 795, "OffsetEndMs": 930}, {"Word": "the", "OffsetStartMs": 930, "OffsetEndMs": 1065}, {"Word": "problems", "OffsetStartMs": 1065, "OffsetEndMs": 1340}, {"Word": "in", "OffsetStartMs": 1570, "OffsetEndMs": 1970}, {"Word": "modern", "OffsetStartMs": 2050, "OffsetEndMs": 2430}, {"Word": "day", "OffsetStartMs": 2430, "OffsetEndMs": 2790}, {"Word": "AI", "OffsetStartMs": 2790, "OffsetEndMs": 3120}, {"Word": "are", "OffsetStartMs": 3120, "OffsetEndMs": 3360}, {"Word": "the", "OffsetStartMs": 3360, "OffsetEndMs": 3570}, {"Word": "result", "OffsetStartMs": 3570, "OffsetEndMs": 3810}, {"Word": "of", "OffsetStartMs": 3810, "OffsetEndMs": 3990}, {"Word": "a", "OffsetStartMs": 3990, "OffsetEndMs": 4155}, {"Word": "combination", "OffsetStartMs": 4155, "OffsetEndMs": 4460}, {"Word": "of", "OffsetStartMs": 4720, "OffsetEndMs": 4995}, {"Word": "unmitigated", "OffsetStartMs": 4995, "OffsetEndMs": 5640}, {"Word": "bias", "OffsetStartMs": 5640, "OffsetEndMs": 6120}, {"Word": "and", "OffsetStartMs": 6120, "OffsetEndMs": 6435}, {"Word": "uncertainty", "OffsetStartMs": 6435, "OffsetEndMs": 6800}], "SpeechSpeed": 13.5}, {"FinalSentence": "So today in this lecture, we're going to focus on investigating the root causes of all of these problems, these two big challenges to robust deep learning. We'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone. And we'll start by talking about bias.", "SliceSentence": "So today in this lecture we're going to focus on investigating the root causes of all of these problems these two big challenges to robust deep learning .We'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone . Andwe'll start by talking about bias", "StartMs": 450820, "EndMs": 469980, "WordsNum": 55, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 555}, {"Word": "today", "OffsetStartMs": 555, "OffsetEndMs": 950}, {"Word": "in", "OffsetStartMs": 1000, "OffsetEndMs": 1275}, {"Word": "this", "OffsetStartMs": 1275, "OffsetEndMs": 1440}, {"Word": "lecture", "OffsetStartMs": 1440, "OffsetEndMs": 1730}, {"Word": "we're", "OffsetStartMs": 1930, "OffsetEndMs": 2250}, {"Word": "going", "OffsetStartMs": 2250, "OffsetEndMs": 2370}, {"Word": "to", "OffsetStartMs": 2370, "OffsetEndMs": 2505}, {"Word": "focus", "OffsetStartMs": 2505, "OffsetEndMs": 2730}, {"Word": "on", "OffsetStartMs": 2730, "OffsetEndMs": 3060}, {"Word": "investigating", "OffsetStartMs": 3060, "OffsetEndMs": 3645}, {"Word": "the", "OffsetStartMs": 3645, "OffsetEndMs": 3855}, {"Word": "root", "OffsetStartMs": 3855, "OffsetEndMs": 4080}, {"Word": "causes", "OffsetStartMs": 4080, "OffsetEndMs": 4430}, {"Word": "of", "OffsetStartMs": 4480, "OffsetEndMs": 4800}, {"Word": "all", "OffsetStartMs": 4800, "OffsetEndMs": 5010}, {"Word": "of", "OffsetStartMs": 5010, "OffsetEndMs": 5175}, {"Word": "these", "OffsetStartMs": 5175, "OffsetEndMs": 5355}, {"Word": "problems", "OffsetStartMs": 5355, "OffsetEndMs": 5660}, {"Word": "these", "OffsetStartMs": 5920, "OffsetEndMs": 6300}, {"Word": "two", "OffsetStartMs": 6300, "OffsetEndMs": 6645}, {"Word": "big", "OffsetStartMs": 6645, "OffsetEndMs": 6960}, {"Word": "challenges", "OffsetStartMs": 6960, "OffsetEndMs": 7310}, {"Word": "to", "OffsetStartMs": 7510, "OffsetEndMs": 7830}, {"Word": "robust", "OffsetStartMs": 7830, "OffsetEndMs": 8130}, {"Word": "deep", "OffsetStartMs": 8130, "OffsetEndMs": 8400}, {"Word": "learning", "OffsetStartMs": 8400, "OffsetEndMs": 8690}, {"Word": ".We'll", "OffsetStartMs": 9400, "OffsetEndMs": 9810}, {"Word": "also", "OffsetStartMs": 9810, "OffsetEndMs": 10020}, {"Word": "talk", "OffsetStartMs": 10020, "OffsetEndMs": 10230}, {"Word": "about", "OffsetStartMs": 10230, "OffsetEndMs": 10455}, {"Word": "solutions", "OffsetStartMs": 10455, "OffsetEndMs": 10760}, {"Word": "for", "OffsetStartMs": 10840, "OffsetEndMs": 11100}, {"Word": "them", "OffsetStartMs": 11100, "OffsetEndMs": 11280}, {"Word": "that", "OffsetStartMs": 11280, "OffsetEndMs": 11475}, {"Word": "can", "OffsetStartMs": 11475, "OffsetEndMs": 11685}, {"Word": "improve", "OffsetStartMs": 11685, "OffsetEndMs": 11940}, {"Word": "the", "OffsetStartMs": 11940, "OffsetEndMs": 12180}, {"Word": "robustness", "OffsetStartMs": 12180, "OffsetEndMs": 12660}, {"Word": "and", "OffsetStartMs": 12660, "OffsetEndMs": 12840}, {"Word": "safety", "OffsetStartMs": 12840, "OffsetEndMs": 13130}, {"Word": "of", "OffsetStartMs": 13300, "OffsetEndMs": 13590}, {"Word": "all", "OffsetStartMs": 13590, "OffsetEndMs": 13770}, {"Word": "of", "OffsetStartMs": 13770, "OffsetEndMs": 13920}, {"Word": "these", "OffsetStartMs": 13920, "OffsetEndMs": 14180}, {"Word": "algorithms", "OffsetStartMs": 14230, "OffsetEndMs": 14745}, {"Word": "for", "OffsetStartMs": 14745, "OffsetEndMs": 15105}, {"Word": "everyone", "OffsetStartMs": 15105, "OffsetEndMs": 15500}, {"Word": ".", "OffsetStartMs": 16030, "OffsetEndMs": 16320}, {"Word": "Andwe'll", "OffsetStartMs": 16320, "OffsetEndMs": 16545}, {"Word": "start", "OffsetStartMs": 16545, "OffsetEndMs": 16740}, {"Word": "by", "OffsetStartMs": 16740, "OffsetEndMs": 16950}, {"Word": "talking", "OffsetStartMs": 16950, "OffsetEndMs": 17240}, {"Word": "about", "OffsetStartMs": 17290, "OffsetEndMs": 17580}, {"Word": "bias", "OffsetStartMs": 17580, "OffsetEndMs": 18080}], "SpeechSpeed": 16.4}, {"FinalSentence": "Bias is a word that we've all heard outside the context of deep learning, but in the context of machine learning, it can be quantified and mathematically defined.", "SliceSentence": "Bias is a word that we've all heard outside the context of deep learning but in the context of machine learning it can be quantified and mathematically defined", "StartMs": 470180, "EndMs": 479060, "WordsNum": 28, "Words": [{"Word": "Bias", "OffsetStartMs": 100, "OffsetEndMs": 525}, {"Word": "is", "OffsetStartMs": 525, "OffsetEndMs": 660}, {"Word": "a", "OffsetStartMs": 660, "OffsetEndMs": 780}, {"Word": "word", "OffsetStartMs": 780, "OffsetEndMs": 960}, {"Word": "that", "OffsetStartMs": 960, "OffsetEndMs": 1140}, {"Word": "we've", "OffsetStartMs": 1140, "OffsetEndMs": 1350}, {"Word": "all", "OffsetStartMs": 1350, "OffsetEndMs": 1575}, {"Word": "heard", "OffsetStartMs": 1575, "OffsetEndMs": 1940}, {"Word": "outside", "OffsetStartMs": 2050, "OffsetEndMs": 2310}, {"Word": "the", "OffsetStartMs": 2310, "OffsetEndMs": 2430}, {"Word": "context", "OffsetStartMs": 2430, "OffsetEndMs": 2685}, {"Word": "of", "OffsetStartMs": 2685, "OffsetEndMs": 2955}, {"Word": "deep", "OffsetStartMs": 2955, "OffsetEndMs": 3120}, {"Word": "learning", "OffsetStartMs": 3120, "OffsetEndMs": 3410}, {"Word": "but", "OffsetStartMs": 3670, "OffsetEndMs": 3945}, {"Word": "in", "OffsetStartMs": 3945, "OffsetEndMs": 4095}, {"Word": "the", "OffsetStartMs": 4095, "OffsetEndMs": 4245}, {"Word": "context", "OffsetStartMs": 4245, "OffsetEndMs": 4515}, {"Word": "of", "OffsetStartMs": 4515, "OffsetEndMs": 4845}, {"Word": "machine", "OffsetStartMs": 4845, "OffsetEndMs": 5070}, {"Word": "learning", "OffsetStartMs": 5070, "OffsetEndMs": 5360}, {"Word": "it", "OffsetStartMs": 5500, "OffsetEndMs": 5775}, {"Word": "can", "OffsetStartMs": 5775, "OffsetEndMs": 5910}, {"Word": "be", "OffsetStartMs": 5910, "OffsetEndMs": 6090}, {"Word": "quantified", "OffsetStartMs": 6090, "OffsetEndMs": 6705}, {"Word": "and", "OffsetStartMs": 6705, "OffsetEndMs": 6945}, {"Word": "mathematically", "OffsetStartMs": 6945, "OffsetEndMs": 7490}, {"Word": "defined", "OffsetStartMs": 7540, "OffsetEndMs": 7940}], "SpeechSpeed": 17.9}, {"FinalSentence": "Today we'll talk about how to do this and methods for mitigation of this bias algorithmically and how themus is innovating in these areas in order to bring new algorithms in this space to industries around the world.", "SliceSentence": "Today we'll talk about how to do this and methods for mitigation of this bias algorithmically and how themus is innovating in these areas in order to bring new algorithms in this space to industries around the world", "StartMs": 479060, "EndMs": 492200, "WordsNum": 38, "Words": [{"Word": "Today", "OffsetStartMs": 70, "OffsetEndMs": 390}, {"Word": "we'll", "OffsetStartMs": 390, "OffsetEndMs": 630}, {"Word": "talk", "OffsetStartMs": 630, "OffsetEndMs": 780}, {"Word": "about", "OffsetStartMs": 780, "OffsetEndMs": 975}, {"Word": "how", "OffsetStartMs": 975, "OffsetEndMs": 1110}, {"Word": "to", "OffsetStartMs": 1110, "OffsetEndMs": 1245}, {"Word": "do", "OffsetStartMs": 1245, "OffsetEndMs": 1380}, {"Word": "this", "OffsetStartMs": 1380, "OffsetEndMs": 1640}, {"Word": "and", "OffsetStartMs": 1660, "OffsetEndMs": 1965}, {"Word": "methods", "OffsetStartMs": 1965, "OffsetEndMs": 2235}, {"Word": "for", "OffsetStartMs": 2235, "OffsetEndMs": 2475}, {"Word": "mitigation", "OffsetStartMs": 2475, "OffsetEndMs": 2970}, {"Word": "of", "OffsetStartMs": 2970, "OffsetEndMs": 3240}, {"Word": "this", "OffsetStartMs": 3240, "OffsetEndMs": 3435}, {"Word": "bias", "OffsetStartMs": 3435, "OffsetEndMs": 3890}, {"Word": "algorithmically", "OffsetStartMs": 3910, "OffsetEndMs": 4820}, {"Word": "and", "OffsetStartMs": 4900, "OffsetEndMs": 5190}, {"Word": "how", "OffsetStartMs": 5190, "OffsetEndMs": 5415}, {"Word": "themus", "OffsetStartMs": 5415, "OffsetEndMs": 5790}, {"Word": "is", "OffsetStartMs": 5790, "OffsetEndMs": 5970}, {"Word": "innovating", "OffsetStartMs": 5970, "OffsetEndMs": 6435}, {"Word": "in", "OffsetStartMs": 6435, "OffsetEndMs": 6630}, {"Word": "these", "OffsetStartMs": 6630, "OffsetEndMs": 6780}, {"Word": "areas", "OffsetStartMs": 6780, "OffsetEndMs": 7070}, {"Word": "in", "OffsetStartMs": 7150, "OffsetEndMs": 7395}, {"Word": "order", "OffsetStartMs": 7395, "OffsetEndMs": 7560}, {"Word": "to", "OffsetStartMs": 7560, "OffsetEndMs": 7725}, {"Word": "bring", "OffsetStartMs": 7725, "OffsetEndMs": 7890}, {"Word": "new", "OffsetStartMs": 7890, "OffsetEndMs": 8210}, {"Word": "algorithms", "OffsetStartMs": 8290, "OffsetEndMs": 8700}, {"Word": "in", "OffsetStartMs": 8700, "OffsetEndMs": 8835}, {"Word": "this", "OffsetStartMs": 8835, "OffsetEndMs": 9030}, {"Word": "space", "OffsetStartMs": 9030, "OffsetEndMs": 9380}, {"Word": "to", "OffsetStartMs": 9580, "OffsetEndMs": 9980}, {"Word": "industries", "OffsetStartMs": 11050, "OffsetEndMs": 11385}, {"Word": "around", "OffsetStartMs": 11385, "OffsetEndMs": 11610}, {"Word": "the", "OffsetStartMs": 11610, "OffsetEndMs": 11760}, {"Word": "world", "OffsetStartMs": 11760, "OffsetEndMs": 12020}], "SpeechSpeed": 16.4}, {"FinalSentence": "Afterwards, we'll talk about uncertainty, which is can we teach a model when it does or doesn't know the answer to to its given task? And we'll talk about the ramifications for this for real world AI.", "SliceSentence": "Afterwards we'll talk about uncertainty which is can we teach a model when it does or doesn't know the answer to to its given task And we'll talk about the ramifications for this for real world AI", "StartMs": 492200, "EndMs": 504780, "WordsNum": 37, "Words": [{"Word": "Afterwards", "OffsetStartMs": 40, "OffsetEndMs": 440}, {"Word": "we'll", "OffsetStartMs": 670, "OffsetEndMs": 975}, {"Word": "talk", "OffsetStartMs": 975, "OffsetEndMs": 1155}, {"Word": "about", "OffsetStartMs": 1155, "OffsetEndMs": 1485}, {"Word": "uncertainty", "OffsetStartMs": 1485, "OffsetEndMs": 1880}, {"Word": "which", "OffsetStartMs": 2260, "OffsetEndMs": 2535}, {"Word": "is", "OffsetStartMs": 2535, "OffsetEndMs": 2810}, {"Word": "can", "OffsetStartMs": 2950, "OffsetEndMs": 3225}, {"Word": "we", "OffsetStartMs": 3225, "OffsetEndMs": 3405}, {"Word": "teach", "OffsetStartMs": 3405, "OffsetEndMs": 3600}, {"Word": "a", "OffsetStartMs": 3600, "OffsetEndMs": 3765}, {"Word": "model", "OffsetStartMs": 3765, "OffsetEndMs": 4035}, {"Word": "when", "OffsetStartMs": 4035, "OffsetEndMs": 4350}, {"Word": "it", "OffsetStartMs": 4350, "OffsetEndMs": 4560}, {"Word": "does", "OffsetStartMs": 4560, "OffsetEndMs": 4815}, {"Word": "or", "OffsetStartMs": 4815, "OffsetEndMs": 5055}, {"Word": "doesn't", "OffsetStartMs": 5055, "OffsetEndMs": 5385}, {"Word": "know", "OffsetStartMs": 5385, "OffsetEndMs": 5580}, {"Word": "the", "OffsetStartMs": 5580, "OffsetEndMs": 5730}, {"Word": "answer", "OffsetStartMs": 5730, "OffsetEndMs": 5990}, {"Word": "to", "OffsetStartMs": 6100, "OffsetEndMs": 6500}, {"Word": "to", "OffsetStartMs": 6610, "OffsetEndMs": 6840}, {"Word": "its", "OffsetStartMs": 6840, "OffsetEndMs": 6960}, {"Word": "given", "OffsetStartMs": 6960, "OffsetEndMs": 7200}, {"Word": "task", "OffsetStartMs": 7200, "OffsetEndMs": 7550}, {"Word": "And", "OffsetStartMs": 7900, "OffsetEndMs": 8160}, {"Word": "we'll", "OffsetStartMs": 8160, "OffsetEndMs": 8325}, {"Word": "talk", "OffsetStartMs": 8325, "OffsetEndMs": 8490}, {"Word": "about", "OffsetStartMs": 8490, "OffsetEndMs": 8700}, {"Word": "the", "OffsetStartMs": 8700, "OffsetEndMs": 8895}, {"Word": "ramifications", "OffsetStartMs": 8895, "OffsetEndMs": 9570}, {"Word": "for", "OffsetStartMs": 9570, "OffsetEndMs": 9810}, {"Word": "this", "OffsetStartMs": 9810, "OffsetEndMs": 10070}, {"Word": "for", "OffsetStartMs": 10270, "OffsetEndMs": 10545}, {"Word": "real", "OffsetStartMs": 10545, "OffsetEndMs": 10740}, {"Word": "world", "OffsetStartMs": 10740, "OffsetEndMs": 11060}, {"Word": "AI", "OffsetStartMs": 11110, "OffsetEndMs": 11510}], "SpeechSpeed": 15.6}, {"FinalSentence": "So what exactly does bias mean, and where is it present in the artificial intelligence lifecycle?", "SliceSentence": "So what exactly does bias mean and where is it present in the artificial intelligence lifecycle", "StartMs": 507740, "EndMs": 514860, "WordsNum": 16, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "what", "OffsetStartMs": 790, "OffsetEndMs": 1140}, {"Word": "exactly", "OffsetStartMs": 1140, "OffsetEndMs": 1485}, {"Word": "does", "OffsetStartMs": 1485, "OffsetEndMs": 1770}, {"Word": "bias", "OffsetStartMs": 1770, "OffsetEndMs": 2160}, {"Word": "mean", "OffsetStartMs": 2160, "OffsetEndMs": 2480}, {"Word": "and", "OffsetStartMs": 2740, "OffsetEndMs": 3045}, {"Word": "where", "OffsetStartMs": 3045, "OffsetEndMs": 3255}, {"Word": "is", "OffsetStartMs": 3255, "OffsetEndMs": 3420}, {"Word": "it", "OffsetStartMs": 3420, "OffsetEndMs": 3555}, {"Word": "present", "OffsetStartMs": 3555, "OffsetEndMs": 3795}, {"Word": "in", "OffsetStartMs": 3795, "OffsetEndMs": 4020}, {"Word": "the", "OffsetStartMs": 4020, "OffsetEndMs": 4230}, {"Word": "artificial", "OffsetStartMs": 4230, "OffsetEndMs": 4545}, {"Word": "intelligence", "OffsetStartMs": 4545, "OffsetEndMs": 4905}, {"Word": "lifecycle", "OffsetStartMs": 4905, "OffsetEndMs": 5690}], "SpeechSpeed": 13.3}, {"FinalSentence": "The most intuitive form of bias comes from data. We have two different two main types of bias here. The first is sampling bias, which is when we over sample from some regions of our input, data distribution and under sample from others. A good example of this is a lot of clinical data sets, where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts.", "SliceSentence": "The most intuitive form of bias comes from data . Wehave two different two main types of bias here . Thefirst is sampling bias which is when we over sample from some regions of our input data distribution and under sample from others . Agood example of this is a lot of clinical data sets where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts", "StartMs": 514860, "EndMs": 542040, "WordsNum": 81, "Words": [{"Word": "The", "OffsetStartMs": 0, "OffsetEndMs": 135}, {"Word": "most", "OffsetStartMs": 135, "OffsetEndMs": 300}, {"Word": "intuitive", "OffsetStartMs": 300, "OffsetEndMs": 915}, {"Word": "form", "OffsetStartMs": 915, "OffsetEndMs": 1125}, {"Word": "of", "OffsetStartMs": 1125, "OffsetEndMs": 1305}, {"Word": "bias", "OffsetStartMs": 1305, "OffsetEndMs": 1760}, {"Word": "comes", "OffsetStartMs": 1810, "OffsetEndMs": 2175}, {"Word": "from", "OffsetStartMs": 2175, "OffsetEndMs": 2445}, {"Word": "data", "OffsetStartMs": 2445, "OffsetEndMs": 2750}, {"Word": ".", "OffsetStartMs": 3400, "OffsetEndMs": 3675}, {"Word": "Wehave", "OffsetStartMs": 3675, "OffsetEndMs": 3915}, {"Word": "two", "OffsetStartMs": 3915, "OffsetEndMs": 4170}, {"Word": "different", "OffsetStartMs": 4170, "OffsetEndMs": 4455}, {"Word": "two", "OffsetStartMs": 4455, "OffsetEndMs": 4725}, {"Word": "main", "OffsetStartMs": 4725, "OffsetEndMs": 4920}, {"Word": "types", "OffsetStartMs": 4920, "OffsetEndMs": 5145}, {"Word": "of", "OffsetStartMs": 5145, "OffsetEndMs": 5310}, {"Word": "bias", "OffsetStartMs": 5310, "OffsetEndMs": 5625}, {"Word": "here", "OffsetStartMs": 5625, "OffsetEndMs": 5930}, {"Word": ".", "OffsetStartMs": 6070, "OffsetEndMs": 6345}, {"Word": "Thefirst", "OffsetStartMs": 6345, "OffsetEndMs": 6620}, {"Word": "is", "OffsetStartMs": 6670, "OffsetEndMs": 7005}, {"Word": "sampling", "OffsetStartMs": 7005, "OffsetEndMs": 7470}, {"Word": "bias", "OffsetStartMs": 7470, "OffsetEndMs": 7940}, {"Word": "which", "OffsetStartMs": 8170, "OffsetEndMs": 8430}, {"Word": "is", "OffsetStartMs": 8430, "OffsetEndMs": 8580}, {"Word": "when", "OffsetStartMs": 8580, "OffsetEndMs": 8805}, {"Word": "we", "OffsetStartMs": 8805, "OffsetEndMs": 9030}, {"Word": "over", "OffsetStartMs": 9030, "OffsetEndMs": 9300}, {"Word": "sample", "OffsetStartMs": 9300, "OffsetEndMs": 9675}, {"Word": "from", "OffsetStartMs": 9675, "OffsetEndMs": 9975}, {"Word": "some", "OffsetStartMs": 9975, "OffsetEndMs": 10200}, {"Word": "regions", "OffsetStartMs": 10200, "OffsetEndMs": 10515}, {"Word": "of", "OffsetStartMs": 10515, "OffsetEndMs": 10755}, {"Word": "our", "OffsetStartMs": 10755, "OffsetEndMs": 10980}, {"Word": "input", "OffsetStartMs": 10980, "OffsetEndMs": 11235}, {"Word": "data", "OffsetStartMs": 11235, "OffsetEndMs": 11510}, {"Word": "distribution", "OffsetStartMs": 11530, "OffsetEndMs": 11930}, {"Word": "and", "OffsetStartMs": 12310, "OffsetEndMs": 12600}, {"Word": "under", "OffsetStartMs": 12600, "OffsetEndMs": 12840}, {"Word": "sample", "OffsetStartMs": 12840, "OffsetEndMs": 13170}, {"Word": "from", "OffsetStartMs": 13170, "OffsetEndMs": 13425}, {"Word": "others", "OffsetStartMs": 13425, "OffsetEndMs": 13700}, {"Word": ".", "OffsetStartMs": 14200, "OffsetEndMs": 14460}, {"Word": "Agood", "OffsetStartMs": 14460, "OffsetEndMs": 14640}, {"Word": "example", "OffsetStartMs": 14640, "OffsetEndMs": 14895}, {"Word": "of", "OffsetStartMs": 14895, "OffsetEndMs": 15090}, {"Word": "this", "OffsetStartMs": 15090, "OffsetEndMs": 15315}, {"Word": "is", "OffsetStartMs": 15315, "OffsetEndMs": 15540}, {"Word": "a", "OffsetStartMs": 15540, "OffsetEndMs": 15645}, {"Word": "lot", "OffsetStartMs": 15645, "OffsetEndMs": 15765}, {"Word": "of", "OffsetStartMs": 15765, "OffsetEndMs": 15930}, {"Word": "clinical", "OffsetStartMs": 15930, "OffsetEndMs": 16200}, {"Word": "data", "OffsetStartMs": 16200, "OffsetEndMs": 16515}, {"Word": "sets", "OffsetStartMs": 16515, "OffsetEndMs": 16850}, {"Word": "where", "OffsetStartMs": 17080, "OffsetEndMs": 17480}, {"Word": "they", "OffsetStartMs": 17680, "OffsetEndMs": 17970}, {"Word": "often", "OffsetStartMs": 17970, "OffsetEndMs": 18260}, {"Word": "contain", "OffsetStartMs": 18340, "OffsetEndMs": 18660}, {"Word": "fewer", "OffsetStartMs": 18660, "OffsetEndMs": 18975}, {"Word": "examples", "OffsetStartMs": 18975, "OffsetEndMs": 19370}, {"Word": "of", "OffsetStartMs": 19480, "OffsetEndMs": 19880}, {"Word": "diseased", "OffsetStartMs": 20110, "OffsetEndMs": 20580}, {"Word": "patients", "OffsetStartMs": 20580, "OffsetEndMs": 20930}, {"Word": "than", "OffsetStartMs": 21010, "OffsetEndMs": 21285}, {"Word": "healthy", "OffsetStartMs": 21285, "OffsetEndMs": 21525}, {"Word": "patients", "OffsetStartMs": 21525, "OffsetEndMs": 21890}, {"Word": "because", "OffsetStartMs": 22030, "OffsetEndMs": 22290}, {"Word": "it's", "OffsetStartMs": 22290, "OffsetEndMs": 22515}, {"Word": "much", "OffsetStartMs": 22515, "OffsetEndMs": 22725}, {"Word": "easier", "OffsetStartMs": 22725, "OffsetEndMs": 23055}, {"Word": "to", "OffsetStartMs": 23055, "OffsetEndMs": 23355}, {"Word": "acquire", "OffsetStartMs": 23355, "OffsetEndMs": 23640}, {"Word": "data", "OffsetStartMs": 23640, "OffsetEndMs": 23990}, {"Word": "for", "OffsetStartMs": 24160, "OffsetEndMs": 24405}, {"Word": "healthy", "OffsetStartMs": 24405, "OffsetEndMs": 24600}, {"Word": "patients", "OffsetStartMs": 24600, "OffsetEndMs": 24950}, {"Word": "than", "OffsetStartMs": 25030, "OffsetEndMs": 25305}, {"Word": "their", "OffsetStartMs": 25305, "OffsetEndMs": 25500}, {"Word": "disease", "OffsetStartMs": 25500, "OffsetEndMs": 25740}, {"Word": "counterparts", "OffsetStartMs": 25740, "OffsetEndMs": 26420}], "SpeechSpeed": 17.0}, {"FinalSentence": "In addition, we also have selection bias at the data portion of the AI life cycle. Think about Apple's series voice recognition algorithm. This model is trained largely on flawless American English, but it's deployed across the real world to be able to recognize voices with accents from all over the world.", "SliceSentence": "In addition we also have selection bias at the data portion of the AI life cycle . Thinkabout Apple's series voice recognition algorithm . Thismodel is trained largely on flawless American English but it's deployed across the real world to be able to recognize voices with accents from all over the world", "StartMs": 542540, "EndMs": 561260, "WordsNum": 52, "Words": [{"Word": "In", "OffsetStartMs": 70, "OffsetEndMs": 375}, {"Word": "addition", "OffsetStartMs": 375, "OffsetEndMs": 680}, {"Word": "we", "OffsetStartMs": 790, "OffsetEndMs": 1140}, {"Word": "also", "OffsetStartMs": 1140, "OffsetEndMs": 1350}, {"Word": "have", "OffsetStartMs": 1350, "OffsetEndMs": 1590}, {"Word": "selection", "OffsetStartMs": 1590, "OffsetEndMs": 1950}, {"Word": "bias", "OffsetStartMs": 1950, "OffsetEndMs": 2430}, {"Word": "at", "OffsetStartMs": 2430, "OffsetEndMs": 2655}, {"Word": "the", "OffsetStartMs": 2655, "OffsetEndMs": 2790}, {"Word": "data", "OffsetStartMs": 2790, "OffsetEndMs": 3000}, {"Word": "portion", "OffsetStartMs": 3000, "OffsetEndMs": 3350}, {"Word": "of", "OffsetStartMs": 3370, "OffsetEndMs": 3630}, {"Word": "the", "OffsetStartMs": 3630, "OffsetEndMs": 3810}, {"Word": "AI", "OffsetStartMs": 3810, "OffsetEndMs": 4020}, {"Word": "life", "OffsetStartMs": 4020, "OffsetEndMs": 4215}, {"Word": "cycle", "OffsetStartMs": 4215, "OffsetEndMs": 4520}, {"Word": ".", "OffsetStartMs": 5320, "OffsetEndMs": 5640}, {"Word": "Thinkabout", "OffsetStartMs": 5640, "OffsetEndMs": 5895}, {"Word": "Apple's", "OffsetStartMs": 5895, "OffsetEndMs": 6510}, {"Word": "series", "OffsetStartMs": 6510, "OffsetEndMs": 6800}, {"Word": "voice", "OffsetStartMs": 6850, "OffsetEndMs": 7170}, {"Word": "recognition", "OffsetStartMs": 7170, "OffsetEndMs": 7490}, {"Word": "algorithm", "OffsetStartMs": 7810, "OffsetEndMs": 8390}, {"Word": ".", "OffsetStartMs": 9040, "OffsetEndMs": 9360}, {"Word": "Thismodel", "OffsetStartMs": 9360, "OffsetEndMs": 9680}, {"Word": "is", "OffsetStartMs": 10000, "OffsetEndMs": 10320}, {"Word": "trained", "OffsetStartMs": 10320, "OffsetEndMs": 10590}, {"Word": "largely", "OffsetStartMs": 10590, "OffsetEndMs": 10940}, {"Word": "on", "OffsetStartMs": 11020, "OffsetEndMs": 11385}, {"Word": "flawless", "OffsetStartMs": 11385, "OffsetEndMs": 11880}, {"Word": "American", "OffsetStartMs": 11880, "OffsetEndMs": 12230}, {"Word": "English", "OffsetStartMs": 12340, "OffsetEndMs": 12740}, {"Word": "but", "OffsetStartMs": 13060, "OffsetEndMs": 13365}, {"Word": "it's", "OffsetStartMs": 13365, "OffsetEndMs": 13680}, {"Word": "deployed", "OffsetStartMs": 13680, "OffsetEndMs": 13995}, {"Word": "across", "OffsetStartMs": 13995, "OffsetEndMs": 14220}, {"Word": "the", "OffsetStartMs": 14220, "OffsetEndMs": 14415}, {"Word": "real", "OffsetStartMs": 14415, "OffsetEndMs": 14610}, {"Word": "world", "OffsetStartMs": 14610, "OffsetEndMs": 14910}, {"Word": "to", "OffsetStartMs": 14910, "OffsetEndMs": 15135}, {"Word": "be", "OffsetStartMs": 15135, "OffsetEndMs": 15225}, {"Word": "able", "OffsetStartMs": 15225, "OffsetEndMs": 15390}, {"Word": "to", "OffsetStartMs": 15390, "OffsetEndMs": 15555}, {"Word": "recognize", "OffsetStartMs": 15555, "OffsetEndMs": 15800}, {"Word": "voices", "OffsetStartMs": 15940, "OffsetEndMs": 16340}, {"Word": "with", "OffsetStartMs": 16360, "OffsetEndMs": 16650}, {"Word": "accents", "OffsetStartMs": 16650, "OffsetEndMs": 17145}, {"Word": "from", "OffsetStartMs": 17145, "OffsetEndMs": 17355}, {"Word": "all", "OffsetStartMs": 17355, "OffsetEndMs": 17550}, {"Word": "over", "OffsetStartMs": 17550, "OffsetEndMs": 17805}, {"Word": "the", "OffsetStartMs": 17805, "OffsetEndMs": 17985}, {"Word": "world", "OffsetStartMs": 17985, "OffsetEndMs": 18230}], "SpeechSpeed": 16.1}, {"FinalSentence": "The distribution of the model's training data doesn't match the distribution of this type of language in the real world, because American English is highly overrepresented, as opposed to other demographics.", "SliceSentence": "The distribution of the model's training data doesn't match the distribution of this type of language in the real world because American English is highly overrepresented as opposed to other demographics", "StartMs": 561780, "EndMs": 573900, "WordsNum": 31, "Words": [{"Word": "The", "OffsetStartMs": 70, "OffsetEndMs": 435}, {"Word": "distribution", "OffsetStartMs": 435, "OffsetEndMs": 800}, {"Word": "of", "OffsetStartMs": 940, "OffsetEndMs": 1215}, {"Word": "the", "OffsetStartMs": 1215, "OffsetEndMs": 1350}, {"Word": "model's", "OffsetStartMs": 1350, "OffsetEndMs": 1725}, {"Word": "training", "OffsetStartMs": 1725, "OffsetEndMs": 1935}, {"Word": "data", "OffsetStartMs": 1935, "OffsetEndMs": 2270}, {"Word": "doesn't", "OffsetStartMs": 2350, "OffsetEndMs": 2820}, {"Word": "match", "OffsetStartMs": 2820, "OffsetEndMs": 3140}, {"Word": "the", "OffsetStartMs": 3160, "OffsetEndMs": 3540}, {"Word": "distribution", "OffsetStartMs": 3540, "OffsetEndMs": 3920}, {"Word": "of", "OffsetStartMs": 3970, "OffsetEndMs": 4245}, {"Word": "this", "OffsetStartMs": 4245, "OffsetEndMs": 4410}, {"Word": "type", "OffsetStartMs": 4410, "OffsetEndMs": 4575}, {"Word": "of", "OffsetStartMs": 4575, "OffsetEndMs": 4850}, {"Word": "language", "OffsetStartMs": 5290, "OffsetEndMs": 5640}, {"Word": "in", "OffsetStartMs": 5640, "OffsetEndMs": 5835}, {"Word": "the", "OffsetStartMs": 5835, "OffsetEndMs": 5955}, {"Word": "real", "OffsetStartMs": 5955, "OffsetEndMs": 6150}, {"Word": "world", "OffsetStartMs": 6150, "OffsetEndMs": 6470}, {"Word": "because", "OffsetStartMs": 6790, "OffsetEndMs": 7140}, {"Word": "American", "OffsetStartMs": 7140, "OffsetEndMs": 7490}, {"Word": "English", "OffsetStartMs": 7570, "OffsetEndMs": 7890}, {"Word": "is", "OffsetStartMs": 7890, "OffsetEndMs": 8130}, {"Word": "highly", "OffsetStartMs": 8130, "OffsetEndMs": 8415}, {"Word": "overrepresented", "OffsetStartMs": 8415, "OffsetEndMs": 9260}, {"Word": "as", "OffsetStartMs": 9400, "OffsetEndMs": 9735}, {"Word": "opposed", "OffsetStartMs": 9735, "OffsetEndMs": 9975}, {"Word": "to", "OffsetStartMs": 9975, "OffsetEndMs": 10110}, {"Word": "other", "OffsetStartMs": 10110, "OffsetEndMs": 10260}, {"Word": "demographics", "OffsetStartMs": 10260, "OffsetEndMs": 11000}], "SpeechSpeed": 16.7}, {"FinalSentence": "But that's not where, that's not where bias and data stops. These biases can be propagated towards models, training cycles themselves, which is what we'll focus on in the second half of this lecture.", "SliceSentence": "But that's not where that's not where bias and data stops . Thesebiases can be propagated towards models training cycles themselves which is what we'll focus on in the second half of this lecture", "StartMs": 575000, "EndMs": 587140, "WordsNum": 34, "Words": [{"Word": "But", "OffsetStartMs": 70, "OffsetEndMs": 390}, {"Word": "that's", "OffsetStartMs": 390, "OffsetEndMs": 690}, {"Word": "not", "OffsetStartMs": 690, "OffsetEndMs": 870}, {"Word": "where", "OffsetStartMs": 870, "OffsetEndMs": 1190}, {"Word": "that's", "OffsetStartMs": 1570, "OffsetEndMs": 1920}, {"Word": "not", "OffsetStartMs": 1920, "OffsetEndMs": 2025}, {"Word": "where", "OffsetStartMs": 2025, "OffsetEndMs": 2190}, {"Word": "bias", "OffsetStartMs": 2190, "OffsetEndMs": 2550}, {"Word": "and", "OffsetStartMs": 2550, "OffsetEndMs": 2700}, {"Word": "data", "OffsetStartMs": 2700, "OffsetEndMs": 2955}, {"Word": "stops", "OffsetStartMs": 2955, "OffsetEndMs": 3350}, {"Word": ".", "OffsetStartMs": 4000, "OffsetEndMs": 4320}, {"Word": "Thesebiases", "OffsetStartMs": 4320, "OffsetEndMs": 4965}, {"Word": "can", "OffsetStartMs": 4965, "OffsetEndMs": 5330}, {"Word": "be", "OffsetStartMs": 5440, "OffsetEndMs": 5745}, {"Word": "propagated", "OffsetStartMs": 5745, "OffsetEndMs": 6315}, {"Word": "towards", "OffsetStartMs": 6315, "OffsetEndMs": 6645}, {"Word": "models", "OffsetStartMs": 6645, "OffsetEndMs": 7010}, {"Word": "training", "OffsetStartMs": 7030, "OffsetEndMs": 7380}, {"Word": "cycles", "OffsetStartMs": 7380, "OffsetEndMs": 7875}, {"Word": "themselves", "OffsetStartMs": 7875, "OffsetEndMs": 8270}, {"Word": "which", "OffsetStartMs": 8440, "OffsetEndMs": 8700}, {"Word": "is", "OffsetStartMs": 8700, "OffsetEndMs": 8820}, {"Word": "what", "OffsetStartMs": 8820, "OffsetEndMs": 8940}, {"Word": "we'll", "OffsetStartMs": 8940, "OffsetEndMs": 9105}, {"Word": "focus", "OffsetStartMs": 9105, "OffsetEndMs": 9300}, {"Word": "on", "OffsetStartMs": 9300, "OffsetEndMs": 9555}, {"Word": "in", "OffsetStartMs": 9555, "OffsetEndMs": 9705}, {"Word": "the", "OffsetStartMs": 9705, "OffsetEndMs": 9825}, {"Word": "second", "OffsetStartMs": 9825, "OffsetEndMs": 10035}, {"Word": "half", "OffsetStartMs": 10035, "OffsetEndMs": 10305}, {"Word": "of", "OffsetStartMs": 10305, "OffsetEndMs": 10500}, {"Word": "this", "OffsetStartMs": 10500, "OffsetEndMs": 10635}, {"Word": "lecture", "OffsetStartMs": 10635, "OffsetEndMs": 10910}], "SpeechSpeed": 16.0}, {"FinalSentence": "And then once the model is actually deployed, which means it's actually put out into the real world and customers or users can actually get the predictions from it, we may see further biases perpetuated that we haven't seen before. The first of these is distribution shifts. Let's say I have a model that I trained on the past twenty years of data, and then I deploy it into the real world in 2023. This model will probably do fine because the data input distribution is quite similar to data in the training distribution.", "SliceSentence": "And then once the model is actually deployed which means it's actually put out into the real world and customers or users can actually get the predictions from it we may see further biases perpetuated that we haven't seen before . Thefirst of these is distribution shifts .Let's say I have a model that I trained on the past twenty years of data and then I deploy it into the real world in 20 2 3 .Thismodel will probably do fine because the data input distribution is quite similar to data in the training distribution", "StartMs": 588060, "EndMs": 618900, "WordsNum": 95, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 330}, {"Word": "then", "OffsetStartMs": 330, "OffsetEndMs": 590}, {"Word": "once", "OffsetStartMs": 670, "OffsetEndMs": 1005}, {"Word": "the", "OffsetStartMs": 1005, "OffsetEndMs": 1200}, {"Word": "model", "OffsetStartMs": 1200, "OffsetEndMs": 1380}, {"Word": "is", "OffsetStartMs": 1380, "OffsetEndMs": 1700}, {"Word": "actually", "OffsetStartMs": 1780, "OffsetEndMs": 2180}, {"Word": "deployed", "OffsetStartMs": 2230, "OffsetEndMs": 2700}, {"Word": "which", "OffsetStartMs": 2700, "OffsetEndMs": 2925}, {"Word": "means", "OffsetStartMs": 2925, "OffsetEndMs": 3135}, {"Word": "it's", "OffsetStartMs": 3135, "OffsetEndMs": 3465}, {"Word": "actually", "OffsetStartMs": 3465, "OffsetEndMs": 3735}, {"Word": "put", "OffsetStartMs": 3735, "OffsetEndMs": 3915}, {"Word": "out", "OffsetStartMs": 3915, "OffsetEndMs": 4095}, {"Word": "into", "OffsetStartMs": 4095, "OffsetEndMs": 4290}, {"Word": "the", "OffsetStartMs": 4290, "OffsetEndMs": 4455}, {"Word": "real", "OffsetStartMs": 4455, "OffsetEndMs": 4635}, {"Word": "world", "OffsetStartMs": 4635, "OffsetEndMs": 4875}, {"Word": "and", "OffsetStartMs": 4875, "OffsetEndMs": 5100}, {"Word": "customers", "OffsetStartMs": 5100, "OffsetEndMs": 5390}, {"Word": "or", "OffsetStartMs": 5530, "OffsetEndMs": 5805}, {"Word": "users", "OffsetStartMs": 5805, "OffsetEndMs": 6080}, {"Word": "can", "OffsetStartMs": 6100, "OffsetEndMs": 6480}, {"Word": "actually", "OffsetStartMs": 6480, "OffsetEndMs": 6860}, {"Word": "get", "OffsetStartMs": 6880, "OffsetEndMs": 7185}, {"Word": "the", "OffsetStartMs": 7185, "OffsetEndMs": 7350}, {"Word": "predictions", "OffsetStartMs": 7350, "OffsetEndMs": 7740}, {"Word": "from", "OffsetStartMs": 7740, "OffsetEndMs": 7905}, {"Word": "it", "OffsetStartMs": 7905, "OffsetEndMs": 8180}, {"Word": "we", "OffsetStartMs": 8500, "OffsetEndMs": 8775}, {"Word": "may", "OffsetStartMs": 8775, "OffsetEndMs": 9015}, {"Word": "see", "OffsetStartMs": 9015, "OffsetEndMs": 9380}, {"Word": "further", "OffsetStartMs": 10210, "OffsetEndMs": 10545}, {"Word": "biases", "OffsetStartMs": 10545, "OffsetEndMs": 11070}, {"Word": "perpetuated", "OffsetStartMs": 11070, "OffsetEndMs": 11745}, {"Word": "that", "OffsetStartMs": 11745, "OffsetEndMs": 11985}, {"Word": "we", "OffsetStartMs": 11985, "OffsetEndMs": 12135}, {"Word": "haven't", "OffsetStartMs": 12135, "OffsetEndMs": 12435}, {"Word": "seen", "OffsetStartMs": 12435, "OffsetEndMs": 12630}, {"Word": "before", "OffsetStartMs": 12630, "OffsetEndMs": 12920}, {"Word": ".", "OffsetStartMs": 13570, "OffsetEndMs": 13830}, {"Word": "Thefirst", "OffsetStartMs": 13830, "OffsetEndMs": 14010}, {"Word": "of", "OffsetStartMs": 14010, "OffsetEndMs": 14190}, {"Word": "these", "OffsetStartMs": 14190, "OffsetEndMs": 14400}, {"Word": "is", "OffsetStartMs": 14400, "OffsetEndMs": 14745}, {"Word": "distribution", "OffsetStartMs": 14745, "OffsetEndMs": 15140}, {"Word": "shifts", "OffsetStartMs": 15220, "OffsetEndMs": 15800}, {"Word": ".Let's", "OffsetStartMs": 16270, "OffsetEndMs": 16650}, {"Word": "say", "OffsetStartMs": 16650, "OffsetEndMs": 16815}, {"Word": "I", "OffsetStartMs": 16815, "OffsetEndMs": 16995}, {"Word": "have", "OffsetStartMs": 16995, "OffsetEndMs": 17130}, {"Word": "a", "OffsetStartMs": 17130, "OffsetEndMs": 17250}, {"Word": "model", "OffsetStartMs": 17250, "OffsetEndMs": 17510}, {"Word": "that", "OffsetStartMs": 17560, "OffsetEndMs": 17925}, {"Word": "I", "OffsetStartMs": 17925, "OffsetEndMs": 18210}, {"Word": "trained", "OffsetStartMs": 18210, "OffsetEndMs": 18480}, {"Word": "on", "OffsetStartMs": 18480, "OffsetEndMs": 18795}, {"Word": "the", "OffsetStartMs": 18795, "OffsetEndMs": 19035}, {"Word": "past", "OffsetStartMs": 19035, "OffsetEndMs": 19260}, {"Word": "twenty", "OffsetStartMs": 19260, "OffsetEndMs": 19500}, {"Word": "years", "OffsetStartMs": 19500, "OffsetEndMs": 19695}, {"Word": "of", "OffsetStartMs": 19695, "OffsetEndMs": 19860}, {"Word": "data", "OffsetStartMs": 19860, "OffsetEndMs": 20120}, {"Word": "and", "OffsetStartMs": 20470, "OffsetEndMs": 20745}, {"Word": "then", "OffsetStartMs": 20745, "OffsetEndMs": 20925}, {"Word": "I", "OffsetStartMs": 20925, "OffsetEndMs": 21165}, {"Word": "deploy", "OffsetStartMs": 21165, "OffsetEndMs": 21375}, {"Word": "it", "OffsetStartMs": 21375, "OffsetEndMs": 21510}, {"Word": "into", "OffsetStartMs": 21510, "OffsetEndMs": 21675}, {"Word": "the", "OffsetStartMs": 21675, "OffsetEndMs": 21840}, {"Word": "real", "OffsetStartMs": 21840, "OffsetEndMs": 21990}, {"Word": "world", "OffsetStartMs": 21990, "OffsetEndMs": 22280}, {"Word": "in", "OffsetStartMs": 22750, "OffsetEndMs": 23055}, {"Word": "20", "OffsetStartMs": 23055, "OffsetEndMs": 23580}, {"Word": "2", "OffsetStartMs": 23580, "OffsetEndMs": 23930}, {"Word": "3", "OffsetStartMs": 24040, "OffsetEndMs": 24330}, {"Word": ".Thismodel", "OffsetStartMs": 24330, "OffsetEndMs": 24540}, {"Word": "will", "OffsetStartMs": 24540, "OffsetEndMs": 24735}, {"Word": "probably", "OffsetStartMs": 24735, "OffsetEndMs": 25010}, {"Word": "do", "OffsetStartMs": 25060, "OffsetEndMs": 25350}, {"Word": "fine", "OffsetStartMs": 25350, "OffsetEndMs": 25640}, {"Word": "because", "OffsetStartMs": 25780, "OffsetEndMs": 26040}, {"Word": "the", "OffsetStartMs": 26040, "OffsetEndMs": 26160}, {"Word": "data", "OffsetStartMs": 26160, "OffsetEndMs": 26420}, {"Word": "input", "OffsetStartMs": 26470, "OffsetEndMs": 26835}, {"Word": "distribution", "OffsetStartMs": 26835, "OffsetEndMs": 27200}, {"Word": "is", "OffsetStartMs": 27280, "OffsetEndMs": 27600}, {"Word": "quite", "OffsetStartMs": 27600, "OffsetEndMs": 27870}, {"Word": "similar", "OffsetStartMs": 27870, "OffsetEndMs": 28220}, {"Word": "to", "OffsetStartMs": 28300, "OffsetEndMs": 28545}, {"Word": "data", "OffsetStartMs": 28545, "OffsetEndMs": 28755}, {"Word": "in", "OffsetStartMs": 28755, "OffsetEndMs": 29025}, {"Word": "the", "OffsetStartMs": 29025, "OffsetEndMs": 29190}, {"Word": "training", "OffsetStartMs": 29190, "OffsetEndMs": 29445}, {"Word": "distribution", "OffsetStartMs": 29445, "OffsetEndMs": 29840}], "SpeechSpeed": 17.2}, {"FinalSentence": "But what would happen to this model in 2033 it, it probably would not work as well because the distribution that the data is coming from would shift significantly across this decade. And if we don't continue to update our models with this input stream of data, we're going to have obsolete and incorrect predictions.", "SliceSentence": "But what would happen to this model in 2033it it probably would not work as well because the distribution that the data is coming from would shift significantly across this decade . Andif we don't continue to update our models with this input stream of data we're going to have obsolete and incorrect predictions", "StartMs": 618900, "EndMs": 637480, "WordsNum": 54, "Words": [{"Word": "But", "OffsetStartMs": 0, "OffsetEndMs": 320}, {"Word": "what", "OffsetStartMs": 340, "OffsetEndMs": 615}, {"Word": "would", "OffsetStartMs": 615, "OffsetEndMs": 750}, {"Word": "happen", "OffsetStartMs": 750, "OffsetEndMs": 945}, {"Word": "to", "OffsetStartMs": 945, "OffsetEndMs": 1125}, {"Word": "this", "OffsetStartMs": 1125, "OffsetEndMs": 1260}, {"Word": "model", "OffsetStartMs": 1260, "OffsetEndMs": 1530}, {"Word": "in", "OffsetStartMs": 1530, "OffsetEndMs": 1860}, {"Word": "2033it", "OffsetStartMs": 1860, "OffsetEndMs": 3495}, {"Word": "it", "OffsetStartMs": 3495, "OffsetEndMs": 3750}, {"Word": "probably", "OffsetStartMs": 3750, "OffsetEndMs": 4040}, {"Word": "would", "OffsetStartMs": 4060, "OffsetEndMs": 4350}, {"Word": "not", "OffsetStartMs": 4350, "OffsetEndMs": 4640}, {"Word": "work", "OffsetStartMs": 4750, "OffsetEndMs": 5145}, {"Word": "as", "OffsetStartMs": 5145, "OffsetEndMs": 5460}, {"Word": "well", "OffsetStartMs": 5460, "OffsetEndMs": 5780}, {"Word": "because", "OffsetStartMs": 6160, "OffsetEndMs": 6480}, {"Word": "the", "OffsetStartMs": 6480, "OffsetEndMs": 6750}, {"Word": "distribution", "OffsetStartMs": 6750, "OffsetEndMs": 7095}, {"Word": "that", "OffsetStartMs": 7095, "OffsetEndMs": 7350}, {"Word": "the", "OffsetStartMs": 7350, "OffsetEndMs": 7455}, {"Word": "data", "OffsetStartMs": 7455, "OffsetEndMs": 7605}, {"Word": "is", "OffsetStartMs": 7605, "OffsetEndMs": 7800}, {"Word": "coming", "OffsetStartMs": 7800, "OffsetEndMs": 8025}, {"Word": "from", "OffsetStartMs": 8025, "OffsetEndMs": 8250}, {"Word": "would", "OffsetStartMs": 8250, "OffsetEndMs": 8460}, {"Word": "shift", "OffsetStartMs": 8460, "OffsetEndMs": 8780}, {"Word": "significantly", "OffsetStartMs": 8800, "OffsetEndMs": 9200}, {"Word": "across", "OffsetStartMs": 9880, "OffsetEndMs": 10215}, {"Word": "this", "OffsetStartMs": 10215, "OffsetEndMs": 10455}, {"Word": "decade", "OffsetStartMs": 10455, "OffsetEndMs": 10760}, {"Word": ".", "OffsetStartMs": 11020, "OffsetEndMs": 11280}, {"Word": "Andif", "OffsetStartMs": 11280, "OffsetEndMs": 11400}, {"Word": "we", "OffsetStartMs": 11400, "OffsetEndMs": 11520}, {"Word": "don't", "OffsetStartMs": 11520, "OffsetEndMs": 11805}, {"Word": "continue", "OffsetStartMs": 11805, "OffsetEndMs": 12045}, {"Word": "to", "OffsetStartMs": 12045, "OffsetEndMs": 12330}, {"Word": "update", "OffsetStartMs": 12330, "OffsetEndMs": 12540}, {"Word": "our", "OffsetStartMs": 12540, "OffsetEndMs": 12690}, {"Word": "models", "OffsetStartMs": 12690, "OffsetEndMs": 12975}, {"Word": "with", "OffsetStartMs": 12975, "OffsetEndMs": 13230}, {"Word": "this", "OffsetStartMs": 13230, "OffsetEndMs": 13490}, {"Word": "input", "OffsetStartMs": 13900, "OffsetEndMs": 14205}, {"Word": "stream", "OffsetStartMs": 14205, "OffsetEndMs": 14370}, {"Word": "of", "OffsetStartMs": 14370, "OffsetEndMs": 14505}, {"Word": "data", "OffsetStartMs": 14505, "OffsetEndMs": 14780}, {"Word": "we're", "OffsetStartMs": 14980, "OffsetEndMs": 15285}, {"Word": "going", "OffsetStartMs": 15285, "OffsetEndMs": 15375}, {"Word": "to", "OffsetStartMs": 15375, "OffsetEndMs": 15495}, {"Word": "have", "OffsetStartMs": 15495, "OffsetEndMs": 15645}, {"Word": "obsolete", "OffsetStartMs": 15645, "OffsetEndMs": 16275}, {"Word": "and", "OffsetStartMs": 16275, "OffsetEndMs": 16500}, {"Word": "incorrect", "OffsetStartMs": 16500, "OffsetEndMs": 17010}, {"Word": "predictions", "OffsetStartMs": 17010, "OffsetEndMs": 17630}], "SpeechSpeed": 17.6}, {"FinalSentence": "And finally, after deployment there, the evaluation aspect. So think back to the Apple Siri example that we've been talking about. EM, if the evaluation metric or the evaluation data set that Siri was evaluated on was also mostly comprised of American English, then to anybody this model will look like it does extremely well, right? It can detect, it, can recognize American English voices with extremely high accuracy and therefore is deployed into the real world. But what about its accuracy on subgroups, on accented voices, on people who for whom English is not their first language? If we don't also test on subgroups in our evaluation metrics, we're going to faceation bias.", "SliceSentence": "And finally after deployment there the evaluation aspect . Sothink back to the Apple Siri example that we've been talking about . EMif the evaluation metric or the evaluation data set that Siri was evaluated on was also mostly comprised of American English then to anybody this model will look like it does extremely well right It can detect it can recognize American English voices with extremely high accuracy and therefore is deployed into the real world . Butwhat about its accuracy on subgroups on accented voices on people who for whom English is not their first language If we don't also test on subgroups in our evaluation metrics we're going to faceation bias", "StartMs": 638440, "EndMs": 680340, "WordsNum": 114, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "finally", "OffsetStartMs": 360, "OffsetEndMs": 650}, {"Word": "after", "OffsetStartMs": 850, "OffsetEndMs": 1250}, {"Word": "deployment", "OffsetStartMs": 1360, "OffsetEndMs": 1875}, {"Word": "there", "OffsetStartMs": 1875, "OffsetEndMs": 2250}, {"Word": "the", "OffsetStartMs": 2250, "OffsetEndMs": 2445}, {"Word": "evaluation", "OffsetStartMs": 2445, "OffsetEndMs": 3050}, {"Word": "aspect", "OffsetStartMs": 3250, "OffsetEndMs": 3650}, {"Word": ".", "OffsetStartMs": 4030, "OffsetEndMs": 4430}, {"Word": "Sothink", "OffsetStartMs": 4630, "OffsetEndMs": 4950}, {"Word": "back", "OffsetStartMs": 4950, "OffsetEndMs": 5250}, {"Word": "to", "OffsetStartMs": 5250, "OffsetEndMs": 5550}, {"Word": "the", "OffsetStartMs": 5550, "OffsetEndMs": 5790}, {"Word": "Apple", "OffsetStartMs": 5790, "OffsetEndMs": 6075}, {"Word": "Siri", "OffsetStartMs": 6075, "OffsetEndMs": 6480}, {"Word": "example", "OffsetStartMs": 6480, "OffsetEndMs": 6735}, {"Word": "that", "OffsetStartMs": 6735, "OffsetEndMs": 6930}, {"Word": "we've", "OffsetStartMs": 6930, "OffsetEndMs": 7125}, {"Word": "been", "OffsetStartMs": 7125, "OffsetEndMs": 7260}, {"Word": "talking", "OffsetStartMs": 7260, "OffsetEndMs": 7545}, {"Word": "about", "OffsetStartMs": 7545, "OffsetEndMs": 7940}, {"Word": ".", "OffsetStartMs": 8380, "OffsetEndMs": 8780}, {"Word": "EMif", "OffsetStartMs": 9190, "OffsetEndMs": 9525}, {"Word": "the", "OffsetStartMs": 9525, "OffsetEndMs": 9750}, {"Word": "evaluation", "OffsetStartMs": 9750, "OffsetEndMs": 10275}, {"Word": "metric", "OffsetStartMs": 10275, "OffsetEndMs": 10770}, {"Word": "or", "OffsetStartMs": 10770, "OffsetEndMs": 10995}, {"Word": "the", "OffsetStartMs": 10995, "OffsetEndMs": 11145}, {"Word": "evaluation", "OffsetStartMs": 11145, "OffsetEndMs": 11550}, {"Word": "data", "OffsetStartMs": 11550, "OffsetEndMs": 11850}, {"Word": "set", "OffsetStartMs": 11850, "OffsetEndMs": 12090}, {"Word": "that", "OffsetStartMs": 12090, "OffsetEndMs": 12300}, {"Word": "Siri", "OffsetStartMs": 12300, "OffsetEndMs": 12630}, {"Word": "was", "OffsetStartMs": 12630, "OffsetEndMs": 12795}, {"Word": "evaluated", "OffsetStartMs": 12795, "OffsetEndMs": 13245}, {"Word": "on", "OffsetStartMs": 13245, "OffsetEndMs": 13550}, {"Word": "was", "OffsetStartMs": 13720, "OffsetEndMs": 14120}, {"Word": "also", "OffsetStartMs": 14170, "OffsetEndMs": 14460}, {"Word": "mostly", "OffsetStartMs": 14460, "OffsetEndMs": 14745}, {"Word": "comprised", "OffsetStartMs": 14745, "OffsetEndMs": 15285}, {"Word": "of", "OffsetStartMs": 15285, "OffsetEndMs": 15645}, {"Word": "American", "OffsetStartMs": 15645, "OffsetEndMs": 16010}, {"Word": "English", "OffsetStartMs": 16090, "OffsetEndMs": 16490}, {"Word": "then", "OffsetStartMs": 16810, "OffsetEndMs": 17210}, {"Word": "to", "OffsetStartMs": 17770, "OffsetEndMs": 18170}, {"Word": "anybody", "OffsetStartMs": 18190, "OffsetEndMs": 18570}, {"Word": "this", "OffsetStartMs": 18570, "OffsetEndMs": 18840}, {"Word": "model", "OffsetStartMs": 18840, "OffsetEndMs": 19125}, {"Word": "will", "OffsetStartMs": 19125, "OffsetEndMs": 19380}, {"Word": "look", "OffsetStartMs": 19380, "OffsetEndMs": 19515}, {"Word": "like", "OffsetStartMs": 19515, "OffsetEndMs": 19650}, {"Word": "it", "OffsetStartMs": 19650, "OffsetEndMs": 19770}, {"Word": "does", "OffsetStartMs": 19770, "OffsetEndMs": 20010}, {"Word": "extremely", "OffsetStartMs": 20010, "OffsetEndMs": 20355}, {"Word": "well", "OffsetStartMs": 20355, "OffsetEndMs": 20720}, {"Word": "right", "OffsetStartMs": 20890, "OffsetEndMs": 21180}, {"Word": "It", "OffsetStartMs": 21180, "OffsetEndMs": 21330}, {"Word": "can", "OffsetStartMs": 21330, "OffsetEndMs": 21540}, {"Word": "detect", "OffsetStartMs": 21540, "OffsetEndMs": 21890}, {"Word": "it", "OffsetStartMs": 22090, "OffsetEndMs": 22350}, {"Word": "can", "OffsetStartMs": 22350, "OffsetEndMs": 22515}, {"Word": "recognize", "OffsetStartMs": 22515, "OffsetEndMs": 22820}, {"Word": "American", "OffsetStartMs": 23050, "OffsetEndMs": 23445}, {"Word": "English", "OffsetStartMs": 23445, "OffsetEndMs": 23730}, {"Word": "voices", "OffsetStartMs": 23730, "OffsetEndMs": 24000}, {"Word": "with", "OffsetStartMs": 24000, "OffsetEndMs": 24345}, {"Word": "extremely", "OffsetStartMs": 24345, "OffsetEndMs": 24660}, {"Word": "high", "OffsetStartMs": 24660, "OffsetEndMs": 25005}, {"Word": "accuracy", "OffsetStartMs": 25005, "OffsetEndMs": 25610}, {"Word": "and", "OffsetStartMs": 25690, "OffsetEndMs": 26055}, {"Word": "therefore", "OffsetStartMs": 26055, "OffsetEndMs": 26310}, {"Word": "is", "OffsetStartMs": 26310, "OffsetEndMs": 26550}, {"Word": "deployed", "OffsetStartMs": 26550, "OffsetEndMs": 26880}, {"Word": "into", "OffsetStartMs": 26880, "OffsetEndMs": 27045}, {"Word": "the", "OffsetStartMs": 27045, "OffsetEndMs": 27210}, {"Word": "real", "OffsetStartMs": 27210, "OffsetEndMs": 27375}, {"Word": "world", "OffsetStartMs": 27375, "OffsetEndMs": 27680}, {"Word": ".", "OffsetStartMs": 28150, "OffsetEndMs": 28410}, {"Word": "Butwhat", "OffsetStartMs": 28410, "OffsetEndMs": 28575}, {"Word": "about", "OffsetStartMs": 28575, "OffsetEndMs": 28860}, {"Word": "its", "OffsetStartMs": 28860, "OffsetEndMs": 29235}, {"Word": "accuracy", "OffsetStartMs": 29235, "OffsetEndMs": 29715}, {"Word": "on", "OffsetStartMs": 29715, "OffsetEndMs": 29910}, {"Word": "subgroups", "OffsetStartMs": 29910, "OffsetEndMs": 30570}, {"Word": "on", "OffsetStartMs": 30570, "OffsetEndMs": 30900}, {"Word": "accented", "OffsetStartMs": 30900, "OffsetEndMs": 31470}, {"Word": "voices", "OffsetStartMs": 31470, "OffsetEndMs": 31820}, {"Word": "on", "OffsetStartMs": 31960, "OffsetEndMs": 32295}, {"Word": "people", "OffsetStartMs": 32295, "OffsetEndMs": 32595}, {"Word": "who", "OffsetStartMs": 32595, "OffsetEndMs": 32960}, {"Word": "for", "OffsetStartMs": 33340, "OffsetEndMs": 33615}, {"Word": "whom", "OffsetStartMs": 33615, "OffsetEndMs": 33840}, {"Word": "English", "OffsetStartMs": 33840, "OffsetEndMs": 34065}, {"Word": "is", "OffsetStartMs": 34065, "OffsetEndMs": 34200}, {"Word": "not", "OffsetStartMs": 34200, "OffsetEndMs": 34350}, {"Word": "their", "OffsetStartMs": 34350, "OffsetEndMs": 34515}, {"Word": "first", "OffsetStartMs": 34515, "OffsetEndMs": 34695}, {"Word": "language", "OffsetStartMs": 34695, "OffsetEndMs": 35000}, {"Word": "If", "OffsetStartMs": 35440, "OffsetEndMs": 35715}, {"Word": "we", "OffsetStartMs": 35715, "OffsetEndMs": 35880}, {"Word": "don't", "OffsetStartMs": 35880, "OffsetEndMs": 36285}, {"Word": "also", "OffsetStartMs": 36285, "OffsetEndMs": 36585}, {"Word": "test", "OffsetStartMs": 36585, "OffsetEndMs": 36825}, {"Word": "on", "OffsetStartMs": 36825, "OffsetEndMs": 37050}, {"Word": "subgroups", "OffsetStartMs": 37050, "OffsetEndMs": 37545}, {"Word": "in", "OffsetStartMs": 37545, "OffsetEndMs": 37665}, {"Word": "our", "OffsetStartMs": 37665, "OffsetEndMs": 37815}, {"Word": "evaluation", "OffsetStartMs": 37815, "OffsetEndMs": 38340}, {"Word": "metrics", "OffsetStartMs": 38340, "OffsetEndMs": 38690}, {"Word": "we're", "OffsetStartMs": 38950, "OffsetEndMs": 39240}, {"Word": "going", "OffsetStartMs": 39240, "OffsetEndMs": 39360}, {"Word": "to", "OffsetStartMs": 39360, "OffsetEndMs": 39510}, {"Word": "faceation", "OffsetStartMs": 39510, "OffsetEndMs": 40365}, {"Word": "bias", "OffsetStartMs": 40365, "OffsetEndMs": 40880}], "SpeechSpeed": 15.9}, {"FinalSentence": "So now let's talk about another example in the real world of how bias can perpetuate throughout the course of this artificial intelligence life cycle.", "SliceSentence": "So now let's talk about another example in the real world of how bias can perpetuate throughout the course of this artificial intelligence life cycle", "StartMs": 680540, "EndMs": 688720, "WordsNum": 25, "Words": [{"Word": "So", "OffsetStartMs": 100, "OffsetEndMs": 360}, {"Word": "now", "OffsetStartMs": 360, "OffsetEndMs": 620}, {"Word": "let's", "OffsetStartMs": 790, "OffsetEndMs": 1155}, {"Word": "talk", "OffsetStartMs": 1155, "OffsetEndMs": 1335}, {"Word": "about", "OffsetStartMs": 1335, "OffsetEndMs": 1575}, {"Word": "another", "OffsetStartMs": 1575, "OffsetEndMs": 1845}, {"Word": "example", "OffsetStartMs": 1845, "OffsetEndMs": 2190}, {"Word": "in", "OffsetStartMs": 2190, "OffsetEndMs": 2430}, {"Word": "the", "OffsetStartMs": 2430, "OffsetEndMs": 2535}, {"Word": "real", "OffsetStartMs": 2535, "OffsetEndMs": 2700}, {"Word": "world", "OffsetStartMs": 2700, "OffsetEndMs": 3000}, {"Word": "of", "OffsetStartMs": 3000, "OffsetEndMs": 3270}, {"Word": "how", "OffsetStartMs": 3270, "OffsetEndMs": 3465}, {"Word": "bias", "OffsetStartMs": 3465, "OffsetEndMs": 3825}, {"Word": "can", "OffsetStartMs": 3825, "OffsetEndMs": 3990}, {"Word": "perpetuate", "OffsetStartMs": 3990, "OffsetEndMs": 4605}, {"Word": "throughout", "OffsetStartMs": 4605, "OffsetEndMs": 4890}, {"Word": "the", "OffsetStartMs": 4890, "OffsetEndMs": 5115}, {"Word": "course", "OffsetStartMs": 5115, "OffsetEndMs": 5355}, {"Word": "of", "OffsetStartMs": 5355, "OffsetEndMs": 5580}, {"Word": "this", "OffsetStartMs": 5580, "OffsetEndMs": 5820}, {"Word": "artificial", "OffsetStartMs": 5820, "OffsetEndMs": 6165}, {"Word": "intelligence", "OffsetStartMs": 6165, "OffsetEndMs": 6510}, {"Word": "life", "OffsetStartMs": 6510, "OffsetEndMs": 6795}, {"Word": "cycle", "OffsetStartMs": 6795, "OffsetEndMs": 7100}], "SpeechSpeed": 18.2}, {"FinalSentence": "Commercial facial detection systems are everywhere. You actually played around with some of them in lab two when you trained your v on a facial detection data set.", "SliceSentence": "Commercial facial detection systems are everywhere . Youactually played around with some of them in lab two when you trained your v on a facial detection data set", "StartMs": 690120, "EndMs": 700520, "WordsNum": 28, "Words": [{"Word": "Commercial", "OffsetStartMs": 190, "OffsetEndMs": 585}, {"Word": "facial", "OffsetStartMs": 585, "OffsetEndMs": 1035}, {"Word": "detection", "OffsetStartMs": 1035, "OffsetEndMs": 1410}, {"Word": "systems", "OffsetStartMs": 1410, "OffsetEndMs": 1700}, {"Word": "are", "OffsetStartMs": 1720, "OffsetEndMs": 2120}, {"Word": "everywhere", "OffsetStartMs": 2200, "OffsetEndMs": 2600}, {"Word": ".", "OffsetStartMs": 2770, "OffsetEndMs": 3170}, {"Word": "Youactually", "OffsetStartMs": 3280, "OffsetEndMs": 3570}, {"Word": "played", "OffsetStartMs": 3570, "OffsetEndMs": 3765}, {"Word": "around", "OffsetStartMs": 3765, "OffsetEndMs": 3975}, {"Word": "with", "OffsetStartMs": 3975, "OffsetEndMs": 4170}, {"Word": "some", "OffsetStartMs": 4170, "OffsetEndMs": 4335}, {"Word": "of", "OffsetStartMs": 4335, "OffsetEndMs": 4470}, {"Word": "them", "OffsetStartMs": 4470, "OffsetEndMs": 4680}, {"Word": "in", "OffsetStartMs": 4680, "OffsetEndMs": 5030}, {"Word": "lab", "OffsetStartMs": 5290, "OffsetEndMs": 5655}, {"Word": "two", "OffsetStartMs": 5655, "OffsetEndMs": 5970}, {"Word": "when", "OffsetStartMs": 5970, "OffsetEndMs": 6165}, {"Word": "you", "OffsetStartMs": 6165, "OffsetEndMs": 6375}, {"Word": "trained", "OffsetStartMs": 6375, "OffsetEndMs": 6675}, {"Word": "your", "OffsetStartMs": 6675, "OffsetEndMs": 6900}, {"Word": "v", "OffsetStartMs": 6900, "OffsetEndMs": 7190}, {"Word": "on", "OffsetStartMs": 7540, "OffsetEndMs": 7875}, {"Word": "a", "OffsetStartMs": 7875, "OffsetEndMs": 8100}, {"Word": "facial", "OffsetStartMs": 8100, "OffsetEndMs": 8460}, {"Word": "detection", "OffsetStartMs": 8460, "OffsetEndMs": 8790}, {"Word": "data", "OffsetStartMs": 8790, "OffsetEndMs": 8985}, {"Word": "set", "OffsetStartMs": 8985, "OffsetEndMs": 9320}], "SpeechSpeed": 15.5}, {"FinalSentence": "In addition to the lock screens on your cell phones, facial detection systems are also present in the automatic filters that your phone cameras apply whenever you try to take a picture, and they're also used in criminal investigations.", "SliceSentence": "In addition to the lock screens on your cell phones facial detection systems are also present in the automatic filters that your phone cameras apply whenever you try to take a picture and they 're also used in criminal investigations", "StartMs": 700520, "EndMs": 714080, "WordsNum": 40, "Words": [{"Word": "In", "OffsetStartMs": 0, "OffsetEndMs": 165}, {"Word": "addition", "OffsetStartMs": 165, "OffsetEndMs": 470}, {"Word": "to", "OffsetStartMs": 490, "OffsetEndMs": 795}, {"Word": "the", "OffsetStartMs": 795, "OffsetEndMs": 945}, {"Word": "lock", "OffsetStartMs": 945, "OffsetEndMs": 1170}, {"Word": "screens", "OffsetStartMs": 1170, "OffsetEndMs": 1470}, {"Word": "on", "OffsetStartMs": 1470, "OffsetEndMs": 1620}, {"Word": "your", "OffsetStartMs": 1620, "OffsetEndMs": 1785}, {"Word": "cell", "OffsetStartMs": 1785, "OffsetEndMs": 1995}, {"Word": "phones", "OffsetStartMs": 1995, "OffsetEndMs": 2300}, {"Word": "facial", "OffsetStartMs": 3520, "OffsetEndMs": 3975}, {"Word": "detection", "OffsetStartMs": 3975, "OffsetEndMs": 4305}, {"Word": "systems", "OffsetStartMs": 4305, "OffsetEndMs": 4560}, {"Word": "are", "OffsetStartMs": 4560, "OffsetEndMs": 4920}, {"Word": "also", "OffsetStartMs": 4920, "OffsetEndMs": 5175}, {"Word": "present", "OffsetStartMs": 5175, "OffsetEndMs": 5430}, {"Word": "in", "OffsetStartMs": 5430, "OffsetEndMs": 5685}, {"Word": "the", "OffsetStartMs": 5685, "OffsetEndMs": 5955}, {"Word": "automatic", "OffsetStartMs": 5955, "OffsetEndMs": 6285}, {"Word": "filters", "OffsetStartMs": 6285, "OffsetEndMs": 6830}, {"Word": "that", "OffsetStartMs": 6850, "OffsetEndMs": 7110}, {"Word": "your", "OffsetStartMs": 7110, "OffsetEndMs": 7245}, {"Word": "phone", "OffsetStartMs": 7245, "OffsetEndMs": 7440}, {"Word": "cameras", "OffsetStartMs": 7440, "OffsetEndMs": 7760}, {"Word": "apply", "OffsetStartMs": 7780, "OffsetEndMs": 8130}, {"Word": "whenever", "OffsetStartMs": 8130, "OffsetEndMs": 8370}, {"Word": "you", "OffsetStartMs": 8370, "OffsetEndMs": 8550}, {"Word": "try", "OffsetStartMs": 8550, "OffsetEndMs": 8700}, {"Word": "to", "OffsetStartMs": 8700, "OffsetEndMs": 8820}, {"Word": "take", "OffsetStartMs": 8820, "OffsetEndMs": 8925}, {"Word": "a", "OffsetStartMs": 8925, "OffsetEndMs": 9030}, {"Word": "picture", "OffsetStartMs": 9030, "OffsetEndMs": 9290}, {"Word": "and", "OffsetStartMs": 9910, "OffsetEndMs": 10260}, {"Word": "they", "OffsetStartMs": 10260, "OffsetEndMs": 10440}, {"Word": "'re", "OffsetStartMs": 10440, "OffsetEndMs": 10620}, {"Word": "also", "OffsetStartMs": 10620, "OffsetEndMs": 10845}, {"Word": "used", "OffsetStartMs": 10845, "OffsetEndMs": 11055}, {"Word": "in", "OffsetStartMs": 11055, "OffsetEndMs": 11310}, {"Word": "criminal", "OffsetStartMs": 11310, "OffsetEndMs": 11625}, {"Word": "investigations", "OffsetStartMs": 11625, "OffsetEndMs": 12320}], "SpeechSpeed": 17.1}, {"FinalSentence": "These are three commercial facial detection systems that were deployed, and we'll analyze the biases that might have been present in all of them for in the next few minutes.", "SliceSentence": "These are three commercial facial detection systems that were deployed and we'll analyze the biases that might have been present in all of them for in the next few minutes", "StartMs": 714080, "EndMs": 724940, "WordsNum": 30, "Words": [{"Word": "These", "OffsetStartMs": 40, "OffsetEndMs": 405}, {"Word": "are", "OffsetStartMs": 405, "OffsetEndMs": 720}, {"Word": "three", "OffsetStartMs": 720, "OffsetEndMs": 1065}, {"Word": "commercial", "OffsetStartMs": 1065, "OffsetEndMs": 1440}, {"Word": "facial", "OffsetStartMs": 1440, "OffsetEndMs": 1905}, {"Word": "detection", "OffsetStartMs": 1905, "OffsetEndMs": 2250}, {"Word": "systems", "OffsetStartMs": 2250, "OffsetEndMs": 2540}, {"Word": "that", "OffsetStartMs": 2560, "OffsetEndMs": 2835}, {"Word": "were", "OffsetStartMs": 2835, "OffsetEndMs": 3075}, {"Word": "deployed", "OffsetStartMs": 3075, "OffsetEndMs": 3620}, {"Word": "and", "OffsetStartMs": 3700, "OffsetEndMs": 4100}, {"Word": "we'll", "OffsetStartMs": 4810, "OffsetEndMs": 5145}, {"Word": "analyze", "OffsetStartMs": 5145, "OffsetEndMs": 5535}, {"Word": "the", "OffsetStartMs": 5535, "OffsetEndMs": 5685}, {"Word": "biases", "OffsetStartMs": 5685, "OffsetEndMs": 6135}, {"Word": "that", "OffsetStartMs": 6135, "OffsetEndMs": 6285}, {"Word": "might", "OffsetStartMs": 6285, "OffsetEndMs": 6420}, {"Word": "have", "OffsetStartMs": 6420, "OffsetEndMs": 6540}, {"Word": "been", "OffsetStartMs": 6540, "OffsetEndMs": 6690}, {"Word": "present", "OffsetStartMs": 6690, "OffsetEndMs": 6930}, {"Word": "in", "OffsetStartMs": 6930, "OffsetEndMs": 7155}, {"Word": "all", "OffsetStartMs": 7155, "OffsetEndMs": 7320}, {"Word": "of", "OffsetStartMs": 7320, "OffsetEndMs": 7485}, {"Word": "them", "OffsetStartMs": 7485, "OffsetEndMs": 7760}, {"Word": "for", "OffsetStartMs": 8080, "OffsetEndMs": 8430}, {"Word": "in", "OffsetStartMs": 8430, "OffsetEndMs": 8760}, {"Word": "the", "OffsetStartMs": 8760, "OffsetEndMs": 8985}, {"Word": "next", "OffsetStartMs": 8985, "OffsetEndMs": 9150}, {"Word": "few", "OffsetStartMs": 9150, "OffsetEndMs": 9330}, {"Word": "minutes", "OffsetStartMs": 9330, "OffsetEndMs": 9590}], "SpeechSpeed": 15.7}, {"FinalSentence": "So the first thing you may notice is that there is a huge accuracy gap between two different demographics in this plot.", "SliceSentence": "So the first thing you may notice is that there is a huge accuracy gap between two different demographics in this plot", "StartMs": 725600, "EndMs": 734520, "WordsNum": 22, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 480}, {"Word": "the", "OffsetStartMs": 480, "OffsetEndMs": 690}, {"Word": "first", "OffsetStartMs": 690, "OffsetEndMs": 855}, {"Word": "thing", "OffsetStartMs": 855, "OffsetEndMs": 1035}, {"Word": "you", "OffsetStartMs": 1035, "OffsetEndMs": 1170}, {"Word": "may", "OffsetStartMs": 1170, "OffsetEndMs": 1305}, {"Word": "notice", "OffsetStartMs": 1305, "OffsetEndMs": 1560}, {"Word": "is", "OffsetStartMs": 1560, "OffsetEndMs": 1815}, {"Word": "that", "OffsetStartMs": 1815, "OffsetEndMs": 1995}, {"Word": "there", "OffsetStartMs": 1995, "OffsetEndMs": 2175}, {"Word": "is", "OffsetStartMs": 2175, "OffsetEndMs": 2325}, {"Word": "a", "OffsetStartMs": 2325, "OffsetEndMs": 2595}, {"Word": "huge", "OffsetStartMs": 2595, "OffsetEndMs": 2990}, {"Word": "accuracy", "OffsetStartMs": 3340, "OffsetEndMs": 3840}, {"Word": "gap", "OffsetStartMs": 3840, "OffsetEndMs": 4100}, {"Word": "between", "OffsetStartMs": 4300, "OffsetEndMs": 4650}, {"Word": "two", "OffsetStartMs": 4650, "OffsetEndMs": 4905}, {"Word": "different", "OffsetStartMs": 4905, "OffsetEndMs": 5160}, {"Word": "demographics", "OffsetStartMs": 5160, "OffsetEndMs": 5900}, {"Word": "in", "OffsetStartMs": 6010, "OffsetEndMs": 6285}, {"Word": "this", "OffsetStartMs": 6285, "OffsetEndMs": 6560}, {"Word": "plot", "OffsetStartMs": 7420, "OffsetEndMs": 7820}], "SpeechSpeed": 13.2}, {"FinalSentence": "This accuracy gap can get up to 34%. Keep in mind that this facial detection is a binary classification task. Everything is either a face or it's not a face. This means that a randomly initialized model would be expected to have an accuracy of 50% because it's going to randomly assign whether or not something is a face or not.", "SliceSentence": "This accuracy gap can get up to 34 % . Keepin mind that this facial detection is a binary classification task . Everythingis either a face or it's not a face . Thismeans that a randomly initialized model would be expected to have an accuracy of 50 %because it's going to randomly assign whether or not something is a face or not", "StartMs": 734520, "EndMs": 754860, "WordsNum": 62, "Words": [{"Word": "This", "OffsetStartMs": 0, "OffsetEndMs": 380}, {"Word": "accuracy", "OffsetStartMs": 430, "OffsetEndMs": 885}, {"Word": "gap", "OffsetStartMs": 885, "OffsetEndMs": 1080}, {"Word": "can", "OffsetStartMs": 1080, "OffsetEndMs": 1320}, {"Word": "get", "OffsetStartMs": 1320, "OffsetEndMs": 1530}, {"Word": "up", "OffsetStartMs": 1530, "OffsetEndMs": 1770}, {"Word": "to", "OffsetStartMs": 1770, "OffsetEndMs": 2010}, {"Word": "34", "OffsetStartMs": 2010, "OffsetEndMs": 2625}, {"Word": "%", "OffsetStartMs": 2625, "OffsetEndMs": 2930}, {"Word": ".", "OffsetStartMs": 3460, "OffsetEndMs": 3735}, {"Word": "Keepin", "OffsetStartMs": 3735, "OffsetEndMs": 3885}, {"Word": "mind", "OffsetStartMs": 3885, "OffsetEndMs": 4160}, {"Word": "that", "OffsetStartMs": 4180, "OffsetEndMs": 4580}, {"Word": "this", "OffsetStartMs": 4600, "OffsetEndMs": 5000}, {"Word": "facial", "OffsetStartMs": 5170, "OffsetEndMs": 5625}, {"Word": "detection", "OffsetStartMs": 5625, "OffsetEndMs": 5985}, {"Word": "is", "OffsetStartMs": 5985, "OffsetEndMs": 6120}, {"Word": "a", "OffsetStartMs": 6120, "OffsetEndMs": 6270}, {"Word": "binary", "OffsetStartMs": 6270, "OffsetEndMs": 6675}, {"Word": "classification", "OffsetStartMs": 6675, "OffsetEndMs": 7250}, {"Word": "task", "OffsetStartMs": 7300, "OffsetEndMs": 7700}, {"Word": ".", "OffsetStartMs": 8200, "OffsetEndMs": 8445}, {"Word": "Everythingis", "OffsetStartMs": 8445, "OffsetEndMs": 8565}, {"Word": "either", "OffsetStartMs": 8565, "OffsetEndMs": 8730}, {"Word": "a", "OffsetStartMs": 8730, "OffsetEndMs": 8880}, {"Word": "face", "OffsetStartMs": 8880, "OffsetEndMs": 9140}, {"Word": "or", "OffsetStartMs": 9160, "OffsetEndMs": 9420}, {"Word": "it's", "OffsetStartMs": 9420, "OffsetEndMs": 9600}, {"Word": "not", "OffsetStartMs": 9600, "OffsetEndMs": 9735}, {"Word": "a", "OffsetStartMs": 9735, "OffsetEndMs": 9870}, {"Word": "face", "OffsetStartMs": 9870, "OffsetEndMs": 10130}, {"Word": ".", "OffsetStartMs": 10720, "OffsetEndMs": 11025}, {"Word": "Thismeans", "OffsetStartMs": 11025, "OffsetEndMs": 11265}, {"Word": "that", "OffsetStartMs": 11265, "OffsetEndMs": 11490}, {"Word": "a", "OffsetStartMs": 11490, "OffsetEndMs": 11700}, {"Word": "randomly", "OffsetStartMs": 11700, "OffsetEndMs": 12270}, {"Word": "initialized", "OffsetStartMs": 12270, "OffsetEndMs": 12735}, {"Word": "model", "OffsetStartMs": 12735, "OffsetEndMs": 13070}, {"Word": "would", "OffsetStartMs": 13270, "OffsetEndMs": 13530}, {"Word": "be", "OffsetStartMs": 13530, "OffsetEndMs": 13740}, {"Word": "expected", "OffsetStartMs": 13740, "OffsetEndMs": 14010}, {"Word": "to", "OffsetStartMs": 14010, "OffsetEndMs": 14190}, {"Word": "have", "OffsetStartMs": 14190, "OffsetEndMs": 14415}, {"Word": "an", "OffsetStartMs": 14415, "OffsetEndMs": 14760}, {"Word": "accuracy", "OffsetStartMs": 14760, "OffsetEndMs": 15210}, {"Word": "of", "OffsetStartMs": 15210, "OffsetEndMs": 15390}, {"Word": "50", "OffsetStartMs": 15390, "OffsetEndMs": 16010}, {"Word": "%because", "OffsetStartMs": 16750, "OffsetEndMs": 17010}, {"Word": "it's", "OffsetStartMs": 17010, "OffsetEndMs": 17175}, {"Word": "going", "OffsetStartMs": 17175, "OffsetEndMs": 17295}, {"Word": "to", "OffsetStartMs": 17295, "OffsetEndMs": 17415}, {"Word": "randomly", "OffsetStartMs": 17415, "OffsetEndMs": 17880}, {"Word": "assign", "OffsetStartMs": 17880, "OffsetEndMs": 18120}, {"Word": "whether", "OffsetStartMs": 18120, "OffsetEndMs": 18345}, {"Word": "or", "OffsetStartMs": 18345, "OffsetEndMs": 18495}, {"Word": "not", "OffsetStartMs": 18495, "OffsetEndMs": 18645}, {"Word": "something", "OffsetStartMs": 18645, "OffsetEndMs": 18870}, {"Word": "is", "OffsetStartMs": 18870, "OffsetEndMs": 19020}, {"Word": "a", "OffsetStartMs": 19020, "OffsetEndMs": 19110}, {"Word": "face", "OffsetStartMs": 19110, "OffsetEndMs": 19305}, {"Word": "or", "OffsetStartMs": 19305, "OffsetEndMs": 19500}, {"Word": "not", "OffsetStartMs": 19500, "OffsetEndMs": 19760}], "SpeechSpeed": 17.2}, {"FinalSentence": "Some of these facial detection classifiers do only barely better than random on these underrepresented data on these underrepresented samples in this population.", "SliceSentence": "Some of these facial detection classifiers do only barely better than random on these underrepresented data on these underrepresented samples in this population", "StartMs": 755020, "EndMs": 766520, "WordsNum": 23, "Words": [{"Word": "Some", "OffsetStartMs": 130, "OffsetEndMs": 405}, {"Word": "of", "OffsetStartMs": 405, "OffsetEndMs": 540}, {"Word": "these", "OffsetStartMs": 540, "OffsetEndMs": 690}, {"Word": "facial", "OffsetStartMs": 690, "OffsetEndMs": 1050}, {"Word": "detection", "OffsetStartMs": 1050, "OffsetEndMs": 1395}, {"Word": "classifiers", "OffsetStartMs": 1395, "OffsetEndMs": 1940}, {"Word": "do", "OffsetStartMs": 2140, "OffsetEndMs": 2460}, {"Word": "only", "OffsetStartMs": 2460, "OffsetEndMs": 2780}, {"Word": "barely", "OffsetStartMs": 2860, "OffsetEndMs": 3260}, {"Word": "better", "OffsetStartMs": 3310, "OffsetEndMs": 3660}, {"Word": "than", "OffsetStartMs": 3660, "OffsetEndMs": 3915}, {"Word": "random", "OffsetStartMs": 3915, "OffsetEndMs": 4220}, {"Word": "on", "OffsetStartMs": 4270, "OffsetEndMs": 4670}, {"Word": "these", "OffsetStartMs": 4810, "OffsetEndMs": 5085}, {"Word": "underrepresented", "OffsetStartMs": 5085, "OffsetEndMs": 5870}, {"Word": "data", "OffsetStartMs": 6400, "OffsetEndMs": 6800}, {"Word": "on", "OffsetStartMs": 6850, "OffsetEndMs": 7110}, {"Word": "these", "OffsetStartMs": 7110, "OffsetEndMs": 7230}, {"Word": "underrepresented", "OffsetStartMs": 7230, "OffsetEndMs": 7890}, {"Word": "samples", "OffsetStartMs": 7890, "OffsetEndMs": 8415}, {"Word": "in", "OffsetStartMs": 8415, "OffsetEndMs": 8780}, {"Word": "this", "OffsetStartMs": 9370, "OffsetEndMs": 9660}, {"Word": "population", "OffsetStartMs": 9660, "OffsetEndMs": 9950}], "SpeechSpeed": 13.9}, {"FinalSentence": "So how did this happen? Why is there such a blatant gap in accuracy between these different demographic groups? And how did these models ever get deployed in the first place? What types of biases were present in these models?", "SliceSentence": "So how did this happen Why is there such a blatant gap in accuracy between these different demographic groups And how did these models ever get deployed in the first place What types of biases were present in these models", "StartMs": 766660, "EndMs": 780700, "WordsNum": 40, "Words": [{"Word": "So", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "how", "OffsetStartMs": 730, "OffsetEndMs": 1005}, {"Word": "did", "OffsetStartMs": 1005, "OffsetEndMs": 1140}, {"Word": "this", "OffsetStartMs": 1140, "OffsetEndMs": 1290}, {"Word": "happen", "OffsetStartMs": 1290, "OffsetEndMs": 1580}, {"Word": "Why", "OffsetStartMs": 1960, "OffsetEndMs": 2280}, {"Word": "is", "OffsetStartMs": 2280, "OffsetEndMs": 2475}, {"Word": "there", "OffsetStartMs": 2475, "OffsetEndMs": 2640}, {"Word": "such", "OffsetStartMs": 2640, "OffsetEndMs": 2790}, {"Word": "a", "OffsetStartMs": 2790, "OffsetEndMs": 2940}, {"Word": "blatant", "OffsetStartMs": 2940, "OffsetEndMs": 3300}, {"Word": "gap", "OffsetStartMs": 3300, "OffsetEndMs": 3480}, {"Word": "in", "OffsetStartMs": 3480, "OffsetEndMs": 3750}, {"Word": "accuracy", "OffsetStartMs": 3750, "OffsetEndMs": 4370}, {"Word": "between", "OffsetStartMs": 4600, "OffsetEndMs": 4875}, {"Word": "these", "OffsetStartMs": 4875, "OffsetEndMs": 5025}, {"Word": "different", "OffsetStartMs": 5025, "OffsetEndMs": 5235}, {"Word": "demographic", "OffsetStartMs": 5235, "OffsetEndMs": 5880}, {"Word": "groups", "OffsetStartMs": 5880, "OffsetEndMs": 6170}, {"Word": "And", "OffsetStartMs": 6550, "OffsetEndMs": 6855}, {"Word": "how", "OffsetStartMs": 6855, "OffsetEndMs": 7035}, {"Word": "did", "OffsetStartMs": 7035, "OffsetEndMs": 7155}, {"Word": "these", "OffsetStartMs": 7155, "OffsetEndMs": 7400}, {"Word": "models", "OffsetStartMs": 7690, "OffsetEndMs": 8085}, {"Word": "ever", "OffsetStartMs": 8085, "OffsetEndMs": 8415}, {"Word": "get", "OffsetStartMs": 8415, "OffsetEndMs": 8700}, {"Word": "deployed", "OffsetStartMs": 8700, "OffsetEndMs": 9060}, {"Word": "in", "OffsetStartMs": 9060, "OffsetEndMs": 9195}, {"Word": "the", "OffsetStartMs": 9195, "OffsetEndMs": 9285}, {"Word": "first", "OffsetStartMs": 9285, "OffsetEndMs": 9480}, {"Word": "place", "OffsetStartMs": 9480, "OffsetEndMs": 9830}, {"Word": "What", "OffsetStartMs": 10300, "OffsetEndMs": 10635}, {"Word": "types", "OffsetStartMs": 10635, "OffsetEndMs": 10890}, {"Word": "of", "OffsetStartMs": 10890, "OffsetEndMs": 11070}, {"Word": "biases", "OffsetStartMs": 11070, "OffsetEndMs": 11610}, {"Word": "were", "OffsetStartMs": 11610, "OffsetEndMs": 11880}, {"Word": "present", "OffsetStartMs": 11880, "OffsetEndMs": 12170}, {"Word": "in", "OffsetStartMs": 12250, "OffsetEndMs": 12510}, {"Word": "these", "OffsetStartMs": 12510, "OffsetEndMs": 12660}, {"Word": "models", "OffsetStartMs": 12660, "OffsetEndMs": 12950}], "SpeechSpeed": 15.7}, {"FinalSentence": "So a lot of facial detection systems exhibit very clear selection bias. This model was likely trained mostly on lighter skin faces and therefore EM learned those much more effectively than it learned to classify darker skin faces. But that's not the only bias that was present. The second bias that's often EM very present in facial detection systems is evaluation bias because originally this data set that you see on the screen is not the data set that these models were evaluated on. They were evaluated on one big bulk dataas set without any classification into subgroups at all. And therefore you can imagine if the dataset was also comprised mostly of lighter skinned faces, these accuracy metrics would be incredibly inflated and therefore would cause unnecessary confidence and we could deploy them into the real world.", "SliceSentence": "So a lot of facial detection systems exhibit very clear selection bias . Thismodel was likely trained mostly on lighter skin faces and therefore EM learned those much more effectively than it learned to classify darker skin faces . Butthat's not the only bias that was present . Thesecond bias that's often EM very present in facial detection systems is evaluation bias because originally this data set that you see on the screen is not the data set that these models were evaluated on . Theywere evaluated on one big bulk dataas set without any classification into subgroups at all . Andtherefore you can imagine if the dataset was also comprised mostly of lighter skinned faces these accuracy metrics would be incredibly inflated and therefore would cause unnecessary confidence and we could deploy them into the real world", "StartMs": 781180, "EndMs": 829960, "WordsNum": 138, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "a", "OffsetStartMs": 700, "OffsetEndMs": 960}, {"Word": "lot", "OffsetStartMs": 960, "OffsetEndMs": 1080}, {"Word": "of", "OffsetStartMs": 1080, "OffsetEndMs": 1200}, {"Word": "facial", "OffsetStartMs": 1200, "OffsetEndMs": 1575}, {"Word": "detection", "OffsetStartMs": 1575, "OffsetEndMs": 1935}, {"Word": "systems", "OffsetStartMs": 1935, "OffsetEndMs": 2210}, {"Word": "exhibit", "OffsetStartMs": 2500, "OffsetEndMs": 2940}, {"Word": "very", "OffsetStartMs": 2940, "OffsetEndMs": 3255}, {"Word": "clear", "OffsetStartMs": 3255, "OffsetEndMs": 3645}, {"Word": "selection", "OffsetStartMs": 3645, "OffsetEndMs": 4020}, {"Word": "bias", "OffsetStartMs": 4020, "OffsetEndMs": 4580}, {"Word": ".", "OffsetStartMs": 4870, "OffsetEndMs": 5175}, {"Word": "Thismodel", "OffsetStartMs": 5175, "OffsetEndMs": 5475}, {"Word": "was", "OffsetStartMs": 5475, "OffsetEndMs": 5760}, {"Word": "likely", "OffsetStartMs": 5760, "OffsetEndMs": 6045}, {"Word": "trained", "OffsetStartMs": 6045, "OffsetEndMs": 6420}, {"Word": "mostly", "OffsetStartMs": 6420, "OffsetEndMs": 6800}, {"Word": "on", "OffsetStartMs": 6880, "OffsetEndMs": 7170}, {"Word": "lighter", "OffsetStartMs": 7170, "OffsetEndMs": 7515}, {"Word": "skin", "OffsetStartMs": 7515, "OffsetEndMs": 7725}, {"Word": "faces", "OffsetStartMs": 7725, "OffsetEndMs": 8030}, {"Word": "and", "OffsetStartMs": 8380, "OffsetEndMs": 8780}, {"Word": "therefore", "OffsetStartMs": 8830, "OffsetEndMs": 9230}, {"Word": "EM", "OffsetStartMs": 9370, "OffsetEndMs": 9720}, {"Word": "learned", "OffsetStartMs": 9720, "OffsetEndMs": 10020}, {"Word": "those", "OffsetStartMs": 10020, "OffsetEndMs": 10320}, {"Word": "much", "OffsetStartMs": 10320, "OffsetEndMs": 10575}, {"Word": "more", "OffsetStartMs": 10575, "OffsetEndMs": 10770}, {"Word": "effectively", "OffsetStartMs": 10770, "OffsetEndMs": 11060}, {"Word": "than", "OffsetStartMs": 11110, "OffsetEndMs": 11340}, {"Word": "it", "OffsetStartMs": 11340, "OffsetEndMs": 11445}, {"Word": "learned", "OffsetStartMs": 11445, "OffsetEndMs": 11610}, {"Word": "to", "OffsetStartMs": 11610, "OffsetEndMs": 11760}, {"Word": "classify", "OffsetStartMs": 11760, "OffsetEndMs": 12225}, {"Word": "darker", "OffsetStartMs": 12225, "OffsetEndMs": 12570}, {"Word": "skin", "OffsetStartMs": 12570, "OffsetEndMs": 12750}, {"Word": "faces", "OffsetStartMs": 12750, "OffsetEndMs": 13040}, {"Word": ".", "OffsetStartMs": 13840, "OffsetEndMs": 14100}, {"Word": "Butthat's", "OffsetStartMs": 14100, "OffsetEndMs": 14325}, {"Word": "not", "OffsetStartMs": 14325, "OffsetEndMs": 14475}, {"Word": "the", "OffsetStartMs": 14475, "OffsetEndMs": 14625}, {"Word": "only", "OffsetStartMs": 14625, "OffsetEndMs": 14805}, {"Word": "bias", "OffsetStartMs": 14805, "OffsetEndMs": 15165}, {"Word": "that", "OffsetStartMs": 15165, "OffsetEndMs": 15300}, {"Word": "was", "OffsetStartMs": 15300, "OffsetEndMs": 15450}, {"Word": "present", "OffsetStartMs": 15450, "OffsetEndMs": 15740}, {"Word": ".", "OffsetStartMs": 16420, "OffsetEndMs": 16680}, {"Word": "Thesecond", "OffsetStartMs": 16680, "OffsetEndMs": 16890}, {"Word": "bias", "OffsetStartMs": 16890, "OffsetEndMs": 17280}, {"Word": "that's", "OffsetStartMs": 17280, "OffsetEndMs": 17505}, {"Word": "often", "OffsetStartMs": 17505, "OffsetEndMs": 17750}, {"Word": "EM", "OffsetStartMs": 17830, "OffsetEndMs": 18230}, {"Word": "very", "OffsetStartMs": 19360, "OffsetEndMs": 19725}, {"Word": "present", "OffsetStartMs": 19725, "OffsetEndMs": 20040}, {"Word": "in", "OffsetStartMs": 20040, "OffsetEndMs": 20340}, {"Word": "facial", "OffsetStartMs": 20340, "OffsetEndMs": 20745}, {"Word": "detection", "OffsetStartMs": 20745, "OffsetEndMs": 21075}, {"Word": "systems", "OffsetStartMs": 21075, "OffsetEndMs": 21350}, {"Word": "is", "OffsetStartMs": 21430, "OffsetEndMs": 21765}, {"Word": "evaluation", "OffsetStartMs": 21765, "OffsetEndMs": 22350}, {"Word": "bias", "OffsetStartMs": 22350, "OffsetEndMs": 22910}, {"Word": "because", "OffsetStartMs": 23350, "OffsetEndMs": 23750}, {"Word": "originally", "OffsetStartMs": 24160, "OffsetEndMs": 24560}, {"Word": "this", "OffsetStartMs": 25180, "OffsetEndMs": 25515}, {"Word": "data", "OffsetStartMs": 25515, "OffsetEndMs": 25785}, {"Word": "set", "OffsetStartMs": 25785, "OffsetEndMs": 25980}, {"Word": "that", "OffsetStartMs": 25980, "OffsetEndMs": 26100}, {"Word": "you", "OffsetStartMs": 26100, "OffsetEndMs": 26235}, {"Word": "see", "OffsetStartMs": 26235, "OffsetEndMs": 26355}, {"Word": "on", "OffsetStartMs": 26355, "OffsetEndMs": 26445}, {"Word": "the", "OffsetStartMs": 26445, "OffsetEndMs": 26595}, {"Word": "screen", "OffsetStartMs": 26595, "OffsetEndMs": 26835}, {"Word": "is", "OffsetStartMs": 26835, "OffsetEndMs": 27060}, {"Word": "not", "OffsetStartMs": 27060, "OffsetEndMs": 27255}, {"Word": "the", "OffsetStartMs": 27255, "OffsetEndMs": 27420}, {"Word": "data", "OffsetStartMs": 27420, "OffsetEndMs": 27600}, {"Word": "set", "OffsetStartMs": 27600, "OffsetEndMs": 27795}, {"Word": "that", "OffsetStartMs": 27795, "OffsetEndMs": 27930}, {"Word": "these", "OffsetStartMs": 27930, "OffsetEndMs": 28080}, {"Word": "models", "OffsetStartMs": 28080, "OffsetEndMs": 28335}, {"Word": "were", "OffsetStartMs": 28335, "OffsetEndMs": 28590}, {"Word": "evaluated", "OffsetStartMs": 28590, "OffsetEndMs": 29025}, {"Word": "on", "OffsetStartMs": 29025, "OffsetEndMs": 29330}, {"Word": ".", "OffsetStartMs": 29620, "OffsetEndMs": 29880}, {"Word": "Theywere", "OffsetStartMs": 29880, "OffsetEndMs": 30030}, {"Word": "evaluated", "OffsetStartMs": 30030, "OffsetEndMs": 30540}, {"Word": "on", "OffsetStartMs": 30540, "OffsetEndMs": 30840}, {"Word": "one", "OffsetStartMs": 30840, "OffsetEndMs": 31190}, {"Word": "big", "OffsetStartMs": 31300, "OffsetEndMs": 31695}, {"Word": "bulk", "OffsetStartMs": 31695, "OffsetEndMs": 32055}, {"Word": "dataas", "OffsetStartMs": 32055, "OffsetEndMs": 32415}, {"Word": "set", "OffsetStartMs": 32415, "OffsetEndMs": 32690}, {"Word": "without", "OffsetStartMs": 32800, "OffsetEndMs": 33165}, {"Word": "any", "OffsetStartMs": 33165, "OffsetEndMs": 33530}, {"Word": "classification", "OffsetStartMs": 33790, "OffsetEndMs": 34380}, {"Word": "into", "OffsetStartMs": 34380, "OffsetEndMs": 34680}, {"Word": "subgroups", "OffsetStartMs": 34680, "OffsetEndMs": 35220}, {"Word": "at", "OffsetStartMs": 35220, "OffsetEndMs": 35355}, {"Word": "all", "OffsetStartMs": 35355, "OffsetEndMs": 35630}, {"Word": ".", "OffsetStartMs": 35950, "OffsetEndMs": 36350}, {"Word": "Andtherefore", "OffsetStartMs": 36370, "OffsetEndMs": 36720}, {"Word": "you", "OffsetStartMs": 36720, "OffsetEndMs": 36930}, {"Word": "can", "OffsetStartMs": 36930, "OffsetEndMs": 37080}, {"Word": "imagine", "OffsetStartMs": 37080, "OffsetEndMs": 37370}, {"Word": "if", "OffsetStartMs": 37390, "OffsetEndMs": 37680}, {"Word": "the", "OffsetStartMs": 37680, "OffsetEndMs": 37815}, {"Word": "dataset", "OffsetStartMs": 37815, "OffsetEndMs": 38160}, {"Word": "was", "OffsetStartMs": 38160, "OffsetEndMs": 38400}, {"Word": "also", "OffsetStartMs": 38400, "OffsetEndMs": 38640}, {"Word": "comprised", "OffsetStartMs": 38640, "OffsetEndMs": 38940}, {"Word": "mostly", "OffsetStartMs": 38940, "OffsetEndMs": 39260}, {"Word": "of", "OffsetStartMs": 39280, "OffsetEndMs": 39555}, {"Word": "lighter", "OffsetStartMs": 39555, "OffsetEndMs": 39870}, {"Word": "skinned", "OffsetStartMs": 39870, "OffsetEndMs": 40080}, {"Word": "faces", "OffsetStartMs": 40080, "OffsetEndMs": 40340}, {"Word": "these", "OffsetStartMs": 40720, "OffsetEndMs": 41120}, {"Word": "accuracy", "OffsetStartMs": 41140, "OffsetEndMs": 41610}, {"Word": "metrics", "OffsetStartMs": 41610, "OffsetEndMs": 41850}, {"Word": "would", "OffsetStartMs": 41850, "OffsetEndMs": 42090}, {"Word": "be", "OffsetStartMs": 42090, "OffsetEndMs": 42285}, {"Word": "incredibly", "OffsetStartMs": 42285, "OffsetEndMs": 42585}, {"Word": "inflated", "OffsetStartMs": 42585, "OffsetEndMs": 43160}, {"Word": "and", "OffsetStartMs": 43210, "OffsetEndMs": 43610}, {"Word": "therefore", "OffsetStartMs": 43630, "OffsetEndMs": 43920}, {"Word": "would", "OffsetStartMs": 43920, "OffsetEndMs": 44115}, {"Word": "cause", "OffsetStartMs": 44115, "OffsetEndMs": 44370}, {"Word": "unnecessary", "OffsetStartMs": 44370, "OffsetEndMs": 45105}, {"Word": "confidence", "OffsetStartMs": 45105, "OffsetEndMs": 45440}, {"Word": "and", "OffsetStartMs": 45790, "OffsetEndMs": 46080}, {"Word": "we", "OffsetStartMs": 46080, "OffsetEndMs": 46215}, {"Word": "could", "OffsetStartMs": 46215, "OffsetEndMs": 46425}, {"Word": "deploy", "OffsetStartMs": 46425, "OffsetEndMs": 46695}, {"Word": "them", "OffsetStartMs": 46695, "OffsetEndMs": 46905}, {"Word": "into", "OffsetStartMs": 46905, "OffsetEndMs": 47145}, {"Word": "the", "OffsetStartMs": 47145, "OffsetEndMs": 47340}, {"Word": "real", "OffsetStartMs": 47340, "OffsetEndMs": 47520}, {"Word": "world", "OffsetStartMs": 47520, "OffsetEndMs": 47840}], "SpeechSpeed": 16.8}, {"FinalSentence": "In fact, the biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender.", "SliceSentence": "In fact the biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender", "StartMs": 830560, "EndMs": 844200, "WordsNum": 35, "Words": [{"Word": "In", "OffsetStartMs": 100, "OffsetEndMs": 405}, {"Word": "fact", "OffsetStartMs": 405, "OffsetEndMs": 710}, {"Word": "the", "OffsetStartMs": 730, "OffsetEndMs": 990}, {"Word": "biases", "OffsetStartMs": 990, "OffsetEndMs": 1470}, {"Word": "in", "OffsetStartMs": 1470, "OffsetEndMs": 1620}, {"Word": "these", "OffsetStartMs": 1620, "OffsetEndMs": 1785}, {"Word": "models", "OffsetStartMs": 1785, "OffsetEndMs": 2070}, {"Word": "were", "OffsetStartMs": 2070, "OffsetEndMs": 2325}, {"Word": "only", "OffsetStartMs": 2325, "OffsetEndMs": 2565}, {"Word": "uncovered", "OffsetStartMs": 2565, "OffsetEndMs": 3285}, {"Word": "once", "OffsetStartMs": 3285, "OffsetEndMs": 3555}, {"Word": "an", "OffsetStartMs": 3555, "OffsetEndMs": 3890}, {"Word": "independent", "OffsetStartMs": 3970, "OffsetEndMs": 4350}, {"Word": "study", "OffsetStartMs": 4350, "OffsetEndMs": 4730}, {"Word": "actually", "OffsetStartMs": 4900, "OffsetEndMs": 5300}, {"Word": "constructed", "OffsetStartMs": 5380, "OffsetEndMs": 5835}, {"Word": "a", "OffsetStartMs": 5835, "OffsetEndMs": 5985}, {"Word": "data", "OffsetStartMs": 5985, "OffsetEndMs": 6195}, {"Word": "set", "OffsetStartMs": 6195, "OffsetEndMs": 6530}, {"Word": "that", "OffsetStartMs": 6610, "OffsetEndMs": 6870}, {"Word": "is", "OffsetStartMs": 6870, "OffsetEndMs": 7050}, {"Word": "specifically", "OffsetStartMs": 7050, "OffsetEndMs": 7370}, {"Word": "designed", "OffsetStartMs": 7660, "OffsetEndMs": 8060}, {"Word": "to", "OffsetStartMs": 8380, "OffsetEndMs": 8780}, {"Word": "uncover", "OffsetStartMs": 8950, "OffsetEndMs": 9525}, {"Word": "these", "OffsetStartMs": 9525, "OffsetEndMs": 9660}, {"Word": "sorts", "OffsetStartMs": 9660, "OffsetEndMs": 9840}, {"Word": "of", "OffsetStartMs": 9840, "OffsetEndMs": 10005}, {"Word": "biases", "OffsetStartMs": 10005, "OffsetEndMs": 10500}, {"Word": "by", "OffsetStartMs": 10500, "OffsetEndMs": 10695}, {"Word": "balancing", "OffsetStartMs": 10695, "OffsetEndMs": 11295}, {"Word": "across", "OffsetStartMs": 11295, "OffsetEndMs": 11610}, {"Word": "race", "OffsetStartMs": 11610, "OffsetEndMs": 11925}, {"Word": "and", "OffsetStartMs": 11925, "OffsetEndMs": 12195}, {"Word": "gender", "OffsetStartMs": 12195, "OffsetEndMs": 12500}], "SpeechSpeed": 15.5}, {"FinalSentence": "However, there are other ways that data sets can be biased that we haven't yet talked about.", "SliceSentence": "However there are other ways that data sets can be biased that we haven 't yet talked about", "StartMs": 844620, "EndMs": 850080, "WordsNum": 18, "Words": [{"Word": "However", "OffsetStartMs": 190, "OffsetEndMs": 570}, {"Word": "there", "OffsetStartMs": 570, "OffsetEndMs": 810}, {"Word": "are", "OffsetStartMs": 810, "OffsetEndMs": 930}, {"Word": "other", "OffsetStartMs": 930, "OffsetEndMs": 1140}, {"Word": "ways", "OffsetStartMs": 1140, "OffsetEndMs": 1410}, {"Word": "that", "OffsetStartMs": 1410, "OffsetEndMs": 1730}, {"Word": "data", "OffsetStartMs": 1810, "OffsetEndMs": 2130}, {"Word": "sets", "OffsetStartMs": 2130, "OffsetEndMs": 2355}, {"Word": "can", "OffsetStartMs": 2355, "OffsetEndMs": 2520}, {"Word": "be", "OffsetStartMs": 2520, "OffsetEndMs": 2655}, {"Word": "biased", "OffsetStartMs": 2655, "OffsetEndMs": 3000}, {"Word": "that", "OffsetStartMs": 3000, "OffsetEndMs": 3165}, {"Word": "we", "OffsetStartMs": 3165, "OffsetEndMs": 3285}, {"Word": "haven", "OffsetStartMs": 3285, "OffsetEndMs": 3420}, {"Word": "'t", "OffsetStartMs": 3420, "OffsetEndMs": 3540}, {"Word": "yet", "OffsetStartMs": 3540, "OffsetEndMs": 3720}, {"Word": "talked", "OffsetStartMs": 3720, "OffsetEndMs": 3990}, {"Word": "about", "OffsetStartMs": 3990, "OffsetEndMs": 4340}], "SpeechSpeed": 16.5}, {"FinalSentence": "So, so far we've assumed a pretty key assumption in our data set, which is that the number of faces in our data is the exact same as the number of non faces in our data. But you can imagine, especially if you're looking at things like security feeds, this might not always be the case. You might be faced with many more negative samples than positive samples in your data set.", "SliceSentence": "So so far we've assumed a pretty key assumption in our data set which is that the number of faces in our data is the exact same as the number of non faces in our data . Butyou can imagine especially if you're looking at things like security feeds this might not always be the case . Youmight be faced with many more negative samples than positive samples in your data set", "StartMs": 850900, "EndMs": 870860, "WordsNum": 72, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "so", "OffsetStartMs": 580, "OffsetEndMs": 885}, {"Word": "far", "OffsetStartMs": 885, "OffsetEndMs": 1155}, {"Word": "we've", "OffsetStartMs": 1155, "OffsetEndMs": 1545}, {"Word": "assumed", "OffsetStartMs": 1545, "OffsetEndMs": 2090}, {"Word": "a", "OffsetStartMs": 2470, "OffsetEndMs": 2850}, {"Word": "pretty", "OffsetStartMs": 2850, "OffsetEndMs": 3165}, {"Word": "key", "OffsetStartMs": 3165, "OffsetEndMs": 3390}, {"Word": "assumption", "OffsetStartMs": 3390, "OffsetEndMs": 3795}, {"Word": "in", "OffsetStartMs": 3795, "OffsetEndMs": 3900}, {"Word": "our", "OffsetStartMs": 3900, "OffsetEndMs": 4020}, {"Word": "data", "OffsetStartMs": 4020, "OffsetEndMs": 4230}, {"Word": "set", "OffsetStartMs": 4230, "OffsetEndMs": 4500}, {"Word": "which", "OffsetStartMs": 4500, "OffsetEndMs": 4710}, {"Word": "is", "OffsetStartMs": 4710, "OffsetEndMs": 4860}, {"Word": "that", "OffsetStartMs": 4860, "OffsetEndMs": 5070}, {"Word": "the", "OffsetStartMs": 5070, "OffsetEndMs": 5235}, {"Word": "number", "OffsetStartMs": 5235, "OffsetEndMs": 5400}, {"Word": "of", "OffsetStartMs": 5400, "OffsetEndMs": 5595}, {"Word": "faces", "OffsetStartMs": 5595, "OffsetEndMs": 5820}, {"Word": "in", "OffsetStartMs": 5820, "OffsetEndMs": 6015}, {"Word": "our", "OffsetStartMs": 6015, "OffsetEndMs": 6135}, {"Word": "data", "OffsetStartMs": 6135, "OffsetEndMs": 6410}, {"Word": "is", "OffsetStartMs": 6670, "OffsetEndMs": 6945}, {"Word": "the", "OffsetStartMs": 6945, "OffsetEndMs": 7110}, {"Word": "exact", "OffsetStartMs": 7110, "OffsetEndMs": 7335}, {"Word": "same", "OffsetStartMs": 7335, "OffsetEndMs": 7620}, {"Word": "as", "OffsetStartMs": 7620, "OffsetEndMs": 7830}, {"Word": "the", "OffsetStartMs": 7830, "OffsetEndMs": 7935}, {"Word": "number", "OffsetStartMs": 7935, "OffsetEndMs": 8145}, {"Word": "of", "OffsetStartMs": 8145, "OffsetEndMs": 8400}, {"Word": "non", "OffsetStartMs": 8400, "OffsetEndMs": 8595}, {"Word": "faces", "OffsetStartMs": 8595, "OffsetEndMs": 8835}, {"Word": "in", "OffsetStartMs": 8835, "OffsetEndMs": 9015}, {"Word": "our", "OffsetStartMs": 9015, "OffsetEndMs": 9135}, {"Word": "data", "OffsetStartMs": 9135, "OffsetEndMs": 9410}, {"Word": ".", "OffsetStartMs": 9880, "OffsetEndMs": 10140}, {"Word": "Butyou", "OffsetStartMs": 10140, "OffsetEndMs": 10260}, {"Word": "can", "OffsetStartMs": 10260, "OffsetEndMs": 10410}, {"Word": "imagine", "OffsetStartMs": 10410, "OffsetEndMs": 10700}, {"Word": "especially", "OffsetStartMs": 10780, "OffsetEndMs": 11100}, {"Word": "if", "OffsetStartMs": 11100, "OffsetEndMs": 11265}, {"Word": "you're", "OffsetStartMs": 11265, "OffsetEndMs": 11460}, {"Word": "looking", "OffsetStartMs": 11460, "OffsetEndMs": 11580}, {"Word": "at", "OffsetStartMs": 11580, "OffsetEndMs": 11745}, {"Word": "things", "OffsetStartMs": 11745, "OffsetEndMs": 11910}, {"Word": "like", "OffsetStartMs": 11910, "OffsetEndMs": 12165}, {"Word": "security", "OffsetStartMs": 12165, "OffsetEndMs": 12495}, {"Word": "feeds", "OffsetStartMs": 12495, "OffsetEndMs": 12990}, {"Word": "this", "OffsetStartMs": 12990, "OffsetEndMs": 13260}, {"Word": "might", "OffsetStartMs": 13260, "OffsetEndMs": 13425}, {"Word": "not", "OffsetStartMs": 13425, "OffsetEndMs": 13620}, {"Word": "always", "OffsetStartMs": 13620, "OffsetEndMs": 13830}, {"Word": "be", "OffsetStartMs": 13830, "OffsetEndMs": 13965}, {"Word": "the", "OffsetStartMs": 13965, "OffsetEndMs": 14070}, {"Word": "case", "OffsetStartMs": 14070, "OffsetEndMs": 14330}, {"Word": ".", "OffsetStartMs": 14650, "OffsetEndMs": 14925}, {"Word": "Youmight", "OffsetStartMs": 14925, "OffsetEndMs": 15105}, {"Word": "be", "OffsetStartMs": 15105, "OffsetEndMs": 15270}, {"Word": "faced", "OffsetStartMs": 15270, "OffsetEndMs": 15465}, {"Word": "with", "OffsetStartMs": 15465, "OffsetEndMs": 15705}, {"Word": "many", "OffsetStartMs": 15705, "OffsetEndMs": 15945}, {"Word": "more", "OffsetStartMs": 15945, "OffsetEndMs": 16215}, {"Word": "negative", "OffsetStartMs": 16215, "OffsetEndMs": 16545}, {"Word": "samples", "OffsetStartMs": 16545, "OffsetEndMs": 17070}, {"Word": "than", "OffsetStartMs": 17070, "OffsetEndMs": 17295}, {"Word": "positive", "OffsetStartMs": 17295, "OffsetEndMs": 17595}, {"Word": "samples", "OffsetStartMs": 17595, "OffsetEndMs": 18135}, {"Word": "in", "OffsetStartMs": 18135, "OffsetEndMs": 18315}, {"Word": "your", "OffsetStartMs": 18315, "OffsetEndMs": 18450}, {"Word": "data", "OffsetStartMs": 18450, "OffsetEndMs": 18660}, {"Word": "set", "OffsetStartMs": 18660, "OffsetEndMs": 18980}], "SpeechSpeed": 18.5}, {"FinalSentence": "In the most so, what's the problem here? In the most extreme case, we may assign the label non face to every item in the data set, because the model sees items that are labeled as faces so infrequently that it isn't able to learn an accurate class boundary between the two Sam between the two classes.", "SliceSentence": "In the most so what's the problem here In the most extreme case we may assign the label non face to every item in the data set because the model sees items that are labeled as faces so infrequently that it isn't able to learn an accurate class boundary between the two Sam between the two classes", "StartMs": 871400, "EndMs": 890680, "WordsNum": 57, "Words": [{"Word": "In", "OffsetStartMs": 70, "OffsetEndMs": 345}, {"Word": "the", "OffsetStartMs": 345, "OffsetEndMs": 465}, {"Word": "most", "OffsetStartMs": 465, "OffsetEndMs": 710}, {"Word": "so", "OffsetStartMs": 760, "OffsetEndMs": 1020}, {"Word": "what's", "OffsetStartMs": 1020, "OffsetEndMs": 1425}, {"Word": "the", "OffsetStartMs": 1425, "OffsetEndMs": 1545}, {"Word": "problem", "OffsetStartMs": 1545, "OffsetEndMs": 1815}, {"Word": "here", "OffsetStartMs": 1815, "OffsetEndMs": 2210}, {"Word": "In", "OffsetStartMs": 2860, "OffsetEndMs": 3135}, {"Word": "the", "OffsetStartMs": 3135, "OffsetEndMs": 3255}, {"Word": "most", "OffsetStartMs": 3255, "OffsetEndMs": 3500}, {"Word": "extreme", "OffsetStartMs": 3550, "OffsetEndMs": 3885}, {"Word": "case", "OffsetStartMs": 3885, "OffsetEndMs": 4220}, {"Word": "we", "OffsetStartMs": 4360, "OffsetEndMs": 4635}, {"Word": "may", "OffsetStartMs": 4635, "OffsetEndMs": 4845}, {"Word": "assign", "OffsetStartMs": 4845, "OffsetEndMs": 5100}, {"Word": "the", "OffsetStartMs": 5100, "OffsetEndMs": 5280}, {"Word": "label", "OffsetStartMs": 5280, "OffsetEndMs": 5540}, {"Word": "non", "OffsetStartMs": 5860, "OffsetEndMs": 6240}, {"Word": "face", "OffsetStartMs": 6240, "OffsetEndMs": 6620}, {"Word": "to", "OffsetStartMs": 6820, "OffsetEndMs": 7080}, {"Word": "every", "OffsetStartMs": 7080, "OffsetEndMs": 7260}, {"Word": "item", "OffsetStartMs": 7260, "OffsetEndMs": 7500}, {"Word": "in", "OffsetStartMs": 7500, "OffsetEndMs": 7665}, {"Word": "the", "OffsetStartMs": 7665, "OffsetEndMs": 7755}, {"Word": "data", "OffsetStartMs": 7755, "OffsetEndMs": 7935}, {"Word": "set", "OffsetStartMs": 7935, "OffsetEndMs": 8270}, {"Word": "because", "OffsetStartMs": 8740, "OffsetEndMs": 9015}, {"Word": "the", "OffsetStartMs": 9015, "OffsetEndMs": 9150}, {"Word": "model", "OffsetStartMs": 9150, "OffsetEndMs": 9405}, {"Word": "sees", "OffsetStartMs": 9405, "OffsetEndMs": 9720}, {"Word": "items", "OffsetStartMs": 9720, "OffsetEndMs": 10040}, {"Word": "that", "OffsetStartMs": 10060, "OffsetEndMs": 10320}, {"Word": "are", "OffsetStartMs": 10320, "OffsetEndMs": 10455}, {"Word": "labeled", "OffsetStartMs": 10455, "OffsetEndMs": 10755}, {"Word": "as", "OffsetStartMs": 10755, "OffsetEndMs": 10935}, {"Word": "faces", "OffsetStartMs": 10935, "OffsetEndMs": 11240}, {"Word": "so", "OffsetStartMs": 11380, "OffsetEndMs": 11715}, {"Word": "infrequently", "OffsetStartMs": 11715, "OffsetEndMs": 12555}, {"Word": "that", "OffsetStartMs": 12555, "OffsetEndMs": 12765}, {"Word": "it", "OffsetStartMs": 12765, "OffsetEndMs": 12870}, {"Word": "isn't", "OffsetStartMs": 12870, "OffsetEndMs": 13110}, {"Word": "able", "OffsetStartMs": 13110, "OffsetEndMs": 13320}, {"Word": "to", "OffsetStartMs": 13320, "OffsetEndMs": 13545}, {"Word": "learn", "OffsetStartMs": 13545, "OffsetEndMs": 13820}, {"Word": "an", "OffsetStartMs": 14530, "OffsetEndMs": 14910}, {"Word": "accurate", "OffsetStartMs": 14910, "OffsetEndMs": 15225}, {"Word": "class", "OffsetStartMs": 15225, "OffsetEndMs": 15560}, {"Word": "boundary", "OffsetStartMs": 15730, "OffsetEndMs": 16320}, {"Word": "between", "OffsetStartMs": 16320, "OffsetEndMs": 16545}, {"Word": "the", "OffsetStartMs": 16545, "OffsetEndMs": 16710}, {"Word": "two", "OffsetStartMs": 16710, "OffsetEndMs": 16890}, {"Word": "Sam", "OffsetStartMs": 16890, "OffsetEndMs": 17180}, {"Word": "between", "OffsetStartMs": 17230, "OffsetEndMs": 17490}, {"Word": "the", "OffsetStartMs": 17490, "OffsetEndMs": 17610}, {"Word": "two", "OffsetStartMs": 17610, "OffsetEndMs": 17745}, {"Word": "classes", "OffsetStartMs": 17745, "OffsetEndMs": 18020}], "SpeechSpeed": 15.4}, {"FinalSentence": "So how can we mitigate this? This is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets. And the first way that we can EM try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set at a rate EM, we instead sample at a rate that is inversely proportional to the incidence of a class in our data set. So in the previous example, if EM the likelihood if faces were much, if the number of faces was much lower than the number of non faces in our data set, we would sample the faces with a higher probability than the negatives so that the model sees both classes equally.", "SliceSentence": "So how can we mitigate this This is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets . Andthe first way that we can EM try to mitigate class imbalance is using sample re -weighting which is when instead of uniformly sampling from our data set at a rate EM we instead sample at a rate that is inversely proportional to the incidence of a class in our data set . Soin the previous example if EM the likelihood if faces were much if the number of faces was much lower than the number of non faces in our data set we would sample the faces with a higher probability than the negatives so that the model sees both classes equally", "StartMs": 892360, "EndMs": 931400, "WordsNum": 133, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "how", "OffsetStartMs": 820, "OffsetEndMs": 1095}, {"Word": "can", "OffsetStartMs": 1095, "OffsetEndMs": 1215}, {"Word": "we", "OffsetStartMs": 1215, "OffsetEndMs": 1320}, {"Word": "mitigate", "OffsetStartMs": 1320, "OffsetEndMs": 1680}, {"Word": "this", "OffsetStartMs": 1680, "OffsetEndMs": 1920}, {"Word": "This", "OffsetStartMs": 1920, "OffsetEndMs": 2130}, {"Word": "is", "OffsetStartMs": 2130, "OffsetEndMs": 2235}, {"Word": "a", "OffsetStartMs": 2235, "OffsetEndMs": 2355}, {"Word": "really", "OffsetStartMs": 2355, "OffsetEndMs": 2595}, {"Word": "big", "OffsetStartMs": 2595, "OffsetEndMs": 2895}, {"Word": "problem", "OffsetStartMs": 2895, "OffsetEndMs": 3210}, {"Word": "and", "OffsetStartMs": 3210, "OffsetEndMs": 3435}, {"Word": "it's", "OffsetStartMs": 3435, "OffsetEndMs": 3615}, {"Word": "very", "OffsetStartMs": 3615, "OffsetEndMs": 3840}, {"Word": "common", "OffsetStartMs": 3840, "OffsetEndMs": 4220}, {"Word": "across", "OffsetStartMs": 4330, "OffsetEndMs": 4695}, {"Word": "a", "OffsetStartMs": 4695, "OffsetEndMs": 4920}, {"Word": "lot", "OffsetStartMs": 4920, "OffsetEndMs": 5070}, {"Word": "of", "OffsetStartMs": 5070, "OffsetEndMs": 5220}, {"Word": "different", "OffsetStartMs": 5220, "OffsetEndMs": 5415}, {"Word": "types", "OffsetStartMs": 5415, "OffsetEndMs": 5655}, {"Word": "of", "OffsetStartMs": 5655, "OffsetEndMs": 5880}, {"Word": "machine", "OffsetStartMs": 5880, "OffsetEndMs": 6075}, {"Word": "learning", "OffsetStartMs": 6075, "OffsetEndMs": 6300}, {"Word": "tasks", "OffsetStartMs": 6300, "OffsetEndMs": 6650}, {"Word": "and", "OffsetStartMs": 6700, "OffsetEndMs": 6960}, {"Word": "data", "OffsetStartMs": 6960, "OffsetEndMs": 7140}, {"Word": "sets", "OffsetStartMs": 7140, "OffsetEndMs": 7460}, {"Word": ".", "OffsetStartMs": 8080, "OffsetEndMs": 8480}, {"Word": "Andthe", "OffsetStartMs": 8680, "OffsetEndMs": 8940}, {"Word": "first", "OffsetStartMs": 8940, "OffsetEndMs": 9120}, {"Word": "way", "OffsetStartMs": 9120, "OffsetEndMs": 9300}, {"Word": "that", "OffsetStartMs": 9300, "OffsetEndMs": 9420}, {"Word": "we", "OffsetStartMs": 9420, "OffsetEndMs": 9540}, {"Word": "can", "OffsetStartMs": 9540, "OffsetEndMs": 9660}, {"Word": "EM", "OffsetStartMs": 9660, "OffsetEndMs": 9900}, {"Word": "try", "OffsetStartMs": 9900, "OffsetEndMs": 10140}, {"Word": "to", "OffsetStartMs": 10140, "OffsetEndMs": 10245}, {"Word": "mitigate", "OffsetStartMs": 10245, "OffsetEndMs": 10620}, {"Word": "class", "OffsetStartMs": 10620, "OffsetEndMs": 10845}, {"Word": "imbalance", "OffsetStartMs": 10845, "OffsetEndMs": 11400}, {"Word": "is", "OffsetStartMs": 11400, "OffsetEndMs": 11640}, {"Word": "using", "OffsetStartMs": 11640, "OffsetEndMs": 11930}, {"Word": "sample", "OffsetStartMs": 11980, "OffsetEndMs": 12375}, {"Word": "re", "OffsetStartMs": 12375, "OffsetEndMs": 12630}, {"Word": "-weighting", "OffsetStartMs": 12630, "OffsetEndMs": 13070}, {"Word": "which", "OffsetStartMs": 13300, "OffsetEndMs": 13560}, {"Word": "is", "OffsetStartMs": 13560, "OffsetEndMs": 13710}, {"Word": "when", "OffsetStartMs": 13710, "OffsetEndMs": 14000}, {"Word": "instead", "OffsetStartMs": 14050, "OffsetEndMs": 14325}, {"Word": "of", "OffsetStartMs": 14325, "OffsetEndMs": 14600}, {"Word": "uniformly", "OffsetStartMs": 14710, "OffsetEndMs": 15150}, {"Word": "sampling", "OffsetStartMs": 15150, "OffsetEndMs": 15675}, {"Word": "from", "OffsetStartMs": 15675, "OffsetEndMs": 15930}, {"Word": "our", "OffsetStartMs": 15930, "OffsetEndMs": 16080}, {"Word": "data", "OffsetStartMs": 16080, "OffsetEndMs": 16320}, {"Word": "set", "OffsetStartMs": 16320, "OffsetEndMs": 16590}, {"Word": "at", "OffsetStartMs": 16590, "OffsetEndMs": 16785}, {"Word": "a", "OffsetStartMs": 16785, "OffsetEndMs": 16920}, {"Word": "rate", "OffsetStartMs": 16920, "OffsetEndMs": 17115}, {"Word": "EM", "OffsetStartMs": 17115, "OffsetEndMs": 17450}, {"Word": "we", "OffsetStartMs": 17710, "OffsetEndMs": 18060}, {"Word": "instead", "OffsetStartMs": 18060, "OffsetEndMs": 18330}, {"Word": "sample", "OffsetStartMs": 18330, "OffsetEndMs": 18650}, {"Word": "at", "OffsetStartMs": 18700, "OffsetEndMs": 18960}, {"Word": "a", "OffsetStartMs": 18960, "OffsetEndMs": 19080}, {"Word": "rate", "OffsetStartMs": 19080, "OffsetEndMs": 19245}, {"Word": "that", "OffsetStartMs": 19245, "OffsetEndMs": 19410}, {"Word": "is", "OffsetStartMs": 19410, "OffsetEndMs": 19545}, {"Word": "inversely", "OffsetStartMs": 19545, "OffsetEndMs": 20115}, {"Word": "proportional", "OffsetStartMs": 20115, "OffsetEndMs": 20760}, {"Word": "to", "OffsetStartMs": 20760, "OffsetEndMs": 20955}, {"Word": "the", "OffsetStartMs": 20955, "OffsetEndMs": 21060}, {"Word": "incidence", "OffsetStartMs": 21060, "OffsetEndMs": 21495}, {"Word": "of", "OffsetStartMs": 21495, "OffsetEndMs": 21735}, {"Word": "a", "OffsetStartMs": 21735, "OffsetEndMs": 21855}, {"Word": "class", "OffsetStartMs": 21855, "OffsetEndMs": 22095}, {"Word": "in", "OffsetStartMs": 22095, "OffsetEndMs": 22305}, {"Word": "our", "OffsetStartMs": 22305, "OffsetEndMs": 22425}, {"Word": "data", "OffsetStartMs": 22425, "OffsetEndMs": 22620}, {"Word": "set", "OffsetStartMs": 22620, "OffsetEndMs": 22940}, {"Word": ".", "OffsetStartMs": 23560, "OffsetEndMs": 23895}, {"Word": "Soin", "OffsetStartMs": 23895, "OffsetEndMs": 24105}, {"Word": "the", "OffsetStartMs": 24105, "OffsetEndMs": 24225}, {"Word": "previous", "OffsetStartMs": 24225, "OffsetEndMs": 24470}, {"Word": "example", "OffsetStartMs": 24580, "OffsetEndMs": 24980}, {"Word": "if", "OffsetStartMs": 25270, "OffsetEndMs": 25605}, {"Word": "EM", "OffsetStartMs": 25605, "OffsetEndMs": 25920}, {"Word": "the", "OffsetStartMs": 25920, "OffsetEndMs": 26220}, {"Word": "likelihood", "OffsetStartMs": 26220, "OffsetEndMs": 26865}, {"Word": "if", "OffsetStartMs": 26865, "OffsetEndMs": 27195}, {"Word": "faces", "OffsetStartMs": 27195, "OffsetEndMs": 27510}, {"Word": "were", "OffsetStartMs": 27510, "OffsetEndMs": 27780}, {"Word": "much", "OffsetStartMs": 27780, "OffsetEndMs": 28070}, {"Word": "if", "OffsetStartMs": 28660, "OffsetEndMs": 28920}, {"Word": "the", "OffsetStartMs": 28920, "OffsetEndMs": 29025}, {"Word": "number", "OffsetStartMs": 29025, "OffsetEndMs": 29160}, {"Word": "of", "OffsetStartMs": 29160, "OffsetEndMs": 29325}, {"Word": "faces", "OffsetStartMs": 29325, "OffsetEndMs": 29600}, {"Word": "was", "OffsetStartMs": 29770, "OffsetEndMs": 30075}, {"Word": "much", "OffsetStartMs": 30075, "OffsetEndMs": 30315}, {"Word": "lower", "OffsetStartMs": 30315, "OffsetEndMs": 30600}, {"Word": "than", "OffsetStartMs": 30600, "OffsetEndMs": 30810}, {"Word": "the", "OffsetStartMs": 30810, "OffsetEndMs": 30900}, {"Word": "number", "OffsetStartMs": 30900, "OffsetEndMs": 31080}, {"Word": "of", "OffsetStartMs": 31080, "OffsetEndMs": 31320}, {"Word": "non", "OffsetStartMs": 31320, "OffsetEndMs": 31545}, {"Word": "faces", "OffsetStartMs": 31545, "OffsetEndMs": 31875}, {"Word": "in", "OffsetStartMs": 31875, "OffsetEndMs": 32100}, {"Word": "our", "OffsetStartMs": 32100, "OffsetEndMs": 32220}, {"Word": "data", "OffsetStartMs": 32220, "OffsetEndMs": 32445}, {"Word": "set", "OffsetStartMs": 32445, "OffsetEndMs": 32780}, {"Word": "we", "OffsetStartMs": 32890, "OffsetEndMs": 33135}, {"Word": "would", "OffsetStartMs": 33135, "OffsetEndMs": 33285}, {"Word": "sample", "OffsetStartMs": 33285, "OffsetEndMs": 33590}, {"Word": "the", "OffsetStartMs": 33730, "OffsetEndMs": 33990}, {"Word": "faces", "OffsetStartMs": 33990, "OffsetEndMs": 34215}, {"Word": "with", "OffsetStartMs": 34215, "OffsetEndMs": 34440}, {"Word": "a", "OffsetStartMs": 34440, "OffsetEndMs": 34560}, {"Word": "higher", "OffsetStartMs": 34560, "OffsetEndMs": 34770}, {"Word": "probability", "OffsetStartMs": 34770, "OffsetEndMs": 35330}, {"Word": "than", "OffsetStartMs": 35530, "OffsetEndMs": 35790}, {"Word": "the", "OffsetStartMs": 35790, "OffsetEndMs": 35895}, {"Word": "negatives", "OffsetStartMs": 35895, "OffsetEndMs": 36380}, {"Word": "so", "OffsetStartMs": 36460, "OffsetEndMs": 36705}, {"Word": "that", "OffsetStartMs": 36705, "OffsetEndMs": 36870}, {"Word": "the", "OffsetStartMs": 36870, "OffsetEndMs": 37050}, {"Word": "model", "OffsetStartMs": 37050, "OffsetEndMs": 37275}, {"Word": "sees", "OffsetStartMs": 37275, "OffsetEndMs": 37545}, {"Word": "both", "OffsetStartMs": 37545, "OffsetEndMs": 37785}, {"Word": "classes", "OffsetStartMs": 37785, "OffsetEndMs": 38100}, {"Word": "equally", "OffsetStartMs": 38100, "OffsetEndMs": 38480}], "SpeechSpeed": 17.6}, {"FinalSentence": "The second example, the second way we can mitigate class imbalance is through loss re-rating, which is when instead of having every single mistake that the model makes contribute equally to our total loss function, we're weight the samples such that samples from underrepresented classes contribute more to the loss function. So instead of the model assigning every single input face to a as a negative, it'll be highly penalized if it does so because the loss of the faces would contribute more to the total loss function than the loss of the negatives.", "SliceSentence": "The second example the second way we can mitigate class imbalance is through loss re -rating which is when instead of having every single mistake that the model makes contribute equally to our total loss function we 're weight the samples such that samples from underrepresented classes contribute more to the loss function . Soinstead of the model assigning every single input face to a as a negative it'll be highly penalized if it does so because the loss of the faces would contribute more to the total loss function than the loss of the negatives", "StartMs": 933060, "EndMs": 965520, "WordsNum": 96, "Words": [{"Word": "The", "OffsetStartMs": 100, "OffsetEndMs": 360}, {"Word": "second", "OffsetStartMs": 360, "OffsetEndMs": 615}, {"Word": "example", "OffsetStartMs": 615, "OffsetEndMs": 1010}, {"Word": "the", "OffsetStartMs": 1060, "OffsetEndMs": 1305}, {"Word": "second", "OffsetStartMs": 1305, "OffsetEndMs": 1470}, {"Word": "way", "OffsetStartMs": 1470, "OffsetEndMs": 1665}, {"Word": "we", "OffsetStartMs": 1665, "OffsetEndMs": 1800}, {"Word": "can", "OffsetStartMs": 1800, "OffsetEndMs": 1935}, {"Word": "mitigate", "OffsetStartMs": 1935, "OffsetEndMs": 2325}, {"Word": "class", "OffsetStartMs": 2325, "OffsetEndMs": 2535}, {"Word": "imbalance", "OffsetStartMs": 2535, "OffsetEndMs": 3120}, {"Word": "is", "OffsetStartMs": 3120, "OffsetEndMs": 3405}, {"Word": "through", "OffsetStartMs": 3405, "OffsetEndMs": 3645}, {"Word": "loss", "OffsetStartMs": 3645, "OffsetEndMs": 3915}, {"Word": "re", "OffsetStartMs": 3915, "OffsetEndMs": 4140}, {"Word": "-rating", "OffsetStartMs": 4140, "OffsetEndMs": 4400}, {"Word": "which", "OffsetStartMs": 4960, "OffsetEndMs": 5235}, {"Word": "is", "OffsetStartMs": 5235, "OffsetEndMs": 5400}, {"Word": "when", "OffsetStartMs": 5400, "OffsetEndMs": 5690}, {"Word": "instead", "OffsetStartMs": 5950, "OffsetEndMs": 6315}, {"Word": "of", "OffsetStartMs": 6315, "OffsetEndMs": 6600}, {"Word": "having", "OffsetStartMs": 6600, "OffsetEndMs": 6885}, {"Word": "every", "OffsetStartMs": 6885, "OffsetEndMs": 7230}, {"Word": "single", "OffsetStartMs": 7230, "OffsetEndMs": 7610}, {"Word": "mistake", "OffsetStartMs": 7750, "OffsetEndMs": 8085}, {"Word": "that", "OffsetStartMs": 8085, "OffsetEndMs": 8265}, {"Word": "the", "OffsetStartMs": 8265, "OffsetEndMs": 8370}, {"Word": "model", "OffsetStartMs": 8370, "OffsetEndMs": 8580}, {"Word": "makes", "OffsetStartMs": 8580, "OffsetEndMs": 8930}, {"Word": "contribute", "OffsetStartMs": 9130, "OffsetEndMs": 9480}, {"Word": "equally", "OffsetStartMs": 9480, "OffsetEndMs": 9830}, {"Word": "to", "OffsetStartMs": 9880, "OffsetEndMs": 10110}, {"Word": "our", "OffsetStartMs": 10110, "OffsetEndMs": 10230}, {"Word": "total", "OffsetStartMs": 10230, "OffsetEndMs": 10455}, {"Word": "loss", "OffsetStartMs": 10455, "OffsetEndMs": 10725}, {"Word": "function", "OffsetStartMs": 10725, "OffsetEndMs": 11060}, {"Word": "we", "OffsetStartMs": 11710, "OffsetEndMs": 12105}, {"Word": "'re", "OffsetStartMs": 12105, "OffsetEndMs": 12420}, {"Word": "weight", "OffsetStartMs": 12420, "OffsetEndMs": 12675}, {"Word": "the", "OffsetStartMs": 12675, "OffsetEndMs": 12870}, {"Word": "samples", "OffsetStartMs": 12870, "OffsetEndMs": 13395}, {"Word": "such", "OffsetStartMs": 13395, "OffsetEndMs": 13710}, {"Word": "that", "OffsetStartMs": 13710, "OffsetEndMs": 14030}, {"Word": "samples", "OffsetStartMs": 14050, "OffsetEndMs": 14535}, {"Word": "from", "OffsetStartMs": 14535, "OffsetEndMs": 14685}, {"Word": "underrepresented", "OffsetStartMs": 14685, "OffsetEndMs": 15450}, {"Word": "classes", "OffsetStartMs": 15450, "OffsetEndMs": 15800}, {"Word": "contribute", "OffsetStartMs": 16150, "OffsetEndMs": 16515}, {"Word": "more", "OffsetStartMs": 16515, "OffsetEndMs": 16830}, {"Word": "to", "OffsetStartMs": 16830, "OffsetEndMs": 17010}, {"Word": "the", "OffsetStartMs": 17010, "OffsetEndMs": 17100}, {"Word": "loss", "OffsetStartMs": 17100, "OffsetEndMs": 17295}, {"Word": "function", "OffsetStartMs": 17295, "OffsetEndMs": 17630}, {"Word": ".", "OffsetStartMs": 18190, "OffsetEndMs": 18590}, {"Word": "Soinstead", "OffsetStartMs": 18910, "OffsetEndMs": 19170}, {"Word": "of", "OffsetStartMs": 19170, "OffsetEndMs": 19290}, {"Word": "the", "OffsetStartMs": 19290, "OffsetEndMs": 19410}, {"Word": "model", "OffsetStartMs": 19410, "OffsetEndMs": 19605}, {"Word": "assigning", "OffsetStartMs": 19605, "OffsetEndMs": 20070}, {"Word": "every", "OffsetStartMs": 20070, "OffsetEndMs": 20430}, {"Word": "single", "OffsetStartMs": 20430, "OffsetEndMs": 20810}, {"Word": "input", "OffsetStartMs": 21610, "OffsetEndMs": 21900}, {"Word": "face", "OffsetStartMs": 21900, "OffsetEndMs": 22190}, {"Word": "to", "OffsetStartMs": 22420, "OffsetEndMs": 22725}, {"Word": "a", "OffsetStartMs": 22725, "OffsetEndMs": 22995}, {"Word": "as", "OffsetStartMs": 22995, "OffsetEndMs": 23235}, {"Word": "a", "OffsetStartMs": 23235, "OffsetEndMs": 23385}, {"Word": "negative", "OffsetStartMs": 23385, "OffsetEndMs": 23660}, {"Word": "it'll", "OffsetStartMs": 24340, "OffsetEndMs": 24750}, {"Word": "be", "OffsetStartMs": 24750, "OffsetEndMs": 24930}, {"Word": "highly", "OffsetStartMs": 24930, "OffsetEndMs": 25245}, {"Word": "penalized", "OffsetStartMs": 25245, "OffsetEndMs": 25710}, {"Word": "if", "OffsetStartMs": 25710, "OffsetEndMs": 25860}, {"Word": "it", "OffsetStartMs": 25860, "OffsetEndMs": 25995}, {"Word": "does", "OffsetStartMs": 25995, "OffsetEndMs": 26190}, {"Word": "so", "OffsetStartMs": 26190, "OffsetEndMs": 26510}, {"Word": "because", "OffsetStartMs": 26770, "OffsetEndMs": 27170}, {"Word": "the", "OffsetStartMs": 27490, "OffsetEndMs": 27750}, {"Word": "loss", "OffsetStartMs": 27750, "OffsetEndMs": 27960}, {"Word": "of", "OffsetStartMs": 27960, "OffsetEndMs": 28245}, {"Word": "the", "OffsetStartMs": 28245, "OffsetEndMs": 28485}, {"Word": "faces", "OffsetStartMs": 28485, "OffsetEndMs": 28755}, {"Word": "would", "OffsetStartMs": 28755, "OffsetEndMs": 29070}, {"Word": "contribute", "OffsetStartMs": 29070, "OffsetEndMs": 29385}, {"Word": "more", "OffsetStartMs": 29385, "OffsetEndMs": 29700}, {"Word": "to", "OffsetStartMs": 29700, "OffsetEndMs": 29895}, {"Word": "the", "OffsetStartMs": 29895, "OffsetEndMs": 30015}, {"Word": "total", "OffsetStartMs": 30015, "OffsetEndMs": 30210}, {"Word": "loss", "OffsetStartMs": 30210, "OffsetEndMs": 30465}, {"Word": "function", "OffsetStartMs": 30465, "OffsetEndMs": 30800}, {"Word": "than", "OffsetStartMs": 30820, "OffsetEndMs": 31080}, {"Word": "the", "OffsetStartMs": 31080, "OffsetEndMs": 31200}, {"Word": "loss", "OffsetStartMs": 31200, "OffsetEndMs": 31380}, {"Word": "of", "OffsetStartMs": 31380, "OffsetEndMs": 31560}, {"Word": "the", "OffsetStartMs": 31560, "OffsetEndMs": 31650}, {"Word": "negatives", "OffsetStartMs": 31650, "OffsetEndMs": 32120}], "SpeechSpeed": 16.9}, {"FinalSentence": "And the final way that we can mitigate class imbalance is through batch selection, which is when we choose randomly from classes so that every single batch has an equal number of data points per class.", "SliceSentence": "And the final way that we can mitigate class imbalance is through batch selection which is when we choose randomly from classes so that every single batch has an equal number of data points per class", "StartMs": 967140, "EndMs": 979780, "WordsNum": 36, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 405}, {"Word": "the", "OffsetStartMs": 405, "OffsetEndMs": 600}, {"Word": "final", "OffsetStartMs": 600, "OffsetEndMs": 840}, {"Word": "way", "OffsetStartMs": 840, "OffsetEndMs": 1185}, {"Word": "that", "OffsetStartMs": 1185, "OffsetEndMs": 1440}, {"Word": "we", "OffsetStartMs": 1440, "OffsetEndMs": 1605}, {"Word": "can", "OffsetStartMs": 1605, "OffsetEndMs": 1880}, {"Word": "mitigate", "OffsetStartMs": 2140, "OffsetEndMs": 2610}, {"Word": "class", "OffsetStartMs": 2610, "OffsetEndMs": 2805}, {"Word": "imbalance", "OffsetStartMs": 2805, "OffsetEndMs": 3360}, {"Word": "is", "OffsetStartMs": 3360, "OffsetEndMs": 3710}, {"Word": "through", "OffsetStartMs": 3940, "OffsetEndMs": 4245}, {"Word": "batch", "OffsetStartMs": 4245, "OffsetEndMs": 4580}, {"Word": "selection", "OffsetStartMs": 4600, "OffsetEndMs": 5000}, {"Word": "which", "OffsetStartMs": 5320, "OffsetEndMs": 5580}, {"Word": "is", "OffsetStartMs": 5580, "OffsetEndMs": 5715}, {"Word": "when", "OffsetStartMs": 5715, "OffsetEndMs": 5865}, {"Word": "we", "OffsetStartMs": 5865, "OffsetEndMs": 6045}, {"Word": "choose", "OffsetStartMs": 6045, "OffsetEndMs": 6285}, {"Word": "randomly", "OffsetStartMs": 6285, "OffsetEndMs": 6870}, {"Word": "from", "OffsetStartMs": 6870, "OffsetEndMs": 7050}, {"Word": "classes", "OffsetStartMs": 7050, "OffsetEndMs": 7340}, {"Word": "so", "OffsetStartMs": 7600, "OffsetEndMs": 7860}, {"Word": "that", "OffsetStartMs": 7860, "OffsetEndMs": 8025}, {"Word": "every", "OffsetStartMs": 8025, "OffsetEndMs": 8310}, {"Word": "single", "OffsetStartMs": 8310, "OffsetEndMs": 8625}, {"Word": "batch", "OffsetStartMs": 8625, "OffsetEndMs": 8990}, {"Word": "has", "OffsetStartMs": 9130, "OffsetEndMs": 9405}, {"Word": "an", "OffsetStartMs": 9405, "OffsetEndMs": 9540}, {"Word": "equal", "OffsetStartMs": 9540, "OffsetEndMs": 9765}, {"Word": "number", "OffsetStartMs": 9765, "OffsetEndMs": 10065}, {"Word": "of", "OffsetStartMs": 10065, "OffsetEndMs": 10260}, {"Word": "data", "OffsetStartMs": 10260, "OffsetEndMs": 10455}, {"Word": "points", "OffsetStartMs": 10455, "OffsetEndMs": 10785}, {"Word": "per", "OffsetStartMs": 10785, "OffsetEndMs": 11115}, {"Word": "class", "OffsetStartMs": 11115, "OffsetEndMs": 11450}], "SpeechSpeed": 15.7}, {"FinalSentence": "So is everything solved?", "SliceSentence": "So is everything solved", "StartMs": 982140, "EndMs": 985080, "WordsNum": 4, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "is", "OffsetStartMs": 610, "OffsetEndMs": 990}, {"Word": "everything", "OffsetStartMs": 990, "OffsetEndMs": 1275}, {"Word": "solved", "OffsetStartMs": 1275, "OffsetEndMs": 1760}], "SpeechSpeed": 7.8}, {"FinalSentence": "Clearly, there are other forms of bias that exist even when the classes are completely balanced, because the thing that we haven't thought about yet is latent features.", "SliceSentence": "Clearly there are other forms of bias that exist even when the classes are completely balanced because the thing that we haven't thought about yet is latent features", "StartMs": 985320, "EndMs": 997220, "WordsNum": 28, "Words": [{"Word": "Clearly", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "there", "OffsetStartMs": 670, "OffsetEndMs": 945}, {"Word": "are", "OffsetStartMs": 945, "OffsetEndMs": 1125}, {"Word": "other", "OffsetStartMs": 1125, "OffsetEndMs": 1430}, {"Word": "forms", "OffsetStartMs": 2170, "OffsetEndMs": 2475}, {"Word": "of", "OffsetStartMs": 2475, "OffsetEndMs": 2640}, {"Word": "bias", "OffsetStartMs": 2640, "OffsetEndMs": 3075}, {"Word": "that", "OffsetStartMs": 3075, "OffsetEndMs": 3465}, {"Word": "exist", "OffsetStartMs": 3465, "OffsetEndMs": 3855}, {"Word": "even", "OffsetStartMs": 3855, "OffsetEndMs": 4215}, {"Word": "when", "OffsetStartMs": 4215, "OffsetEndMs": 4580}, {"Word": "the", "OffsetStartMs": 4780, "OffsetEndMs": 5160}, {"Word": "classes", "OffsetStartMs": 5160, "OffsetEndMs": 5540}, {"Word": "are", "OffsetStartMs": 5560, "OffsetEndMs": 5940}, {"Word": "completely", "OffsetStartMs": 5940, "OffsetEndMs": 6270}, {"Word": "balanced", "OffsetStartMs": 6270, "OffsetEndMs": 6800}, {"Word": "because", "OffsetStartMs": 7210, "OffsetEndMs": 7610}, {"Word": "the", "OffsetStartMs": 7780, "OffsetEndMs": 8025}, {"Word": "thing", "OffsetStartMs": 8025, "OffsetEndMs": 8130}, {"Word": "that", "OffsetStartMs": 8130, "OffsetEndMs": 8235}, {"Word": "we", "OffsetStartMs": 8235, "OffsetEndMs": 8355}, {"Word": "haven't", "OffsetStartMs": 8355, "OffsetEndMs": 8610}, {"Word": "thought", "OffsetStartMs": 8610, "OffsetEndMs": 8790}, {"Word": "about", "OffsetStartMs": 8790, "OffsetEndMs": 9015}, {"Word": "yet", "OffsetStartMs": 9015, "OffsetEndMs": 9320}, {"Word": "is", "OffsetStartMs": 9520, "OffsetEndMs": 9870}, {"Word": "latent", "OffsetStartMs": 9870, "OffsetEndMs": 10365}, {"Word": "features", "OffsetStartMs": 10365, "OffsetEndMs": 10700}], "SpeechSpeed": 13.9}, {"FinalSentence": "So if you remember from lab two and the last lecture, latent features are the actual represent is the actual representation of this image according to the model. And so far we've mitigated the problem of when we know that we have underrepresented classes, but we haven't mitigated the problem of when we have a lot of variability within the same class. Let's say we have an equal number of faces and negative examples in our data set. What happens if the majority of the faces are from a certain demographic or they have a certain set of features? Can we still apply the techniques that we just learned about?", "SliceSentence": "So if you remember from lab two and the last lecture latent features are the actual represent is the actual representation of this image according to the model . Andso far we've mitigated the problem of when we know that we have underrepresented classes but we haven't mitigated the problem of when we have a lot of variability within the same class .Let's say we have an equal number of faces and negative examples in our data set . Whathappens if the majority of the faces are from a certain demographic or they have a certain set of features Can we still apply the techniques that we just learned about", "StartMs": 997260, "EndMs": 1031960, "WordsNum": 110, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "if", "OffsetStartMs": 550, "OffsetEndMs": 810}, {"Word": "you", "OffsetStartMs": 810, "OffsetEndMs": 960}, {"Word": "remember", "OffsetStartMs": 960, "OffsetEndMs": 1185}, {"Word": "from", "OffsetStartMs": 1185, "OffsetEndMs": 1410}, {"Word": "lab", "OffsetStartMs": 1410, "OffsetEndMs": 1665}, {"Word": "two", "OffsetStartMs": 1665, "OffsetEndMs": 2030}, {"Word": "and", "OffsetStartMs": 2050, "OffsetEndMs": 2415}, {"Word": "the", "OffsetStartMs": 2415, "OffsetEndMs": 2640}, {"Word": "last", "OffsetStartMs": 2640, "OffsetEndMs": 2850}, {"Word": "lecture", "OffsetStartMs": 2850, "OffsetEndMs": 3200}, {"Word": "latent", "OffsetStartMs": 3640, "OffsetEndMs": 4140}, {"Word": "features", "OffsetStartMs": 4140, "OffsetEndMs": 4430}, {"Word": "are", "OffsetStartMs": 4540, "OffsetEndMs": 4940}, {"Word": "the", "OffsetStartMs": 4960, "OffsetEndMs": 5360}, {"Word": "actual", "OffsetStartMs": 5440, "OffsetEndMs": 5730}, {"Word": "represent", "OffsetStartMs": 5730, "OffsetEndMs": 6015}, {"Word": "is", "OffsetStartMs": 6015, "OffsetEndMs": 6300}, {"Word": "the", "OffsetStartMs": 6300, "OffsetEndMs": 6540}, {"Word": "actual", "OffsetStartMs": 6540, "OffsetEndMs": 6750}, {"Word": "representation", "OffsetStartMs": 6750, "OffsetEndMs": 7430}, {"Word": "of", "OffsetStartMs": 7480, "OffsetEndMs": 7770}, {"Word": "this", "OffsetStartMs": 7770, "OffsetEndMs": 7965}, {"Word": "image", "OffsetStartMs": 7965, "OffsetEndMs": 8270}, {"Word": "according", "OffsetStartMs": 8710, "OffsetEndMs": 9045}, {"Word": "to", "OffsetStartMs": 9045, "OffsetEndMs": 9225}, {"Word": "the", "OffsetStartMs": 9225, "OffsetEndMs": 9330}, {"Word": "model", "OffsetStartMs": 9330, "OffsetEndMs": 9590}, {"Word": ".", "OffsetStartMs": 10360, "OffsetEndMs": 10760}, {"Word": "Andso", "OffsetStartMs": 11020, "OffsetEndMs": 11340}, {"Word": "far", "OffsetStartMs": 11340, "OffsetEndMs": 11655}, {"Word": "we've", "OffsetStartMs": 11655, "OffsetEndMs": 12000}, {"Word": "mitigated", "OffsetStartMs": 12000, "OffsetEndMs": 12390}, {"Word": "the", "OffsetStartMs": 12390, "OffsetEndMs": 12570}, {"Word": "problem", "OffsetStartMs": 12570, "OffsetEndMs": 12830}, {"Word": "of", "OffsetStartMs": 12940, "OffsetEndMs": 13260}, {"Word": "when", "OffsetStartMs": 13260, "OffsetEndMs": 13470}, {"Word": "we", "OffsetStartMs": 13470, "OffsetEndMs": 13665}, {"Word": "know", "OffsetStartMs": 13665, "OffsetEndMs": 13950}, {"Word": "that", "OffsetStartMs": 13950, "OffsetEndMs": 14190}, {"Word": "we", "OffsetStartMs": 14190, "OffsetEndMs": 14325}, {"Word": "have", "OffsetStartMs": 14325, "OffsetEndMs": 14505}, {"Word": "underrepresented", "OffsetStartMs": 14505, "OffsetEndMs": 15300}, {"Word": "classes", "OffsetStartMs": 15300, "OffsetEndMs": 15650}, {"Word": "but", "OffsetStartMs": 16150, "OffsetEndMs": 16410}, {"Word": "we", "OffsetStartMs": 16410, "OffsetEndMs": 16545}, {"Word": "haven't", "OffsetStartMs": 16545, "OffsetEndMs": 16845}, {"Word": "mitigated", "OffsetStartMs": 16845, "OffsetEndMs": 17220}, {"Word": "the", "OffsetStartMs": 17220, "OffsetEndMs": 17415}, {"Word": "problem", "OffsetStartMs": 17415, "OffsetEndMs": 17690}, {"Word": "of", "OffsetStartMs": 17710, "OffsetEndMs": 17985}, {"Word": "when", "OffsetStartMs": 17985, "OffsetEndMs": 18150}, {"Word": "we", "OffsetStartMs": 18150, "OffsetEndMs": 18315}, {"Word": "have", "OffsetStartMs": 18315, "OffsetEndMs": 18480}, {"Word": "a", "OffsetStartMs": 18480, "OffsetEndMs": 18660}, {"Word": "lot", "OffsetStartMs": 18660, "OffsetEndMs": 18855}, {"Word": "of", "OffsetStartMs": 18855, "OffsetEndMs": 19035}, {"Word": "variability", "OffsetStartMs": 19035, "OffsetEndMs": 19640}, {"Word": "within", "OffsetStartMs": 19810, "OffsetEndMs": 20205}, {"Word": "the", "OffsetStartMs": 20205, "OffsetEndMs": 20490}, {"Word": "same", "OffsetStartMs": 20490, "OffsetEndMs": 20745}, {"Word": "class", "OffsetStartMs": 20745, "OffsetEndMs": 21110}, {"Word": ".Let's", "OffsetStartMs": 21850, "OffsetEndMs": 22230}, {"Word": "say", "OffsetStartMs": 22230, "OffsetEndMs": 22380}, {"Word": "we", "OffsetStartMs": 22380, "OffsetEndMs": 22485}, {"Word": "have", "OffsetStartMs": 22485, "OffsetEndMs": 22545}, {"Word": "an", "OffsetStartMs": 22545, "OffsetEndMs": 22665}, {"Word": "equal", "OffsetStartMs": 22665, "OffsetEndMs": 22905}, {"Word": "number", "OffsetStartMs": 22905, "OffsetEndMs": 23205}, {"Word": "of", "OffsetStartMs": 23205, "OffsetEndMs": 23430}, {"Word": "faces", "OffsetStartMs": 23430, "OffsetEndMs": 23720}, {"Word": "and", "OffsetStartMs": 23830, "OffsetEndMs": 24120}, {"Word": "negative", "OffsetStartMs": 24120, "OffsetEndMs": 24405}, {"Word": "examples", "OffsetStartMs": 24405, "OffsetEndMs": 24795}, {"Word": "in", "OffsetStartMs": 24795, "OffsetEndMs": 25035}, {"Word": "our", "OffsetStartMs": 25035, "OffsetEndMs": 25170}, {"Word": "data", "OffsetStartMs": 25170, "OffsetEndMs": 25395}, {"Word": "set", "OffsetStartMs": 25395, "OffsetEndMs": 25730}, {"Word": ".", "OffsetStartMs": 26320, "OffsetEndMs": 26610}, {"Word": "Whathappens", "OffsetStartMs": 26610, "OffsetEndMs": 26900}, {"Word": "if", "OffsetStartMs": 26950, "OffsetEndMs": 27225}, {"Word": "the", "OffsetStartMs": 27225, "OffsetEndMs": 27345}, {"Word": "majority", "OffsetStartMs": 27345, "OffsetEndMs": 27585}, {"Word": "of", "OffsetStartMs": 27585, "OffsetEndMs": 27840}, {"Word": "the", "OffsetStartMs": 27840, "OffsetEndMs": 27960}, {"Word": "faces", "OffsetStartMs": 27960, "OffsetEndMs": 28220}, {"Word": "are", "OffsetStartMs": 28240, "OffsetEndMs": 28515}, {"Word": "from", "OffsetStartMs": 28515, "OffsetEndMs": 28650}, {"Word": "a", "OffsetStartMs": 28650, "OffsetEndMs": 28770}, {"Word": "certain", "OffsetStartMs": 28770, "OffsetEndMs": 28965}, {"Word": "demographic", "OffsetStartMs": 28965, "OffsetEndMs": 29745}, {"Word": "or", "OffsetStartMs": 29745, "OffsetEndMs": 30015}, {"Word": "they", "OffsetStartMs": 30015, "OffsetEndMs": 30135}, {"Word": "have", "OffsetStartMs": 30135, "OffsetEndMs": 30225}, {"Word": "a", "OffsetStartMs": 30225, "OffsetEndMs": 30330}, {"Word": "certain", "OffsetStartMs": 30330, "OffsetEndMs": 30525}, {"Word": "set", "OffsetStartMs": 30525, "OffsetEndMs": 30705}, {"Word": "of", "OffsetStartMs": 30705, "OffsetEndMs": 30825}, {"Word": "features", "OffsetStartMs": 30825, "OffsetEndMs": 31100}, {"Word": "Can", "OffsetStartMs": 31930, "OffsetEndMs": 32175}, {"Word": "we", "OffsetStartMs": 32175, "OffsetEndMs": 32310}, {"Word": "still", "OffsetStartMs": 32310, "OffsetEndMs": 32565}, {"Word": "apply", "OffsetStartMs": 32565, "OffsetEndMs": 32850}, {"Word": "the", "OffsetStartMs": 32850, "OffsetEndMs": 33030}, {"Word": "techniques", "OffsetStartMs": 33030, "OffsetEndMs": 33270}, {"Word": "that", "OffsetStartMs": 33270, "OffsetEndMs": 33510}, {"Word": "we", "OffsetStartMs": 33510, "OffsetEndMs": 33615}, {"Word": "just", "OffsetStartMs": 33615, "OffsetEndMs": 33735}, {"Word": "learned", "OffsetStartMs": 33735, "OffsetEndMs": 33930}, {"Word": "about", "OffsetStartMs": 33930, "OffsetEndMs": 34250}], "SpeechSpeed": 17.3}, {"FinalSentence": "The answer is that we cannot do this, and the problem is that the bias present right now is in our latent features.", "SliceSentence": "The answer is that we cannot do this and the problem is that the bias present right now is in our latent features", "StartMs": 1031960, "EndMs": 1039380, "WordsNum": 23, "Words": [{"Word": "The", "OffsetStartMs": 340, "OffsetEndMs": 585}, {"Word": "answer", "OffsetStartMs": 585, "OffsetEndMs": 795}, {"Word": "is", "OffsetStartMs": 795, "OffsetEndMs": 1065}, {"Word": "that", "OffsetStartMs": 1065, "OffsetEndMs": 1215}, {"Word": "we", "OffsetStartMs": 1215, "OffsetEndMs": 1350}, {"Word": "cannot", "OffsetStartMs": 1350, "OffsetEndMs": 1605}, {"Word": "do", "OffsetStartMs": 1605, "OffsetEndMs": 1845}, {"Word": "this", "OffsetStartMs": 1845, "OffsetEndMs": 2120}, {"Word": "and", "OffsetStartMs": 2440, "OffsetEndMs": 2700}, {"Word": "the", "OffsetStartMs": 2700, "OffsetEndMs": 2850}, {"Word": "problem", "OffsetStartMs": 2850, "OffsetEndMs": 3105}, {"Word": "is", "OffsetStartMs": 3105, "OffsetEndMs": 3375}, {"Word": "that", "OffsetStartMs": 3375, "OffsetEndMs": 3555}, {"Word": "the", "OffsetStartMs": 3555, "OffsetEndMs": 3690}, {"Word": "bias", "OffsetStartMs": 3690, "OffsetEndMs": 4095}, {"Word": "present", "OffsetStartMs": 4095, "OffsetEndMs": 4380}, {"Word": "right", "OffsetStartMs": 4380, "OffsetEndMs": 4665}, {"Word": "now", "OffsetStartMs": 4665, "OffsetEndMs": 5000}, {"Word": "is", "OffsetStartMs": 5050, "OffsetEndMs": 5325}, {"Word": "in", "OffsetStartMs": 5325, "OffsetEndMs": 5445}, {"Word": "our", "OffsetStartMs": 5445, "OffsetEndMs": 5595}, {"Word": "latent", "OffsetStartMs": 5595, "OffsetEndMs": 5955}, {"Word": "features", "OffsetStartMs": 5955, "OffsetEndMs": 6230}], "SpeechSpeed": 15.2}, {"FinalSentence": "All of these images are labeled with the exact same label, so according to as the model, all we know is that they're all faces. So we have no information about any of these features, only from the label. Therefore, we can't apply any of the previous approaches that we used to mitigate class imbalanced because our classes are balanced, but we have feature imbalance now.", "SliceSentence": "All of these images are labeled with the exact same label so according to as the model all we know is that they're all faces . Sowe have no information about any of these features only from the label . Thereforewe can't apply any of the previous approaches that we used to mitigate class imbalanced because our classes are balanced but we have feature imbalance now", "StartMs": 1039500, "EndMs": 1062180, "WordsNum": 66, "Words": [{"Word": "All", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "of", "OffsetStartMs": 360, "OffsetEndMs": 525}, {"Word": "these", "OffsetStartMs": 525, "OffsetEndMs": 660}, {"Word": "images", "OffsetStartMs": 660, "OffsetEndMs": 920}, {"Word": "are", "OffsetStartMs": 1000, "OffsetEndMs": 1290}, {"Word": "labeled", "OffsetStartMs": 1290, "OffsetEndMs": 1650}, {"Word": "with", "OffsetStartMs": 1650, "OffsetEndMs": 1800}, {"Word": "the", "OffsetStartMs": 1800, "OffsetEndMs": 1965}, {"Word": "exact", "OffsetStartMs": 1965, "OffsetEndMs": 2250}, {"Word": "same", "OffsetStartMs": 2250, "OffsetEndMs": 2565}, {"Word": "label", "OffsetStartMs": 2565, "OffsetEndMs": 2900}, {"Word": "so", "OffsetStartMs": 3100, "OffsetEndMs": 3435}, {"Word": "according", "OffsetStartMs": 3435, "OffsetEndMs": 3705}, {"Word": "to", "OffsetStartMs": 3705, "OffsetEndMs": 4040}, {"Word": "as", "OffsetStartMs": 4390, "OffsetEndMs": 4790}, {"Word": "the", "OffsetStartMs": 5110, "OffsetEndMs": 5370}, {"Word": "model", "OffsetStartMs": 5370, "OffsetEndMs": 5630}, {"Word": "all", "OffsetStartMs": 5710, "OffsetEndMs": 6030}, {"Word": "we", "OffsetStartMs": 6030, "OffsetEndMs": 6240}, {"Word": "know", "OffsetStartMs": 6240, "OffsetEndMs": 6525}, {"Word": "is", "OffsetStartMs": 6525, "OffsetEndMs": 6810}, {"Word": "that", "OffsetStartMs": 6810, "OffsetEndMs": 7065}, {"Word": "they're", "OffsetStartMs": 7065, "OffsetEndMs": 7350}, {"Word": "all", "OffsetStartMs": 7350, "OffsetEndMs": 7530}, {"Word": "faces", "OffsetStartMs": 7530, "OffsetEndMs": 7880}, {"Word": ".", "OffsetStartMs": 8710, "OffsetEndMs": 8955}, {"Word": "Sowe", "OffsetStartMs": 8955, "OffsetEndMs": 9060}, {"Word": "have", "OffsetStartMs": 9060, "OffsetEndMs": 9240}, {"Word": "no", "OffsetStartMs": 9240, "OffsetEndMs": 9560}, {"Word": "information", "OffsetStartMs": 9670, "OffsetEndMs": 10070}, {"Word": "about", "OffsetStartMs": 10180, "OffsetEndMs": 10470}, {"Word": "any", "OffsetStartMs": 10470, "OffsetEndMs": 10680}, {"Word": "of", "OffsetStartMs": 10680, "OffsetEndMs": 10875}, {"Word": "these", "OffsetStartMs": 10875, "OffsetEndMs": 11055}, {"Word": "features", "OffsetStartMs": 11055, "OffsetEndMs": 11360}, {"Word": "only", "OffsetStartMs": 11470, "OffsetEndMs": 11870}, {"Word": "from", "OffsetStartMs": 11890, "OffsetEndMs": 12180}, {"Word": "the", "OffsetStartMs": 12180, "OffsetEndMs": 12330}, {"Word": "label", "OffsetStartMs": 12330, "OffsetEndMs": 12590}, {"Word": ".", "OffsetStartMs": 13630, "OffsetEndMs": 13950}, {"Word": "Thereforewe", "OffsetStartMs": 13950, "OffsetEndMs": 14190}, {"Word": "can't", "OffsetStartMs": 14190, "OffsetEndMs": 14595}, {"Word": "apply", "OffsetStartMs": 14595, "OffsetEndMs": 14865}, {"Word": "any", "OffsetStartMs": 14865, "OffsetEndMs": 15090}, {"Word": "of", "OffsetStartMs": 15090, "OffsetEndMs": 15240}, {"Word": "the", "OffsetStartMs": 15240, "OffsetEndMs": 15390}, {"Word": "previous", "OffsetStartMs": 15390, "OffsetEndMs": 15675}, {"Word": "approaches", "OffsetStartMs": 15675, "OffsetEndMs": 16035}, {"Word": "that", "OffsetStartMs": 16035, "OffsetEndMs": 16260}, {"Word": "we", "OffsetStartMs": 16260, "OffsetEndMs": 16410}, {"Word": "used", "OffsetStartMs": 16410, "OffsetEndMs": 16700}, {"Word": "to", "OffsetStartMs": 17080, "OffsetEndMs": 17480}, {"Word": "mitigate", "OffsetStartMs": 17770, "OffsetEndMs": 18300}, {"Word": "class", "OffsetStartMs": 18300, "OffsetEndMs": 18525}, {"Word": "imbalanced", "OffsetStartMs": 18525, "OffsetEndMs": 19125}, {"Word": "because", "OffsetStartMs": 19125, "OffsetEndMs": 19365}, {"Word": "our", "OffsetStartMs": 19365, "OffsetEndMs": 19515}, {"Word": "classes", "OffsetStartMs": 19515, "OffsetEndMs": 19815}, {"Word": "are", "OffsetStartMs": 19815, "OffsetEndMs": 20130}, {"Word": "balanced", "OffsetStartMs": 20130, "OffsetEndMs": 20625}, {"Word": "but", "OffsetStartMs": 20625, "OffsetEndMs": 20850}, {"Word": "we", "OffsetStartMs": 20850, "OffsetEndMs": 20985}, {"Word": "have", "OffsetStartMs": 20985, "OffsetEndMs": 21165}, {"Word": "feature", "OffsetStartMs": 21165, "OffsetEndMs": 21405}, {"Word": "imbalance", "OffsetStartMs": 21405, "OffsetEndMs": 21915}, {"Word": "now", "OffsetStartMs": 21915, "OffsetEndMs": 22220}], "SpeechSpeed": 16.0}, {"FinalSentence": "However, we can adapt the previous methods to account for bias in latent features, which we'll do in just a few slides.", "SliceSentence": "However we can adapt the previous methods to account for bias in latent features which we'll do in just a few slides", "StartMs": 1062240, "EndMs": 1070520, "WordsNum": 22, "Words": [{"Word": "However", "OffsetStartMs": 400, "OffsetEndMs": 780}, {"Word": "we", "OffsetStartMs": 780, "OffsetEndMs": 1065}, {"Word": "can", "OffsetStartMs": 1065, "OffsetEndMs": 1350}, {"Word": "adapt", "OffsetStartMs": 1350, "OffsetEndMs": 1680}, {"Word": "the", "OffsetStartMs": 1680, "OffsetEndMs": 1905}, {"Word": "previous", "OffsetStartMs": 1905, "OffsetEndMs": 2160}, {"Word": "methods", "OffsetStartMs": 2160, "OffsetEndMs": 2540}, {"Word": "to", "OffsetStartMs": 2740, "OffsetEndMs": 3075}, {"Word": "account", "OffsetStartMs": 3075, "OffsetEndMs": 3330}, {"Word": "for", "OffsetStartMs": 3330, "OffsetEndMs": 3525}, {"Word": "bias", "OffsetStartMs": 3525, "OffsetEndMs": 3900}, {"Word": "in", "OffsetStartMs": 3900, "OffsetEndMs": 4095}, {"Word": "latent", "OffsetStartMs": 4095, "OffsetEndMs": 4395}, {"Word": "features", "OffsetStartMs": 4395, "OffsetEndMs": 4670}, {"Word": "which", "OffsetStartMs": 4960, "OffsetEndMs": 5265}, {"Word": "we'll", "OffsetStartMs": 5265, "OffsetEndMs": 5505}, {"Word": "do", "OffsetStartMs": 5505, "OffsetEndMs": 5750}, {"Word": "in", "OffsetStartMs": 6010, "OffsetEndMs": 6300}, {"Word": "just", "OffsetStartMs": 6300, "OffsetEndMs": 6495}, {"Word": "a", "OffsetStartMs": 6495, "OffsetEndMs": 6660}, {"Word": "few", "OffsetStartMs": 6660, "OffsetEndMs": 6810}, {"Word": "slides", "OffsetStartMs": 6810, "OffsetEndMs": 7100}], "SpeechSpeed": 14.0}, {"FinalSentence": "So let's unpack this a little bit further. We have our potentially biased data set, and we're trying to build and deploy a model that classifies the faces in a traditional training pipeline. This is what that pipeline would look like. We would train our classifier and we would deploy it into the real world. But this training pipeline doesn't de bias our inputs in any way.", "SliceSentence": "So let's unpack this a little bit further . Wehave our potentially biased data set and we're trying to build and deploy a model that classifies the faces in a traditional training pipeline . Thisis what that pipeline would look like . Wewould train our classifier and we would deploy it into the real world . Butthis training pipeline doesn't de bias our inputs in any way", "StartMs": 1071900, "EndMs": 1092020, "WordsNum": 67, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "let's", "OffsetStartMs": 670, "OffsetEndMs": 1020}, {"Word": "unpack", "OffsetStartMs": 1020, "OffsetEndMs": 1335}, {"Word": "this", "OffsetStartMs": 1335, "OffsetEndMs": 1530}, {"Word": "a", "OffsetStartMs": 1530, "OffsetEndMs": 1665}, {"Word": "little", "OffsetStartMs": 1665, "OffsetEndMs": 1800}, {"Word": "bit", "OffsetStartMs": 1800, "OffsetEndMs": 1965}, {"Word": "further", "OffsetStartMs": 1965, "OffsetEndMs": 2240}, {"Word": ".", "OffsetStartMs": 2590, "OffsetEndMs": 2880}, {"Word": "Wehave", "OffsetStartMs": 2880, "OffsetEndMs": 3120}, {"Word": "our", "OffsetStartMs": 3120, "OffsetEndMs": 3450}, {"Word": "potentially", "OffsetStartMs": 3450, "OffsetEndMs": 3830}, {"Word": "biased", "OffsetStartMs": 3850, "OffsetEndMs": 4335}, {"Word": "data", "OffsetStartMs": 4335, "OffsetEndMs": 4575}, {"Word": "set", "OffsetStartMs": 4575, "OffsetEndMs": 4910}, {"Word": "and", "OffsetStartMs": 5080, "OffsetEndMs": 5355}, {"Word": "we're", "OffsetStartMs": 5355, "OffsetEndMs": 5580}, {"Word": "trying", "OffsetStartMs": 5580, "OffsetEndMs": 5775}, {"Word": "to", "OffsetStartMs": 5775, "OffsetEndMs": 5970}, {"Word": "build", "OffsetStartMs": 5970, "OffsetEndMs": 6195}, {"Word": "and", "OffsetStartMs": 6195, "OffsetEndMs": 6510}, {"Word": "deploy", "OffsetStartMs": 6510, "OffsetEndMs": 6860}, {"Word": "a", "OffsetStartMs": 6910, "OffsetEndMs": 7185}, {"Word": "model", "OffsetStartMs": 7185, "OffsetEndMs": 7410}, {"Word": "that", "OffsetStartMs": 7410, "OffsetEndMs": 7650}, {"Word": "classifies", "OffsetStartMs": 7650, "OffsetEndMs": 8130}, {"Word": "the", "OffsetStartMs": 8130, "OffsetEndMs": 8280}, {"Word": "faces", "OffsetStartMs": 8280, "OffsetEndMs": 8540}, {"Word": "in", "OffsetStartMs": 8860, "OffsetEndMs": 9210}, {"Word": "a", "OffsetStartMs": 9210, "OffsetEndMs": 9480}, {"Word": "traditional", "OffsetStartMs": 9480, "OffsetEndMs": 9765}, {"Word": "training", "OffsetStartMs": 9765, "OffsetEndMs": 10065}, {"Word": "pipeline", "OffsetStartMs": 10065, "OffsetEndMs": 10400}, {"Word": ".", "OffsetStartMs": 10900, "OffsetEndMs": 11175}, {"Word": "Thisis", "OffsetStartMs": 11175, "OffsetEndMs": 11325}, {"Word": "what", "OffsetStartMs": 11325, "OffsetEndMs": 11475}, {"Word": "that", "OffsetStartMs": 11475, "OffsetEndMs": 11640}, {"Word": "pipeline", "OffsetStartMs": 11640, "OffsetEndMs": 11910}, {"Word": "would", "OffsetStartMs": 11910, "OffsetEndMs": 12150}, {"Word": "look", "OffsetStartMs": 12150, "OffsetEndMs": 12315}, {"Word": "like", "OffsetStartMs": 12315, "OffsetEndMs": 12600}, {"Word": ".", "OffsetStartMs": 12600, "OffsetEndMs": 12840}, {"Word": "Wewould", "OffsetStartMs": 12840, "OffsetEndMs": 12975}, {"Word": "train", "OffsetStartMs": 12975, "OffsetEndMs": 13125}, {"Word": "our", "OffsetStartMs": 13125, "OffsetEndMs": 13290}, {"Word": "classifier", "OffsetStartMs": 13290, "OffsetEndMs": 13875}, {"Word": "and", "OffsetStartMs": 13875, "OffsetEndMs": 14130}, {"Word": "we", "OffsetStartMs": 14130, "OffsetEndMs": 14235}, {"Word": "would", "OffsetStartMs": 14235, "OffsetEndMs": 14415}, {"Word": "deploy", "OffsetStartMs": 14415, "OffsetEndMs": 14610}, {"Word": "it", "OffsetStartMs": 14610, "OffsetEndMs": 14730}, {"Word": "into", "OffsetStartMs": 14730, "OffsetEndMs": 14895}, {"Word": "the", "OffsetStartMs": 14895, "OffsetEndMs": 15060}, {"Word": "real", "OffsetStartMs": 15060, "OffsetEndMs": 15225}, {"Word": "world", "OffsetStartMs": 15225, "OffsetEndMs": 15530}, {"Word": ".", "OffsetStartMs": 15820, "OffsetEndMs": 16095}, {"Word": "Butthis", "OffsetStartMs": 16095, "OffsetEndMs": 16305}, {"Word": "training", "OffsetStartMs": 16305, "OffsetEndMs": 16575}, {"Word": "pipeline", "OffsetStartMs": 16575, "OffsetEndMs": 16910}, {"Word": "doesn't", "OffsetStartMs": 16930, "OffsetEndMs": 17385}, {"Word": "de", "OffsetStartMs": 17385, "OffsetEndMs": 17580}, {"Word": "bias", "OffsetStartMs": 17580, "OffsetEndMs": 17895}, {"Word": "our", "OffsetStartMs": 17895, "OffsetEndMs": 18135}, {"Word": "inputs", "OffsetStartMs": 18135, "OffsetEndMs": 18480}, {"Word": "in", "OffsetStartMs": 18480, "OffsetEndMs": 18645}, {"Word": "any", "OffsetStartMs": 18645, "OffsetEndMs": 18855}, {"Word": "way", "OffsetStartMs": 18855, "OffsetEndMs": 19190}], "SpeechSpeed": 18.3}, {"FinalSentence": "So one thing we could do is label our biased features and then apply resampling. So let's say in reality that this data set was biased on hair color. Most of the data set is made up of people with blonde hair, with faces with black hair and red hair underrepresented. If we knew this information, we could label the hair color of every single person in this data set, and we could apply either sample weighting or loss re-weighting, just as we did previously.", "SliceSentence": "So one thing we could do is label our biased features and then apply resampling . Solet's say in reality that this data set was biased on hair color . Mostof the data set is made up of people with blonde hair with faces with black hair and red hair underrepresented . Ifwe knew this information we could label the hair color of every single person in this data set and we could apply either sample weighting or loss re -weighting just as we did previously", "StartMs": 1093280, "EndMs": 1119480, "WordsNum": 86, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 555}, {"Word": "one", "OffsetStartMs": 555, "OffsetEndMs": 855}, {"Word": "thing", "OffsetStartMs": 855, "OffsetEndMs": 1035}, {"Word": "we", "OffsetStartMs": 1035, "OffsetEndMs": 1170}, {"Word": "could", "OffsetStartMs": 1170, "OffsetEndMs": 1305}, {"Word": "do", "OffsetStartMs": 1305, "OffsetEndMs": 1580}, {"Word": "is", "OffsetStartMs": 1600, "OffsetEndMs": 1920}, {"Word": "label", "OffsetStartMs": 1920, "OffsetEndMs": 2240}, {"Word": "our", "OffsetStartMs": 2260, "OffsetEndMs": 2550}, {"Word": "biased", "OffsetStartMs": 2550, "OffsetEndMs": 2940}, {"Word": "features", "OffsetStartMs": 2940, "OffsetEndMs": 3260}, {"Word": "and", "OffsetStartMs": 3460, "OffsetEndMs": 3720}, {"Word": "then", "OffsetStartMs": 3720, "OffsetEndMs": 3915}, {"Word": "apply", "OffsetStartMs": 3915, "OffsetEndMs": 4155}, {"Word": "resampling", "OffsetStartMs": 4155, "OffsetEndMs": 4880}, {"Word": ".", "OffsetStartMs": 5230, "OffsetEndMs": 5490}, {"Word": "Solet's", "OffsetStartMs": 5490, "OffsetEndMs": 5745}, {"Word": "say", "OffsetStartMs": 5745, "OffsetEndMs": 5955}, {"Word": "in", "OffsetStartMs": 5955, "OffsetEndMs": 6180}, {"Word": "reality", "OffsetStartMs": 6180, "OffsetEndMs": 6470}, {"Word": "that", "OffsetStartMs": 6760, "OffsetEndMs": 7035}, {"Word": "this", "OffsetStartMs": 7035, "OffsetEndMs": 7200}, {"Word": "data", "OffsetStartMs": 7200, "OffsetEndMs": 7425}, {"Word": "set", "OffsetStartMs": 7425, "OffsetEndMs": 7665}, {"Word": "was", "OffsetStartMs": 7665, "OffsetEndMs": 7875}, {"Word": "biased", "OffsetStartMs": 7875, "OffsetEndMs": 8250}, {"Word": "on", "OffsetStartMs": 8250, "OffsetEndMs": 8475}, {"Word": "hair", "OffsetStartMs": 8475, "OffsetEndMs": 8685}, {"Word": "color", "OffsetStartMs": 8685, "OffsetEndMs": 8990}, {"Word": ".", "OffsetStartMs": 9310, "OffsetEndMs": 9630}, {"Word": "Mostof", "OffsetStartMs": 9630, "OffsetEndMs": 9810}, {"Word": "the", "OffsetStartMs": 9810, "OffsetEndMs": 9915}, {"Word": "data", "OffsetStartMs": 9915, "OffsetEndMs": 10080}, {"Word": "set", "OffsetStartMs": 10080, "OffsetEndMs": 10305}, {"Word": "is", "OffsetStartMs": 10305, "OffsetEndMs": 10530}, {"Word": "made", "OffsetStartMs": 10530, "OffsetEndMs": 10710}, {"Word": "up", "OffsetStartMs": 10710, "OffsetEndMs": 10830}, {"Word": "of", "OffsetStartMs": 10830, "OffsetEndMs": 10965}, {"Word": "people", "OffsetStartMs": 10965, "OffsetEndMs": 11190}, {"Word": "with", "OffsetStartMs": 11190, "OffsetEndMs": 11415}, {"Word": "blonde", "OffsetStartMs": 11415, "OffsetEndMs": 11700}, {"Word": "hair", "OffsetStartMs": 11700, "OffsetEndMs": 11960}, {"Word": "with", "OffsetStartMs": 12130, "OffsetEndMs": 12530}, {"Word": "faces", "OffsetStartMs": 12700, "OffsetEndMs": 13065}, {"Word": "with", "OffsetStartMs": 13065, "OffsetEndMs": 13305}, {"Word": "black", "OffsetStartMs": 13305, "OffsetEndMs": 13500}, {"Word": "hair", "OffsetStartMs": 13500, "OffsetEndMs": 13680}, {"Word": "and", "OffsetStartMs": 13680, "OffsetEndMs": 13815}, {"Word": "red", "OffsetStartMs": 13815, "OffsetEndMs": 13995}, {"Word": "hair", "OffsetStartMs": 13995, "OffsetEndMs": 14205}, {"Word": "underrepresented", "OffsetStartMs": 14205, "OffsetEndMs": 15020}, {"Word": ".", "OffsetStartMs": 15880, "OffsetEndMs": 16200}, {"Word": "Ifwe", "OffsetStartMs": 16200, "OffsetEndMs": 16395}, {"Word": "knew", "OffsetStartMs": 16395, "OffsetEndMs": 16545}, {"Word": "this", "OffsetStartMs": 16545, "OffsetEndMs": 16815}, {"Word": "information", "OffsetStartMs": 16815, "OffsetEndMs": 17210}, {"Word": "we", "OffsetStartMs": 17560, "OffsetEndMs": 17835}, {"Word": "could", "OffsetStartMs": 17835, "OffsetEndMs": 18000}, {"Word": "label", "OffsetStartMs": 18000, "OffsetEndMs": 18255}, {"Word": "the", "OffsetStartMs": 18255, "OffsetEndMs": 18495}, {"Word": "hair", "OffsetStartMs": 18495, "OffsetEndMs": 18705}, {"Word": "color", "OffsetStartMs": 18705, "OffsetEndMs": 18990}, {"Word": "of", "OffsetStartMs": 18990, "OffsetEndMs": 19230}, {"Word": "every", "OffsetStartMs": 19230, "OffsetEndMs": 19485}, {"Word": "single", "OffsetStartMs": 19485, "OffsetEndMs": 19785}, {"Word": "person", "OffsetStartMs": 19785, "OffsetEndMs": 20085}, {"Word": "in", "OffsetStartMs": 20085, "OffsetEndMs": 20295}, {"Word": "this", "OffsetStartMs": 20295, "OffsetEndMs": 20430}, {"Word": "data", "OffsetStartMs": 20430, "OffsetEndMs": 20655}, {"Word": "set", "OffsetStartMs": 20655, "OffsetEndMs": 20990}, {"Word": "and", "OffsetStartMs": 21130, "OffsetEndMs": 21405}, {"Word": "we", "OffsetStartMs": 21405, "OffsetEndMs": 21525}, {"Word": "could", "OffsetStartMs": 21525, "OffsetEndMs": 21690}, {"Word": "apply", "OffsetStartMs": 21690, "OffsetEndMs": 21930}, {"Word": "either", "OffsetStartMs": 21930, "OffsetEndMs": 22200}, {"Word": "sample", "OffsetStartMs": 22200, "OffsetEndMs": 22550}, {"Word": "weighting", "OffsetStartMs": 22600, "OffsetEndMs": 23055}, {"Word": "or", "OffsetStartMs": 23055, "OffsetEndMs": 23265}, {"Word": "loss", "OffsetStartMs": 23265, "OffsetEndMs": 23505}, {"Word": "re", "OffsetStartMs": 23505, "OffsetEndMs": 23700}, {"Word": "-weighting", "OffsetStartMs": 23700, "OffsetEndMs": 23985}, {"Word": "just", "OffsetStartMs": 23985, "OffsetEndMs": 24165}, {"Word": "as", "OffsetStartMs": 24165, "OffsetEndMs": 24345}, {"Word": "we", "OffsetStartMs": 24345, "OffsetEndMs": 24480}, {"Word": "did", "OffsetStartMs": 24480, "OffsetEndMs": 24645}, {"Word": "previously", "OffsetStartMs": 24645, "OffsetEndMs": 24950}], "SpeechSpeed": 17.2}, {"FinalSentence": "But does anyone want to tell me what the problem is here?", "SliceSentence": "But does anyone want to tell me what the problem is here", "StartMs": 1119620, "EndMs": 1123160, "WordsNum": 12, "Words": [{"Word": "But", "OffsetStartMs": 100, "OffsetEndMs": 450}, {"Word": "does", "OffsetStartMs": 450, "OffsetEndMs": 720}, {"Word": "anyone", "OffsetStartMs": 720, "OffsetEndMs": 900}, {"Word": "want", "OffsetStartMs": 900, "OffsetEndMs": 1005}, {"Word": "to", "OffsetStartMs": 1005, "OffsetEndMs": 1095}, {"Word": "tell", "OffsetStartMs": 1095, "OffsetEndMs": 1215}, {"Word": "me", "OffsetStartMs": 1215, "OffsetEndMs": 1365}, {"Word": "what", "OffsetStartMs": 1365, "OffsetEndMs": 1515}, {"Word": "the", "OffsetStartMs": 1515, "OffsetEndMs": 1650}, {"Word": "problem", "OffsetStartMs": 1650, "OffsetEndMs": 1860}, {"Word": "is", "OffsetStartMs": 1860, "OffsetEndMs": 2100}, {"Word": "here", "OffsetStartMs": 2100, "OffsetEndMs": 2390}], "SpeechSpeed": 15.8}, {"FinalSentence": "You go through eat samples, it takes a lot of time.", "SliceSentence": "You go through eat samples it takes a lot of time", "StartMs": 1125920, "EndMs": 1128800, "WordsNum": 11, "Words": [{"Word": "You", "OffsetStartMs": 0, "OffsetEndMs": 240}, {"Word": "go", "OffsetStartMs": 240, "OffsetEndMs": 420}, {"Word": "through", "OffsetStartMs": 420, "OffsetEndMs": 690}, {"Word": "eat", "OffsetStartMs": 690, "OffsetEndMs": 1040}, {"Word": "samples", "OffsetStartMs": 1060, "OffsetEndMs": 1635}, {"Word": "it", "OffsetStartMs": 1635, "OffsetEndMs": 1830}, {"Word": "takes", "OffsetStartMs": 1830, "OffsetEndMs": 2010}, {"Word": "a", "OffsetStartMs": 2010, "OffsetEndMs": 2145}, {"Word": "lot", "OffsetStartMs": 2145, "OffsetEndMs": 2280}, {"Word": "of", "OffsetStartMs": 2280, "OffsetEndMs": 2475}, {"Word": "time", "OffsetStartMs": 2475, "OffsetEndMs": 2780}], "SpeechSpeed": 17.0}, {"FinalSentence": "Yeah, so there are a couple problems here, and that's definitely one of them. The first is how do we know that hair color is a biased feature in this data set? Unless we visually inspect every single sample in this data set, we're not going to know what the biased features are. And the second thing is exactly what you said, which is once we have our biased features, going through and annotating every image with this feature is an extremely labor intensive task that is infeasible in the real world.", "SliceSentence": "Yeah so there are a couple problems here and that's definitely one of them . Thefirst is how do we know that hair color is a biased feature in this data set Unless we visually inspect every single sample in this data set we're not going to know what the biased features are . Andthe second thing is exactly what you said which is once we have our biased features going through and annotating every image with this feature is an extremely labor intensive task that is infeasible in the real world", "StartMs": 1128840, "EndMs": 1156680, "WordsNum": 92, "Words": [{"Word": "Yeah", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "so", "OffsetStartMs": 1000, "OffsetEndMs": 1395}, {"Word": "there", "OffsetStartMs": 1395, "OffsetEndMs": 1620}, {"Word": "are", "OffsetStartMs": 1620, "OffsetEndMs": 1680}, {"Word": "a", "OffsetStartMs": 1680, "OffsetEndMs": 1755}, {"Word": "couple", "OffsetStartMs": 1755, "OffsetEndMs": 1920}, {"Word": "problems", "OffsetStartMs": 1920, "OffsetEndMs": 2220}, {"Word": "here", "OffsetStartMs": 2220, "OffsetEndMs": 2430}, {"Word": "and", "OffsetStartMs": 2430, "OffsetEndMs": 2520}, {"Word": "that's", "OffsetStartMs": 2520, "OffsetEndMs": 2760}, {"Word": "definitely", "OffsetStartMs": 2760, "OffsetEndMs": 3020}, {"Word": "one", "OffsetStartMs": 3070, "OffsetEndMs": 3330}, {"Word": "of", "OffsetStartMs": 3330, "OffsetEndMs": 3465}, {"Word": "them", "OffsetStartMs": 3465, "OffsetEndMs": 3740}, {"Word": ".", "OffsetStartMs": 4240, "OffsetEndMs": 4500}, {"Word": "Thefirst", "OffsetStartMs": 4500, "OffsetEndMs": 4695}, {"Word": "is", "OffsetStartMs": 4695, "OffsetEndMs": 5030}, {"Word": "how", "OffsetStartMs": 5080, "OffsetEndMs": 5355}, {"Word": "do", "OffsetStartMs": 5355, "OffsetEndMs": 5490}, {"Word": "we", "OffsetStartMs": 5490, "OffsetEndMs": 5685}, {"Word": "know", "OffsetStartMs": 5685, "OffsetEndMs": 6000}, {"Word": "that", "OffsetStartMs": 6000, "OffsetEndMs": 6270}, {"Word": "hair", "OffsetStartMs": 6270, "OffsetEndMs": 6480}, {"Word": "color", "OffsetStartMs": 6480, "OffsetEndMs": 6720}, {"Word": "is", "OffsetStartMs": 6720, "OffsetEndMs": 6885}, {"Word": "a", "OffsetStartMs": 6885, "OffsetEndMs": 7005}, {"Word": "biased", "OffsetStartMs": 7005, "OffsetEndMs": 7335}, {"Word": "feature", "OffsetStartMs": 7335, "OffsetEndMs": 7575}, {"Word": "in", "OffsetStartMs": 7575, "OffsetEndMs": 7755}, {"Word": "this", "OffsetStartMs": 7755, "OffsetEndMs": 7890}, {"Word": "data", "OffsetStartMs": 7890, "OffsetEndMs": 8100}, {"Word": "set", "OffsetStartMs": 8100, "OffsetEndMs": 8420}, {"Word": "Unless", "OffsetStartMs": 8770, "OffsetEndMs": 9135}, {"Word": "we", "OffsetStartMs": 9135, "OffsetEndMs": 9450}, {"Word": "visually", "OffsetStartMs": 9450, "OffsetEndMs": 9950}, {"Word": "inspect", "OffsetStartMs": 10030, "OffsetEndMs": 10430}, {"Word": "every", "OffsetStartMs": 10450, "OffsetEndMs": 10850}, {"Word": "single", "OffsetStartMs": 10900, "OffsetEndMs": 11265}, {"Word": "sample", "OffsetStartMs": 11265, "OffsetEndMs": 11595}, {"Word": "in", "OffsetStartMs": 11595, "OffsetEndMs": 11820}, {"Word": "this", "OffsetStartMs": 11820, "OffsetEndMs": 11955}, {"Word": "data", "OffsetStartMs": 11955, "OffsetEndMs": 12165}, {"Word": "set", "OffsetStartMs": 12165, "OffsetEndMs": 12420}, {"Word": "we're", "OffsetStartMs": 12420, "OffsetEndMs": 12675}, {"Word": "not", "OffsetStartMs": 12675, "OffsetEndMs": 12840}, {"Word": "going", "OffsetStartMs": 12840, "OffsetEndMs": 13050}, {"Word": "to", "OffsetStartMs": 13050, "OffsetEndMs": 13200}, {"Word": "know", "OffsetStartMs": 13200, "OffsetEndMs": 13395}, {"Word": "what", "OffsetStartMs": 13395, "OffsetEndMs": 13605}, {"Word": "the", "OffsetStartMs": 13605, "OffsetEndMs": 13725}, {"Word": "biased", "OffsetStartMs": 13725, "OffsetEndMs": 14025}, {"Word": "features", "OffsetStartMs": 14025, "OffsetEndMs": 14280}, {"Word": "are", "OffsetStartMs": 14280, "OffsetEndMs": 14630}, {"Word": ".", "OffsetStartMs": 15340, "OffsetEndMs": 15615}, {"Word": "Andthe", "OffsetStartMs": 15615, "OffsetEndMs": 15750}, {"Word": "second", "OffsetStartMs": 15750, "OffsetEndMs": 15945}, {"Word": "thing", "OffsetStartMs": 15945, "OffsetEndMs": 16170}, {"Word": "is", "OffsetStartMs": 16170, "OffsetEndMs": 16380}, {"Word": "exactly", "OffsetStartMs": 16380, "OffsetEndMs": 16665}, {"Word": "what", "OffsetStartMs": 16665, "OffsetEndMs": 16890}, {"Word": "you", "OffsetStartMs": 16890, "OffsetEndMs": 17040}, {"Word": "said", "OffsetStartMs": 17040, "OffsetEndMs": 17330}, {"Word": "which", "OffsetStartMs": 17500, "OffsetEndMs": 17775}, {"Word": "is", "OffsetStartMs": 17775, "OffsetEndMs": 18050}, {"Word": "once", "OffsetStartMs": 18100, "OffsetEndMs": 18420}, {"Word": "we", "OffsetStartMs": 18420, "OffsetEndMs": 18645}, {"Word": "have", "OffsetStartMs": 18645, "OffsetEndMs": 18855}, {"Word": "our", "OffsetStartMs": 18855, "OffsetEndMs": 19050}, {"Word": "biased", "OffsetStartMs": 19050, "OffsetEndMs": 19395}, {"Word": "features", "OffsetStartMs": 19395, "OffsetEndMs": 19700}, {"Word": "going", "OffsetStartMs": 20050, "OffsetEndMs": 20430}, {"Word": "through", "OffsetStartMs": 20430, "OffsetEndMs": 20745}, {"Word": "and", "OffsetStartMs": 20745, "OffsetEndMs": 20970}, {"Word": "annotating", "OffsetStartMs": 20970, "OffsetEndMs": 21510}, {"Word": "every", "OffsetStartMs": 21510, "OffsetEndMs": 21780}, {"Word": "image", "OffsetStartMs": 21780, "OffsetEndMs": 22110}, {"Word": "with", "OffsetStartMs": 22110, "OffsetEndMs": 22365}, {"Word": "this", "OffsetStartMs": 22365, "OffsetEndMs": 22545}, {"Word": "feature", "OffsetStartMs": 22545, "OffsetEndMs": 22850}, {"Word": "is", "OffsetStartMs": 23020, "OffsetEndMs": 23265}, {"Word": "an", "OffsetStartMs": 23265, "OffsetEndMs": 23505}, {"Word": "extremely", "OffsetStartMs": 23505, "OffsetEndMs": 23895}, {"Word": "labor", "OffsetStartMs": 23895, "OffsetEndMs": 24210}, {"Word": "intensive", "OffsetStartMs": 24210, "OffsetEndMs": 24675}, {"Word": "task", "OffsetStartMs": 24675, "OffsetEndMs": 25040}, {"Word": "that", "OffsetStartMs": 25240, "OffsetEndMs": 25500}, {"Word": "is", "OffsetStartMs": 25500, "OffsetEndMs": 25635}, {"Word": "infeasible", "OffsetStartMs": 25635, "OffsetEndMs": 26235}, {"Word": "in", "OffsetStartMs": 26235, "OffsetEndMs": 26355}, {"Word": "the", "OffsetStartMs": 26355, "OffsetEndMs": 26445}, {"Word": "real", "OffsetStartMs": 26445, "OffsetEndMs": 26595}, {"Word": "world", "OffsetStartMs": 26595, "OffsetEndMs": 26900}], "SpeechSpeed": 17.7}, {"FinalSentence": "So now the question is, what if we had a way to automatically learn latent features and use this learn feature representation to de bi a model?", "SliceSentence": "So now the question is what if we had a way to automatically learn latent features and use this learn feature representation to de bi a model", "StartMs": 1157180, "EndMs": 1167620, "WordsNum": 27, "Words": [{"Word": "So", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "now", "OffsetStartMs": 610, "OffsetEndMs": 900}, {"Word": "the", "OffsetStartMs": 900, "OffsetEndMs": 1050}, {"Word": "question", "OffsetStartMs": 1050, "OffsetEndMs": 1245}, {"Word": "is", "OffsetStartMs": 1245, "OffsetEndMs": 1580}, {"Word": "what", "OffsetStartMs": 1870, "OffsetEndMs": 2145}, {"Word": "if", "OffsetStartMs": 2145, "OffsetEndMs": 2295}, {"Word": "we", "OffsetStartMs": 2295, "OffsetEndMs": 2445}, {"Word": "had", "OffsetStartMs": 2445, "OffsetEndMs": 2610}, {"Word": "a", "OffsetStartMs": 2610, "OffsetEndMs": 2775}, {"Word": "way", "OffsetStartMs": 2775, "OffsetEndMs": 3030}, {"Word": "to", "OffsetStartMs": 3030, "OffsetEndMs": 3410}, {"Word": "automatically", "OffsetStartMs": 3430, "OffsetEndMs": 3830}, {"Word": "learn", "OffsetStartMs": 4000, "OffsetEndMs": 4400}, {"Word": "latent", "OffsetStartMs": 4480, "OffsetEndMs": 4950}, {"Word": "features", "OffsetStartMs": 4950, "OffsetEndMs": 5240}, {"Word": "and", "OffsetStartMs": 5710, "OffsetEndMs": 6030}, {"Word": "use", "OffsetStartMs": 6030, "OffsetEndMs": 6300}, {"Word": "this", "OffsetStartMs": 6300, "OffsetEndMs": 6585}, {"Word": "learn", "OffsetStartMs": 6585, "OffsetEndMs": 6885}, {"Word": "feature", "OffsetStartMs": 6885, "OffsetEndMs": 7185}, {"Word": "representation", "OffsetStartMs": 7185, "OffsetEndMs": 7940}, {"Word": "to", "OffsetStartMs": 8140, "OffsetEndMs": 8400}, {"Word": "de", "OffsetStartMs": 8400, "OffsetEndMs": 8520}, {"Word": "bi", "OffsetStartMs": 8520, "OffsetEndMs": 8700}, {"Word": "a", "OffsetStartMs": 8700, "OffsetEndMs": 8865}, {"Word": "model", "OffsetStartMs": 8865, "OffsetEndMs": 9110}], "SpeechSpeed": 13.5}, {"FinalSentence": "So what we want is a way to learn the features of this data set and then automatically determine the EM samples with the highest feature bias and the samples with the lowest feature bias. We've already learned a method of doing this in the generative modeling lecture. You all learned about variational auto encoders, which are models that learn the latent features of a data set as a recap variational auto encoders work by probabilistically sampling from a learn latent space, and then they decode this new latent vector into back into the original input space, measure the reconstruction loss between the inputs and the outputs, and continue to update their representation of the latent space.", "SliceSentence": "So what we want is a way to learn the features of this data set and then automatically determine the EM samples with the highest feature bias and the samples with the lowest feature bias .We've already learned a method of doing this in the generative modeling lecture . Youall learned about variational auto encoders which are models that learn the latent features of a data set as a recap variational auto encoders work by probabilistically sampling from a learn latent space and then they decode this new latent vector into back into the original input space measure the reconstruction loss between the inputs and the outputs and continue to update their representation of the latent space", "StartMs": 1169360, "EndMs": 1208160, "WordsNum": 117, "Words": [{"Word": "So", "OffsetStartMs": 100, "OffsetEndMs": 500}, {"Word": "what", "OffsetStartMs": 520, "OffsetEndMs": 780}, {"Word": "we", "OffsetStartMs": 780, "OffsetEndMs": 930}, {"Word": "want", "OffsetStartMs": 930, "OffsetEndMs": 1200}, {"Word": "is", "OffsetStartMs": 1200, "OffsetEndMs": 1440}, {"Word": "a", "OffsetStartMs": 1440, "OffsetEndMs": 1560}, {"Word": "way", "OffsetStartMs": 1560, "OffsetEndMs": 1785}, {"Word": "to", "OffsetStartMs": 1785, "OffsetEndMs": 2055}, {"Word": "learn", "OffsetStartMs": 2055, "OffsetEndMs": 2280}, {"Word": "the", "OffsetStartMs": 2280, "OffsetEndMs": 2460}, {"Word": "features", "OffsetStartMs": 2460, "OffsetEndMs": 2670}, {"Word": "of", "OffsetStartMs": 2670, "OffsetEndMs": 2895}, {"Word": "this", "OffsetStartMs": 2895, "OffsetEndMs": 3045}, {"Word": "data", "OffsetStartMs": 3045, "OffsetEndMs": 3255}, {"Word": "set", "OffsetStartMs": 3255, "OffsetEndMs": 3590}, {"Word": "and", "OffsetStartMs": 3700, "OffsetEndMs": 3960}, {"Word": "then", "OffsetStartMs": 3960, "OffsetEndMs": 4220}, {"Word": "automatically", "OffsetStartMs": 4270, "OffsetEndMs": 4670}, {"Word": "determine", "OffsetStartMs": 4840, "OffsetEndMs": 5240}, {"Word": "the", "OffsetStartMs": 5470, "OffsetEndMs": 5850}, {"Word": "EM", "OffsetStartMs": 5850, "OffsetEndMs": 6230}, {"Word": "samples", "OffsetStartMs": 6520, "OffsetEndMs": 6990}, {"Word": "with", "OffsetStartMs": 6990, "OffsetEndMs": 7125}, {"Word": "the", "OffsetStartMs": 7125, "OffsetEndMs": 7245}, {"Word": "highest", "OffsetStartMs": 7245, "OffsetEndMs": 7485}, {"Word": "feature", "OffsetStartMs": 7485, "OffsetEndMs": 7800}, {"Word": "bias", "OffsetStartMs": 7800, "OffsetEndMs": 8295}, {"Word": "and", "OffsetStartMs": 8295, "OffsetEndMs": 8505}, {"Word": "the", "OffsetStartMs": 8505, "OffsetEndMs": 8610}, {"Word": "samples", "OffsetStartMs": 8610, "OffsetEndMs": 8955}, {"Word": "with", "OffsetStartMs": 8955, "OffsetEndMs": 9075}, {"Word": "the", "OffsetStartMs": 9075, "OffsetEndMs": 9165}, {"Word": "lowest", "OffsetStartMs": 9165, "OffsetEndMs": 9360}, {"Word": "feature", "OffsetStartMs": 9360, "OffsetEndMs": 9645}, {"Word": "bias", "OffsetStartMs": 9645, "OffsetEndMs": 10130}, {"Word": ".We've", "OffsetStartMs": 10780, "OffsetEndMs": 11190}, {"Word": "already", "OffsetStartMs": 11190, "OffsetEndMs": 11445}, {"Word": "learned", "OffsetStartMs": 11445, "OffsetEndMs": 11715}, {"Word": "a", "OffsetStartMs": 11715, "OffsetEndMs": 11910}, {"Word": "method", "OffsetStartMs": 11910, "OffsetEndMs": 12075}, {"Word": "of", "OffsetStartMs": 12075, "OffsetEndMs": 12255}, {"Word": "doing", "OffsetStartMs": 12255, "OffsetEndMs": 12450}, {"Word": "this", "OffsetStartMs": 12450, "OffsetEndMs": 12770}, {"Word": "in", "OffsetStartMs": 13090, "OffsetEndMs": 13350}, {"Word": "the", "OffsetStartMs": 13350, "OffsetEndMs": 13485}, {"Word": "generative", "OffsetStartMs": 13485, "OffsetEndMs": 13845}, {"Word": "modeling", "OffsetStartMs": 13845, "OffsetEndMs": 14220}, {"Word": "lecture", "OffsetStartMs": 14220, "OffsetEndMs": 14480}, {"Word": ".", "OffsetStartMs": 14740, "OffsetEndMs": 15000}, {"Word": "Youall", "OffsetStartMs": 15000, "OffsetEndMs": 15180}, {"Word": "learned", "OffsetStartMs": 15180, "OffsetEndMs": 15500}, {"Word": "about", "OffsetStartMs": 15550, "OffsetEndMs": 15885}, {"Word": "variational", "OffsetStartMs": 15885, "OffsetEndMs": 16430}, {"Word": "auto", "OffsetStartMs": 16450, "OffsetEndMs": 16770}, {"Word": "encoders", "OffsetStartMs": 16770, "OffsetEndMs": 17360}, {"Word": "which", "OffsetStartMs": 17500, "OffsetEndMs": 17760}, {"Word": "are", "OffsetStartMs": 17760, "OffsetEndMs": 17910}, {"Word": "models", "OffsetStartMs": 17910, "OffsetEndMs": 18180}, {"Word": "that", "OffsetStartMs": 18180, "OffsetEndMs": 18480}, {"Word": "learn", "OffsetStartMs": 18480, "OffsetEndMs": 18750}, {"Word": "the", "OffsetStartMs": 18750, "OffsetEndMs": 18975}, {"Word": "latent", "OffsetStartMs": 18975, "OffsetEndMs": 19305}, {"Word": "features", "OffsetStartMs": 19305, "OffsetEndMs": 19580}, {"Word": "of", "OffsetStartMs": 19630, "OffsetEndMs": 19890}, {"Word": "a", "OffsetStartMs": 19890, "OffsetEndMs": 20010}, {"Word": "data", "OffsetStartMs": 20010, "OffsetEndMs": 20190}, {"Word": "set", "OffsetStartMs": 20190, "OffsetEndMs": 20510}, {"Word": "as", "OffsetStartMs": 21280, "OffsetEndMs": 21555}, {"Word": "a", "OffsetStartMs": 21555, "OffsetEndMs": 21690}, {"Word": "recap", "OffsetStartMs": 21690, "OffsetEndMs": 22110}, {"Word": "variational", "OffsetStartMs": 22110, "OffsetEndMs": 22635}, {"Word": "auto", "OffsetStartMs": 22635, "OffsetEndMs": 22905}, {"Word": "encoders", "OffsetStartMs": 22905, "OffsetEndMs": 23400}, {"Word": "work", "OffsetStartMs": 23400, "OffsetEndMs": 23715}, {"Word": "by", "OffsetStartMs": 23715, "OffsetEndMs": 24030}, {"Word": "probabilistically", "OffsetStartMs": 24030, "OffsetEndMs": 24900}, {"Word": "sampling", "OffsetStartMs": 24900, "OffsetEndMs": 25425}, {"Word": "from", "OffsetStartMs": 25425, "OffsetEndMs": 25710}, {"Word": "a", "OffsetStartMs": 25710, "OffsetEndMs": 25935}, {"Word": "learn", "OffsetStartMs": 25935, "OffsetEndMs": 26240}, {"Word": "latent", "OffsetStartMs": 26560, "OffsetEndMs": 26985}, {"Word": "space", "OffsetStartMs": 26985, "OffsetEndMs": 27290}, {"Word": "and", "OffsetStartMs": 27640, "OffsetEndMs": 27930}, {"Word": "then", "OffsetStartMs": 27930, "OffsetEndMs": 28215}, {"Word": "they", "OffsetStartMs": 28215, "OffsetEndMs": 28530}, {"Word": "decode", "OffsetStartMs": 28530, "OffsetEndMs": 29010}, {"Word": "this", "OffsetStartMs": 29010, "OffsetEndMs": 29175}, {"Word": "new", "OffsetStartMs": 29175, "OffsetEndMs": 29370}, {"Word": "latent", "OffsetStartMs": 29370, "OffsetEndMs": 29685}, {"Word": "vector", "OffsetStartMs": 29685, "OffsetEndMs": 29990}, {"Word": "into", "OffsetStartMs": 30190, "OffsetEndMs": 30590}, {"Word": "back", "OffsetStartMs": 30610, "OffsetEndMs": 30900}, {"Word": "into", "OffsetStartMs": 30900, "OffsetEndMs": 31095}, {"Word": "the", "OffsetStartMs": 31095, "OffsetEndMs": 31275}, {"Word": "original", "OffsetStartMs": 31275, "OffsetEndMs": 31550}, {"Word": "input", "OffsetStartMs": 31660, "OffsetEndMs": 31965}, {"Word": "space", "OffsetStartMs": 31965, "OffsetEndMs": 32270}, {"Word": "measure", "OffsetStartMs": 32500, "OffsetEndMs": 32850}, {"Word": "the", "OffsetStartMs": 32850, "OffsetEndMs": 33060}, {"Word": "reconstruction", "OffsetStartMs": 33060, "OffsetEndMs": 33615}, {"Word": "loss", "OffsetStartMs": 33615, "OffsetEndMs": 33960}, {"Word": "between", "OffsetStartMs": 33960, "OffsetEndMs": 34200}, {"Word": "the", "OffsetStartMs": 34200, "OffsetEndMs": 34380}, {"Word": "inputs", "OffsetStartMs": 34380, "OffsetEndMs": 34620}, {"Word": "and", "OffsetStartMs": 34620, "OffsetEndMs": 34740}, {"Word": "the", "OffsetStartMs": 34740, "OffsetEndMs": 34965}, {"Word": "outputs", "OffsetStartMs": 34965, "OffsetEndMs": 35445}, {"Word": "and", "OffsetStartMs": 35445, "OffsetEndMs": 35775}, {"Word": "continue", "OffsetStartMs": 35775, "OffsetEndMs": 36030}, {"Word": "to", "OffsetStartMs": 36030, "OffsetEndMs": 36300}, {"Word": "update", "OffsetStartMs": 36300, "OffsetEndMs": 36540}, {"Word": "their", "OffsetStartMs": 36540, "OffsetEndMs": 36690}, {"Word": "representation", "OffsetStartMs": 36690, "OffsetEndMs": 37335}, {"Word": "of", "OffsetStartMs": 37335, "OffsetEndMs": 37590}, {"Word": "the", "OffsetStartMs": 37590, "OffsetEndMs": 37740}, {"Word": "latent", "OffsetStartMs": 37740, "OffsetEndMs": 38040}, {"Word": "space", "OffsetStartMs": 38040, "OffsetEndMs": 38330}], "SpeechSpeed": 17.8}, {"FinalSentence": "And the reason why we care so much about this latent space is that we want samples that are similar to each other in the input to decode to latent vectors that are very close to each other in this latent space. And samples that are far from each other or samples that are dissimilar to each other in the input should decode encode to latent vectors that are far from each other in the latent space.", "SliceSentence": "And the reason why we care so much about this latent space is that we want samples that are similar to each other in the input to decode to latent vectors that are very close to each other in this latent space . Andsamples that are far from each other or samples that are dissimilar to each other in the input should decode encode to latent vectors that are far from each other in the latent space", "StartMs": 1208460, "EndMs": 1230120, "WordsNum": 77, "Words": [{"Word": "And", "OffsetStartMs": 130, "OffsetEndMs": 390}, {"Word": "the", "OffsetStartMs": 390, "OffsetEndMs": 510}, {"Word": "reason", "OffsetStartMs": 510, "OffsetEndMs": 720}, {"Word": "why", "OffsetStartMs": 720, "OffsetEndMs": 960}, {"Word": "we", "OffsetStartMs": 960, "OffsetEndMs": 1140}, {"Word": "care", "OffsetStartMs": 1140, "OffsetEndMs": 1350}, {"Word": "so", "OffsetStartMs": 1350, "OffsetEndMs": 1545}, {"Word": "much", "OffsetStartMs": 1545, "OffsetEndMs": 1785}, {"Word": "about", "OffsetStartMs": 1785, "OffsetEndMs": 2025}, {"Word": "this", "OffsetStartMs": 2025, "OffsetEndMs": 2175}, {"Word": "latent", "OffsetStartMs": 2175, "OffsetEndMs": 2505}, {"Word": "space", "OffsetStartMs": 2505, "OffsetEndMs": 2810}, {"Word": "is", "OffsetStartMs": 2980, "OffsetEndMs": 3255}, {"Word": "that", "OffsetStartMs": 3255, "OffsetEndMs": 3530}, {"Word": "we", "OffsetStartMs": 3550, "OffsetEndMs": 3840}, {"Word": "want", "OffsetStartMs": 3840, "OffsetEndMs": 4095}, {"Word": "samples", "OffsetStartMs": 4095, "OffsetEndMs": 4575}, {"Word": "that", "OffsetStartMs": 4575, "OffsetEndMs": 4710}, {"Word": "are", "OffsetStartMs": 4710, "OffsetEndMs": 4890}, {"Word": "similar", "OffsetStartMs": 4890, "OffsetEndMs": 5205}, {"Word": "to", "OffsetStartMs": 5205, "OffsetEndMs": 5430}, {"Word": "each", "OffsetStartMs": 5430, "OffsetEndMs": 5520}, {"Word": "other", "OffsetStartMs": 5520, "OffsetEndMs": 5715}, {"Word": "in", "OffsetStartMs": 5715, "OffsetEndMs": 5925}, {"Word": "the", "OffsetStartMs": 5925, "OffsetEndMs": 6165}, {"Word": "input", "OffsetStartMs": 6165, "OffsetEndMs": 6530}, {"Word": "to", "OffsetStartMs": 6700, "OffsetEndMs": 6975}, {"Word": "decode", "OffsetStartMs": 6975, "OffsetEndMs": 7635}, {"Word": "to", "OffsetStartMs": 7635, "OffsetEndMs": 7905}, {"Word": "latent", "OffsetStartMs": 7905, "OffsetEndMs": 8250}, {"Word": "vectors", "OffsetStartMs": 8250, "OffsetEndMs": 8655}, {"Word": "that", "OffsetStartMs": 8655, "OffsetEndMs": 8835}, {"Word": "are", "OffsetStartMs": 8835, "OffsetEndMs": 8940}, {"Word": "very", "OffsetStartMs": 8940, "OffsetEndMs": 9135}, {"Word": "close", "OffsetStartMs": 9135, "OffsetEndMs": 9390}, {"Word": "to", "OffsetStartMs": 9390, "OffsetEndMs": 9540}, {"Word": "each", "OffsetStartMs": 9540, "OffsetEndMs": 9630}, {"Word": "other", "OffsetStartMs": 9630, "OffsetEndMs": 9825}, {"Word": "in", "OffsetStartMs": 9825, "OffsetEndMs": 10020}, {"Word": "this", "OffsetStartMs": 10020, "OffsetEndMs": 10170}, {"Word": "latent", "OffsetStartMs": 10170, "OffsetEndMs": 10485}, {"Word": "space", "OffsetStartMs": 10485, "OffsetEndMs": 10790}, {"Word": ".", "OffsetStartMs": 11230, "OffsetEndMs": 11535}, {"Word": "Andsamples", "OffsetStartMs": 11535, "OffsetEndMs": 11925}, {"Word": "that", "OffsetStartMs": 11925, "OffsetEndMs": 12045}, {"Word": "are", "OffsetStartMs": 12045, "OffsetEndMs": 12210}, {"Word": "far", "OffsetStartMs": 12210, "OffsetEndMs": 12450}, {"Word": "from", "OffsetStartMs": 12450, "OffsetEndMs": 12630}, {"Word": "each", "OffsetStartMs": 12630, "OffsetEndMs": 12765}, {"Word": "other", "OffsetStartMs": 12765, "OffsetEndMs": 12915}, {"Word": "or", "OffsetStartMs": 12915, "OffsetEndMs": 13185}, {"Word": "samples", "OffsetStartMs": 13185, "OffsetEndMs": 13590}, {"Word": "that", "OffsetStartMs": 13590, "OffsetEndMs": 13695}, {"Word": "are", "OffsetStartMs": 13695, "OffsetEndMs": 13830}, {"Word": "dissimilar", "OffsetStartMs": 13830, "OffsetEndMs": 14400}, {"Word": "to", "OffsetStartMs": 14400, "OffsetEndMs": 14550}, {"Word": "each", "OffsetStartMs": 14550, "OffsetEndMs": 14655}, {"Word": "other", "OffsetStartMs": 14655, "OffsetEndMs": 14895}, {"Word": "in", "OffsetStartMs": 14895, "OffsetEndMs": 15120}, {"Word": "the", "OffsetStartMs": 15120, "OffsetEndMs": 15345}, {"Word": "input", "OffsetStartMs": 15345, "OffsetEndMs": 15710}, {"Word": "should", "OffsetStartMs": 15940, "OffsetEndMs": 16215}, {"Word": "decode", "OffsetStartMs": 16215, "OffsetEndMs": 16760}, {"Word": "encode", "OffsetStartMs": 16960, "OffsetEndMs": 17445}, {"Word": "to", "OffsetStartMs": 17445, "OffsetEndMs": 17550}, {"Word": "latent", "OffsetStartMs": 17550, "OffsetEndMs": 17850}, {"Word": "vectors", "OffsetStartMs": 17850, "OffsetEndMs": 18225}, {"Word": "that", "OffsetStartMs": 18225, "OffsetEndMs": 18420}, {"Word": "are", "OffsetStartMs": 18420, "OffsetEndMs": 18680}, {"Word": "far", "OffsetStartMs": 18910, "OffsetEndMs": 19215}, {"Word": "from", "OffsetStartMs": 19215, "OffsetEndMs": 19365}, {"Word": "each", "OffsetStartMs": 19365, "OffsetEndMs": 19485}, {"Word": "other", "OffsetStartMs": 19485, "OffsetEndMs": 19710}, {"Word": "in", "OffsetStartMs": 19710, "OffsetEndMs": 19935}, {"Word": "the", "OffsetStartMs": 19935, "OffsetEndMs": 20070}, {"Word": "latent", "OffsetStartMs": 20070, "OffsetEndMs": 20370}, {"Word": "space", "OffsetStartMs": 20370, "OffsetEndMs": 20690}], "SpeechSpeed": 18.3}, {"FinalSentence": "So now we'll walk through step by step, a deb biasing algorithm that automatically uses the latent features learned by a variational auto encoder EM to under sample and over sample from regions in our data set EM. Before I start, I want to point out that this deb biasing model is actually the foundation of themos's work. This work comes out of a paper that we published a few years ago that has been demonstrated to deias commercial facial detection algorithms. And it was so impactful that we decided to make it available and work with companies and industries. And that's how themis was started.", "SliceSentence": "So now we'll walk through step by step a deb biasing algorithm that automatically uses the latent features learned by a variational auto encoder EM to under sample and over sample from regions in our data set EM . Before Istart I want to point out that this deb biasing model is actually the foundation of themos's work . Thiswork comes out of a paper that we published a few years ago that has been demonstrated to deias commercial facial detection algorithms . Andit was so impactful that we decided to make it available and work with companies and industries . Andthat's how themis was started", "StartMs": 1231960, "EndMs": 1266440, "WordsNum": 106, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 510}, {"Word": "now", "OffsetStartMs": 510, "OffsetEndMs": 825}, {"Word": "we'll", "OffsetStartMs": 825, "OffsetEndMs": 1050}, {"Word": "walk", "OffsetStartMs": 1050, "OffsetEndMs": 1215}, {"Word": "through", "OffsetStartMs": 1215, "OffsetEndMs": 1500}, {"Word": "step", "OffsetStartMs": 1500, "OffsetEndMs": 1740}, {"Word": "by", "OffsetStartMs": 1740, "OffsetEndMs": 1965}, {"Word": "step", "OffsetStartMs": 1965, "OffsetEndMs": 2235}, {"Word": "a", "OffsetStartMs": 2235, "OffsetEndMs": 2445}, {"Word": "deb", "OffsetStartMs": 2445, "OffsetEndMs": 2580}, {"Word": "biasing", "OffsetStartMs": 2580, "OffsetEndMs": 3090}, {"Word": "algorithm", "OffsetStartMs": 3090, "OffsetEndMs": 3495}, {"Word": "that", "OffsetStartMs": 3495, "OffsetEndMs": 3800}, {"Word": "automatically", "OffsetStartMs": 3820, "OffsetEndMs": 4215}, {"Word": "uses", "OffsetStartMs": 4215, "OffsetEndMs": 4610}, {"Word": "the", "OffsetStartMs": 4720, "OffsetEndMs": 5120}, {"Word": "latent", "OffsetStartMs": 5260, "OffsetEndMs": 5700}, {"Word": "features", "OffsetStartMs": 5700, "OffsetEndMs": 5960}, {"Word": "learned", "OffsetStartMs": 5980, "OffsetEndMs": 6330}, {"Word": "by", "OffsetStartMs": 6330, "OffsetEndMs": 6555}, {"Word": "a", "OffsetStartMs": 6555, "OffsetEndMs": 6675}, {"Word": "variational", "OffsetStartMs": 6675, "OffsetEndMs": 7110}, {"Word": "auto", "OffsetStartMs": 7110, "OffsetEndMs": 7410}, {"Word": "encoder", "OffsetStartMs": 7410, "OffsetEndMs": 8030}, {"Word": "EM", "OffsetStartMs": 8200, "OffsetEndMs": 8600}, {"Word": "to", "OffsetStartMs": 8680, "OffsetEndMs": 9080}, {"Word": "under", "OffsetStartMs": 9190, "OffsetEndMs": 9540}, {"Word": "sample", "OffsetStartMs": 9540, "OffsetEndMs": 9870}, {"Word": "and", "OffsetStartMs": 9870, "OffsetEndMs": 10140}, {"Word": "over", "OffsetStartMs": 10140, "OffsetEndMs": 10395}, {"Word": "sample", "OffsetStartMs": 10395, "OffsetEndMs": 10760}, {"Word": "from", "OffsetStartMs": 10840, "OffsetEndMs": 11115}, {"Word": "regions", "OffsetStartMs": 11115, "OffsetEndMs": 11385}, {"Word": "in", "OffsetStartMs": 11385, "OffsetEndMs": 11625}, {"Word": "our", "OffsetStartMs": 11625, "OffsetEndMs": 11760}, {"Word": "data", "OffsetStartMs": 11760, "OffsetEndMs": 12050}, {"Word": "set", "OffsetStartMs": 12100, "OffsetEndMs": 12500}, {"Word": "EM", "OffsetStartMs": 13180, "OffsetEndMs": 13530}, {"Word": ".", "OffsetStartMs": 13530, "OffsetEndMs": 13815}, {"Word": "Before", "OffsetStartMs": 13815, "OffsetEndMs": 14055}, {"Word": "Istart", "OffsetStartMs": 14055, "OffsetEndMs": 14265}, {"Word": "I", "OffsetStartMs": 14265, "OffsetEndMs": 14445}, {"Word": "want", "OffsetStartMs": 14445, "OffsetEndMs": 14580}, {"Word": "to", "OffsetStartMs": 14580, "OffsetEndMs": 14715}, {"Word": "point", "OffsetStartMs": 14715, "OffsetEndMs": 14850}, {"Word": "out", "OffsetStartMs": 14850, "OffsetEndMs": 15015}, {"Word": "that", "OffsetStartMs": 15015, "OffsetEndMs": 15255}, {"Word": "this", "OffsetStartMs": 15255, "OffsetEndMs": 15510}, {"Word": "deb", "OffsetStartMs": 15510, "OffsetEndMs": 15690}, {"Word": "biasing", "OffsetStartMs": 15690, "OffsetEndMs": 16080}, {"Word": "model", "OffsetStartMs": 16080, "OffsetEndMs": 16290}, {"Word": "is", "OffsetStartMs": 16290, "OffsetEndMs": 16640}, {"Word": "actually", "OffsetStartMs": 16690, "OffsetEndMs": 16995}, {"Word": "the", "OffsetStartMs": 16995, "OffsetEndMs": 17205}, {"Word": "foundation", "OffsetStartMs": 17205, "OffsetEndMs": 17510}, {"Word": "of", "OffsetStartMs": 17770, "OffsetEndMs": 18045}, {"Word": "themos's", "OffsetStartMs": 18045, "OffsetEndMs": 18555}, {"Word": "work", "OffsetStartMs": 18555, "OffsetEndMs": 18830}, {"Word": ".", "OffsetStartMs": 19240, "OffsetEndMs": 19545}, {"Word": "Thiswork", "OffsetStartMs": 19545, "OffsetEndMs": 19740}, {"Word": "comes", "OffsetStartMs": 19740, "OffsetEndMs": 19920}, {"Word": "out", "OffsetStartMs": 19920, "OffsetEndMs": 20085}, {"Word": "of", "OffsetStartMs": 20085, "OffsetEndMs": 20190}, {"Word": "a", "OffsetStartMs": 20190, "OffsetEndMs": 20310}, {"Word": "paper", "OffsetStartMs": 20310, "OffsetEndMs": 20580}, {"Word": "that", "OffsetStartMs": 20580, "OffsetEndMs": 20820}, {"Word": "we", "OffsetStartMs": 20820, "OffsetEndMs": 20970}, {"Word": "published", "OffsetStartMs": 20970, "OffsetEndMs": 21225}, {"Word": "a", "OffsetStartMs": 21225, "OffsetEndMs": 21465}, {"Word": "few", "OffsetStartMs": 21465, "OffsetEndMs": 21600}, {"Word": "years", "OffsetStartMs": 21600, "OffsetEndMs": 21825}, {"Word": "ago", "OffsetStartMs": 21825, "OffsetEndMs": 22190}, {"Word": "that", "OffsetStartMs": 22270, "OffsetEndMs": 22515}, {"Word": "has", "OffsetStartMs": 22515, "OffsetEndMs": 22635}, {"Word": "been", "OffsetStartMs": 22635, "OffsetEndMs": 22785}, {"Word": "demonstrated", "OffsetStartMs": 22785, "OffsetEndMs": 23355}, {"Word": "to", "OffsetStartMs": 23355, "OffsetEndMs": 23550}, {"Word": "deias", "OffsetStartMs": 23550, "OffsetEndMs": 23930}, {"Word": "commercial", "OffsetStartMs": 23980, "OffsetEndMs": 24345}, {"Word": "facial", "OffsetStartMs": 24345, "OffsetEndMs": 24765}, {"Word": "detection", "OffsetStartMs": 24765, "OffsetEndMs": 25215}, {"Word": "algorithms", "OffsetStartMs": 25215, "OffsetEndMs": 25730}, {"Word": ".", "OffsetStartMs": 25870, "OffsetEndMs": 26270}, {"Word": "Andit", "OffsetStartMs": 26860, "OffsetEndMs": 27135}, {"Word": "was", "OffsetStartMs": 27135, "OffsetEndMs": 27300}, {"Word": "so", "OffsetStartMs": 27300, "OffsetEndMs": 27540}, {"Word": "impactful", "OffsetStartMs": 27540, "OffsetEndMs": 28035}, {"Word": "that", "OffsetStartMs": 28035, "OffsetEndMs": 28185}, {"Word": "we", "OffsetStartMs": 28185, "OffsetEndMs": 28380}, {"Word": "decided", "OffsetStartMs": 28380, "OffsetEndMs": 28700}, {"Word": "to", "OffsetStartMs": 28750, "OffsetEndMs": 29150}, {"Word": "make", "OffsetStartMs": 29200, "OffsetEndMs": 29460}, {"Word": "it", "OffsetStartMs": 29460, "OffsetEndMs": 29595}, {"Word": "available", "OffsetStartMs": 29595, "OffsetEndMs": 29870}, {"Word": "and", "OffsetStartMs": 29950, "OffsetEndMs": 30350}, {"Word": "work", "OffsetStartMs": 30730, "OffsetEndMs": 31020}, {"Word": "with", "OffsetStartMs": 31020, "OffsetEndMs": 31200}, {"Word": "companies", "OffsetStartMs": 31200, "OffsetEndMs": 31470}, {"Word": "and", "OffsetStartMs": 31470, "OffsetEndMs": 31850}, {"Word": "industries", "OffsetStartMs": 31900, "OffsetEndMs": 32175}, {"Word": ".", "OffsetStartMs": 32175, "OffsetEndMs": 32310}, {"Word": "Andthat's", "OffsetStartMs": 32310, "OffsetEndMs": 32565}, {"Word": "how", "OffsetStartMs": 32565, "OffsetEndMs": 32715}, {"Word": "themis", "OffsetStartMs": 32715, "OffsetEndMs": 33045}, {"Word": "was", "OffsetStartMs": 33045, "OffsetEndMs": 33240}, {"Word": "started", "OffsetStartMs": 33240, "OffsetEndMs": 33530}], "SpeechSpeed": 17.2}, {"FinalSentence": "So let's first start by training a vae on this data set. The z shown here in this diagram ends up being our latent space, and the latent space automatically captures features that were important for classification.", "SliceSentence": "So let's first start by training a vae on this data set . Thez shown here in this diagram ends up being our latent space and the latent space automatically captures features that were important for classification", "StartMs": 1266540, "EndMs": 1280800, "WordsNum": 37, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "let's", "OffsetStartMs": 700, "OffsetEndMs": 1080}, {"Word": "first", "OffsetStartMs": 1080, "OffsetEndMs": 1275}, {"Word": "start", "OffsetStartMs": 1275, "OffsetEndMs": 1530}, {"Word": "by", "OffsetStartMs": 1530, "OffsetEndMs": 1755}, {"Word": "training", "OffsetStartMs": 1755, "OffsetEndMs": 2025}, {"Word": "a", "OffsetStartMs": 2025, "OffsetEndMs": 2265}, {"Word": "vae", "OffsetStartMs": 2265, "OffsetEndMs": 2630}, {"Word": "on", "OffsetStartMs": 2680, "OffsetEndMs": 2985}, {"Word": "this", "OffsetStartMs": 2985, "OffsetEndMs": 3180}, {"Word": "data", "OffsetStartMs": 3180, "OffsetEndMs": 3390}, {"Word": "set", "OffsetStartMs": 3390, "OffsetEndMs": 3710}, {"Word": ".", "OffsetStartMs": 4090, "OffsetEndMs": 4395}, {"Word": "Thez", "OffsetStartMs": 4395, "OffsetEndMs": 4650}, {"Word": "shown", "OffsetStartMs": 4650, "OffsetEndMs": 4935}, {"Word": "here", "OffsetStartMs": 4935, "OffsetEndMs": 5190}, {"Word": "in", "OffsetStartMs": 5190, "OffsetEndMs": 5370}, {"Word": "this", "OffsetStartMs": 5370, "OffsetEndMs": 5505}, {"Word": "diagram", "OffsetStartMs": 5505, "OffsetEndMs": 6050}, {"Word": "ends", "OffsetStartMs": 6250, "OffsetEndMs": 6540}, {"Word": "up", "OffsetStartMs": 6540, "OffsetEndMs": 6720}, {"Word": "being", "OffsetStartMs": 6720, "OffsetEndMs": 6960}, {"Word": "our", "OffsetStartMs": 6960, "OffsetEndMs": 7215}, {"Word": "latent", "OffsetStartMs": 7215, "OffsetEndMs": 7575}, {"Word": "space", "OffsetStartMs": 7575, "OffsetEndMs": 7880}, {"Word": "and", "OffsetStartMs": 8260, "OffsetEndMs": 8655}, {"Word": "the", "OffsetStartMs": 8655, "OffsetEndMs": 8955}, {"Word": "latent", "OffsetStartMs": 8955, "OffsetEndMs": 9300}, {"Word": "space", "OffsetStartMs": 9300, "OffsetEndMs": 9590}, {"Word": "automatically", "OffsetStartMs": 9880, "OffsetEndMs": 10280}, {"Word": "captures", "OffsetStartMs": 10390, "OffsetEndMs": 10980}, {"Word": "features", "OffsetStartMs": 10980, "OffsetEndMs": 11300}, {"Word": "that", "OffsetStartMs": 11440, "OffsetEndMs": 11715}, {"Word": "were", "OffsetStartMs": 11715, "OffsetEndMs": 11925}, {"Word": "important", "OffsetStartMs": 11925, "OffsetEndMs": 12255}, {"Word": "for", "OffsetStartMs": 12255, "OffsetEndMs": 12525}, {"Word": "classification", "OffsetStartMs": 12525, "OffsetEndMs": 13010}], "SpeechSpeed": 14.8}, {"FinalSentence": "So here's an example, latent feature that this model captured EM. This is the facial position of an input face. And something that's really crucial here is that we never told the model to calculate, to encode the feature vector of the facial position of a given face. It learned this automatically, because this feature is important for the model to develop a good representation of what a face actually is.", "SliceSentence": "So here's an example latent feature that this model captured EM . Thisis the facial position of an input face . Andsomething that's really crucial here is that we never told the model to calculate to encode the feature vector of the facial position of a given face . Itlearned this automatically because this feature is important for the model to develop a good representation of what a face actually is", "StartMs": 1280800, "EndMs": 1307740, "WordsNum": 71, "Words": [{"Word": "So", "OffsetStartMs": 70, "OffsetEndMs": 470}, {"Word": "here's", "OffsetStartMs": 850, "OffsetEndMs": 1215}, {"Word": "an", "OffsetStartMs": 1215, "OffsetEndMs": 1365}, {"Word": "example", "OffsetStartMs": 1365, "OffsetEndMs": 1635}, {"Word": "latent", "OffsetStartMs": 1635, "OffsetEndMs": 2010}, {"Word": "feature", "OffsetStartMs": 2010, "OffsetEndMs": 2250}, {"Word": "that", "OffsetStartMs": 2250, "OffsetEndMs": 2490}, {"Word": "this", "OffsetStartMs": 2490, "OffsetEndMs": 2655}, {"Word": "model", "OffsetStartMs": 2655, "OffsetEndMs": 2925}, {"Word": "captured", "OffsetStartMs": 2925, "OffsetEndMs": 3290}, {"Word": "EM", "OffsetStartMs": 3730, "OffsetEndMs": 4095}, {"Word": ".", "OffsetStartMs": 4095, "OffsetEndMs": 4320}, {"Word": "Thisis", "OffsetStartMs": 4320, "OffsetEndMs": 4470}, {"Word": "the", "OffsetStartMs": 4470, "OffsetEndMs": 4620}, {"Word": "facial", "OffsetStartMs": 4620, "OffsetEndMs": 4965}, {"Word": "position", "OffsetStartMs": 4965, "OffsetEndMs": 5220}, {"Word": "of", "OffsetStartMs": 5220, "OffsetEndMs": 5415}, {"Word": "an", "OffsetStartMs": 5415, "OffsetEndMs": 5625}, {"Word": "input", "OffsetStartMs": 5625, "OffsetEndMs": 5880}, {"Word": "face", "OffsetStartMs": 5880, "OffsetEndMs": 6170}, {"Word": ".", "OffsetStartMs": 6640, "OffsetEndMs": 7040}, {"Word": "Andsomething", "OffsetStartMs": 7330, "OffsetEndMs": 7650}, {"Word": "that's", "OffsetStartMs": 7650, "OffsetEndMs": 7935}, {"Word": "really", "OffsetStartMs": 7935, "OffsetEndMs": 8190}, {"Word": "crucial", "OffsetStartMs": 8190, "OffsetEndMs": 8550}, {"Word": "here", "OffsetStartMs": 8550, "OffsetEndMs": 8925}, {"Word": "is", "OffsetStartMs": 8925, "OffsetEndMs": 9195}, {"Word": "that", "OffsetStartMs": 9195, "OffsetEndMs": 9420}, {"Word": "we", "OffsetStartMs": 9420, "OffsetEndMs": 9735}, {"Word": "never", "OffsetStartMs": 9735, "OffsetEndMs": 10080}, {"Word": "told", "OffsetStartMs": 10080, "OffsetEndMs": 10365}, {"Word": "the", "OffsetStartMs": 10365, "OffsetEndMs": 10545}, {"Word": "model", "OffsetStartMs": 10545, "OffsetEndMs": 10820}, {"Word": "to", "OffsetStartMs": 11140, "OffsetEndMs": 11540}, {"Word": "calculate", "OffsetStartMs": 12070, "OffsetEndMs": 12770}, {"Word": "to", "OffsetStartMs": 13210, "OffsetEndMs": 13610}, {"Word": "encode", "OffsetStartMs": 13660, "OffsetEndMs": 14175}, {"Word": "the", "OffsetStartMs": 14175, "OffsetEndMs": 14385}, {"Word": "feature", "OffsetStartMs": 14385, "OffsetEndMs": 14670}, {"Word": "vector", "OffsetStartMs": 14670, "OffsetEndMs": 15015}, {"Word": "of", "OffsetStartMs": 15015, "OffsetEndMs": 15380}, {"Word": "the", "OffsetStartMs": 15850, "OffsetEndMs": 16110}, {"Word": "facial", "OffsetStartMs": 16110, "OffsetEndMs": 16440}, {"Word": "position", "OffsetStartMs": 16440, "OffsetEndMs": 16740}, {"Word": "of", "OffsetStartMs": 16740, "OffsetEndMs": 16980}, {"Word": "a", "OffsetStartMs": 16980, "OffsetEndMs": 17085}, {"Word": "given", "OffsetStartMs": 17085, "OffsetEndMs": 17295}, {"Word": "face", "OffsetStartMs": 17295, "OffsetEndMs": 17660}, {"Word": ".", "OffsetStartMs": 17920, "OffsetEndMs": 18225}, {"Word": "Itlearned", "OffsetStartMs": 18225, "OffsetEndMs": 18495}, {"Word": "this", "OffsetStartMs": 18495, "OffsetEndMs": 18860}, {"Word": "automatically", "OffsetStartMs": 19150, "OffsetEndMs": 19550}, {"Word": "because", "OffsetStartMs": 20170, "OffsetEndMs": 20505}, {"Word": "this", "OffsetStartMs": 20505, "OffsetEndMs": 20760}, {"Word": "feature", "OffsetStartMs": 20760, "OffsetEndMs": 21060}, {"Word": "is", "OffsetStartMs": 21060, "OffsetEndMs": 21435}, {"Word": "important", "OffsetStartMs": 21435, "OffsetEndMs": 21795}, {"Word": "for", "OffsetStartMs": 21795, "OffsetEndMs": 22005}, {"Word": "the", "OffsetStartMs": 22005, "OffsetEndMs": 22095}, {"Word": "model", "OffsetStartMs": 22095, "OffsetEndMs": 22305}, {"Word": "to", "OffsetStartMs": 22305, "OffsetEndMs": 22665}, {"Word": "develop", "OffsetStartMs": 22665, "OffsetEndMs": 22995}, {"Word": "a", "OffsetStartMs": 22995, "OffsetEndMs": 23235}, {"Word": "good", "OffsetStartMs": 23235, "OffsetEndMs": 23445}, {"Word": "representation", "OffsetStartMs": 23445, "OffsetEndMs": 24180}, {"Word": "of", "OffsetStartMs": 24180, "OffsetEndMs": 24435}, {"Word": "what", "OffsetStartMs": 24435, "OffsetEndMs": 24585}, {"Word": "a", "OffsetStartMs": 24585, "OffsetEndMs": 24765}, {"Word": "face", "OffsetStartMs": 24765, "OffsetEndMs": 25070}, {"Word": "actually", "OffsetStartMs": 25270, "OffsetEndMs": 25605}, {"Word": "is", "OffsetStartMs": 25605, "OffsetEndMs": 25940}], "SpeechSpeed": 14.8}, {"FinalSentence": "So now that we have our latent structure, we can use it to calculate a distribution of the inputs across every latent variable, and we can estimate a probability distribution depending on that's based on the features of every item in this data set. Essentially, what this means is that we can calculate the probability that a certain combination of features appears in our data set based on the latent space that we just learned.", "SliceSentence": "So now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on that's based on the features of every item in this data set . Essentiallywhat this means is that we can calculate the probability that a certain combination of features appears in our data set based on the latent space that we just learned", "StartMs": 1308500, "EndMs": 1333640, "WordsNum": 75, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "now", "OffsetStartMs": 610, "OffsetEndMs": 885}, {"Word": "that", "OffsetStartMs": 885, "OffsetEndMs": 1005}, {"Word": "we", "OffsetStartMs": 1005, "OffsetEndMs": 1140}, {"Word": "have", "OffsetStartMs": 1140, "OffsetEndMs": 1320}, {"Word": "our", "OffsetStartMs": 1320, "OffsetEndMs": 1485}, {"Word": "latent", "OffsetStartMs": 1485, "OffsetEndMs": 1785}, {"Word": "structure", "OffsetStartMs": 1785, "OffsetEndMs": 2090}, {"Word": "we", "OffsetStartMs": 2410, "OffsetEndMs": 2670}, {"Word": "can", "OffsetStartMs": 2670, "OffsetEndMs": 2820}, {"Word": "use", "OffsetStartMs": 2820, "OffsetEndMs": 3015}, {"Word": "it", "OffsetStartMs": 3015, "OffsetEndMs": 3225}, {"Word": "to", "OffsetStartMs": 3225, "OffsetEndMs": 3405}, {"Word": "calculate", "OffsetStartMs": 3405, "OffsetEndMs": 3915}, {"Word": "a", "OffsetStartMs": 3915, "OffsetEndMs": 4215}, {"Word": "distribution", "OffsetStartMs": 4215, "OffsetEndMs": 4610}, {"Word": "of", "OffsetStartMs": 4720, "OffsetEndMs": 4995}, {"Word": "the", "OffsetStartMs": 4995, "OffsetEndMs": 5235}, {"Word": "inputs", "OffsetStartMs": 5235, "OffsetEndMs": 5700}, {"Word": "across", "OffsetStartMs": 5700, "OffsetEndMs": 6030}, {"Word": "every", "OffsetStartMs": 6030, "OffsetEndMs": 6360}, {"Word": "latent", "OffsetStartMs": 6360, "OffsetEndMs": 6780}, {"Word": "variable", "OffsetStartMs": 6780, "OffsetEndMs": 7040}, {"Word": "and", "OffsetStartMs": 7570, "OffsetEndMs": 7845}, {"Word": "we", "OffsetStartMs": 7845, "OffsetEndMs": 7980}, {"Word": "can", "OffsetStartMs": 7980, "OffsetEndMs": 8240}, {"Word": "estimate", "OffsetStartMs": 8320, "OffsetEndMs": 8595}, {"Word": "a", "OffsetStartMs": 8595, "OffsetEndMs": 8775}, {"Word": "probability", "OffsetStartMs": 8775, "OffsetEndMs": 9320}, {"Word": "distribution", "OffsetStartMs": 9370, "OffsetEndMs": 9770}, {"Word": "depending", "OffsetStartMs": 10330, "OffsetEndMs": 10710}, {"Word": "on", "OffsetStartMs": 10710, "OffsetEndMs": 11090}, {"Word": "that's", "OffsetStartMs": 11500, "OffsetEndMs": 11835}, {"Word": "based", "OffsetStartMs": 11835, "OffsetEndMs": 12015}, {"Word": "on", "OffsetStartMs": 12015, "OffsetEndMs": 12255}, {"Word": "the", "OffsetStartMs": 12255, "OffsetEndMs": 12465}, {"Word": "features", "OffsetStartMs": 12465, "OffsetEndMs": 12770}, {"Word": "of", "OffsetStartMs": 12910, "OffsetEndMs": 13200}, {"Word": "every", "OffsetStartMs": 13200, "OffsetEndMs": 13440}, {"Word": "item", "OffsetStartMs": 13440, "OffsetEndMs": 13740}, {"Word": "in", "OffsetStartMs": 13740, "OffsetEndMs": 13965}, {"Word": "this", "OffsetStartMs": 13965, "OffsetEndMs": 14115}, {"Word": "data", "OffsetStartMs": 14115, "OffsetEndMs": 14310}, {"Word": "set", "OffsetStartMs": 14310, "OffsetEndMs": 14630}, {"Word": ".", "OffsetStartMs": 15370, "OffsetEndMs": 15770}, {"Word": "Essentiallywhat", "OffsetStartMs": 15850, "OffsetEndMs": 16140}, {"Word": "this", "OffsetStartMs": 16140, "OffsetEndMs": 16335}, {"Word": "means", "OffsetStartMs": 16335, "OffsetEndMs": 16620}, {"Word": "is", "OffsetStartMs": 16620, "OffsetEndMs": 16860}, {"Word": "that", "OffsetStartMs": 16860, "OffsetEndMs": 17025}, {"Word": "we", "OffsetStartMs": 17025, "OffsetEndMs": 17190}, {"Word": "can", "OffsetStartMs": 17190, "OffsetEndMs": 17355}, {"Word": "calculate", "OffsetStartMs": 17355, "OffsetEndMs": 17835}, {"Word": "the", "OffsetStartMs": 17835, "OffsetEndMs": 17985}, {"Word": "probability", "OffsetStartMs": 17985, "OffsetEndMs": 18500}, {"Word": "that", "OffsetStartMs": 18580, "OffsetEndMs": 18825}, {"Word": "a", "OffsetStartMs": 18825, "OffsetEndMs": 18990}, {"Word": "certain", "OffsetStartMs": 18990, "OffsetEndMs": 19275}, {"Word": "combination", "OffsetStartMs": 19275, "OffsetEndMs": 19640}, {"Word": "of", "OffsetStartMs": 19750, "OffsetEndMs": 20040}, {"Word": "features", "OffsetStartMs": 20040, "OffsetEndMs": 20330}, {"Word": "appears", "OffsetStartMs": 20590, "OffsetEndMs": 20925}, {"Word": "in", "OffsetStartMs": 20925, "OffsetEndMs": 21090}, {"Word": "our", "OffsetStartMs": 21090, "OffsetEndMs": 21195}, {"Word": "data", "OffsetStartMs": 21195, "OffsetEndMs": 21405}, {"Word": "set", "OffsetStartMs": 21405, "OffsetEndMs": 21740}, {"Word": "based", "OffsetStartMs": 21880, "OffsetEndMs": 22230}, {"Word": "on", "OffsetStartMs": 22230, "OffsetEndMs": 22440}, {"Word": "the", "OffsetStartMs": 22440, "OffsetEndMs": 22545}, {"Word": "latent", "OffsetStartMs": 22545, "OffsetEndMs": 22830}, {"Word": "space", "OffsetStartMs": 22830, "OffsetEndMs": 23025}, {"Word": "that", "OffsetStartMs": 23025, "OffsetEndMs": 23205}, {"Word": "we", "OffsetStartMs": 23205, "OffsetEndMs": 23370}, {"Word": "just", "OffsetStartMs": 23370, "OffsetEndMs": 23660}, {"Word": "learned", "OffsetStartMs": 24130, "OffsetEndMs": 24530}], "SpeechSpeed": 16.9}, {"FinalSentence": "And then we can over sample denser, sparser areas of this data set and under sample from denser areas of this data set.", "SliceSentence": "And then we can over sample denser sparser areas of this data set and under sample from denser areas of this data set", "StartMs": 1333640, "EndMs": 1340640, "WordsNum": 23, "Words": [{"Word": "And", "OffsetStartMs": 0, "OffsetEndMs": 150}, {"Word": "then", "OffsetStartMs": 150, "OffsetEndMs": 285}, {"Word": "we", "OffsetStartMs": 285, "OffsetEndMs": 420}, {"Word": "can", "OffsetStartMs": 420, "OffsetEndMs": 555}, {"Word": "over", "OffsetStartMs": 555, "OffsetEndMs": 810}, {"Word": "sample", "OffsetStartMs": 810, "OffsetEndMs": 1170}, {"Word": "denser", "OffsetStartMs": 1170, "OffsetEndMs": 1760}, {"Word": "sparser", "OffsetStartMs": 2200, "OffsetEndMs": 2700}, {"Word": "areas", "OffsetStartMs": 2700, "OffsetEndMs": 2910}, {"Word": "of", "OffsetStartMs": 2910, "OffsetEndMs": 3120}, {"Word": "this", "OffsetStartMs": 3120, "OffsetEndMs": 3255}, {"Word": "data", "OffsetStartMs": 3255, "OffsetEndMs": 3465}, {"Word": "set", "OffsetStartMs": 3465, "OffsetEndMs": 3735}, {"Word": "and", "OffsetStartMs": 3735, "OffsetEndMs": 3960}, {"Word": "under", "OffsetStartMs": 3960, "OffsetEndMs": 4215}, {"Word": "sample", "OffsetStartMs": 4215, "OffsetEndMs": 4545}, {"Word": "from", "OffsetStartMs": 4545, "OffsetEndMs": 4785}, {"Word": "denser", "OffsetStartMs": 4785, "OffsetEndMs": 5145}, {"Word": "areas", "OffsetStartMs": 5145, "OffsetEndMs": 5385}, {"Word": "of", "OffsetStartMs": 5385, "OffsetEndMs": 5625}, {"Word": "this", "OffsetStartMs": 5625, "OffsetEndMs": 5775}, {"Word": "data", "OffsetStartMs": 5775, "OffsetEndMs": 5970}, {"Word": "set", "OffsetStartMs": 5970, "OffsetEndMs": 6290}], "SpeechSpeed": 16.7}, {"FinalSentence": "So let's say our distribution looks something like this. This is an oversimplification, but for visualization purposes and the denser portions of this data set, we would expect to have a homogeneous skin color and pose in hair color and very good lighting. And then in the sparser portions of this data set, we would expect to see diverse skin color, pose and illumination.", "SliceSentence": "So let's say our distribution looks something like this . Thisis an oversimplification but for visualization purposes and the denser portions of this data set we would expect to have a homogeneous skin color and pose in hair color and very good lighting . Andthen in the sparser portions of this data set we would expect to see diverse skin color pose and illumination", "StartMs": 1340640, "EndMs": 1363020, "WordsNum": 64, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "let's", "OffsetStartMs": 730, "OffsetEndMs": 1080}, {"Word": "say", "OffsetStartMs": 1080, "OffsetEndMs": 1170}, {"Word": "our", "OffsetStartMs": 1170, "OffsetEndMs": 1350}, {"Word": "distribution", "OffsetStartMs": 1350, "OffsetEndMs": 1665}, {"Word": "looks", "OffsetStartMs": 1665, "OffsetEndMs": 1935}, {"Word": "something", "OffsetStartMs": 1935, "OffsetEndMs": 2175}, {"Word": "like", "OffsetStartMs": 2175, "OffsetEndMs": 2445}, {"Word": "this", "OffsetStartMs": 2445, "OffsetEndMs": 2715}, {"Word": ".", "OffsetStartMs": 2715, "OffsetEndMs": 2925}, {"Word": "Thisis", "OffsetStartMs": 2925, "OffsetEndMs": 3015}, {"Word": "an", "OffsetStartMs": 3015, "OffsetEndMs": 3135}, {"Word": "oversimplification", "OffsetStartMs": 3135, "OffsetEndMs": 3860}, {"Word": "but", "OffsetStartMs": 3970, "OffsetEndMs": 4245}, {"Word": "for", "OffsetStartMs": 4245, "OffsetEndMs": 4380}, {"Word": "visualization", "OffsetStartMs": 4380, "OffsetEndMs": 4950}, {"Word": "purposes", "OffsetStartMs": 4950, "OffsetEndMs": 5300}, {"Word": "and", "OffsetStartMs": 5950, "OffsetEndMs": 6350}, {"Word": "the", "OffsetStartMs": 6550, "OffsetEndMs": 6885}, {"Word": "denser", "OffsetStartMs": 6885, "OffsetEndMs": 7350}, {"Word": "portions", "OffsetStartMs": 7350, "OffsetEndMs": 7740}, {"Word": "of", "OffsetStartMs": 7740, "OffsetEndMs": 7890}, {"Word": "this", "OffsetStartMs": 7890, "OffsetEndMs": 8025}, {"Word": "data", "OffsetStartMs": 8025, "OffsetEndMs": 8250}, {"Word": "set", "OffsetStartMs": 8250, "OffsetEndMs": 8600}, {"Word": "we", "OffsetStartMs": 8740, "OffsetEndMs": 9000}, {"Word": "would", "OffsetStartMs": 9000, "OffsetEndMs": 9195}, {"Word": "expect", "OffsetStartMs": 9195, "OffsetEndMs": 9480}, {"Word": "to", "OffsetStartMs": 9480, "OffsetEndMs": 9690}, {"Word": "have", "OffsetStartMs": 9690, "OffsetEndMs": 9950}, {"Word": "a", "OffsetStartMs": 10390, "OffsetEndMs": 10665}, {"Word": "homogeneous", "OffsetStartMs": 10665, "OffsetEndMs": 11235}, {"Word": "skin", "OffsetStartMs": 11235, "OffsetEndMs": 11460}, {"Word": "color", "OffsetStartMs": 11460, "OffsetEndMs": 11750}, {"Word": "and", "OffsetStartMs": 11830, "OffsetEndMs": 12120}, {"Word": "pose", "OffsetStartMs": 12120, "OffsetEndMs": 12440}, {"Word": "in", "OffsetStartMs": 12490, "OffsetEndMs": 12765}, {"Word": "hair", "OffsetStartMs": 12765, "OffsetEndMs": 12945}, {"Word": "color", "OffsetStartMs": 12945, "OffsetEndMs": 13245}, {"Word": "and", "OffsetStartMs": 13245, "OffsetEndMs": 13515}, {"Word": "very", "OffsetStartMs": 13515, "OffsetEndMs": 13740}, {"Word": "good", "OffsetStartMs": 13740, "OffsetEndMs": 14025}, {"Word": "lighting", "OffsetStartMs": 14025, "OffsetEndMs": 14360}, {"Word": ".", "OffsetStartMs": 15130, "OffsetEndMs": 15405}, {"Word": "Andthen", "OffsetStartMs": 15405, "OffsetEndMs": 15540}, {"Word": "in", "OffsetStartMs": 15540, "OffsetEndMs": 15675}, {"Word": "the", "OffsetStartMs": 15675, "OffsetEndMs": 15825}, {"Word": "sparser", "OffsetStartMs": 15825, "OffsetEndMs": 16215}, {"Word": "portions", "OffsetStartMs": 16215, "OffsetEndMs": 16515}, {"Word": "of", "OffsetStartMs": 16515, "OffsetEndMs": 16650}, {"Word": "this", "OffsetStartMs": 16650, "OffsetEndMs": 16800}, {"Word": "data", "OffsetStartMs": 16800, "OffsetEndMs": 17025}, {"Word": "set", "OffsetStartMs": 17025, "OffsetEndMs": 17360}, {"Word": "we", "OffsetStartMs": 17440, "OffsetEndMs": 17685}, {"Word": "would", "OffsetStartMs": 17685, "OffsetEndMs": 17880}, {"Word": "expect", "OffsetStartMs": 17880, "OffsetEndMs": 18165}, {"Word": "to", "OffsetStartMs": 18165, "OffsetEndMs": 18390}, {"Word": "see", "OffsetStartMs": 18390, "OffsetEndMs": 18615}, {"Word": "diverse", "OffsetStartMs": 18615, "OffsetEndMs": 18950}, {"Word": "skin", "OffsetStartMs": 19060, "OffsetEndMs": 19350}, {"Word": "color", "OffsetStartMs": 19350, "OffsetEndMs": 19640}, {"Word": "pose", "OffsetStartMs": 19900, "OffsetEndMs": 20360}, {"Word": "and", "OffsetStartMs": 20410, "OffsetEndMs": 20670}, {"Word": "illumination", "OffsetStartMs": 20670, "OffsetEndMs": 21200}], "SpeechSpeed": 16.4}, {"FinalSentence": "So now that we have this distribution and we know what areas of our distribution are dense and which areas are sparse, we want to under sample areas from the under sample samples that fall in the denser areas of this distribution and over sample data points that fall in the sparser areas of this distribution. So for example, we would probably under sample points with the very common skin colors, hair colors and good lighting that is extremely present in this data set and over sample the diverse images that we saw on the last slide. And this allows us to train in a fair and unbiased manner.", "SliceSentence": "So now that we have this distribution and we know what areas of our distribution are dense and which areas are sparse we want to under sample areas from the under sample samples that fall in the denser areas of this distribution and over sample data points that fall in the sparser areas of this distribution . Sofor example we would probably under sample points with the very common skin colors hair colors and good lighting that is extremely present in this data set and over sample the diverse images that we saw on the last slide . Andthis allows us to train in a fair and unbiased manner", "StartMs": 1366180, "EndMs": 1402000, "WordsNum": 109, "Words": [{"Word": "So", "OffsetStartMs": 100, "OffsetEndMs": 465}, {"Word": "now", "OffsetStartMs": 465, "OffsetEndMs": 705}, {"Word": "that", "OffsetStartMs": 705, "OffsetEndMs": 840}, {"Word": "we", "OffsetStartMs": 840, "OffsetEndMs": 990}, {"Word": "have", "OffsetStartMs": 990, "OffsetEndMs": 1200}, {"Word": "this", "OffsetStartMs": 1200, "OffsetEndMs": 1485}, {"Word": "distribution", "OffsetStartMs": 1485, "OffsetEndMs": 1850}, {"Word": "and", "OffsetStartMs": 1900, "OffsetEndMs": 2175}, {"Word": "we", "OffsetStartMs": 2175, "OffsetEndMs": 2340}, {"Word": "know", "OffsetStartMs": 2340, "OffsetEndMs": 2565}, {"Word": "what", "OffsetStartMs": 2565, "OffsetEndMs": 2775}, {"Word": "areas", "OffsetStartMs": 2775, "OffsetEndMs": 3050}, {"Word": "of", "OffsetStartMs": 3100, "OffsetEndMs": 3360}, {"Word": "our", "OffsetStartMs": 3360, "OffsetEndMs": 3585}, {"Word": "distribution", "OffsetStartMs": 3585, "OffsetEndMs": 3950}, {"Word": "are", "OffsetStartMs": 3970, "OffsetEndMs": 4305}, {"Word": "dense", "OffsetStartMs": 4305, "OffsetEndMs": 4635}, {"Word": "and", "OffsetStartMs": 4635, "OffsetEndMs": 4905}, {"Word": "which", "OffsetStartMs": 4905, "OffsetEndMs": 5085}, {"Word": "areas", "OffsetStartMs": 5085, "OffsetEndMs": 5355}, {"Word": "are", "OffsetStartMs": 5355, "OffsetEndMs": 5655}, {"Word": "sparse", "OffsetStartMs": 5655, "OffsetEndMs": 6200}, {"Word": "we", "OffsetStartMs": 6370, "OffsetEndMs": 6645}, {"Word": "want", "OffsetStartMs": 6645, "OffsetEndMs": 6915}, {"Word": "to", "OffsetStartMs": 6915, "OffsetEndMs": 7200}, {"Word": "under", "OffsetStartMs": 7200, "OffsetEndMs": 7470}, {"Word": "sample", "OffsetStartMs": 7470, "OffsetEndMs": 7830}, {"Word": "areas", "OffsetStartMs": 7830, "OffsetEndMs": 8210}, {"Word": "from", "OffsetStartMs": 8380, "OffsetEndMs": 8715}, {"Word": "the", "OffsetStartMs": 8715, "OffsetEndMs": 9045}, {"Word": "under", "OffsetStartMs": 9045, "OffsetEndMs": 9390}, {"Word": "sample", "OffsetStartMs": 9390, "OffsetEndMs": 9720}, {"Word": "samples", "OffsetStartMs": 9720, "OffsetEndMs": 10155}, {"Word": "that", "OffsetStartMs": 10155, "OffsetEndMs": 10320}, {"Word": "fall", "OffsetStartMs": 10320, "OffsetEndMs": 10560}, {"Word": "in", "OffsetStartMs": 10560, "OffsetEndMs": 10770}, {"Word": "the", "OffsetStartMs": 10770, "OffsetEndMs": 10875}, {"Word": "denser", "OffsetStartMs": 10875, "OffsetEndMs": 11330}, {"Word": "areas", "OffsetStartMs": 11350, "OffsetEndMs": 11730}, {"Word": "of", "OffsetStartMs": 11730, "OffsetEndMs": 11970}, {"Word": "this", "OffsetStartMs": 11970, "OffsetEndMs": 12195}, {"Word": "distribution", "OffsetStartMs": 12195, "OffsetEndMs": 12560}, {"Word": "and", "OffsetStartMs": 12940, "OffsetEndMs": 13215}, {"Word": "over", "OffsetStartMs": 13215, "OffsetEndMs": 13455}, {"Word": "sample", "OffsetStartMs": 13455, "OffsetEndMs": 13820}, {"Word": "data", "OffsetStartMs": 14800, "OffsetEndMs": 15135}, {"Word": "points", "OffsetStartMs": 15135, "OffsetEndMs": 15375}, {"Word": "that", "OffsetStartMs": 15375, "OffsetEndMs": 15555}, {"Word": "fall", "OffsetStartMs": 15555, "OffsetEndMs": 15735}, {"Word": "in", "OffsetStartMs": 15735, "OffsetEndMs": 15900}, {"Word": "the", "OffsetStartMs": 15900, "OffsetEndMs": 16020}, {"Word": "sparser", "OffsetStartMs": 16020, "OffsetEndMs": 16425}, {"Word": "areas", "OffsetStartMs": 16425, "OffsetEndMs": 16665}, {"Word": "of", "OffsetStartMs": 16665, "OffsetEndMs": 16905}, {"Word": "this", "OffsetStartMs": 16905, "OffsetEndMs": 17160}, {"Word": "distribution", "OffsetStartMs": 17160, "OffsetEndMs": 17540}, {"Word": ".", "OffsetStartMs": 18580, "OffsetEndMs": 18930}, {"Word": "Sofor", "OffsetStartMs": 18930, "OffsetEndMs": 19170}, {"Word": "example", "OffsetStartMs": 19170, "OffsetEndMs": 19460}, {"Word": "we", "OffsetStartMs": 19600, "OffsetEndMs": 19845}, {"Word": "would", "OffsetStartMs": 19845, "OffsetEndMs": 19980}, {"Word": "probably", "OffsetStartMs": 19980, "OffsetEndMs": 20265}, {"Word": "under", "OffsetStartMs": 20265, "OffsetEndMs": 20640}, {"Word": "sample", "OffsetStartMs": 20640, "OffsetEndMs": 21000}, {"Word": "points", "OffsetStartMs": 21000, "OffsetEndMs": 21375}, {"Word": "with", "OffsetStartMs": 21375, "OffsetEndMs": 21770}, {"Word": "the", "OffsetStartMs": 22150, "OffsetEndMs": 22550}, {"Word": "very", "OffsetStartMs": 22690, "OffsetEndMs": 23025}, {"Word": "common", "OffsetStartMs": 23025, "OffsetEndMs": 23340}, {"Word": "skin", "OffsetStartMs": 23340, "OffsetEndMs": 23610}, {"Word": "colors", "OffsetStartMs": 23610, "OffsetEndMs": 23895}, {"Word": "hair", "OffsetStartMs": 23895, "OffsetEndMs": 24210}, {"Word": "colors", "OffsetStartMs": 24210, "OffsetEndMs": 24525}, {"Word": "and", "OffsetStartMs": 24525, "OffsetEndMs": 24920}, {"Word": "good", "OffsetStartMs": 24970, "OffsetEndMs": 25290}, {"Word": "lighting", "OffsetStartMs": 25290, "OffsetEndMs": 25605}, {"Word": "that", "OffsetStartMs": 25605, "OffsetEndMs": 25860}, {"Word": "is", "OffsetStartMs": 25860, "OffsetEndMs": 26120}, {"Word": "extremely", "OffsetStartMs": 26170, "OffsetEndMs": 26520}, {"Word": "present", "OffsetStartMs": 26520, "OffsetEndMs": 26790}, {"Word": "in", "OffsetStartMs": 26790, "OffsetEndMs": 26955}, {"Word": "this", "OffsetStartMs": 26955, "OffsetEndMs": 27090}, {"Word": "data", "OffsetStartMs": 27090, "OffsetEndMs": 27315}, {"Word": "set", "OffsetStartMs": 27315, "OffsetEndMs": 27650}, {"Word": "and", "OffsetStartMs": 27730, "OffsetEndMs": 28020}, {"Word": "over", "OffsetStartMs": 28020, "OffsetEndMs": 28275}, {"Word": "sample", "OffsetStartMs": 28275, "OffsetEndMs": 28590}, {"Word": "the", "OffsetStartMs": 28590, "OffsetEndMs": 28800}, {"Word": "diverse", "OffsetStartMs": 28800, "OffsetEndMs": 29060}, {"Word": "images", "OffsetStartMs": 29170, "OffsetEndMs": 29550}, {"Word": "that", "OffsetStartMs": 29550, "OffsetEndMs": 29775}, {"Word": "we", "OffsetStartMs": 29775, "OffsetEndMs": 29910}, {"Word": "saw", "OffsetStartMs": 29910, "OffsetEndMs": 30120}, {"Word": "on", "OffsetStartMs": 30120, "OffsetEndMs": 30300}, {"Word": "the", "OffsetStartMs": 30300, "OffsetEndMs": 30420}, {"Word": "last", "OffsetStartMs": 30420, "OffsetEndMs": 30630}, {"Word": "slide", "OffsetStartMs": 30630, "OffsetEndMs": 30980}, {"Word": ".", "OffsetStartMs": 31660, "OffsetEndMs": 31935}, {"Word": "Andthis", "OffsetStartMs": 31935, "OffsetEndMs": 32160}, {"Word": "allows", "OffsetStartMs": 32160, "OffsetEndMs": 32415}, {"Word": "us", "OffsetStartMs": 32415, "OffsetEndMs": 32625}, {"Word": "to", "OffsetStartMs": 32625, "OffsetEndMs": 32820}, {"Word": "train", "OffsetStartMs": 32820, "OffsetEndMs": 33075}, {"Word": "in", "OffsetStartMs": 33075, "OffsetEndMs": 33285}, {"Word": "a", "OffsetStartMs": 33285, "OffsetEndMs": 33450}, {"Word": "fair", "OffsetStartMs": 33450, "OffsetEndMs": 33750}, {"Word": "and", "OffsetStartMs": 33750, "OffsetEndMs": 34020}, {"Word": "unbiased", "OffsetStartMs": 34020, "OffsetEndMs": 34590}, {"Word": "manner", "OffsetStartMs": 34590, "OffsetEndMs": 34910}], "SpeechSpeed": 16.5}, {"FinalSentence": "To dig in a little bit more into the math behind how this resampling works, this approach basically approximates the latent space VIA a joint histogram over the individual latent variables. So we have a histogram for every latent variable z sub I, and what the histogram essentially does is it discretizes the continuous distribution so that we can calculate probabilities more easily.", "SliceSentence": "To dig in a little bit more into the math behind how this resampling works this approach basically approximates the latent space VIA a joint histogram over the individual latent variables . Sowe have a histogram for every latent variable z sub I and what the histogram essentially does is it discretizes the continuous distribution so that we can calculate probabilities more easily", "StartMs": 1404040, "EndMs": 1428920, "WordsNum": 63, "Words": [{"Word": "To", "OffsetStartMs": 130, "OffsetEndMs": 390}, {"Word": "dig", "OffsetStartMs": 390, "OffsetEndMs": 555}, {"Word": "in", "OffsetStartMs": 555, "OffsetEndMs": 860}, {"Word": "a", "OffsetStartMs": 880, "OffsetEndMs": 1155}, {"Word": "little", "OffsetStartMs": 1155, "OffsetEndMs": 1335}, {"Word": "bit", "OffsetStartMs": 1335, "OffsetEndMs": 1530}, {"Word": "more", "OffsetStartMs": 1530, "OffsetEndMs": 1755}, {"Word": "into", "OffsetStartMs": 1755, "OffsetEndMs": 2010}, {"Word": "the", "OffsetStartMs": 2010, "OffsetEndMs": 2205}, {"Word": "math", "OffsetStartMs": 2205, "OffsetEndMs": 2480}, {"Word": "behind", "OffsetStartMs": 2560, "OffsetEndMs": 2880}, {"Word": "how", "OffsetStartMs": 2880, "OffsetEndMs": 3105}, {"Word": "this", "OffsetStartMs": 3105, "OffsetEndMs": 3300}, {"Word": "resampling", "OffsetStartMs": 3300, "OffsetEndMs": 3840}, {"Word": "works", "OffsetStartMs": 3840, "OffsetEndMs": 4130}, {"Word": "this", "OffsetStartMs": 4810, "OffsetEndMs": 5145}, {"Word": "approach", "OffsetStartMs": 5145, "OffsetEndMs": 5415}, {"Word": "basically", "OffsetStartMs": 5415, "OffsetEndMs": 5750}, {"Word": "approximates", "OffsetStartMs": 6010, "OffsetEndMs": 6690}, {"Word": "the", "OffsetStartMs": 6690, "OffsetEndMs": 6870}, {"Word": "latent", "OffsetStartMs": 6870, "OffsetEndMs": 7185}, {"Word": "space", "OffsetStartMs": 7185, "OffsetEndMs": 7490}, {"Word": "VIA", "OffsetStartMs": 7570, "OffsetEndMs": 7890}, {"Word": "a", "OffsetStartMs": 7890, "OffsetEndMs": 8115}, {"Word": "joint", "OffsetStartMs": 8115, "OffsetEndMs": 8355}, {"Word": "histogram", "OffsetStartMs": 8355, "OffsetEndMs": 8985}, {"Word": "over", "OffsetStartMs": 8985, "OffsetEndMs": 9315}, {"Word": "the", "OffsetStartMs": 9315, "OffsetEndMs": 9645}, {"Word": "individual", "OffsetStartMs": 9645, "OffsetEndMs": 10005}, {"Word": "latent", "OffsetStartMs": 10005, "OffsetEndMs": 10410}, {"Word": "variables", "OffsetStartMs": 10410, "OffsetEndMs": 10970}, {"Word": ".", "OffsetStartMs": 11410, "OffsetEndMs": 11810}, {"Word": "Sowe", "OffsetStartMs": 11950, "OffsetEndMs": 12240}, {"Word": "have", "OffsetStartMs": 12240, "OffsetEndMs": 12530}, {"Word": "a", "OffsetStartMs": 12580, "OffsetEndMs": 12980}, {"Word": "histogram", "OffsetStartMs": 13810, "OffsetEndMs": 14445}, {"Word": "for", "OffsetStartMs": 14445, "OffsetEndMs": 14670}, {"Word": "every", "OffsetStartMs": 14670, "OffsetEndMs": 14925}, {"Word": "latent", "OffsetStartMs": 14925, "OffsetEndMs": 15330}, {"Word": "variable", "OffsetStartMs": 15330, "OffsetEndMs": 15620}, {"Word": "z", "OffsetStartMs": 15820, "OffsetEndMs": 16140}, {"Word": "sub", "OffsetStartMs": 16140, "OffsetEndMs": 16335}, {"Word": "I", "OffsetStartMs": 16335, "OffsetEndMs": 16610}, {"Word": "and", "OffsetStartMs": 17020, "OffsetEndMs": 17325}, {"Word": "what", "OffsetStartMs": 17325, "OffsetEndMs": 17505}, {"Word": "the", "OffsetStartMs": 17505, "OffsetEndMs": 17610}, {"Word": "histogram", "OffsetStartMs": 17610, "OffsetEndMs": 18030}, {"Word": "essentially", "OffsetStartMs": 18030, "OffsetEndMs": 18345}, {"Word": "does", "OffsetStartMs": 18345, "OffsetEndMs": 18720}, {"Word": "is", "OffsetStartMs": 18720, "OffsetEndMs": 18990}, {"Word": "it", "OffsetStartMs": 18990, "OffsetEndMs": 19200}, {"Word": "discretizes", "OffsetStartMs": 19200, "OffsetEndMs": 19880}, {"Word": "the", "OffsetStartMs": 20020, "OffsetEndMs": 20370}, {"Word": "continuous", "OffsetStartMs": 20370, "OffsetEndMs": 20720}, {"Word": "distribution", "OffsetStartMs": 20830, "OffsetEndMs": 21230}, {"Word": "so", "OffsetStartMs": 21550, "OffsetEndMs": 21795}, {"Word": "that", "OffsetStartMs": 21795, "OffsetEndMs": 21900}, {"Word": "we", "OffsetStartMs": 21900, "OffsetEndMs": 22005}, {"Word": "can", "OffsetStartMs": 22005, "OffsetEndMs": 22140}, {"Word": "calculate", "OffsetStartMs": 22140, "OffsetEndMs": 22605}, {"Word": "probabilities", "OffsetStartMs": 22605, "OffsetEndMs": 23120}, {"Word": "more", "OffsetStartMs": 23170, "OffsetEndMs": 23460}, {"Word": "easily", "OffsetStartMs": 23460, "OffsetEndMs": 23750}], "SpeechSpeed": 15.3}, {"FinalSentence": "Then we multiply the probabilities together across all of the latent distributions, and then after that we can develop an understanding of the joint distribution of all of the samples in our latent space.", "SliceSentence": "Then we multiply the probabilities together across all of the latent distributions and then after that we can develop an understanding of the joint distribution of all of the samples in our latent space", "StartMs": 1429240, "EndMs": 1441820, "WordsNum": 34, "Words": [{"Word": "Then", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "we", "OffsetStartMs": 610, "OffsetEndMs": 1010}, {"Word": "multiply", "OffsetStartMs": 1060, "OffsetEndMs": 1680}, {"Word": "the", "OffsetStartMs": 1680, "OffsetEndMs": 1830}, {"Word": "probabilities", "OffsetStartMs": 1830, "OffsetEndMs": 2270}, {"Word": "together", "OffsetStartMs": 2290, "OffsetEndMs": 2690}, {"Word": "across", "OffsetStartMs": 2770, "OffsetEndMs": 3120}, {"Word": "all", "OffsetStartMs": 3120, "OffsetEndMs": 3375}, {"Word": "of", "OffsetStartMs": 3375, "OffsetEndMs": 3555}, {"Word": "the", "OffsetStartMs": 3555, "OffsetEndMs": 3735}, {"Word": "latent", "OffsetStartMs": 3735, "OffsetEndMs": 4140}, {"Word": "distributions", "OffsetStartMs": 4140, "OffsetEndMs": 4580}, {"Word": "and", "OffsetStartMs": 5740, "OffsetEndMs": 6015}, {"Word": "then", "OffsetStartMs": 6015, "OffsetEndMs": 6165}, {"Word": "after", "OffsetStartMs": 6165, "OffsetEndMs": 6390}, {"Word": "that", "OffsetStartMs": 6390, "OffsetEndMs": 6675}, {"Word": "we", "OffsetStartMs": 6675, "OffsetEndMs": 6870}, {"Word": "can", "OffsetStartMs": 6870, "OffsetEndMs": 7035}, {"Word": "develop", "OffsetStartMs": 7035, "OffsetEndMs": 7275}, {"Word": "an", "OffsetStartMs": 7275, "OffsetEndMs": 7610}, {"Word": "understanding", "OffsetStartMs": 8200, "OffsetEndMs": 8535}, {"Word": "of", "OffsetStartMs": 8535, "OffsetEndMs": 8730}, {"Word": "the", "OffsetStartMs": 8730, "OffsetEndMs": 8895}, {"Word": "joint", "OffsetStartMs": 8895, "OffsetEndMs": 9200}, {"Word": "distribution", "OffsetStartMs": 9280, "OffsetEndMs": 9680}, {"Word": "of", "OffsetStartMs": 9700, "OffsetEndMs": 9975}, {"Word": "all", "OffsetStartMs": 9975, "OffsetEndMs": 10125}, {"Word": "of", "OffsetStartMs": 10125, "OffsetEndMs": 10245}, {"Word": "the", "OffsetStartMs": 10245, "OffsetEndMs": 10365}, {"Word": "samples", "OffsetStartMs": 10365, "OffsetEndMs": 10725}, {"Word": "in", "OffsetStartMs": 10725, "OffsetEndMs": 10875}, {"Word": "our", "OffsetStartMs": 10875, "OffsetEndMs": 10995}, {"Word": "latent", "OffsetStartMs": 10995, "OffsetEndMs": 11295}, {"Word": "space", "OffsetStartMs": 11295, "OffsetEndMs": 11600}], "SpeechSpeed": 16.1}, {"FinalSentence": "Based on this, we can define the adjusted probability for sampling for a particular data point as follows. The probability of selecting a sample data point X will be based on the latent space of X, such that it is the inverse of the joint approximated distribution. We have a parameter Alpha here, which is a biasing parameter, and as Alpha increases, this probability will tend to the uniform distribution, and if Alpha increases, we tend to de bias more strongly.", "SliceSentence": "Based on this we can define the adjusted probability for sampling for a particular data point as follows . Theprobability of selecting a sample data point X will be based on the latent space of X such that it is the inverse of the joint approximated distribution . Wehave a parameter Alpha here which is a biasing parameter and as Alpha increases this probability will tend to the uniform distribution and if Alpha increases we tend to de bias more strongly", "StartMs": 1443100, "EndMs": 1471080, "WordsNum": 81, "Words": [{"Word": "Based", "OffsetStartMs": 70, "OffsetEndMs": 390}, {"Word": "on", "OffsetStartMs": 390, "OffsetEndMs": 585}, {"Word": "this", "OffsetStartMs": 585, "OffsetEndMs": 855}, {"Word": "we", "OffsetStartMs": 855, "OffsetEndMs": 1110}, {"Word": "can", "OffsetStartMs": 1110, "OffsetEndMs": 1305}, {"Word": "define", "OffsetStartMs": 1305, "OffsetEndMs": 1590}, {"Word": "the", "OffsetStartMs": 1590, "OffsetEndMs": 1890}, {"Word": "adjusted", "OffsetStartMs": 1890, "OffsetEndMs": 2235}, {"Word": "probability", "OffsetStartMs": 2235, "OffsetEndMs": 2840}, {"Word": "for", "OffsetStartMs": 2860, "OffsetEndMs": 3150}, {"Word": "sampling", "OffsetStartMs": 3150, "OffsetEndMs": 3680}, {"Word": "for", "OffsetStartMs": 3790, "OffsetEndMs": 4035}, {"Word": "a", "OffsetStartMs": 4035, "OffsetEndMs": 4215}, {"Word": "particular", "OffsetStartMs": 4215, "OffsetEndMs": 4530}, {"Word": "data", "OffsetStartMs": 4530, "OffsetEndMs": 4830}, {"Word": "point", "OffsetStartMs": 4830, "OffsetEndMs": 5040}, {"Word": "as", "OffsetStartMs": 5040, "OffsetEndMs": 5250}, {"Word": "follows", "OffsetStartMs": 5250, "OffsetEndMs": 5570}, {"Word": ".", "OffsetStartMs": 6160, "OffsetEndMs": 6435}, {"Word": "Theprobability", "OffsetStartMs": 6435, "OffsetEndMs": 6855}, {"Word": "of", "OffsetStartMs": 6855, "OffsetEndMs": 7110}, {"Word": "selecting", "OffsetStartMs": 7110, "OffsetEndMs": 7425}, {"Word": "a", "OffsetStartMs": 7425, "OffsetEndMs": 7575}, {"Word": "sample", "OffsetStartMs": 7575, "OffsetEndMs": 7880}, {"Word": "data", "OffsetStartMs": 7990, "OffsetEndMs": 8325}, {"Word": "point", "OffsetStartMs": 8325, "OffsetEndMs": 8565}, {"Word": "X", "OffsetStartMs": 8565, "OffsetEndMs": 8870}, {"Word": "will", "OffsetStartMs": 9010, "OffsetEndMs": 9270}, {"Word": "be", "OffsetStartMs": 9270, "OffsetEndMs": 9405}, {"Word": "based", "OffsetStartMs": 9405, "OffsetEndMs": 9645}, {"Word": "on", "OffsetStartMs": 9645, "OffsetEndMs": 9885}, {"Word": "the", "OffsetStartMs": 9885, "OffsetEndMs": 10020}, {"Word": "latent", "OffsetStartMs": 10020, "OffsetEndMs": 10335}, {"Word": "space", "OffsetStartMs": 10335, "OffsetEndMs": 10545}, {"Word": "of", "OffsetStartMs": 10545, "OffsetEndMs": 10725}, {"Word": "X", "OffsetStartMs": 10725, "OffsetEndMs": 11000}, {"Word": "such", "OffsetStartMs": 11380, "OffsetEndMs": 11685}, {"Word": "that", "OffsetStartMs": 11685, "OffsetEndMs": 11865}, {"Word": "it", "OffsetStartMs": 11865, "OffsetEndMs": 11985}, {"Word": "is", "OffsetStartMs": 11985, "OffsetEndMs": 12120}, {"Word": "the", "OffsetStartMs": 12120, "OffsetEndMs": 12300}, {"Word": "inverse", "OffsetStartMs": 12300, "OffsetEndMs": 12780}, {"Word": "of", "OffsetStartMs": 12780, "OffsetEndMs": 13035}, {"Word": "the", "OffsetStartMs": 13035, "OffsetEndMs": 13185}, {"Word": "joint", "OffsetStartMs": 13185, "OffsetEndMs": 13455}, {"Word": "approximated", "OffsetStartMs": 13455, "OffsetEndMs": 14115}, {"Word": "distribution", "OffsetStartMs": 14115, "OffsetEndMs": 14510}, {"Word": ".", "OffsetStartMs": 15610, "OffsetEndMs": 15885}, {"Word": "Wehave", "OffsetStartMs": 15885, "OffsetEndMs": 16080}, {"Word": "a", "OffsetStartMs": 16080, "OffsetEndMs": 16305}, {"Word": "parameter", "OffsetStartMs": 16305, "OffsetEndMs": 16740}, {"Word": "Alpha", "OffsetStartMs": 16740, "OffsetEndMs": 17175}, {"Word": "here", "OffsetStartMs": 17175, "OffsetEndMs": 17385}, {"Word": "which", "OffsetStartMs": 17385, "OffsetEndMs": 17580}, {"Word": "is", "OffsetStartMs": 17580, "OffsetEndMs": 17715}, {"Word": "a", "OffsetStartMs": 17715, "OffsetEndMs": 17940}, {"Word": "biasing", "OffsetStartMs": 17940, "OffsetEndMs": 18480}, {"Word": "parameter", "OffsetStartMs": 18480, "OffsetEndMs": 18980}, {"Word": "and", "OffsetStartMs": 19120, "OffsetEndMs": 19395}, {"Word": "as", "OffsetStartMs": 19395, "OffsetEndMs": 19575}, {"Word": "Alpha", "OffsetStartMs": 19575, "OffsetEndMs": 20055}, {"Word": "increases", "OffsetStartMs": 20055, "OffsetEndMs": 20420}, {"Word": "this", "OffsetStartMs": 20740, "OffsetEndMs": 21045}, {"Word": "probability", "OffsetStartMs": 21045, "OffsetEndMs": 21540}, {"Word": "will", "OffsetStartMs": 21540, "OffsetEndMs": 21780}, {"Word": "tend", "OffsetStartMs": 21780, "OffsetEndMs": 22050}, {"Word": "to", "OffsetStartMs": 22050, "OffsetEndMs": 22395}, {"Word": "the", "OffsetStartMs": 22395, "OffsetEndMs": 22710}, {"Word": "uniform", "OffsetStartMs": 22710, "OffsetEndMs": 23025}, {"Word": "distribution", "OffsetStartMs": 23025, "OffsetEndMs": 23390}, {"Word": "and", "OffsetStartMs": 23770, "OffsetEndMs": 24030}, {"Word": "if", "OffsetStartMs": 24030, "OffsetEndMs": 24165}, {"Word": "Alpha", "OffsetStartMs": 24165, "OffsetEndMs": 24645}, {"Word": "increases", "OffsetStartMs": 24645, "OffsetEndMs": 25040}, {"Word": "we", "OffsetStartMs": 25210, "OffsetEndMs": 25485}, {"Word": "tend", "OffsetStartMs": 25485, "OffsetEndMs": 25635}, {"Word": "to", "OffsetStartMs": 25635, "OffsetEndMs": 25755}, {"Word": "de", "OffsetStartMs": 25755, "OffsetEndMs": 25860}, {"Word": "bias", "OffsetStartMs": 25860, "OffsetEndMs": 26220}, {"Word": "more", "OffsetStartMs": 26220, "OffsetEndMs": 26505}, {"Word": "strongly", "OffsetStartMs": 26505, "OffsetEndMs": 26840}], "SpeechSpeed": 16.3}, {"FinalSentence": "And this gives us the final weight of the sample in our data set that we can calculate on the fly and use it to adaptively resample while training.", "SliceSentence": "And this gives us the final weight of the sample in our data set that we can calculate on the fly and use it to adaptively resample while training", "StartMs": 1472500, "EndMs": 1482620, "WordsNum": 29, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 345}, {"Word": "this", "OffsetStartMs": 345, "OffsetEndMs": 540}, {"Word": "gives", "OffsetStartMs": 540, "OffsetEndMs": 750}, {"Word": "us", "OffsetStartMs": 750, "OffsetEndMs": 990}, {"Word": "the", "OffsetStartMs": 990, "OffsetEndMs": 1260}, {"Word": "final", "OffsetStartMs": 1260, "OffsetEndMs": 1580}, {"Word": "weight", "OffsetStartMs": 1630, "OffsetEndMs": 2030}, {"Word": "of", "OffsetStartMs": 2050, "OffsetEndMs": 2325}, {"Word": "the", "OffsetStartMs": 2325, "OffsetEndMs": 2475}, {"Word": "sample", "OffsetStartMs": 2475, "OffsetEndMs": 2750}, {"Word": "in", "OffsetStartMs": 3250, "OffsetEndMs": 3495}, {"Word": "our", "OffsetStartMs": 3495, "OffsetEndMs": 3630}, {"Word": "data", "OffsetStartMs": 3630, "OffsetEndMs": 3855}, {"Word": "set", "OffsetStartMs": 3855, "OffsetEndMs": 4080}, {"Word": "that", "OffsetStartMs": 4080, "OffsetEndMs": 4230}, {"Word": "we", "OffsetStartMs": 4230, "OffsetEndMs": 4350}, {"Word": "can", "OffsetStartMs": 4350, "OffsetEndMs": 4575}, {"Word": "calculate", "OffsetStartMs": 4575, "OffsetEndMs": 5085}, {"Word": "on", "OffsetStartMs": 5085, "OffsetEndMs": 5250}, {"Word": "the", "OffsetStartMs": 5250, "OffsetEndMs": 5445}, {"Word": "fly", "OffsetStartMs": 5445, "OffsetEndMs": 5750}, {"Word": "and", "OffsetStartMs": 5980, "OffsetEndMs": 6380}, {"Word": "use", "OffsetStartMs": 6430, "OffsetEndMs": 6705}, {"Word": "it", "OffsetStartMs": 6705, "OffsetEndMs": 6885}, {"Word": "to", "OffsetStartMs": 6885, "OffsetEndMs": 7190}, {"Word": "adaptively", "OffsetStartMs": 7360, "OffsetEndMs": 7965}, {"Word": "resample", "OffsetStartMs": 7965, "OffsetEndMs": 8460}, {"Word": "while", "OffsetStartMs": 8460, "OffsetEndMs": 8655}, {"Word": "training", "OffsetStartMs": 8655, "OffsetEndMs": 8960}], "SpeechSpeed": 14.4}, {"FinalSentence": "And so once we apply this biasing, we have pretty remarkable results. This is the original EM graph that shows the accuracy gap between EM, the darker Mills and the lighter Mills in this dataset.", "SliceSentence": "And so once we apply this biasing we have pretty remarkable results . Thisis the original EM graph that shows the accuracy gap between EM the darker Mills and the lighter Mills in this dataset", "StartMs": 1483700, "EndMs": 1497340, "WordsNum": 35, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 375}, {"Word": "so", "OffsetStartMs": 375, "OffsetEndMs": 680}, {"Word": "once", "OffsetStartMs": 850, "OffsetEndMs": 1155}, {"Word": "we", "OffsetStartMs": 1155, "OffsetEndMs": 1365}, {"Word": "apply", "OffsetStartMs": 1365, "OffsetEndMs": 1670}, {"Word": "this", "OffsetStartMs": 1930, "OffsetEndMs": 2265}, {"Word": "biasing", "OffsetStartMs": 2265, "OffsetEndMs": 2865}, {"Word": "we", "OffsetStartMs": 2865, "OffsetEndMs": 3135}, {"Word": "have", "OffsetStartMs": 3135, "OffsetEndMs": 3440}, {"Word": "pretty", "OffsetStartMs": 3460, "OffsetEndMs": 3825}, {"Word": "remarkable", "OffsetStartMs": 3825, "OffsetEndMs": 4190}, {"Word": "results", "OffsetStartMs": 4300, "OffsetEndMs": 4700}, {"Word": ".", "OffsetStartMs": 5050, "OffsetEndMs": 5340}, {"Word": "Thisis", "OffsetStartMs": 5340, "OffsetEndMs": 5550}, {"Word": "the", "OffsetStartMs": 5550, "OffsetEndMs": 5790}, {"Word": "original", "OffsetStartMs": 5790, "OffsetEndMs": 6110}, {"Word": "EM", "OffsetStartMs": 6280, "OffsetEndMs": 6645}, {"Word": "graph", "OffsetStartMs": 6645, "OffsetEndMs": 6975}, {"Word": "that", "OffsetStartMs": 6975, "OffsetEndMs": 7260}, {"Word": "shows", "OffsetStartMs": 7260, "OffsetEndMs": 7530}, {"Word": "the", "OffsetStartMs": 7530, "OffsetEndMs": 7845}, {"Word": "accuracy", "OffsetStartMs": 7845, "OffsetEndMs": 8280}, {"Word": "gap", "OffsetStartMs": 8280, "OffsetEndMs": 8540}, {"Word": "between", "OffsetStartMs": 8560, "OffsetEndMs": 8960}, {"Word": "EM", "OffsetStartMs": 9070, "OffsetEndMs": 9470}, {"Word": "the", "OffsetStartMs": 9520, "OffsetEndMs": 9885}, {"Word": "darker", "OffsetStartMs": 9885, "OffsetEndMs": 10335}, {"Word": "Mills", "OffsetStartMs": 10335, "OffsetEndMs": 10695}, {"Word": "and", "OffsetStartMs": 10695, "OffsetEndMs": 10950}, {"Word": "the", "OffsetStartMs": 10950, "OffsetEndMs": 11205}, {"Word": "lighter", "OffsetStartMs": 11205, "OffsetEndMs": 11535}, {"Word": "Mills", "OffsetStartMs": 11535, "OffsetEndMs": 11790}, {"Word": "in", "OffsetStartMs": 11790, "OffsetEndMs": 11910}, {"Word": "this", "OffsetStartMs": 11910, "OffsetEndMs": 12060}, {"Word": "dataset", "OffsetStartMs": 12060, "OffsetEndMs": 12650}], "SpeechSpeed": 14.0}, {"FinalSentence": "Once we apply the device algorithm, where as Alpha gets smaller, we're devising more and more, as we just talked about, this accuracy gap decreases significantly and that's because we tend to over sample samples with darker skin color and therefore the model learns them better and tends to do better on them.", "SliceSentence": "Once we apply the device algorithm where as Alpha gets smaller we're devising more and more as we just talked about this accuracy gap decreases significantly and that's because we tend to over sample samples with darker skin color and therefore the model learns them better and tends to do better on them", "StartMs": 1497640, "EndMs": 1516340, "WordsNum": 53, "Words": [{"Word": "Once", "OffsetStartMs": 130, "OffsetEndMs": 450}, {"Word": "we", "OffsetStartMs": 450, "OffsetEndMs": 705}, {"Word": "apply", "OffsetStartMs": 705, "OffsetEndMs": 945}, {"Word": "the", "OffsetStartMs": 945, "OffsetEndMs": 1170}, {"Word": "device", "OffsetStartMs": 1170, "OffsetEndMs": 1490}, {"Word": "algorithm", "OffsetStartMs": 1690, "OffsetEndMs": 2250}, {"Word": "where", "OffsetStartMs": 2250, "OffsetEndMs": 2550}, {"Word": "as", "OffsetStartMs": 2550, "OffsetEndMs": 2760}, {"Word": "Alpha", "OffsetStartMs": 2760, "OffsetEndMs": 3120}, {"Word": "gets", "OffsetStartMs": 3120, "OffsetEndMs": 3300}, {"Word": "smaller", "OffsetStartMs": 3300, "OffsetEndMs": 3585}, {"Word": "we're", "OffsetStartMs": 3585, "OffsetEndMs": 3870}, {"Word": "devising", "OffsetStartMs": 3870, "OffsetEndMs": 4220}, {"Word": "more", "OffsetStartMs": 4240, "OffsetEndMs": 4515}, {"Word": "and", "OffsetStartMs": 4515, "OffsetEndMs": 4680}, {"Word": "more", "OffsetStartMs": 4680, "OffsetEndMs": 4970}, {"Word": "as", "OffsetStartMs": 5050, "OffsetEndMs": 5340}, {"Word": "we", "OffsetStartMs": 5340, "OffsetEndMs": 5490}, {"Word": "just", "OffsetStartMs": 5490, "OffsetEndMs": 5640}, {"Word": "talked", "OffsetStartMs": 5640, "OffsetEndMs": 5865}, {"Word": "about", "OffsetStartMs": 5865, "OffsetEndMs": 6200}, {"Word": "this", "OffsetStartMs": 6970, "OffsetEndMs": 7365}, {"Word": "accuracy", "OffsetStartMs": 7365, "OffsetEndMs": 7800}, {"Word": "gap", "OffsetStartMs": 7800, "OffsetEndMs": 8060}, {"Word": "decreases", "OffsetStartMs": 8170, "OffsetEndMs": 8790}, {"Word": "significantly", "OffsetStartMs": 8790, "OffsetEndMs": 9170}, {"Word": "and", "OffsetStartMs": 10180, "OffsetEndMs": 10440}, {"Word": "that's", "OffsetStartMs": 10440, "OffsetEndMs": 10770}, {"Word": "because", "OffsetStartMs": 10770, "OffsetEndMs": 10995}, {"Word": "we", "OffsetStartMs": 10995, "OffsetEndMs": 11175}, {"Word": "tend", "OffsetStartMs": 11175, "OffsetEndMs": 11355}, {"Word": "to", "OffsetStartMs": 11355, "OffsetEndMs": 11505}, {"Word": "over", "OffsetStartMs": 11505, "OffsetEndMs": 11730}, {"Word": "sample", "OffsetStartMs": 11730, "OffsetEndMs": 12110}, {"Word": "samples", "OffsetStartMs": 12310, "OffsetEndMs": 12795}, {"Word": "with", "OffsetStartMs": 12795, "OffsetEndMs": 13020}, {"Word": "darker", "OffsetStartMs": 13020, "OffsetEndMs": 13410}, {"Word": "skin", "OffsetStartMs": 13410, "OffsetEndMs": 13590}, {"Word": "color", "OffsetStartMs": 13590, "OffsetEndMs": 13880}, {"Word": "and", "OffsetStartMs": 14080, "OffsetEndMs": 14480}, {"Word": "therefore", "OffsetStartMs": 14620, "OffsetEndMs": 14940}, {"Word": "the", "OffsetStartMs": 14940, "OffsetEndMs": 15120}, {"Word": "model", "OffsetStartMs": 15120, "OffsetEndMs": 15330}, {"Word": "learns", "OffsetStartMs": 15330, "OffsetEndMs": 15705}, {"Word": "them", "OffsetStartMs": 15705, "OffsetEndMs": 15855}, {"Word": "better", "OffsetStartMs": 15855, "OffsetEndMs": 16125}, {"Word": "and", "OffsetStartMs": 16125, "OffsetEndMs": 16425}, {"Word": "tends", "OffsetStartMs": 16425, "OffsetEndMs": 16695}, {"Word": "to", "OffsetStartMs": 16695, "OffsetEndMs": 16830}, {"Word": "do", "OffsetStartMs": 16830, "OffsetEndMs": 16980}, {"Word": "better", "OffsetStartMs": 16980, "OffsetEndMs": 17205}, {"Word": "on", "OffsetStartMs": 17205, "OffsetEndMs": 17445}, {"Word": "them", "OffsetStartMs": 17445, "OffsetEndMs": 17750}], "SpeechSpeed": 16.3}, {"FinalSentence": "Keep this algorithm in mind because you're going to need it for the lab three competition, which I'll talk more about towards the end of this lecture.", "SliceSentence": "Keep this algorithm in mind because you're going to need it for the lab three competition which I'll talk more about towards the end of this lecture", "StartMs": 1516340, "EndMs": 1523560, "WordsNum": 27, "Words": [{"Word": "Keep", "OffsetStartMs": 40, "OffsetEndMs": 330}, {"Word": "this", "OffsetStartMs": 330, "OffsetEndMs": 600}, {"Word": "algorithm", "OffsetStartMs": 600, "OffsetEndMs": 945}, {"Word": "in", "OffsetStartMs": 945, "OffsetEndMs": 1065}, {"Word": "mind", "OffsetStartMs": 1065, "OffsetEndMs": 1340}, {"Word": "because", "OffsetStartMs": 1390, "OffsetEndMs": 1650}, {"Word": "you're", "OffsetStartMs": 1650, "OffsetEndMs": 1800}, {"Word": "going", "OffsetStartMs": 1800, "OffsetEndMs": 1920}, {"Word": "to", "OffsetStartMs": 1920, "OffsetEndMs": 2055}, {"Word": "need", "OffsetStartMs": 2055, "OffsetEndMs": 2160}, {"Word": "it", "OffsetStartMs": 2160, "OffsetEndMs": 2295}, {"Word": "for", "OffsetStartMs": 2295, "OffsetEndMs": 2430}, {"Word": "the", "OffsetStartMs": 2430, "OffsetEndMs": 2535}, {"Word": "lab", "OffsetStartMs": 2535, "OffsetEndMs": 2745}, {"Word": "three", "OffsetStartMs": 2745, "OffsetEndMs": 3110}, {"Word": "competition", "OffsetStartMs": 3190, "OffsetEndMs": 3590}, {"Word": "which", "OffsetStartMs": 3790, "OffsetEndMs": 4050}, {"Word": "I'll", "OffsetStartMs": 4050, "OffsetEndMs": 4245}, {"Word": "talk", "OffsetStartMs": 4245, "OffsetEndMs": 4410}, {"Word": "more", "OffsetStartMs": 4410, "OffsetEndMs": 4635}, {"Word": "about", "OffsetStartMs": 4635, "OffsetEndMs": 4905}, {"Word": "towards", "OffsetStartMs": 4905, "OffsetEndMs": 5175}, {"Word": "the", "OffsetStartMs": 5175, "OffsetEndMs": 5340}, {"Word": "end", "OffsetStartMs": 5340, "OffsetEndMs": 5490}, {"Word": "of", "OffsetStartMs": 5490, "OffsetEndMs": 5625}, {"Word": "this", "OffsetStartMs": 5625, "OffsetEndMs": 5760}, {"Word": "lecture", "OffsetStartMs": 5760, "OffsetEndMs": 6050}], "SpeechSpeed": 20.5}, {"FinalSentence": "So, so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias. However, bias is actually far more widespread in machine learning. Consider the example of autonomous driving. Many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. And this is because the data for these cars for these algorithms is actually just collected by cars driving down roads.", "SliceSentence": "So so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias . Howeverbias is actually far more widespread in machine learning . Considerthe example of autonomous driving . Manydata sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility . Andthis is because the data for these cars for these algorithms is actually just collected by cars driving down roads", "StartMs": 1525260, "EndMs": 1554960, "WordsNum": 82, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "so", "OffsetStartMs": 580, "OffsetEndMs": 885}, {"Word": "far", "OffsetStartMs": 885, "OffsetEndMs": 1155}, {"Word": "we've", "OffsetStartMs": 1155, "OffsetEndMs": 1440}, {"Word": "been", "OffsetStartMs": 1440, "OffsetEndMs": 1560}, {"Word": "focusing", "OffsetStartMs": 1560, "OffsetEndMs": 1850}, {"Word": "mainly", "OffsetStartMs": 1870, "OffsetEndMs": 2250}, {"Word": "on", "OffsetStartMs": 2250, "OffsetEndMs": 2535}, {"Word": "facial", "OffsetStartMs": 2535, "OffsetEndMs": 2850}, {"Word": "recognition", "OffsetStartMs": 2850, "OffsetEndMs": 3110}, {"Word": "systems", "OffsetStartMs": 3280, "OffsetEndMs": 3680}, {"Word": "and", "OffsetStartMs": 3760, "OffsetEndMs": 4005}, {"Word": "a", "OffsetStartMs": 4005, "OffsetEndMs": 4110}, {"Word": "couple", "OffsetStartMs": 4110, "OffsetEndMs": 4275}, {"Word": "of", "OffsetStartMs": 4275, "OffsetEndMs": 4440}, {"Word": "other", "OffsetStartMs": 4440, "OffsetEndMs": 4635}, {"Word": "systems", "OffsetStartMs": 4635, "OffsetEndMs": 4970}, {"Word": "as", "OffsetStartMs": 5080, "OffsetEndMs": 5445}, {"Word": "canonical", "OffsetStartMs": 5445, "OffsetEndMs": 6075}, {"Word": "examples", "OffsetStartMs": 6075, "OffsetEndMs": 6405}, {"Word": "of", "OffsetStartMs": 6405, "OffsetEndMs": 6675}, {"Word": "bias", "OffsetStartMs": 6675, "OffsetEndMs": 7100}, {"Word": ".", "OffsetStartMs": 7840, "OffsetEndMs": 8240}, {"Word": "Howeverbias", "OffsetStartMs": 8320, "OffsetEndMs": 8775}, {"Word": "is", "OffsetStartMs": 8775, "OffsetEndMs": 9030}, {"Word": "actually", "OffsetStartMs": 9030, "OffsetEndMs": 9405}, {"Word": "far", "OffsetStartMs": 9405, "OffsetEndMs": 9800}, {"Word": "more", "OffsetStartMs": 9820, "OffsetEndMs": 10140}, {"Word": "widespread", "OffsetStartMs": 10140, "OffsetEndMs": 10710}, {"Word": "in", "OffsetStartMs": 10710, "OffsetEndMs": 10950}, {"Word": "machine", "OffsetStartMs": 10950, "OffsetEndMs": 11175}, {"Word": "learning", "OffsetStartMs": 11175, "OffsetEndMs": 11450}, {"Word": ".", "OffsetStartMs": 12040, "OffsetEndMs": 12330}, {"Word": "Considerthe", "OffsetStartMs": 12330, "OffsetEndMs": 12525}, {"Word": "example", "OffsetStartMs": 12525, "OffsetEndMs": 12825}, {"Word": "of", "OffsetStartMs": 12825, "OffsetEndMs": 13155}, {"Word": "autonomous", "OffsetStartMs": 13155, "OffsetEndMs": 13710}, {"Word": "driving", "OffsetStartMs": 13710, "OffsetEndMs": 14000}, {"Word": ".", "OffsetStartMs": 14500, "OffsetEndMs": 14835}, {"Word": "Manydata", "OffsetStartMs": 14835, "OffsetEndMs": 15090}, {"Word": "sets", "OffsetStartMs": 15090, "OffsetEndMs": 15390}, {"Word": "are", "OffsetStartMs": 15390, "OffsetEndMs": 15770}, {"Word": "comprised", "OffsetStartMs": 15820, "OffsetEndMs": 16320}, {"Word": "mainly", "OffsetStartMs": 16320, "OffsetEndMs": 16635}, {"Word": "of", "OffsetStartMs": 16635, "OffsetEndMs": 16905}, {"Word": "cars", "OffsetStartMs": 16905, "OffsetEndMs": 17190}, {"Word": "driving", "OffsetStartMs": 17190, "OffsetEndMs": 17520}, {"Word": "down", "OffsetStartMs": 17520, "OffsetEndMs": 17870}, {"Word": "straight", "OffsetStartMs": 17980, "OffsetEndMs": 18315}, {"Word": "and", "OffsetStartMs": 18315, "OffsetEndMs": 18555}, {"Word": "sunny", "OffsetStartMs": 18555, "OffsetEndMs": 18795}, {"Word": "roads", "OffsetStartMs": 18795, "OffsetEndMs": 19130}, {"Word": "in", "OffsetStartMs": 19180, "OffsetEndMs": 19470}, {"Word": "really", "OffsetStartMs": 19470, "OffsetEndMs": 19680}, {"Word": "good", "OffsetStartMs": 19680, "OffsetEndMs": 19890}, {"Word": "weather", "OffsetStartMs": 19890, "OffsetEndMs": 20175}, {"Word": "conditions", "OffsetStartMs": 20175, "OffsetEndMs": 20570}, {"Word": "with", "OffsetStartMs": 20800, "OffsetEndMs": 21120}, {"Word": "very", "OffsetStartMs": 21120, "OffsetEndMs": 21390}, {"Word": "high", "OffsetStartMs": 21390, "OffsetEndMs": 21630}, {"Word": "visibility", "OffsetStartMs": 21630, "OffsetEndMs": 22130}, {"Word": ".", "OffsetStartMs": 22450, "OffsetEndMs": 22725}, {"Word": "Andthis", "OffsetStartMs": 22725, "OffsetEndMs": 22875}, {"Word": "is", "OffsetStartMs": 22875, "OffsetEndMs": 23070}, {"Word": "because", "OffsetStartMs": 23070, "OffsetEndMs": 23310}, {"Word": "the", "OffsetStartMs": 23310, "OffsetEndMs": 23490}, {"Word": "data", "OffsetStartMs": 23490, "OffsetEndMs": 23685}, {"Word": "for", "OffsetStartMs": 23685, "OffsetEndMs": 23865}, {"Word": "these", "OffsetStartMs": 23865, "OffsetEndMs": 24015}, {"Word": "cars", "OffsetStartMs": 24015, "OffsetEndMs": 24320}, {"Word": "for", "OffsetStartMs": 24520, "OffsetEndMs": 24810}, {"Word": "these", "OffsetStartMs": 24810, "OffsetEndMs": 25100}, {"Word": "algorithms", "OffsetStartMs": 25150, "OffsetEndMs": 25605}, {"Word": "is", "OffsetStartMs": 25605, "OffsetEndMs": 25890}, {"Word": "actually", "OffsetStartMs": 25890, "OffsetEndMs": 26145}, {"Word": "just", "OffsetStartMs": 26145, "OffsetEndMs": 26420}, {"Word": "collected", "OffsetStartMs": 27010, "OffsetEndMs": 27360}, {"Word": "by", "OffsetStartMs": 27360, "OffsetEndMs": 27600}, {"Word": "cars", "OffsetStartMs": 27600, "OffsetEndMs": 27825}, {"Word": "driving", "OffsetStartMs": 27825, "OffsetEndMs": 28110}, {"Word": "down", "OffsetStartMs": 28110, "OffsetEndMs": 28395}, {"Word": "roads", "OffsetStartMs": 28395, "OffsetEndMs": 28730}], "SpeechSpeed": 16.7}, {"FinalSentence": "However, in some specific cases, you're going to face adverse weather, bad, bad visibility, near collision scenarios. And these are actually the samples that are the most important for the model to learn because they're the hardest samples and they're the samples where the model is most likely to fail.", "SliceSentence": "However in some specific cases you're going to face adverse weather bad bad visibility near collision scenarios . Andthese are actually the samples that are the most important for the model to learn because they're the hardest samples and they're the samples where the model is most likely to fail", "StartMs": 1555000, "EndMs": 1571900, "WordsNum": 50, "Words": [{"Word": "However", "OffsetStartMs": 220, "OffsetEndMs": 620}, {"Word": "in", "OffsetStartMs": 970, "OffsetEndMs": 1365}, {"Word": "some", "OffsetStartMs": 1365, "OffsetEndMs": 1710}, {"Word": "specific", "OffsetStartMs": 1710, "OffsetEndMs": 2055}, {"Word": "cases", "OffsetStartMs": 2055, "OffsetEndMs": 2450}, {"Word": "you're", "OffsetStartMs": 2560, "OffsetEndMs": 2850}, {"Word": "going", "OffsetStartMs": 2850, "OffsetEndMs": 2955}, {"Word": "to", "OffsetStartMs": 2955, "OffsetEndMs": 3075}, {"Word": "face", "OffsetStartMs": 3075, "OffsetEndMs": 3300}, {"Word": "adverse", "OffsetStartMs": 3300, "OffsetEndMs": 3810}, {"Word": "weather", "OffsetStartMs": 3810, "OffsetEndMs": 4130}, {"Word": "bad", "OffsetStartMs": 4420, "OffsetEndMs": 4820}, {"Word": "bad", "OffsetStartMs": 5260, "OffsetEndMs": 5595}, {"Word": "visibility", "OffsetStartMs": 5595, "OffsetEndMs": 6140}, {"Word": "near", "OffsetStartMs": 6520, "OffsetEndMs": 6855}, {"Word": "collision", "OffsetStartMs": 6855, "OffsetEndMs": 7200}, {"Word": "scenarios", "OffsetStartMs": 7200, "OffsetEndMs": 7820}, {"Word": ".", "OffsetStartMs": 7900, "OffsetEndMs": 8175}, {"Word": "Andthese", "OffsetStartMs": 8175, "OffsetEndMs": 8310}, {"Word": "are", "OffsetStartMs": 8310, "OffsetEndMs": 8565}, {"Word": "actually", "OffsetStartMs": 8565, "OffsetEndMs": 8880}, {"Word": "the", "OffsetStartMs": 8880, "OffsetEndMs": 9060}, {"Word": "samples", "OffsetStartMs": 9060, "OffsetEndMs": 9405}, {"Word": "that", "OffsetStartMs": 9405, "OffsetEndMs": 9540}, {"Word": "are", "OffsetStartMs": 9540, "OffsetEndMs": 9675}, {"Word": "the", "OffsetStartMs": 9675, "OffsetEndMs": 9810}, {"Word": "most", "OffsetStartMs": 9810, "OffsetEndMs": 10070}, {"Word": "important", "OffsetStartMs": 10090, "OffsetEndMs": 10490}, {"Word": "for", "OffsetStartMs": 10510, "OffsetEndMs": 10755}, {"Word": "the", "OffsetStartMs": 10755, "OffsetEndMs": 10845}, {"Word": "model", "OffsetStartMs": 10845, "OffsetEndMs": 11055}, {"Word": "to", "OffsetStartMs": 11055, "OffsetEndMs": 11265}, {"Word": "learn", "OffsetStartMs": 11265, "OffsetEndMs": 11510}, {"Word": "because", "OffsetStartMs": 11890, "OffsetEndMs": 12165}, {"Word": "they're", "OffsetStartMs": 12165, "OffsetEndMs": 12390}, {"Word": "the", "OffsetStartMs": 12390, "OffsetEndMs": 12495}, {"Word": "hardest", "OffsetStartMs": 12495, "OffsetEndMs": 12840}, {"Word": "samples", "OffsetStartMs": 12840, "OffsetEndMs": 13275}, {"Word": "and", "OffsetStartMs": 13275, "OffsetEndMs": 13455}, {"Word": "they're", "OffsetStartMs": 13455, "OffsetEndMs": 13665}, {"Word": "the", "OffsetStartMs": 13665, "OffsetEndMs": 13770}, {"Word": "samples", "OffsetStartMs": 13770, "OffsetEndMs": 14100}, {"Word": "where", "OffsetStartMs": 14100, "OffsetEndMs": 14235}, {"Word": "the", "OffsetStartMs": 14235, "OffsetEndMs": 14325}, {"Word": "model", "OffsetStartMs": 14325, "OffsetEndMs": 14475}, {"Word": "is", "OffsetStartMs": 14475, "OffsetEndMs": 14715}, {"Word": "most", "OffsetStartMs": 14715, "OffsetEndMs": 14985}, {"Word": "likely", "OffsetStartMs": 14985, "OffsetEndMs": 15300}, {"Word": "to", "OffsetStartMs": 15300, "OffsetEndMs": 15555}, {"Word": "fail", "OffsetStartMs": 15555, "OffsetEndMs": 15830}], "SpeechSpeed": 17.5}, {"FinalSentence": "But in a traditional EM autonomous driving pipeline, these samples are often extremely low, have extremely low representation. So this is an example where using the unsupervised latent biasing that we just talked about, we would be able to up sample these EM important data points and under sample the data points of driving down straight and sunny roads.", "SliceSentence": "But in a traditional EM autonomous driving pipeline these samples are often extremely low have extremely low representation . Sothis is an example where using the unsupervised latent biasing that we just talked about we would be able to up sample these EM important data points and under sample the data points of driving down straight and sunny roads", "StartMs": 1571900, "EndMs": 1593480, "WordsNum": 59, "Words": [{"Word": "But", "OffsetStartMs": 0, "OffsetEndMs": 345}, {"Word": "in", "OffsetStartMs": 345, "OffsetEndMs": 570}, {"Word": "a", "OffsetStartMs": 570, "OffsetEndMs": 735}, {"Word": "traditional", "OffsetStartMs": 735, "OffsetEndMs": 1040}, {"Word": "EM", "OffsetStartMs": 1060, "OffsetEndMs": 1395}, {"Word": "autonomous", "OffsetStartMs": 1395, "OffsetEndMs": 1905}, {"Word": "driving", "OffsetStartMs": 1905, "OffsetEndMs": 2115}, {"Word": "pipeline", "OffsetStartMs": 2115, "OffsetEndMs": 2450}, {"Word": "these", "OffsetStartMs": 2650, "OffsetEndMs": 2940}, {"Word": "samples", "OffsetStartMs": 2940, "OffsetEndMs": 3345}, {"Word": "are", "OffsetStartMs": 3345, "OffsetEndMs": 3480}, {"Word": "often", "OffsetStartMs": 3480, "OffsetEndMs": 3740}, {"Word": "extremely", "OffsetStartMs": 3940, "OffsetEndMs": 4335}, {"Word": "low", "OffsetStartMs": 4335, "OffsetEndMs": 4730}, {"Word": "have", "OffsetStartMs": 5110, "OffsetEndMs": 5475}, {"Word": "extremely", "OffsetStartMs": 5475, "OffsetEndMs": 5790}, {"Word": "low", "OffsetStartMs": 5790, "OffsetEndMs": 6045}, {"Word": "representation", "OffsetStartMs": 6045, "OffsetEndMs": 6800}, {"Word": ".", "OffsetStartMs": 7360, "OffsetEndMs": 7605}, {"Word": "Sothis", "OffsetStartMs": 7605, "OffsetEndMs": 7710}, {"Word": "is", "OffsetStartMs": 7710, "OffsetEndMs": 7815}, {"Word": "an", "OffsetStartMs": 7815, "OffsetEndMs": 7995}, {"Word": "example", "OffsetStartMs": 7995, "OffsetEndMs": 8325}, {"Word": "where", "OffsetStartMs": 8325, "OffsetEndMs": 8640}, {"Word": "using", "OffsetStartMs": 8640, "OffsetEndMs": 8955}, {"Word": "the", "OffsetStartMs": 8955, "OffsetEndMs": 9350}, {"Word": "unsupervised", "OffsetStartMs": 9700, "OffsetEndMs": 10545}, {"Word": "latent", "OffsetStartMs": 10545, "OffsetEndMs": 10950}, {"Word": "biasing", "OffsetStartMs": 10950, "OffsetEndMs": 11370}, {"Word": "that", "OffsetStartMs": 11370, "OffsetEndMs": 11490}, {"Word": "we", "OffsetStartMs": 11490, "OffsetEndMs": 11610}, {"Word": "just", "OffsetStartMs": 11610, "OffsetEndMs": 11775}, {"Word": "talked", "OffsetStartMs": 11775, "OffsetEndMs": 12030}, {"Word": "about", "OffsetStartMs": 12030, "OffsetEndMs": 12380}, {"Word": "we", "OffsetStartMs": 12460, "OffsetEndMs": 12720}, {"Word": "would", "OffsetStartMs": 12720, "OffsetEndMs": 12840}, {"Word": "be", "OffsetStartMs": 12840, "OffsetEndMs": 12960}, {"Word": "able", "OffsetStartMs": 12960, "OffsetEndMs": 13200}, {"Word": "to", "OffsetStartMs": 13200, "OffsetEndMs": 13440}, {"Word": "up", "OffsetStartMs": 13440, "OffsetEndMs": 13635}, {"Word": "sample", "OffsetStartMs": 13635, "OffsetEndMs": 13935}, {"Word": "these", "OffsetStartMs": 13935, "OffsetEndMs": 14300}, {"Word": "EM", "OffsetStartMs": 14350, "OffsetEndMs": 14750}, {"Word": "important", "OffsetStartMs": 15640, "OffsetEndMs": 15990}, {"Word": "data", "OffsetStartMs": 15990, "OffsetEndMs": 16260}, {"Word": "points", "OffsetStartMs": 16260, "OffsetEndMs": 16560}, {"Word": "and", "OffsetStartMs": 16560, "OffsetEndMs": 16845}, {"Word": "under", "OffsetStartMs": 16845, "OffsetEndMs": 17150}, {"Word": "sample", "OffsetStartMs": 17170, "OffsetEndMs": 17570}, {"Word": "the", "OffsetStartMs": 17590, "OffsetEndMs": 17990}, {"Word": "data", "OffsetStartMs": 18040, "OffsetEndMs": 18360}, {"Word": "points", "OffsetStartMs": 18360, "OffsetEndMs": 18645}, {"Word": "of", "OffsetStartMs": 18645, "OffsetEndMs": 18900}, {"Word": "driving", "OffsetStartMs": 18900, "OffsetEndMs": 19155}, {"Word": "down", "OffsetStartMs": 19155, "OffsetEndMs": 19520}, {"Word": "straight", "OffsetStartMs": 19540, "OffsetEndMs": 19875}, {"Word": "and", "OffsetStartMs": 19875, "OffsetEndMs": 20115}, {"Word": "sunny", "OffsetStartMs": 20115, "OffsetEndMs": 20340}, {"Word": "roads", "OffsetStartMs": 20340, "OffsetEndMs": 20660}], "SpeechSpeed": 16.2}, {"FinalSentence": "Similarly, consider the example of large language models. EM, an extremely famous paper a couple years ago showed that if you put terms that imply female or woman into a large language model powered job search engine, you're going to get roles such as artist or things in the humanities. But if you input similar things but of the male counterpart, you put things like mail into the the search engine. You'll end up with roles for scientists and engineers, so this type of bias also occurs regardless of the task at hand for a specific model.", "SliceSentence": "Similarly consider the example of large language models . EMan extremely famous paper a couple years ago showed that if you put terms that imply female or woman into a large language model powered job search engine you're going to get roles such as artist or things in the humanities . Butif you input similar things but of the male counterpart you put things like mail into the the search engine .You'll end up with roles for scientists and engineers so this type of bias also occurs regardless of the task at hand for a specific model", "StartMs": 1595460, "EndMs": 1630020, "WordsNum": 97, "Words": [{"Word": "Similarly", "OffsetStartMs": 130, "OffsetEndMs": 890}, {"Word": "consider", "OffsetStartMs": 1060, "OffsetEndMs": 1410}, {"Word": "the", "OffsetStartMs": 1410, "OffsetEndMs": 1665}, {"Word": "example", "OffsetStartMs": 1665, "OffsetEndMs": 1950}, {"Word": "of", "OffsetStartMs": 1950, "OffsetEndMs": 2235}, {"Word": "large", "OffsetStartMs": 2235, "OffsetEndMs": 2490}, {"Word": "language", "OffsetStartMs": 2490, "OffsetEndMs": 2805}, {"Word": "models", "OffsetStartMs": 2805, "OffsetEndMs": 3170}, {"Word": ".", "OffsetStartMs": 3910, "OffsetEndMs": 4310}, {"Word": "EMan", "OffsetStartMs": 4660, "OffsetEndMs": 5025}, {"Word": "extremely", "OffsetStartMs": 5025, "OffsetEndMs": 5385}, {"Word": "famous", "OffsetStartMs": 5385, "OffsetEndMs": 5745}, {"Word": "paper", "OffsetStartMs": 5745, "OffsetEndMs": 6060}, {"Word": "a", "OffsetStartMs": 6060, "OffsetEndMs": 6285}, {"Word": "couple", "OffsetStartMs": 6285, "OffsetEndMs": 6465}, {"Word": "years", "OffsetStartMs": 6465, "OffsetEndMs": 6705}, {"Word": "ago", "OffsetStartMs": 6705, "OffsetEndMs": 7040}, {"Word": "showed", "OffsetStartMs": 7150, "OffsetEndMs": 7455}, {"Word": "that", "OffsetStartMs": 7455, "OffsetEndMs": 7620}, {"Word": "if", "OffsetStartMs": 7620, "OffsetEndMs": 7755}, {"Word": "you", "OffsetStartMs": 7755, "OffsetEndMs": 7920}, {"Word": "put", "OffsetStartMs": 7920, "OffsetEndMs": 8145}, {"Word": "terms", "OffsetStartMs": 8145, "OffsetEndMs": 8460}, {"Word": "that", "OffsetStartMs": 8460, "OffsetEndMs": 8685}, {"Word": "imply", "OffsetStartMs": 8685, "OffsetEndMs": 9140}, {"Word": "female", "OffsetStartMs": 9370, "OffsetEndMs": 9770}, {"Word": "or", "OffsetStartMs": 9790, "OffsetEndMs": 10080}, {"Word": "woman", "OffsetStartMs": 10080, "OffsetEndMs": 10350}, {"Word": "into", "OffsetStartMs": 10350, "OffsetEndMs": 10730}, {"Word": "a", "OffsetStartMs": 10780, "OffsetEndMs": 11085}, {"Word": "large", "OffsetStartMs": 11085, "OffsetEndMs": 11325}, {"Word": "language", "OffsetStartMs": 11325, "OffsetEndMs": 11640}, {"Word": "model", "OffsetStartMs": 11640, "OffsetEndMs": 12000}, {"Word": "powered", "OffsetStartMs": 12000, "OffsetEndMs": 12495}, {"Word": "job", "OffsetStartMs": 12495, "OffsetEndMs": 12795}, {"Word": "search", "OffsetStartMs": 12795, "OffsetEndMs": 13035}, {"Word": "engine", "OffsetStartMs": 13035, "OffsetEndMs": 13340}, {"Word": "you're", "OffsetStartMs": 13630, "OffsetEndMs": 13950}, {"Word": "going", "OffsetStartMs": 13950, "OffsetEndMs": 14055}, {"Word": "to", "OffsetStartMs": 14055, "OffsetEndMs": 14175}, {"Word": "get", "OffsetStartMs": 14175, "OffsetEndMs": 14325}, {"Word": "roles", "OffsetStartMs": 14325, "OffsetEndMs": 14625}, {"Word": "such", "OffsetStartMs": 14625, "OffsetEndMs": 14910}, {"Word": "as", "OffsetStartMs": 14910, "OffsetEndMs": 15200}, {"Word": "artist", "OffsetStartMs": 15220, "OffsetEndMs": 15620}, {"Word": "or", "OffsetStartMs": 15880, "OffsetEndMs": 16155}, {"Word": "things", "OffsetStartMs": 16155, "OffsetEndMs": 16320}, {"Word": "in", "OffsetStartMs": 16320, "OffsetEndMs": 16455}, {"Word": "the", "OffsetStartMs": 16455, "OffsetEndMs": 16545}, {"Word": "humanities", "OffsetStartMs": 16545, "OffsetEndMs": 17120}, {"Word": ".", "OffsetStartMs": 17620, "OffsetEndMs": 17865}, {"Word": "Butif", "OffsetStartMs": 17865, "OffsetEndMs": 17985}, {"Word": "you", "OffsetStartMs": 17985, "OffsetEndMs": 18260}, {"Word": "input", "OffsetStartMs": 18520, "OffsetEndMs": 18920}, {"Word": "similar", "OffsetStartMs": 19000, "OffsetEndMs": 19380}, {"Word": "things", "OffsetStartMs": 19380, "OffsetEndMs": 19710}, {"Word": "but", "OffsetStartMs": 19710, "OffsetEndMs": 19905}, {"Word": "of", "OffsetStartMs": 19905, "OffsetEndMs": 20010}, {"Word": "the", "OffsetStartMs": 20010, "OffsetEndMs": 20130}, {"Word": "male", "OffsetStartMs": 20130, "OffsetEndMs": 20340}, {"Word": "counterpart", "OffsetStartMs": 20340, "OffsetEndMs": 20955}, {"Word": "you", "OffsetStartMs": 20955, "OffsetEndMs": 21165}, {"Word": "put", "OffsetStartMs": 21165, "OffsetEndMs": 21315}, {"Word": "things", "OffsetStartMs": 21315, "OffsetEndMs": 21495}, {"Word": "like", "OffsetStartMs": 21495, "OffsetEndMs": 21720}, {"Word": "mail", "OffsetStartMs": 21720, "OffsetEndMs": 22020}, {"Word": "into", "OffsetStartMs": 22020, "OffsetEndMs": 22335}, {"Word": "the", "OffsetStartMs": 22335, "OffsetEndMs": 22650}, {"Word": "the", "OffsetStartMs": 22650, "OffsetEndMs": 22890}, {"Word": "search", "OffsetStartMs": 22890, "OffsetEndMs": 23040}, {"Word": "engine", "OffsetStartMs": 23040, "OffsetEndMs": 23330}, {"Word": ".You'll", "OffsetStartMs": 23770, "OffsetEndMs": 24105}, {"Word": "end", "OffsetStartMs": 24105, "OffsetEndMs": 24225}, {"Word": "up", "OffsetStartMs": 24225, "OffsetEndMs": 24375}, {"Word": "with", "OffsetStartMs": 24375, "OffsetEndMs": 24540}, {"Word": "roles", "OffsetStartMs": 24540, "OffsetEndMs": 24830}, {"Word": "for", "OffsetStartMs": 24850, "OffsetEndMs": 25155}, {"Word": "scientists", "OffsetStartMs": 25155, "OffsetEndMs": 25460}, {"Word": "and", "OffsetStartMs": 25870, "OffsetEndMs": 26250}, {"Word": "engineers", "OffsetStartMs": 26250, "OffsetEndMs": 26630}, {"Word": "so", "OffsetStartMs": 27340, "OffsetEndMs": 27740}, {"Word": "this", "OffsetStartMs": 28000, "OffsetEndMs": 28320}, {"Word": "type", "OffsetStartMs": 28320, "OffsetEndMs": 28530}, {"Word": "of", "OffsetStartMs": 28530, "OffsetEndMs": 28820}, {"Word": "bias", "OffsetStartMs": 29350, "OffsetEndMs": 29925}, {"Word": "also", "OffsetStartMs": 29925, "OffsetEndMs": 30255}, {"Word": "occurs", "OffsetStartMs": 30255, "OffsetEndMs": 30590}, {"Word": "regardless", "OffsetStartMs": 30790, "OffsetEndMs": 31185}, {"Word": "of", "OffsetStartMs": 31185, "OffsetEndMs": 31455}, {"Word": "the", "OffsetStartMs": 31455, "OffsetEndMs": 31605}, {"Word": "task", "OffsetStartMs": 31605, "OffsetEndMs": 31875}, {"Word": "at", "OffsetStartMs": 31875, "OffsetEndMs": 32175}, {"Word": "hand", "OffsetStartMs": 32175, "OffsetEndMs": 32480}, {"Word": "for", "OffsetStartMs": 32500, "OffsetEndMs": 32820}, {"Word": "a", "OffsetStartMs": 32820, "OffsetEndMs": 33015}, {"Word": "specific", "OffsetStartMs": 33015, "OffsetEndMs": 33285}, {"Word": "model", "OffsetStartMs": 33285, "OffsetEndMs": 33680}], "SpeechSpeed": 15.4}, {"FinalSentence": "And finally, let's talk about health care recommendation algorithms. These recommendation algorithms tend to amplify racial biases. A paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care, and that's because of inherent bias in the data set of this model. And so in all of these examples, we can use the above algorithmic bias mitigation method to try and solve these problems and more.", "SliceSentence": "And finally let's talk about health care recommendation algorithms . Theserecommendation algorithms tend to amplify racial biases . Apaper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care and that's because of inherent bias in the data set of this model . Andso in all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more", "StartMs": 1631120, "EndMs": 1657640, "WordsNum": 81, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "finally", "OffsetStartMs": 360, "OffsetEndMs": 650}, {"Word": "let's", "OffsetStartMs": 820, "OffsetEndMs": 1200}, {"Word": "talk", "OffsetStartMs": 1200, "OffsetEndMs": 1380}, {"Word": "about", "OffsetStartMs": 1380, "OffsetEndMs": 1605}, {"Word": "health", "OffsetStartMs": 1605, "OffsetEndMs": 1845}, {"Word": "care", "OffsetStartMs": 1845, "OffsetEndMs": 2040}, {"Word": "recommendation", "OffsetStartMs": 2040, "OffsetEndMs": 2600}, {"Word": "algorithms", "OffsetStartMs": 2710, "OffsetEndMs": 3230}, {"Word": ".", "OffsetStartMs": 3490, "OffsetEndMs": 3780}, {"Word": "Theserecommendation", "OffsetStartMs": 3780, "OffsetEndMs": 4395}, {"Word": "algorithms", "OffsetStartMs": 4395, "OffsetEndMs": 4845}, {"Word": "tend", "OffsetStartMs": 4845, "OffsetEndMs": 5085}, {"Word": "to", "OffsetStartMs": 5085, "OffsetEndMs": 5235}, {"Word": "amplify", "OffsetStartMs": 5235, "OffsetEndMs": 5685}, {"Word": "racial", "OffsetStartMs": 5685, "OffsetEndMs": 6045}, {"Word": "biases", "OffsetStartMs": 6045, "OffsetEndMs": 6620}, {"Word": ".", "OffsetStartMs": 6790, "OffsetEndMs": 7065}, {"Word": "Apaper", "OffsetStartMs": 7065, "OffsetEndMs": 7275}, {"Word": "from", "OffsetStartMs": 7275, "OffsetEndMs": 7455}, {"Word": "a", "OffsetStartMs": 7455, "OffsetEndMs": 7545}, {"Word": "couple", "OffsetStartMs": 7545, "OffsetEndMs": 7695}, {"Word": "years", "OffsetStartMs": 7695, "OffsetEndMs": 7950}, {"Word": "ago", "OffsetStartMs": 7950, "OffsetEndMs": 8300}, {"Word": "showed", "OffsetStartMs": 8410, "OffsetEndMs": 8760}, {"Word": "that", "OffsetStartMs": 8760, "OffsetEndMs": 9045}, {"Word": "black", "OffsetStartMs": 9045, "OffsetEndMs": 9315}, {"Word": "patients", "OffsetStartMs": 9315, "OffsetEndMs": 9650}, {"Word": "need", "OffsetStartMs": 9670, "OffsetEndMs": 9945}, {"Word": "to", "OffsetStartMs": 9945, "OffsetEndMs": 10065}, {"Word": "be", "OffsetStartMs": 10065, "OffsetEndMs": 10200}, {"Word": "significantly", "OffsetStartMs": 10200, "OffsetEndMs": 10490}, {"Word": "sicker", "OffsetStartMs": 10870, "OffsetEndMs": 11450}, {"Word": "than", "OffsetStartMs": 11530, "OffsetEndMs": 11820}, {"Word": "their", "OffsetStartMs": 11820, "OffsetEndMs": 12000}, {"Word": "white", "OffsetStartMs": 12000, "OffsetEndMs": 12240}, {"Word": "counterparts", "OffsetStartMs": 12240, "OffsetEndMs": 12870}, {"Word": "to", "OffsetStartMs": 12870, "OffsetEndMs": 13005}, {"Word": "get", "OffsetStartMs": 13005, "OffsetEndMs": 13140}, {"Word": "the", "OffsetStartMs": 13140, "OffsetEndMs": 13335}, {"Word": "same", "OffsetStartMs": 13335, "OffsetEndMs": 13575}, {"Word": "level", "OffsetStartMs": 13575, "OffsetEndMs": 13815}, {"Word": "of", "OffsetStartMs": 13815, "OffsetEndMs": 14040}, {"Word": "care", "OffsetStartMs": 14040, "OffsetEndMs": 14360}, {"Word": "and", "OffsetStartMs": 14710, "OffsetEndMs": 15000}, {"Word": "that's", "OffsetStartMs": 15000, "OffsetEndMs": 15375}, {"Word": "because", "OffsetStartMs": 15375, "OffsetEndMs": 15630}, {"Word": "of", "OffsetStartMs": 15630, "OffsetEndMs": 15900}, {"Word": "inherent", "OffsetStartMs": 15900, "OffsetEndMs": 16425}, {"Word": "bias", "OffsetStartMs": 16425, "OffsetEndMs": 16875}, {"Word": "in", "OffsetStartMs": 16875, "OffsetEndMs": 17040}, {"Word": "the", "OffsetStartMs": 17040, "OffsetEndMs": 17160}, {"Word": "data", "OffsetStartMs": 17160, "OffsetEndMs": 17355}, {"Word": "set", "OffsetStartMs": 17355, "OffsetEndMs": 17550}, {"Word": "of", "OffsetStartMs": 17550, "OffsetEndMs": 17685}, {"Word": "this", "OffsetStartMs": 17685, "OffsetEndMs": 17850}, {"Word": "model", "OffsetStartMs": 17850, "OffsetEndMs": 18140}, {"Word": ".", "OffsetStartMs": 18610, "OffsetEndMs": 18930}, {"Word": "Andso", "OffsetStartMs": 18930, "OffsetEndMs": 19140}, {"Word": "in", "OffsetStartMs": 19140, "OffsetEndMs": 19305}, {"Word": "all", "OffsetStartMs": 19305, "OffsetEndMs": 19470}, {"Word": "of", "OffsetStartMs": 19470, "OffsetEndMs": 19620}, {"Word": "these", "OffsetStartMs": 19620, "OffsetEndMs": 19845}, {"Word": "examples", "OffsetStartMs": 19845, "OffsetEndMs": 20210}, {"Word": "we", "OffsetStartMs": 20260, "OffsetEndMs": 20520}, {"Word": "can", "OffsetStartMs": 20520, "OffsetEndMs": 20670}, {"Word": "use", "OffsetStartMs": 20670, "OffsetEndMs": 20895}, {"Word": "the", "OffsetStartMs": 20895, "OffsetEndMs": 21150}, {"Word": "above", "OffsetStartMs": 21150, "OffsetEndMs": 21470}, {"Word": "algorithmic", "OffsetStartMs": 21610, "OffsetEndMs": 22140}, {"Word": "bias", "OffsetStartMs": 22140, "OffsetEndMs": 22485}, {"Word": "mitigation", "OffsetStartMs": 22485, "OffsetEndMs": 22935}, {"Word": "method", "OffsetStartMs": 22935, "OffsetEndMs": 23300}, {"Word": "to", "OffsetStartMs": 23470, "OffsetEndMs": 23760}, {"Word": "try", "OffsetStartMs": 23760, "OffsetEndMs": 23940}, {"Word": "and", "OffsetStartMs": 23940, "OffsetEndMs": 24120}, {"Word": "solve", "OffsetStartMs": 24120, "OffsetEndMs": 24360}, {"Word": "these", "OffsetStartMs": 24360, "OffsetEndMs": 24645}, {"Word": "problems", "OffsetStartMs": 24645, "OffsetEndMs": 24980}, {"Word": "and", "OffsetStartMs": 25060, "OffsetEndMs": 25350}, {"Word": "more", "OffsetStartMs": 25350, "OffsetEndMs": 25640}], "SpeechSpeed": 17.9}, {"FinalSentence": "So we just went through how to mitigate some forms of bias in artificial intelligence and where these solutions may be applied. And we talked about a foundational algorithm that themis uses that you all will also be developing today. And for the next part of the lecture, we'll focus on uncertainty or when a model does not know the answer.", "SliceSentence": "So we just went through how to mitigate some forms of bias in artificial intelligence and where these solutions may be applied . Andwe talked about a foundational algorithm that themis uses that you all will also be developing today . Andfor the next part of the lecture we'll focus on uncertainty or when a model does not know the answer", "StartMs": 1660200, "EndMs": 1680560, "WordsNum": 61, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "we", "OffsetStartMs": 850, "OffsetEndMs": 1125}, {"Word": "just", "OffsetStartMs": 1125, "OffsetEndMs": 1290}, {"Word": "went", "OffsetStartMs": 1290, "OffsetEndMs": 1515}, {"Word": "through", "OffsetStartMs": 1515, "OffsetEndMs": 1800}, {"Word": "how", "OffsetStartMs": 1800, "OffsetEndMs": 2025}, {"Word": "to", "OffsetStartMs": 2025, "OffsetEndMs": 2160}, {"Word": "mitigate", "OffsetStartMs": 2160, "OffsetEndMs": 2580}, {"Word": "some", "OffsetStartMs": 2580, "OffsetEndMs": 2895}, {"Word": "forms", "OffsetStartMs": 2895, "OffsetEndMs": 3165}, {"Word": "of", "OffsetStartMs": 3165, "OffsetEndMs": 3345}, {"Word": "bias", "OffsetStartMs": 3345, "OffsetEndMs": 3675}, {"Word": "in", "OffsetStartMs": 3675, "OffsetEndMs": 3930}, {"Word": "artificial", "OffsetStartMs": 3930, "OffsetEndMs": 4245}, {"Word": "intelligence", "OffsetStartMs": 4245, "OffsetEndMs": 4610}, {"Word": "and", "OffsetStartMs": 4840, "OffsetEndMs": 5130}, {"Word": "where", "OffsetStartMs": 5130, "OffsetEndMs": 5325}, {"Word": "these", "OffsetStartMs": 5325, "OffsetEndMs": 5505}, {"Word": "solutions", "OffsetStartMs": 5505, "OffsetEndMs": 5780}, {"Word": "may", "OffsetStartMs": 5890, "OffsetEndMs": 6165}, {"Word": "be", "OffsetStartMs": 6165, "OffsetEndMs": 6360}, {"Word": "applied", "OffsetStartMs": 6360, "OffsetEndMs": 6680}, {"Word": ".", "OffsetStartMs": 7270, "OffsetEndMs": 7545}, {"Word": "Andwe", "OffsetStartMs": 7545, "OffsetEndMs": 7710}, {"Word": "talked", "OffsetStartMs": 7710, "OffsetEndMs": 7950}, {"Word": "about", "OffsetStartMs": 7950, "OffsetEndMs": 8300}, {"Word": "a", "OffsetStartMs": 8320, "OffsetEndMs": 8610}, {"Word": "foundational", "OffsetStartMs": 8610, "OffsetEndMs": 9080}, {"Word": "algorithm", "OffsetStartMs": 9310, "OffsetEndMs": 9735}, {"Word": "that", "OffsetStartMs": 9735, "OffsetEndMs": 9900}, {"Word": "themis", "OffsetStartMs": 9900, "OffsetEndMs": 10230}, {"Word": "uses", "OffsetStartMs": 10230, "OffsetEndMs": 10520}, {"Word": "that", "OffsetStartMs": 10630, "OffsetEndMs": 10905}, {"Word": "you", "OffsetStartMs": 10905, "OffsetEndMs": 11040}, {"Word": "all", "OffsetStartMs": 11040, "OffsetEndMs": 11205}, {"Word": "will", "OffsetStartMs": 11205, "OffsetEndMs": 11460}, {"Word": "also", "OffsetStartMs": 11460, "OffsetEndMs": 11655}, {"Word": "be", "OffsetStartMs": 11655, "OffsetEndMs": 11790}, {"Word": "developing", "OffsetStartMs": 11790, "OffsetEndMs": 12075}, {"Word": "today", "OffsetStartMs": 12075, "OffsetEndMs": 12470}, {"Word": ".", "OffsetStartMs": 13210, "OffsetEndMs": 13605}, {"Word": "Andfor", "OffsetStartMs": 13605, "OffsetEndMs": 13860}, {"Word": "the", "OffsetStartMs": 13860, "OffsetEndMs": 13965}, {"Word": "next", "OffsetStartMs": 13965, "OffsetEndMs": 14160}, {"Word": "part", "OffsetStartMs": 14160, "OffsetEndMs": 14355}, {"Word": "of", "OffsetStartMs": 14355, "OffsetEndMs": 14445}, {"Word": "the", "OffsetStartMs": 14445, "OffsetEndMs": 14535}, {"Word": "lecture", "OffsetStartMs": 14535, "OffsetEndMs": 14780}, {"Word": "we'll", "OffsetStartMs": 15160, "OffsetEndMs": 15510}, {"Word": "focus", "OffsetStartMs": 15510, "OffsetEndMs": 15770}, {"Word": "on", "OffsetStartMs": 15820, "OffsetEndMs": 16220}, {"Word": "uncertainty", "OffsetStartMs": 16270, "OffsetEndMs": 16670}, {"Word": "or", "OffsetStartMs": 16990, "OffsetEndMs": 17280}, {"Word": "when", "OffsetStartMs": 17280, "OffsetEndMs": 17505}, {"Word": "a", "OffsetStartMs": 17505, "OffsetEndMs": 17730}, {"Word": "model", "OffsetStartMs": 17730, "OffsetEndMs": 18020}, {"Word": "does", "OffsetStartMs": 18070, "OffsetEndMs": 18375}, {"Word": "not", "OffsetStartMs": 18375, "OffsetEndMs": 18615}, {"Word": "know", "OffsetStartMs": 18615, "OffsetEndMs": 18840}, {"Word": "the", "OffsetStartMs": 18840, "OffsetEndMs": 18975}, {"Word": "answer", "OffsetStartMs": 18975, "OffsetEndMs": 19220}], "SpeechSpeed": 16.5}, {"FinalSentence": "We'll talk about why uncertainty is important and how we can estimate it, and also the applications of uncertainty estimation.", "SliceSentence": "We'll talk about why uncertainty is important and how we can estimate it and also the applications of uncertainty estimation", "StartMs": 1680560, "EndMs": 1688320, "WordsNum": 20, "Words": [{"Word": "We'll", "OffsetStartMs": 0, "OffsetEndMs": 195}, {"Word": "talk", "OffsetStartMs": 195, "OffsetEndMs": 360}, {"Word": "about", "OffsetStartMs": 360, "OffsetEndMs": 600}, {"Word": "why", "OffsetStartMs": 600, "OffsetEndMs": 920}, {"Word": "uncertainty", "OffsetStartMs": 970, "OffsetEndMs": 1350}, {"Word": "is", "OffsetStartMs": 1350, "OffsetEndMs": 1665}, {"Word": "important", "OffsetStartMs": 1665, "OffsetEndMs": 2000}, {"Word": "and", "OffsetStartMs": 2290, "OffsetEndMs": 2610}, {"Word": "how", "OffsetStartMs": 2610, "OffsetEndMs": 2925}, {"Word": "we", "OffsetStartMs": 2925, "OffsetEndMs": 3180}, {"Word": "can", "OffsetStartMs": 3180, "OffsetEndMs": 3440}, {"Word": "estimate", "OffsetStartMs": 3490, "OffsetEndMs": 3750}, {"Word": "it", "OffsetStartMs": 3750, "OffsetEndMs": 4005}, {"Word": "and", "OffsetStartMs": 4005, "OffsetEndMs": 4395}, {"Word": "also", "OffsetStartMs": 4395, "OffsetEndMs": 4680}, {"Word": "the", "OffsetStartMs": 4680, "OffsetEndMs": 4965}, {"Word": "applications", "OffsetStartMs": 4965, "OffsetEndMs": 5340}, {"Word": "of", "OffsetStartMs": 5340, "OffsetEndMs": 5670}, {"Word": "uncertainty", "OffsetStartMs": 5670, "OffsetEndMs": 6020}, {"Word": "estimation", "OffsetStartMs": 6220, "OffsetEndMs": 6710}], "SpeechSpeed": 16.0}, {"FinalSentence": "So, to start with, what is uncertainty and why is it necessary to compute?", "SliceSentence": "So to start with what is uncertainty and why is it necessary to compute", "StartMs": 1688440, "EndMs": 1693720, "WordsNum": 14, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 540}, {"Word": "to", "OffsetStartMs": 540, "OffsetEndMs": 795}, {"Word": "start", "OffsetStartMs": 795, "OffsetEndMs": 975}, {"Word": "with", "OffsetStartMs": 975, "OffsetEndMs": 1280}, {"Word": "what", "OffsetStartMs": 1420, "OffsetEndMs": 1740}, {"Word": "is", "OffsetStartMs": 1740, "OffsetEndMs": 2055}, {"Word": "uncertainty", "OffsetStartMs": 2055, "OffsetEndMs": 2450}, {"Word": "and", "OffsetStartMs": 2620, "OffsetEndMs": 2925}, {"Word": "why", "OffsetStartMs": 2925, "OffsetEndMs": 3105}, {"Word": "is", "OffsetStartMs": 3105, "OffsetEndMs": 3240}, {"Word": "it", "OffsetStartMs": 3240, "OffsetEndMs": 3495}, {"Word": "necessary", "OffsetStartMs": 3495, "OffsetEndMs": 3810}, {"Word": "to", "OffsetStartMs": 3810, "OffsetEndMs": 3975}, {"Word": "compute", "OffsetStartMs": 3975, "OffsetEndMs": 4370}], "SpeechSpeed": 13.4}, {"FinalSentence": "Let's look at the following example. This is a binary classifier that is trained on images of cats and dogs. For every single input, it will output a probability distribution over these two classes.", "SliceSentence": "Let's look at the following example . Thisis a binary classifier that is trained on images of cats and dogs . Forevery single input it will output a probability distribution over these two classes", "StartMs": 1693720, "EndMs": 1706380, "WordsNum": 34, "Words": [{"Word": "Let's", "OffsetStartMs": 0, "OffsetEndMs": 285}, {"Word": "look", "OffsetStartMs": 285, "OffsetEndMs": 375}, {"Word": "at", "OffsetStartMs": 375, "OffsetEndMs": 480}, {"Word": "the", "OffsetStartMs": 480, "OffsetEndMs": 600}, {"Word": "following", "OffsetStartMs": 600, "OffsetEndMs": 860}, {"Word": "example", "OffsetStartMs": 970, "OffsetEndMs": 1370}, {"Word": ".", "OffsetStartMs": 1840, "OffsetEndMs": 2130}, {"Word": "Thisis", "OffsetStartMs": 2130, "OffsetEndMs": 2340}, {"Word": "a", "OffsetStartMs": 2340, "OffsetEndMs": 2580}, {"Word": "binary", "OffsetStartMs": 2580, "OffsetEndMs": 3015}, {"Word": "classifier", "OffsetStartMs": 3015, "OffsetEndMs": 3630}, {"Word": "that", "OffsetStartMs": 3630, "OffsetEndMs": 3855}, {"Word": "is", "OffsetStartMs": 3855, "OffsetEndMs": 4020}, {"Word": "trained", "OffsetStartMs": 4020, "OffsetEndMs": 4260}, {"Word": "on", "OffsetStartMs": 4260, "OffsetEndMs": 4470}, {"Word": "images", "OffsetStartMs": 4470, "OffsetEndMs": 4760}, {"Word": "of", "OffsetStartMs": 4780, "OffsetEndMs": 5100}, {"Word": "cats", "OffsetStartMs": 5100, "OffsetEndMs": 5400}, {"Word": "and", "OffsetStartMs": 5400, "OffsetEndMs": 5655}, {"Word": "dogs", "OffsetStartMs": 5655, "OffsetEndMs": 5930}, {"Word": ".", "OffsetStartMs": 6520, "OffsetEndMs": 6795}, {"Word": "Forevery", "OffsetStartMs": 6795, "OffsetEndMs": 7065}, {"Word": "single", "OffsetStartMs": 7065, "OffsetEndMs": 7460}, {"Word": "input", "OffsetStartMs": 7510, "OffsetEndMs": 7860}, {"Word": "it", "OffsetStartMs": 7860, "OffsetEndMs": 8085}, {"Word": "will", "OffsetStartMs": 8085, "OffsetEndMs": 8360}, {"Word": "output", "OffsetStartMs": 8380, "OffsetEndMs": 8640}, {"Word": "a", "OffsetStartMs": 8640, "OffsetEndMs": 8805}, {"Word": "probability", "OffsetStartMs": 8805, "OffsetEndMs": 9320}, {"Word": "distribution", "OffsetStartMs": 9400, "OffsetEndMs": 9800}, {"Word": "over", "OffsetStartMs": 10060, "OffsetEndMs": 10425}, {"Word": "these", "OffsetStartMs": 10425, "OffsetEndMs": 10740}, {"Word": "two", "OffsetStartMs": 10740, "OffsetEndMs": 10995}, {"Word": "classes", "OffsetStartMs": 10995, "OffsetEndMs": 11300}], "SpeechSpeed": 15.3}, {"FinalSentence": "Now, let's say I give this model an image of a horse. It's never seen a horse before. The horse is clearly neither a cat nor a dog. However, the model has no choice but to output a probability distribution, because that's how this model is structured.", "SliceSentence": "Now let's say I give this model an image of a horse .It's never seen a horse before . Thehorse is clearly neither a cat nor a dog . Howeverthe model has no choice but to output a probability distribution because that's how this model is structured", "StartMs": 1707480, "EndMs": 1722040, "WordsNum": 47, "Words": [{"Word": "Now", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "let's", "OffsetStartMs": 580, "OffsetEndMs": 915}, {"Word": "say", "OffsetStartMs": 915, "OffsetEndMs": 1005}, {"Word": "I", "OffsetStartMs": 1005, "OffsetEndMs": 1110}, {"Word": "give", "OffsetStartMs": 1110, "OffsetEndMs": 1245}, {"Word": "this", "OffsetStartMs": 1245, "OffsetEndMs": 1395}, {"Word": "model", "OffsetStartMs": 1395, "OffsetEndMs": 1620}, {"Word": "an", "OffsetStartMs": 1620, "OffsetEndMs": 1815}, {"Word": "image", "OffsetStartMs": 1815, "OffsetEndMs": 1995}, {"Word": "of", "OffsetStartMs": 1995, "OffsetEndMs": 2175}, {"Word": "a", "OffsetStartMs": 2175, "OffsetEndMs": 2280}, {"Word": "horse", "OffsetStartMs": 2280, "OffsetEndMs": 2540}, {"Word": ".It's", "OffsetStartMs": 3010, "OffsetEndMs": 3345}, {"Word": "never", "OffsetStartMs": 3345, "OffsetEndMs": 3525}, {"Word": "seen", "OffsetStartMs": 3525, "OffsetEndMs": 3720}, {"Word": "a", "OffsetStartMs": 3720, "OffsetEndMs": 3825}, {"Word": "horse", "OffsetStartMs": 3825, "OffsetEndMs": 3990}, {"Word": "before", "OffsetStartMs": 3990, "OffsetEndMs": 4310}, {"Word": ".", "OffsetStartMs": 4600, "OffsetEndMs": 4845}, {"Word": "Thehorse", "OffsetStartMs": 4845, "OffsetEndMs": 4995}, {"Word": "is", "OffsetStartMs": 4995, "OffsetEndMs": 5205}, {"Word": "clearly", "OffsetStartMs": 5205, "OffsetEndMs": 5505}, {"Word": "neither", "OffsetStartMs": 5505, "OffsetEndMs": 5835}, {"Word": "a", "OffsetStartMs": 5835, "OffsetEndMs": 6060}, {"Word": "cat", "OffsetStartMs": 6060, "OffsetEndMs": 6350}, {"Word": "nor", "OffsetStartMs": 6370, "OffsetEndMs": 6630}, {"Word": "a", "OffsetStartMs": 6630, "OffsetEndMs": 6750}, {"Word": "dog", "OffsetStartMs": 6750, "OffsetEndMs": 7010}, {"Word": ".", "OffsetStartMs": 7630, "OffsetEndMs": 8010}, {"Word": "Howeverthe", "OffsetStartMs": 8010, "OffsetEndMs": 8250}, {"Word": "model", "OffsetStartMs": 8250, "OffsetEndMs": 8475}, {"Word": "has", "OffsetStartMs": 8475, "OffsetEndMs": 8790}, {"Word": "no", "OffsetStartMs": 8790, "OffsetEndMs": 9105}, {"Word": "choice", "OffsetStartMs": 9105, "OffsetEndMs": 9405}, {"Word": "but", "OffsetStartMs": 9405, "OffsetEndMs": 9630}, {"Word": "to", "OffsetStartMs": 9630, "OffsetEndMs": 9900}, {"Word": "output", "OffsetStartMs": 9900, "OffsetEndMs": 10275}, {"Word": "a", "OffsetStartMs": 10275, "OffsetEndMs": 10560}, {"Word": "probability", "OffsetStartMs": 10560, "OffsetEndMs": 11060}, {"Word": "distribution", "OffsetStartMs": 11080, "OffsetEndMs": 11480}, {"Word": "because", "OffsetStartMs": 11710, "OffsetEndMs": 11985}, {"Word": "that's", "OffsetStartMs": 11985, "OffsetEndMs": 12255}, {"Word": "how", "OffsetStartMs": 12255, "OffsetEndMs": 12375}, {"Word": "this", "OffsetStartMs": 12375, "OffsetEndMs": 12540}, {"Word": "model", "OffsetStartMs": 12540, "OffsetEndMs": 12735}, {"Word": "is", "OffsetStartMs": 12735, "OffsetEndMs": 12930}, {"Word": "structured", "OffsetStartMs": 12930, "OffsetEndMs": 13430}], "SpeechSpeed": 16.8}, {"FinalSentence": "However, what if in addition to this prediction, we also achieved a confidence estimate, in this case the model which should be able to say, I've never seen anything like this before and I have very low confidence in this prediction. So you as the user should not trust my prediction on this model, and that's the core idea behind uncertainty estimation.", "SliceSentence": "However what if in addition to this prediction we also achieved a confidence estimate in this case the model which should be able to say I've never seen anything like this before and I have very low confidence in this prediction . Soyou as the user should not trust my prediction on this model and that's the core idea behind uncertainty estimation", "StartMs": 1722620, "EndMs": 1743080, "WordsNum": 62, "Words": [{"Word": "However", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "what", "OffsetStartMs": 760, "OffsetEndMs": 1035}, {"Word": "if", "OffsetStartMs": 1035, "OffsetEndMs": 1215}, {"Word": "in", "OffsetStartMs": 1215, "OffsetEndMs": 1440}, {"Word": "addition", "OffsetStartMs": 1440, "OffsetEndMs": 1695}, {"Word": "to", "OffsetStartMs": 1695, "OffsetEndMs": 1875}, {"Word": "this", "OffsetStartMs": 1875, "OffsetEndMs": 2025}, {"Word": "prediction", "OffsetStartMs": 2025, "OffsetEndMs": 2390}, {"Word": "we", "OffsetStartMs": 2680, "OffsetEndMs": 3060}, {"Word": "also", "OffsetStartMs": 3060, "OffsetEndMs": 3440}, {"Word": "achieved", "OffsetStartMs": 3670, "OffsetEndMs": 4020}, {"Word": "a", "OffsetStartMs": 4020, "OffsetEndMs": 4275}, {"Word": "confidence", "OffsetStartMs": 4275, "OffsetEndMs": 4580}, {"Word": "estimate", "OffsetStartMs": 4960, "OffsetEndMs": 5360}, {"Word": "in", "OffsetStartMs": 5890, "OffsetEndMs": 6180}, {"Word": "this", "OffsetStartMs": 6180, "OffsetEndMs": 6390}, {"Word": "case", "OffsetStartMs": 6390, "OffsetEndMs": 6710}, {"Word": "the", "OffsetStartMs": 6790, "OffsetEndMs": 7050}, {"Word": "model", "OffsetStartMs": 7050, "OffsetEndMs": 7245}, {"Word": "which", "OffsetStartMs": 7245, "OffsetEndMs": 7485}, {"Word": "should", "OffsetStartMs": 7485, "OffsetEndMs": 7650}, {"Word": "be", "OffsetStartMs": 7650, "OffsetEndMs": 7755}, {"Word": "able", "OffsetStartMs": 7755, "OffsetEndMs": 7890}, {"Word": "to", "OffsetStartMs": 7890, "OffsetEndMs": 8055}, {"Word": "say", "OffsetStartMs": 8055, "OffsetEndMs": 8330}, {"Word": "I've", "OffsetStartMs": 9010, "OffsetEndMs": 9405}, {"Word": "never", "OffsetStartMs": 9405, "OffsetEndMs": 9600}, {"Word": "seen", "OffsetStartMs": 9600, "OffsetEndMs": 9915}, {"Word": "anything", "OffsetStartMs": 9915, "OffsetEndMs": 10140}, {"Word": "like", "OffsetStartMs": 10140, "OffsetEndMs": 10290}, {"Word": "this", "OffsetStartMs": 10290, "OffsetEndMs": 10455}, {"Word": "before", "OffsetStartMs": 10455, "OffsetEndMs": 10710}, {"Word": "and", "OffsetStartMs": 10710, "OffsetEndMs": 10935}, {"Word": "I", "OffsetStartMs": 10935, "OffsetEndMs": 11040}, {"Word": "have", "OffsetStartMs": 11040, "OffsetEndMs": 11175}, {"Word": "very", "OffsetStartMs": 11175, "OffsetEndMs": 11445}, {"Word": "low", "OffsetStartMs": 11445, "OffsetEndMs": 11790}, {"Word": "confidence", "OffsetStartMs": 11790, "OffsetEndMs": 12140}, {"Word": "in", "OffsetStartMs": 12190, "OffsetEndMs": 12450}, {"Word": "this", "OffsetStartMs": 12450, "OffsetEndMs": 12615}, {"Word": "prediction", "OffsetStartMs": 12615, "OffsetEndMs": 12980}, {"Word": ".", "OffsetStartMs": 13270, "OffsetEndMs": 13560}, {"Word": "Soyou", "OffsetStartMs": 13560, "OffsetEndMs": 13785}, {"Word": "as", "OffsetStartMs": 13785, "OffsetEndMs": 14025}, {"Word": "the", "OffsetStartMs": 14025, "OffsetEndMs": 14190}, {"Word": "user", "OffsetStartMs": 14190, "OffsetEndMs": 14450}, {"Word": "should", "OffsetStartMs": 14470, "OffsetEndMs": 14745}, {"Word": "not", "OffsetStartMs": 14745, "OffsetEndMs": 14940}, {"Word": "trust", "OffsetStartMs": 14940, "OffsetEndMs": 15165}, {"Word": "my", "OffsetStartMs": 15165, "OffsetEndMs": 15360}, {"Word": "prediction", "OffsetStartMs": 15360, "OffsetEndMs": 15630}, {"Word": "on", "OffsetStartMs": 15630, "OffsetEndMs": 15810}, {"Word": "this", "OffsetStartMs": 15810, "OffsetEndMs": 15945}, {"Word": "model", "OffsetStartMs": 15945, "OffsetEndMs": 16220}, {"Word": "and", "OffsetStartMs": 16780, "OffsetEndMs": 17055}, {"Word": "that's", "OffsetStartMs": 17055, "OffsetEndMs": 17340}, {"Word": "the", "OffsetStartMs": 17340, "OffsetEndMs": 17475}, {"Word": "core", "OffsetStartMs": 17475, "OffsetEndMs": 17700}, {"Word": "idea", "OffsetStartMs": 17700, "OffsetEndMs": 18030}, {"Word": "behind", "OffsetStartMs": 18030, "OffsetEndMs": 18405}, {"Word": "uncertainty", "OffsetStartMs": 18405, "OffsetEndMs": 18800}, {"Word": "estimation", "OffsetStartMs": 18940, "OffsetEndMs": 19430}], "SpeechSpeed": 17.0}, {"FinalSentence": "So in the real world, uncertainty estimation is useful for scenarios like this. This is an example of a Tesla car driving behind a horse drawn buggy, which are very common in some parts of the United States. It has no idea what this horse drawn buggy is. It first thinks it's a truck and then a car and then a person, and it continues to output predictions, even though it is very clear that the model does not know what this image is.", "SliceSentence": "So in the real world uncertainty estimation is useful for scenarios like this . Thisis an example of a Tesla car driving behind a horse drawn buggy which are very common in some parts of the United States . Ithas no idea what this horse drawn buggy is . Itfirst thinks it's a truck and then a car and then a person and it continues to output predictions even though it is very clear that the model does not know what this image is", "StartMs": 1743540, "EndMs": 1770460, "WordsNum": 84, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 525}, {"Word": "in", "OffsetStartMs": 525, "OffsetEndMs": 750}, {"Word": "the", "OffsetStartMs": 750, "OffsetEndMs": 870}, {"Word": "real", "OffsetStartMs": 870, "OffsetEndMs": 1035}, {"Word": "world", "OffsetStartMs": 1035, "OffsetEndMs": 1340}, {"Word": "uncertainty", "OffsetStartMs": 1510, "OffsetEndMs": 1910}, {"Word": "estimation", "OffsetStartMs": 1990, "OffsetEndMs": 2480}, {"Word": "is", "OffsetStartMs": 2500, "OffsetEndMs": 2760}, {"Word": "useful", "OffsetStartMs": 2760, "OffsetEndMs": 3020}, {"Word": "for", "OffsetStartMs": 3040, "OffsetEndMs": 3315}, {"Word": "scenarios", "OffsetStartMs": 3315, "OffsetEndMs": 3705}, {"Word": "like", "OffsetStartMs": 3705, "OffsetEndMs": 3930}, {"Word": "this", "OffsetStartMs": 3930, "OffsetEndMs": 4250}, {"Word": ".", "OffsetStartMs": 4690, "OffsetEndMs": 4965}, {"Word": "Thisis", "OffsetStartMs": 4965, "OffsetEndMs": 5160}, {"Word": "an", "OffsetStartMs": 5160, "OffsetEndMs": 5385}, {"Word": "example", "OffsetStartMs": 5385, "OffsetEndMs": 5690}, {"Word": "of", "OffsetStartMs": 5740, "OffsetEndMs": 6060}, {"Word": "a", "OffsetStartMs": 6060, "OffsetEndMs": 6285}, {"Word": "Tesla", "OffsetStartMs": 6285, "OffsetEndMs": 6660}, {"Word": "car", "OffsetStartMs": 6660, "OffsetEndMs": 6945}, {"Word": "driving", "OffsetStartMs": 6945, "OffsetEndMs": 7340}, {"Word": "behind", "OffsetStartMs": 7540, "OffsetEndMs": 7860}, {"Word": "a", "OffsetStartMs": 7860, "OffsetEndMs": 8070}, {"Word": "horse", "OffsetStartMs": 8070, "OffsetEndMs": 8295}, {"Word": "drawn", "OffsetStartMs": 8295, "OffsetEndMs": 8550}, {"Word": "buggy", "OffsetStartMs": 8550, "OffsetEndMs": 9050}, {"Word": "which", "OffsetStartMs": 9490, "OffsetEndMs": 9765}, {"Word": "are", "OffsetStartMs": 9765, "OffsetEndMs": 9930}, {"Word": "very", "OffsetStartMs": 9930, "OffsetEndMs": 10140}, {"Word": "common", "OffsetStartMs": 10140, "OffsetEndMs": 10395}, {"Word": "in", "OffsetStartMs": 10395, "OffsetEndMs": 10605}, {"Word": "some", "OffsetStartMs": 10605, "OffsetEndMs": 10770}, {"Word": "parts", "OffsetStartMs": 10770, "OffsetEndMs": 10950}, {"Word": "of", "OffsetStartMs": 10950, "OffsetEndMs": 11070}, {"Word": "the", "OffsetStartMs": 11070, "OffsetEndMs": 11175}, {"Word": "United", "OffsetStartMs": 11175, "OffsetEndMs": 11415}, {"Word": "States", "OffsetStartMs": 11415, "OffsetEndMs": 11780}, {"Word": ".", "OffsetStartMs": 12220, "OffsetEndMs": 12495}, {"Word": "Ithas", "OffsetStartMs": 12495, "OffsetEndMs": 12770}, {"Word": "no", "OffsetStartMs": 12790, "OffsetEndMs": 13155}, {"Word": "idea", "OffsetStartMs": 13155, "OffsetEndMs": 13425}, {"Word": "what", "OffsetStartMs": 13425, "OffsetEndMs": 13590}, {"Word": "this", "OffsetStartMs": 13590, "OffsetEndMs": 13710}, {"Word": "horse", "OffsetStartMs": 13710, "OffsetEndMs": 13890}, {"Word": "drawn", "OffsetStartMs": 13890, "OffsetEndMs": 14100}, {"Word": "buggy", "OffsetStartMs": 14100, "OffsetEndMs": 14400}, {"Word": "is", "OffsetStartMs": 14400, "OffsetEndMs": 14655}, {"Word": ".", "OffsetStartMs": 14655, "OffsetEndMs": 14940}, {"Word": "Itfirst", "OffsetStartMs": 14940, "OffsetEndMs": 15135}, {"Word": "thinks", "OffsetStartMs": 15135, "OffsetEndMs": 15360}, {"Word": "it's", "OffsetStartMs": 15360, "OffsetEndMs": 15600}, {"Word": "a", "OffsetStartMs": 15600, "OffsetEndMs": 15735}, {"Word": "truck", "OffsetStartMs": 15735, "OffsetEndMs": 16040}, {"Word": "and", "OffsetStartMs": 16150, "OffsetEndMs": 16425}, {"Word": "then", "OffsetStartMs": 16425, "OffsetEndMs": 16545}, {"Word": "a", "OffsetStartMs": 16545, "OffsetEndMs": 16710}, {"Word": "car", "OffsetStartMs": 16710, "OffsetEndMs": 17030}, {"Word": "and", "OffsetStartMs": 17380, "OffsetEndMs": 17655}, {"Word": "then", "OffsetStartMs": 17655, "OffsetEndMs": 17775}, {"Word": "a", "OffsetStartMs": 17775, "OffsetEndMs": 17925}, {"Word": "person", "OffsetStartMs": 17925, "OffsetEndMs": 18230}, {"Word": "and", "OffsetStartMs": 18580, "OffsetEndMs": 18980}, {"Word": "it", "OffsetStartMs": 19900, "OffsetEndMs": 20300}, {"Word": "continues", "OffsetStartMs": 20650, "OffsetEndMs": 21015}, {"Word": "to", "OffsetStartMs": 21015, "OffsetEndMs": 21345}, {"Word": "output", "OffsetStartMs": 21345, "OffsetEndMs": 21600}, {"Word": "predictions", "OffsetStartMs": 21600, "OffsetEndMs": 22110}, {"Word": "even", "OffsetStartMs": 22110, "OffsetEndMs": 22395}, {"Word": "though", "OffsetStartMs": 22395, "OffsetEndMs": 22730}, {"Word": "it", "OffsetStartMs": 23110, "OffsetEndMs": 23355}, {"Word": "is", "OffsetStartMs": 23355, "OffsetEndMs": 23475}, {"Word": "very", "OffsetStartMs": 23475, "OffsetEndMs": 23730}, {"Word": "clear", "OffsetStartMs": 23730, "OffsetEndMs": 24030}, {"Word": "that", "OffsetStartMs": 24030, "OffsetEndMs": 24210}, {"Word": "the", "OffsetStartMs": 24210, "OffsetEndMs": 24330}, {"Word": "model", "OffsetStartMs": 24330, "OffsetEndMs": 24590}, {"Word": "does", "OffsetStartMs": 24640, "OffsetEndMs": 24930}, {"Word": "not", "OffsetStartMs": 24930, "OffsetEndMs": 25140}, {"Word": "know", "OffsetStartMs": 25140, "OffsetEndMs": 25395}, {"Word": "what", "OffsetStartMs": 25395, "OffsetEndMs": 25605}, {"Word": "this", "OffsetStartMs": 25605, "OffsetEndMs": 25755}, {"Word": "image", "OffsetStartMs": 25755, "OffsetEndMs": 25965}, {"Word": "is", "OffsetStartMs": 25965, "OffsetEndMs": 26300}], "SpeechSpeed": 15.9}, {"FinalSentence": "And now you might be asking, okay, so what's the big deal? It didn't recognize the horse drawn buggy, but it seemed to drive successfully anyway. However, the exact same problem that resulted in that video has also resulted in numerous autonomous car crashes.", "SliceSentence": "And now you might be asking okay so what's the big deal It didn't recognize the horse drawn buggy but it seemed to drive successfully anyway . Howeverthe exact same problem that resulted in that video has also resulted in numerous autonomous car crashes", "StartMs": 1772060, "EndMs": 1789260, "WordsNum": 44, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 360}, {"Word": "now", "OffsetStartMs": 360, "OffsetEndMs": 540}, {"Word": "you", "OffsetStartMs": 540, "OffsetEndMs": 705}, {"Word": "might", "OffsetStartMs": 705, "OffsetEndMs": 810}, {"Word": "be", "OffsetStartMs": 810, "OffsetEndMs": 915}, {"Word": "asking", "OffsetStartMs": 915, "OffsetEndMs": 1160}, {"Word": "okay", "OffsetStartMs": 1390, "OffsetEndMs": 1790}, {"Word": "so", "OffsetStartMs": 1840, "OffsetEndMs": 2235}, {"Word": "what's", "OffsetStartMs": 2235, "OffsetEndMs": 2610}, {"Word": "the", "OffsetStartMs": 2610, "OffsetEndMs": 2715}, {"Word": "big", "OffsetStartMs": 2715, "OffsetEndMs": 2865}, {"Word": "deal", "OffsetStartMs": 2865, "OffsetEndMs": 3170}, {"Word": "It", "OffsetStartMs": 3340, "OffsetEndMs": 3600}, {"Word": "didn't", "OffsetStartMs": 3600, "OffsetEndMs": 3855}, {"Word": "recognize", "OffsetStartMs": 3855, "OffsetEndMs": 4160}, {"Word": "the", "OffsetStartMs": 4330, "OffsetEndMs": 4575}, {"Word": "horse", "OffsetStartMs": 4575, "OffsetEndMs": 4740}, {"Word": "drawn", "OffsetStartMs": 4740, "OffsetEndMs": 4965}, {"Word": "buggy", "OffsetStartMs": 4965, "OffsetEndMs": 5450}, {"Word": "but", "OffsetStartMs": 5560, "OffsetEndMs": 5820}, {"Word": "it", "OffsetStartMs": 5820, "OffsetEndMs": 6080}, {"Word": "seemed", "OffsetStartMs": 6220, "OffsetEndMs": 6585}, {"Word": "to", "OffsetStartMs": 6585, "OffsetEndMs": 6840}, {"Word": "drive", "OffsetStartMs": 6840, "OffsetEndMs": 7065}, {"Word": "successfully", "OffsetStartMs": 7065, "OffsetEndMs": 7400}, {"Word": "anyway", "OffsetStartMs": 7840, "OffsetEndMs": 8240}, {"Word": ".", "OffsetStartMs": 8950, "OffsetEndMs": 9350}, {"Word": "Howeverthe", "OffsetStartMs": 9460, "OffsetEndMs": 9825}, {"Word": "exact", "OffsetStartMs": 9825, "OffsetEndMs": 10155}, {"Word": "same", "OffsetStartMs": 10155, "OffsetEndMs": 10455}, {"Word": "problem", "OffsetStartMs": 10455, "OffsetEndMs": 10785}, {"Word": "that", "OffsetStartMs": 10785, "OffsetEndMs": 11115}, {"Word": "resulted", "OffsetStartMs": 11115, "OffsetEndMs": 11445}, {"Word": "in", "OffsetStartMs": 11445, "OffsetEndMs": 11730}, {"Word": "that", "OffsetStartMs": 11730, "OffsetEndMs": 11940}, {"Word": "video", "OffsetStartMs": 11940, "OffsetEndMs": 12260}, {"Word": "has", "OffsetStartMs": 12610, "OffsetEndMs": 12960}, {"Word": "also", "OffsetStartMs": 12960, "OffsetEndMs": 13230}, {"Word": "resulted", "OffsetStartMs": 13230, "OffsetEndMs": 13550}, {"Word": "in", "OffsetStartMs": 13600, "OffsetEndMs": 13920}, {"Word": "numerous", "OffsetStartMs": 13920, "OffsetEndMs": 14240}, {"Word": "autonomous", "OffsetStartMs": 14590, "OffsetEndMs": 15225}, {"Word": "car", "OffsetStartMs": 15225, "OffsetEndMs": 15480}, {"Word": "crashes", "OffsetStartMs": 15480, "OffsetEndMs": 16100}], "SpeechSpeed": 14.7}, {"FinalSentence": "So let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks, which may cause incidents like the ones that we just saw. We'll go through a simple example that illustrates the two main types of uncertainty that we'll focus on in this lecture.", "SliceSentence": "So let's go through why something like this might have happened . Thereare multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw .We'll go through a simple example that illustrates the two main types of uncertainty that we'll focus on in this lecture", "StartMs": 1791080, "EndMs": 1808040, "WordsNum": 53, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 495}, {"Word": "let's", "OffsetStartMs": 495, "OffsetEndMs": 840}, {"Word": "go", "OffsetStartMs": 840, "OffsetEndMs": 1005}, {"Word": "through", "OffsetStartMs": 1005, "OffsetEndMs": 1230}, {"Word": "why", "OffsetStartMs": 1230, "OffsetEndMs": 1500}, {"Word": "something", "OffsetStartMs": 1500, "OffsetEndMs": 1785}, {"Word": "like", "OffsetStartMs": 1785, "OffsetEndMs": 2010}, {"Word": "this", "OffsetStartMs": 2010, "OffsetEndMs": 2235}, {"Word": "might", "OffsetStartMs": 2235, "OffsetEndMs": 2475}, {"Word": "have", "OffsetStartMs": 2475, "OffsetEndMs": 2655}, {"Word": "happened", "OffsetStartMs": 2655, "OffsetEndMs": 2930}, {"Word": ".", "OffsetStartMs": 3640, "OffsetEndMs": 3900}, {"Word": "Thereare", "OffsetStartMs": 3900, "OffsetEndMs": 4020}, {"Word": "multiple", "OffsetStartMs": 4020, "OffsetEndMs": 4280}, {"Word": "different", "OffsetStartMs": 4330, "OffsetEndMs": 4710}, {"Word": "types", "OffsetStartMs": 4710, "OffsetEndMs": 5070}, {"Word": "of", "OffsetStartMs": 5070, "OffsetEndMs": 5400}, {"Word": "uncertainty", "OffsetStartMs": 5400, "OffsetEndMs": 5750}, {"Word": "in", "OffsetStartMs": 5800, "OffsetEndMs": 6090}, {"Word": "neural", "OffsetStartMs": 6090, "OffsetEndMs": 6360}, {"Word": "networks", "OffsetStartMs": 6360, "OffsetEndMs": 6650}, {"Word": "which", "OffsetStartMs": 7090, "OffsetEndMs": 7395}, {"Word": "may", "OffsetStartMs": 7395, "OffsetEndMs": 7620}, {"Word": "cause", "OffsetStartMs": 7620, "OffsetEndMs": 7875}, {"Word": "incidents", "OffsetStartMs": 7875, "OffsetEndMs": 8325}, {"Word": "like", "OffsetStartMs": 8325, "OffsetEndMs": 8550}, {"Word": "the", "OffsetStartMs": 8550, "OffsetEndMs": 8700}, {"Word": "ones", "OffsetStartMs": 8700, "OffsetEndMs": 8880}, {"Word": "that", "OffsetStartMs": 8880, "OffsetEndMs": 9075}, {"Word": "we", "OffsetStartMs": 9075, "OffsetEndMs": 9225}, {"Word": "just", "OffsetStartMs": 9225, "OffsetEndMs": 9405}, {"Word": "saw", "OffsetStartMs": 9405, "OffsetEndMs": 9710}, {"Word": ".We'll", "OffsetStartMs": 10240, "OffsetEndMs": 10560}, {"Word": "go", "OffsetStartMs": 10560, "OffsetEndMs": 10680}, {"Word": "through", "OffsetStartMs": 10680, "OffsetEndMs": 10800}, {"Word": "a", "OffsetStartMs": 10800, "OffsetEndMs": 10920}, {"Word": "simple", "OffsetStartMs": 10920, "OffsetEndMs": 11210}, {"Word": "example", "OffsetStartMs": 11230, "OffsetEndMs": 11630}, {"Word": "that", "OffsetStartMs": 11740, "OffsetEndMs": 12000}, {"Word": "illustrates", "OffsetStartMs": 12000, "OffsetEndMs": 12480}, {"Word": "the", "OffsetStartMs": 12480, "OffsetEndMs": 12705}, {"Word": "two", "OffsetStartMs": 12705, "OffsetEndMs": 12930}, {"Word": "main", "OffsetStartMs": 12930, "OffsetEndMs": 13245}, {"Word": "types", "OffsetStartMs": 13245, "OffsetEndMs": 13530}, {"Word": "of", "OffsetStartMs": 13530, "OffsetEndMs": 13770}, {"Word": "uncertainty", "OffsetStartMs": 13770, "OffsetEndMs": 14120}, {"Word": "that", "OffsetStartMs": 14200, "OffsetEndMs": 14460}, {"Word": "we'll", "OffsetStartMs": 14460, "OffsetEndMs": 14655}, {"Word": "focus", "OffsetStartMs": 14655, "OffsetEndMs": 14850}, {"Word": "on", "OffsetStartMs": 14850, "OffsetEndMs": 15105}, {"Word": "in", "OffsetStartMs": 15105, "OffsetEndMs": 15285}, {"Word": "this", "OffsetStartMs": 15285, "OffsetEndMs": 15465}, {"Word": "lecture", "OffsetStartMs": 15465, "OffsetEndMs": 15770}], "SpeechSpeed": 18.3}, {"FinalSentence": "So let's say I'm trying to estimate the curve y equals X cubed as part of a regression task. The input here, X is some real number, and we want it to output f of X, which should be ideally X cubed.", "SliceSentence": "So let's say I'm trying to estimate the curve y equals X cubed as part of a regression task . Theinput here X is some real number and we want it to output f of X which should be ideally X cubed", "StartMs": 1809820, "EndMs": 1824920, "WordsNum": 42, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "let's", "OffsetStartMs": 640, "OffsetEndMs": 1020}, {"Word": "say", "OffsetStartMs": 1020, "OffsetEndMs": 1155}, {"Word": "I'm", "OffsetStartMs": 1155, "OffsetEndMs": 1410}, {"Word": "trying", "OffsetStartMs": 1410, "OffsetEndMs": 1575}, {"Word": "to", "OffsetStartMs": 1575, "OffsetEndMs": 1875}, {"Word": "estimate", "OffsetStartMs": 1875, "OffsetEndMs": 2145}, {"Word": "the", "OffsetStartMs": 2145, "OffsetEndMs": 2295}, {"Word": "curve", "OffsetStartMs": 2295, "OffsetEndMs": 2570}, {"Word": "y", "OffsetStartMs": 2590, "OffsetEndMs": 2990}, {"Word": "equals", "OffsetStartMs": 3010, "OffsetEndMs": 3300}, {"Word": "X", "OffsetStartMs": 3300, "OffsetEndMs": 3555}, {"Word": "cubed", "OffsetStartMs": 3555, "OffsetEndMs": 3975}, {"Word": "as", "OffsetStartMs": 3975, "OffsetEndMs": 4170}, {"Word": "part", "OffsetStartMs": 4170, "OffsetEndMs": 4365}, {"Word": "of", "OffsetStartMs": 4365, "OffsetEndMs": 4500}, {"Word": "a", "OffsetStartMs": 4500, "OffsetEndMs": 4635}, {"Word": "regression", "OffsetStartMs": 4635, "OffsetEndMs": 5010}, {"Word": "task", "OffsetStartMs": 5010, "OffsetEndMs": 5390}, {"Word": ".", "OffsetStartMs": 5800, "OffsetEndMs": 6150}, {"Word": "Theinput", "OffsetStartMs": 6150, "OffsetEndMs": 6405}, {"Word": "here", "OffsetStartMs": 6405, "OffsetEndMs": 6710}, {"Word": "X", "OffsetStartMs": 6760, "OffsetEndMs": 7160}, {"Word": "is", "OffsetStartMs": 7870, "OffsetEndMs": 8190}, {"Word": "some", "OffsetStartMs": 8190, "OffsetEndMs": 8430}, {"Word": "real", "OffsetStartMs": 8430, "OffsetEndMs": 8655}, {"Word": "number", "OffsetStartMs": 8655, "OffsetEndMs": 8960}, {"Word": "and", "OffsetStartMs": 9100, "OffsetEndMs": 9500}, {"Word": "we", "OffsetStartMs": 9580, "OffsetEndMs": 9885}, {"Word": "want", "OffsetStartMs": 9885, "OffsetEndMs": 10125}, {"Word": "it", "OffsetStartMs": 10125, "OffsetEndMs": 10380}, {"Word": "to", "OffsetStartMs": 10380, "OffsetEndMs": 10700}, {"Word": "output", "OffsetStartMs": 10840, "OffsetEndMs": 11145}, {"Word": "f", "OffsetStartMs": 11145, "OffsetEndMs": 11340}, {"Word": "of", "OffsetStartMs": 11340, "OffsetEndMs": 11490}, {"Word": "X", "OffsetStartMs": 11490, "OffsetEndMs": 11730}, {"Word": "which", "OffsetStartMs": 11730, "OffsetEndMs": 12110}, {"Word": "should", "OffsetStartMs": 12520, "OffsetEndMs": 12810}, {"Word": "be", "OffsetStartMs": 12810, "OffsetEndMs": 12990}, {"Word": "ideally", "OffsetStartMs": 12990, "OffsetEndMs": 13500}, {"Word": "X", "OffsetStartMs": 13500, "OffsetEndMs": 13770}, {"Word": "cubed", "OffsetStartMs": 13770, "OffsetEndMs": 14270}], "SpeechSpeed": 12.7}, {"FinalSentence": "So right away you might notice that there are some issues in this data set. Assume the red points in this image are your training samples.", "SliceSentence": "So right away you might notice that there are some issues in this data set . Assumethe red points in this image are your training samples", "StartMs": 1824920, "EndMs": 1833780, "WordsNum": 26, "Words": [{"Word": "So", "OffsetStartMs": 70, "OffsetEndMs": 470}, {"Word": "right", "OffsetStartMs": 490, "OffsetEndMs": 810}, {"Word": "away", "OffsetStartMs": 810, "OffsetEndMs": 1125}, {"Word": "you", "OffsetStartMs": 1125, "OffsetEndMs": 1380}, {"Word": "might", "OffsetStartMs": 1380, "OffsetEndMs": 1530}, {"Word": "notice", "OffsetStartMs": 1530, "OffsetEndMs": 1770}, {"Word": "that", "OffsetStartMs": 1770, "OffsetEndMs": 1995}, {"Word": "there", "OffsetStartMs": 1995, "OffsetEndMs": 2160}, {"Word": "are", "OffsetStartMs": 2160, "OffsetEndMs": 2400}, {"Word": "some", "OffsetStartMs": 2400, "OffsetEndMs": 2750}, {"Word": "issues", "OffsetStartMs": 3070, "OffsetEndMs": 3470}, {"Word": "in", "OffsetStartMs": 3490, "OffsetEndMs": 3750}, {"Word": "this", "OffsetStartMs": 3750, "OffsetEndMs": 3900}, {"Word": "data", "OffsetStartMs": 3900, "OffsetEndMs": 4125}, {"Word": "set", "OffsetStartMs": 4125, "OffsetEndMs": 4460}, {"Word": ".", "OffsetStartMs": 4480, "OffsetEndMs": 4755}, {"Word": "Assumethe", "OffsetStartMs": 4755, "OffsetEndMs": 4875}, {"Word": "red", "OffsetStartMs": 4875, "OffsetEndMs": 5025}, {"Word": "points", "OffsetStartMs": 5025, "OffsetEndMs": 5330}, {"Word": "in", "OffsetStartMs": 5530, "OffsetEndMs": 5865}, {"Word": "this", "OffsetStartMs": 5865, "OffsetEndMs": 6150}, {"Word": "image", "OffsetStartMs": 6150, "OffsetEndMs": 6500}, {"Word": "are", "OffsetStartMs": 6520, "OffsetEndMs": 6870}, {"Word": "your", "OffsetStartMs": 6870, "OffsetEndMs": 7125}, {"Word": "training", "OffsetStartMs": 7125, "OffsetEndMs": 7365}, {"Word": "samples", "OffsetStartMs": 7365, "OffsetEndMs": 7910}], "SpeechSpeed": 15.3}, {"FinalSentence": "So the box area of this image shows data points in our data set where we have really high noise. These points do not follow the curve. Why it equals X cubed. In fact, they don't really seem to follow any distribution at all. And the model won't be able to compute outputs for points in this region accurately, because very similar inputs have extremely different outputs, which is the definition of data uncertainty.", "SliceSentence": "So the box area of this image shows data points in our data set where we have really high noise . Thesepoints do not follow the curve . Whyit equals X cubed . Infact they don't really seem to follow any distribution at all . Andthe model won't be able to compute outputs for points in this region accurately because very similar inputs have extremely different outputs which is the definition of data uncertainty", "StartMs": 1836880, "EndMs": 1866380, "WordsNum": 74, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 540}, {"Word": "the", "OffsetStartMs": 540, "OffsetEndMs": 780}, {"Word": "box", "OffsetStartMs": 780, "OffsetEndMs": 1035}, {"Word": "area", "OffsetStartMs": 1035, "OffsetEndMs": 1430}, {"Word": "of", "OffsetStartMs": 1510, "OffsetEndMs": 1785}, {"Word": "this", "OffsetStartMs": 1785, "OffsetEndMs": 1950}, {"Word": "image", "OffsetStartMs": 1950, "OffsetEndMs": 2240}, {"Word": "shows", "OffsetStartMs": 2770, "OffsetEndMs": 3170}, {"Word": "data", "OffsetStartMs": 3370, "OffsetEndMs": 3770}, {"Word": "points", "OffsetStartMs": 3910, "OffsetEndMs": 4275}, {"Word": "in", "OffsetStartMs": 4275, "OffsetEndMs": 4485}, {"Word": "our", "OffsetStartMs": 4485, "OffsetEndMs": 4620}, {"Word": "data", "OffsetStartMs": 4620, "OffsetEndMs": 4845}, {"Word": "set", "OffsetStartMs": 4845, "OffsetEndMs": 5070}, {"Word": "where", "OffsetStartMs": 5070, "OffsetEndMs": 5220}, {"Word": "we", "OffsetStartMs": 5220, "OffsetEndMs": 5355}, {"Word": "have", "OffsetStartMs": 5355, "OffsetEndMs": 5580}, {"Word": "really", "OffsetStartMs": 5580, "OffsetEndMs": 5910}, {"Word": "high", "OffsetStartMs": 5910, "OffsetEndMs": 6285}, {"Word": "noise", "OffsetStartMs": 6285, "OffsetEndMs": 6680}, {"Word": ".", "OffsetStartMs": 7180, "OffsetEndMs": 7515}, {"Word": "Thesepoints", "OffsetStartMs": 7515, "OffsetEndMs": 7815}, {"Word": "do", "OffsetStartMs": 7815, "OffsetEndMs": 8040}, {"Word": "not", "OffsetStartMs": 8040, "OffsetEndMs": 8205}, {"Word": "follow", "OffsetStartMs": 8205, "OffsetEndMs": 8445}, {"Word": "the", "OffsetStartMs": 8445, "OffsetEndMs": 8640}, {"Word": "curve", "OffsetStartMs": 8640, "OffsetEndMs": 8820}, {"Word": ".", "OffsetStartMs": 8820, "OffsetEndMs": 9015}, {"Word": "Whyit", "OffsetStartMs": 9015, "OffsetEndMs": 9165}, {"Word": "equals", "OffsetStartMs": 9165, "OffsetEndMs": 9330}, {"Word": "X", "OffsetStartMs": 9330, "OffsetEndMs": 9585}, {"Word": "cubed", "OffsetStartMs": 9585, "OffsetEndMs": 10065}, {"Word": ".", "OffsetStartMs": 10065, "OffsetEndMs": 10320}, {"Word": "Infact", "OffsetStartMs": 10320, "OffsetEndMs": 10545}, {"Word": "they", "OffsetStartMs": 10545, "OffsetEndMs": 10740}, {"Word": "don't", "OffsetStartMs": 10740, "OffsetEndMs": 10935}, {"Word": "really", "OffsetStartMs": 10935, "OffsetEndMs": 11115}, {"Word": "seem", "OffsetStartMs": 11115, "OffsetEndMs": 11340}, {"Word": "to", "OffsetStartMs": 11340, "OffsetEndMs": 11490}, {"Word": "follow", "OffsetStartMs": 11490, "OffsetEndMs": 11685}, {"Word": "any", "OffsetStartMs": 11685, "OffsetEndMs": 11985}, {"Word": "distribution", "OffsetStartMs": 11985, "OffsetEndMs": 12285}, {"Word": "at", "OffsetStartMs": 12285, "OffsetEndMs": 12480}, {"Word": "all", "OffsetStartMs": 12480, "OffsetEndMs": 12740}, {"Word": ".", "OffsetStartMs": 13240, "OffsetEndMs": 13640}, {"Word": "Andthe", "OffsetStartMs": 14110, "OffsetEndMs": 14355}, {"Word": "model", "OffsetStartMs": 14355, "OffsetEndMs": 14600}, {"Word": "won't", "OffsetStartMs": 15370, "OffsetEndMs": 15735}, {"Word": "be", "OffsetStartMs": 15735, "OffsetEndMs": 15840}, {"Word": "able", "OffsetStartMs": 15840, "OffsetEndMs": 16095}, {"Word": "to", "OffsetStartMs": 16095, "OffsetEndMs": 16490}, {"Word": "compute", "OffsetStartMs": 17020, "OffsetEndMs": 17600}, {"Word": "outputs", "OffsetStartMs": 18190, "OffsetEndMs": 18525}, {"Word": "for", "OffsetStartMs": 18525, "OffsetEndMs": 18800}, {"Word": "points", "OffsetStartMs": 19930, "OffsetEndMs": 20250}, {"Word": "in", "OffsetStartMs": 20250, "OffsetEndMs": 20445}, {"Word": "this", "OffsetStartMs": 20445, "OffsetEndMs": 20640}, {"Word": "region", "OffsetStartMs": 20640, "OffsetEndMs": 20960}, {"Word": "accurately", "OffsetStartMs": 21550, "OffsetEndMs": 22160}, {"Word": "because", "OffsetStartMs": 22450, "OffsetEndMs": 22850}, {"Word": "very", "OffsetStartMs": 23200, "OffsetEndMs": 23600}, {"Word": "similar", "OffsetStartMs": 23620, "OffsetEndMs": 24020}, {"Word": "inputs", "OffsetStartMs": 24130, "OffsetEndMs": 24480}, {"Word": "have", "OffsetStartMs": 24480, "OffsetEndMs": 24740}, {"Word": "extremely", "OffsetStartMs": 24790, "OffsetEndMs": 25190}, {"Word": "different", "OffsetStartMs": 25240, "OffsetEndMs": 25640}, {"Word": "outputs", "OffsetStartMs": 25810, "OffsetEndMs": 26300}, {"Word": "which", "OffsetStartMs": 26350, "OffsetEndMs": 26610}, {"Word": "is", "OffsetStartMs": 26610, "OffsetEndMs": 26745}, {"Word": "the", "OffsetStartMs": 26745, "OffsetEndMs": 26880}, {"Word": "definition", "OffsetStartMs": 26880, "OffsetEndMs": 27140}, {"Word": "of", "OffsetStartMs": 27340, "OffsetEndMs": 27660}, {"Word": "data", "OffsetStartMs": 27660, "OffsetEndMs": 27980}, {"Word": "uncertainty", "OffsetStartMs": 28060, "OffsetEndMs": 28460}], "SpeechSpeed": 13.8}, {"FinalSentence": "We also have regions in this data set where we have no data. So if we queried the model for a prediction in this part of in this region of the data set, we should not really expect to see an accurate result because the model's never seen anything like this before. And this is what is called model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to output a correct prediction.", "SliceSentence": "We also have regions in this data set where we have no data . Soif we queried the model for a prediction in this part of in this region of the data set we should not really expect to see an accurate result because the model's never seen anything like this before . Andthis is what is called model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to output a correct prediction", "StartMs": 1869620, "EndMs": 1895860, "WordsNum": 84, "Words": [{"Word": "We", "OffsetStartMs": 100, "OffsetEndMs": 435}, {"Word": "also", "OffsetStartMs": 435, "OffsetEndMs": 660}, {"Word": "have", "OffsetStartMs": 660, "OffsetEndMs": 855}, {"Word": "regions", "OffsetStartMs": 855, "OffsetEndMs": 1155}, {"Word": "in", "OffsetStartMs": 1155, "OffsetEndMs": 1440}, {"Word": "this", "OffsetStartMs": 1440, "OffsetEndMs": 1605}, {"Word": "data", "OffsetStartMs": 1605, "OffsetEndMs": 1815}, {"Word": "set", "OffsetStartMs": 1815, "OffsetEndMs": 2055}, {"Word": "where", "OffsetStartMs": 2055, "OffsetEndMs": 2235}, {"Word": "we", "OffsetStartMs": 2235, "OffsetEndMs": 2385}, {"Word": "have", "OffsetStartMs": 2385, "OffsetEndMs": 2625}, {"Word": "no", "OffsetStartMs": 2625, "OffsetEndMs": 2895}, {"Word": "data", "OffsetStartMs": 2895, "OffsetEndMs": 3200}, {"Word": ".", "OffsetStartMs": 3700, "OffsetEndMs": 4020}, {"Word": "Soif", "OffsetStartMs": 4020, "OffsetEndMs": 4230}, {"Word": "we", "OffsetStartMs": 4230, "OffsetEndMs": 4410}, {"Word": "queried", "OffsetStartMs": 4410, "OffsetEndMs": 4725}, {"Word": "the", "OffsetStartMs": 4725, "OffsetEndMs": 4860}, {"Word": "model", "OffsetStartMs": 4860, "OffsetEndMs": 5120}, {"Word": "for", "OffsetStartMs": 5140, "OffsetEndMs": 5385}, {"Word": "a", "OffsetStartMs": 5385, "OffsetEndMs": 5505}, {"Word": "prediction", "OffsetStartMs": 5505, "OffsetEndMs": 5820}, {"Word": "in", "OffsetStartMs": 5820, "OffsetEndMs": 6075}, {"Word": "this", "OffsetStartMs": 6075, "OffsetEndMs": 6350}, {"Word": "part", "OffsetStartMs": 6460, "OffsetEndMs": 6735}, {"Word": "of", "OffsetStartMs": 6735, "OffsetEndMs": 6900}, {"Word": "in", "OffsetStartMs": 6900, "OffsetEndMs": 7050}, {"Word": "this", "OffsetStartMs": 7050, "OffsetEndMs": 7200}, {"Word": "region", "OffsetStartMs": 7200, "OffsetEndMs": 7455}, {"Word": "of", "OffsetStartMs": 7455, "OffsetEndMs": 7770}, {"Word": "the", "OffsetStartMs": 7770, "OffsetEndMs": 8010}, {"Word": "data", "OffsetStartMs": 8010, "OffsetEndMs": 8235}, {"Word": "set", "OffsetStartMs": 8235, "OffsetEndMs": 8570}, {"Word": "we", "OffsetStartMs": 8710, "OffsetEndMs": 9000}, {"Word": "should", "OffsetStartMs": 9000, "OffsetEndMs": 9290}, {"Word": "not", "OffsetStartMs": 9340, "OffsetEndMs": 9660}, {"Word": "really", "OffsetStartMs": 9660, "OffsetEndMs": 9975}, {"Word": "expect", "OffsetStartMs": 9975, "OffsetEndMs": 10275}, {"Word": "to", "OffsetStartMs": 10275, "OffsetEndMs": 10470}, {"Word": "see", "OffsetStartMs": 10470, "OffsetEndMs": 10590}, {"Word": "an", "OffsetStartMs": 10590, "OffsetEndMs": 10815}, {"Word": "accurate", "OffsetStartMs": 10815, "OffsetEndMs": 11160}, {"Word": "result", "OffsetStartMs": 11160, "OffsetEndMs": 11510}, {"Word": "because", "OffsetStartMs": 11560, "OffsetEndMs": 11820}, {"Word": "the", "OffsetStartMs": 11820, "OffsetEndMs": 11925}, {"Word": "model's", "OffsetStartMs": 11925, "OffsetEndMs": 12255}, {"Word": "never", "OffsetStartMs": 12255, "OffsetEndMs": 12435}, {"Word": "seen", "OffsetStartMs": 12435, "OffsetEndMs": 12735}, {"Word": "anything", "OffsetStartMs": 12735, "OffsetEndMs": 12975}, {"Word": "like", "OffsetStartMs": 12975, "OffsetEndMs": 13125}, {"Word": "this", "OffsetStartMs": 13125, "OffsetEndMs": 13305}, {"Word": "before", "OffsetStartMs": 13305, "OffsetEndMs": 13610}, {"Word": ".", "OffsetStartMs": 14380, "OffsetEndMs": 14670}, {"Word": "Andthis", "OffsetStartMs": 14670, "OffsetEndMs": 14835}, {"Word": "is", "OffsetStartMs": 14835, "OffsetEndMs": 14970}, {"Word": "what", "OffsetStartMs": 14970, "OffsetEndMs": 15075}, {"Word": "is", "OffsetStartMs": 15075, "OffsetEndMs": 15210}, {"Word": "called", "OffsetStartMs": 15210, "OffsetEndMs": 15480}, {"Word": "model", "OffsetStartMs": 15480, "OffsetEndMs": 15860}, {"Word": "uncertainty", "OffsetStartMs": 15970, "OffsetEndMs": 16370}, {"Word": "when", "OffsetStartMs": 16810, "OffsetEndMs": 17070}, {"Word": "the", "OffsetStartMs": 17070, "OffsetEndMs": 17190}, {"Word": "model", "OffsetStartMs": 17190, "OffsetEndMs": 17415}, {"Word": "hasn't", "OffsetStartMs": 17415, "OffsetEndMs": 17835}, {"Word": "seen", "OffsetStartMs": 17835, "OffsetEndMs": 18060}, {"Word": "enough", "OffsetStartMs": 18060, "OffsetEndMs": 18270}, {"Word": "data", "OffsetStartMs": 18270, "OffsetEndMs": 18480}, {"Word": "points", "OffsetStartMs": 18480, "OffsetEndMs": 18705}, {"Word": "or", "OffsetStartMs": 18705, "OffsetEndMs": 18930}, {"Word": "cannot", "OffsetStartMs": 18930, "OffsetEndMs": 19250}, {"Word": "estimate", "OffsetStartMs": 19510, "OffsetEndMs": 19815}, {"Word": "that", "OffsetStartMs": 19815, "OffsetEndMs": 20040}, {"Word": "area", "OffsetStartMs": 20040, "OffsetEndMs": 20310}, {"Word": "of", "OffsetStartMs": 20310, "OffsetEndMs": 20520}, {"Word": "the", "OffsetStartMs": 20520, "OffsetEndMs": 20745}, {"Word": "input", "OffsetStartMs": 20745, "OffsetEndMs": 21060}, {"Word": "distribution", "OffsetStartMs": 21060, "OffsetEndMs": 21410}, {"Word": "accurately", "OffsetStartMs": 22210, "OffsetEndMs": 22695}, {"Word": "enough", "OffsetStartMs": 22695, "OffsetEndMs": 23000}, {"Word": "to", "OffsetStartMs": 23050, "OffsetEndMs": 23430}, {"Word": "output", "OffsetStartMs": 23430, "OffsetEndMs": 23685}, {"Word": "a", "OffsetStartMs": 23685, "OffsetEndMs": 23895}, {"Word": "correct", "OffsetStartMs": 23895, "OffsetEndMs": 24230}, {"Word": "prediction", "OffsetStartMs": 24700, "OffsetEndMs": 25190}], "SpeechSpeed": 17.4}, {"FinalSentence": "So what would happen if I added the following blue training points to the areas of the data set with high model uncertainty? Do you think the model uncertainty would decrease? Raise your hand.", "SliceSentence": "So what would happen if I added the following blue training points to the areas of the data set with high model uncertainty Do you think the model uncertainty would decrease Raise your hand", "StartMs": 1897160, "EndMs": 1910840, "WordsNum": 34, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "what", "OffsetStartMs": 730, "OffsetEndMs": 1005}, {"Word": "would", "OffsetStartMs": 1005, "OffsetEndMs": 1140}, {"Word": "happen", "OffsetStartMs": 1140, "OffsetEndMs": 1400}, {"Word": "if", "OffsetStartMs": 1420, "OffsetEndMs": 1680}, {"Word": "I", "OffsetStartMs": 1680, "OffsetEndMs": 1830}, {"Word": "added", "OffsetStartMs": 1830, "OffsetEndMs": 2120}, {"Word": "the", "OffsetStartMs": 2170, "OffsetEndMs": 2460}, {"Word": "following", "OffsetStartMs": 2460, "OffsetEndMs": 2750}, {"Word": "blue", "OffsetStartMs": 2950, "OffsetEndMs": 3285}, {"Word": "training", "OffsetStartMs": 3285, "OffsetEndMs": 3585}, {"Word": "points", "OffsetStartMs": 3585, "OffsetEndMs": 3950}, {"Word": "to", "OffsetStartMs": 4180, "OffsetEndMs": 4580}, {"Word": "the", "OffsetStartMs": 5290, "OffsetEndMs": 5670}, {"Word": "areas", "OffsetStartMs": 5670, "OffsetEndMs": 6050}, {"Word": "of", "OffsetStartMs": 6070, "OffsetEndMs": 6390}, {"Word": "the", "OffsetStartMs": 6390, "OffsetEndMs": 6615}, {"Word": "data", "OffsetStartMs": 6615, "OffsetEndMs": 6840}, {"Word": "set", "OffsetStartMs": 6840, "OffsetEndMs": 7065}, {"Word": "with", "OffsetStartMs": 7065, "OffsetEndMs": 7245}, {"Word": "high", "OffsetStartMs": 7245, "OffsetEndMs": 7520}, {"Word": "model", "OffsetStartMs": 7540, "OffsetEndMs": 7940}, {"Word": "uncertainty", "OffsetStartMs": 7990, "OffsetEndMs": 8390}, {"Word": "Do", "OffsetStartMs": 9040, "OffsetEndMs": 9285}, {"Word": "you", "OffsetStartMs": 9285, "OffsetEndMs": 9420}, {"Word": "think", "OffsetStartMs": 9420, "OffsetEndMs": 9645}, {"Word": "the", "OffsetStartMs": 9645, "OffsetEndMs": 9900}, {"Word": "model", "OffsetStartMs": 9900, "OffsetEndMs": 10215}, {"Word": "uncertainty", "OffsetStartMs": 10215, "OffsetEndMs": 10610}, {"Word": "would", "OffsetStartMs": 10690, "OffsetEndMs": 11090}, {"Word": "decrease", "OffsetStartMs": 11230, "OffsetEndMs": 11630}, {"Word": "Raise", "OffsetStartMs": 12070, "OffsetEndMs": 12345}, {"Word": "your", "OffsetStartMs": 12345, "OffsetEndMs": 12465}, {"Word": "hand", "OffsetStartMs": 12465, "OffsetEndMs": 12710}], "SpeechSpeed": 13.8}, {"FinalSentence": "Does anyone think it would not change?", "SliceSentence": "Does anyone think it would not change", "StartMs": 1911920, "EndMs": 1914760, "WordsNum": 7, "Words": [{"Word": "Does", "OffsetStartMs": 100, "OffsetEndMs": 435}, {"Word": "anyone", "OffsetStartMs": 435, "OffsetEndMs": 630}, {"Word": "think", "OffsetStartMs": 630, "OffsetEndMs": 735}, {"Word": "it", "OffsetStartMs": 735, "OffsetEndMs": 870}, {"Word": "would", "OffsetStartMs": 870, "OffsetEndMs": 1035}, {"Word": "not", "OffsetStartMs": 1035, "OffsetEndMs": 1215}, {"Word": "change", "OffsetStartMs": 1215, "OffsetEndMs": 1520}], "SpeechSpeed": 13.0}, {"FinalSentence": "Okay, so yeah, most of you were correct. Model uncertainty can typically be reduced by adding in data into any region, but specifically regions with high model uncertainty.", "SliceSentence": "Okay so yeah most of you were correct . Modeluncertainty can typically be reduced by adding in data into any region but specifically regions with high model uncertainty", "StartMs": 1915360, "EndMs": 1925980, "WordsNum": 28, "Words": [{"Word": "Okay", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "so", "OffsetStartMs": 610, "OffsetEndMs": 855}, {"Word": "yeah", "OffsetStartMs": 855, "OffsetEndMs": 1005}, {"Word": "most", "OffsetStartMs": 1005, "OffsetEndMs": 1200}, {"Word": "of", "OffsetStartMs": 1200, "OffsetEndMs": 1350}, {"Word": "you", "OffsetStartMs": 1350, "OffsetEndMs": 1455}, {"Word": "were", "OffsetStartMs": 1455, "OffsetEndMs": 1635}, {"Word": "correct", "OffsetStartMs": 1635, "OffsetEndMs": 1970}, {"Word": ".", "OffsetStartMs": 2380, "OffsetEndMs": 2780}, {"Word": "Modeluncertainty", "OffsetStartMs": 2830, "OffsetEndMs": 3230}, {"Word": "can", "OffsetStartMs": 3310, "OffsetEndMs": 3600}, {"Word": "typically", "OffsetStartMs": 3600, "OffsetEndMs": 3890}, {"Word": "be", "OffsetStartMs": 3910, "OffsetEndMs": 4290}, {"Word": "reduced", "OffsetStartMs": 4290, "OffsetEndMs": 4650}, {"Word": "by", "OffsetStartMs": 4650, "OffsetEndMs": 4935}, {"Word": "adding", "OffsetStartMs": 4935, "OffsetEndMs": 5205}, {"Word": "in", "OffsetStartMs": 5205, "OffsetEndMs": 5490}, {"Word": "data", "OffsetStartMs": 5490, "OffsetEndMs": 5810}, {"Word": "into", "OffsetStartMs": 5860, "OffsetEndMs": 6260}, {"Word": "any", "OffsetStartMs": 6370, "OffsetEndMs": 6690}, {"Word": "region", "OffsetStartMs": 6690, "OffsetEndMs": 6975}, {"Word": "but", "OffsetStartMs": 6975, "OffsetEndMs": 7245}, {"Word": "specifically", "OffsetStartMs": 7245, "OffsetEndMs": 7550}, {"Word": "regions", "OffsetStartMs": 7660, "OffsetEndMs": 8025}, {"Word": "with", "OffsetStartMs": 8025, "OffsetEndMs": 8310}, {"Word": "high", "OffsetStartMs": 8310, "OffsetEndMs": 8565}, {"Word": "model", "OffsetStartMs": 8565, "OffsetEndMs": 8895}, {"Word": "uncertainty", "OffsetStartMs": 8895, "OffsetEndMs": 9290}], "SpeechSpeed": 15.7}, {"FinalSentence": "And now, what happens if we add these blue data points into this data set? Would anyone expect the data uncertainty to decrease? You raise your hand.", "SliceSentence": "And now what happens if we add these blue data points into this data set Would anyone expect the data uncertainty to decrease You raise your hand", "StartMs": 1925980, "EndMs": 1937660, "WordsNum": 27, "Words": [{"Word": "And", "OffsetStartMs": 40, "OffsetEndMs": 315}, {"Word": "now", "OffsetStartMs": 315, "OffsetEndMs": 590}, {"Word": "what", "OffsetStartMs": 1090, "OffsetEndMs": 1365}, {"Word": "happens", "OffsetStartMs": 1365, "OffsetEndMs": 1590}, {"Word": "if", "OffsetStartMs": 1590, "OffsetEndMs": 1815}, {"Word": "we", "OffsetStartMs": 1815, "OffsetEndMs": 2090}, {"Word": "add", "OffsetStartMs": 2230, "OffsetEndMs": 2630}, {"Word": "these", "OffsetStartMs": 2680, "OffsetEndMs": 3075}, {"Word": "blue", "OffsetStartMs": 3075, "OffsetEndMs": 3360}, {"Word": "data", "OffsetStartMs": 3360, "OffsetEndMs": 3600}, {"Word": "points", "OffsetStartMs": 3600, "OffsetEndMs": 3945}, {"Word": "into", "OffsetStartMs": 3945, "OffsetEndMs": 4340}, {"Word": "this", "OffsetStartMs": 4510, "OffsetEndMs": 4910}, {"Word": "data", "OffsetStartMs": 5680, "OffsetEndMs": 6015}, {"Word": "set", "OffsetStartMs": 6015, "OffsetEndMs": 6350}, {"Word": "Would", "OffsetStartMs": 6880, "OffsetEndMs": 7245}, {"Word": "anyone", "OffsetStartMs": 7245, "OffsetEndMs": 7575}, {"Word": "expect", "OffsetStartMs": 7575, "OffsetEndMs": 7890}, {"Word": "the", "OffsetStartMs": 7890, "OffsetEndMs": 8205}, {"Word": "data", "OffsetStartMs": 8205, "OffsetEndMs": 8570}, {"Word": "uncertainty", "OffsetStartMs": 8590, "OffsetEndMs": 8990}, {"Word": "to", "OffsetStartMs": 9100, "OffsetEndMs": 9500}, {"Word": "decrease", "OffsetStartMs": 9520, "OffsetEndMs": 9920}, {"Word": "You", "OffsetStartMs": 10000, "OffsetEndMs": 10260}, {"Word": "raise", "OffsetStartMs": 10260, "OffsetEndMs": 10380}, {"Word": "your", "OffsetStartMs": 10380, "OffsetEndMs": 10485}, {"Word": "hand", "OffsetStartMs": 10485, "OffsetEndMs": 10730}], "SpeechSpeed": 12.4}, {"FinalSentence": "That's correct. So data uncertainty is irreducible. In the real world. The blue points and the noisy red points on this image correspond to things like robot sensors. Let's say I I have a robot that's trained to has a sensor that is making measurements of depth. If the sensor has noise in it, there's no way that I can add any more data into the system to reduce that noise, unless I replace my sensor entirely.", "SliceSentence": "That's correct . Sodata uncertainty is irreducible . Inthe real world . Theblue points and the noisy red points on this image correspond to things like robot sensors .Let's say I I have a robot that's trained to has a sensor that is making measurements of depth . Ifthe sensor has noise in it there's no way that I can add any more data into the system to reduce that noise unless I replace my sensor entirely", "StartMs": 1938820, "EndMs": 1966880, "WordsNum": 77, "Words": [{"Word": "That's", "OffsetStartMs": 100, "OffsetEndMs": 510}, {"Word": "correct", "OffsetStartMs": 510, "OffsetEndMs": 800}, {"Word": ".", "OffsetStartMs": 850, "OffsetEndMs": 1155}, {"Word": "Sodata", "OffsetStartMs": 1155, "OffsetEndMs": 1460}, {"Word": "uncertainty", "OffsetStartMs": 1540, "OffsetEndMs": 1940}, {"Word": "is", "OffsetStartMs": 2230, "OffsetEndMs": 2565}, {"Word": "irreducible", "OffsetStartMs": 2565, "OffsetEndMs": 3530}, {"Word": ".", "OffsetStartMs": 3730, "OffsetEndMs": 4005}, {"Word": "Inthe", "OffsetStartMs": 4005, "OffsetEndMs": 4140}, {"Word": "real", "OffsetStartMs": 4140, "OffsetEndMs": 4335}, {"Word": "world", "OffsetStartMs": 4335, "OffsetEndMs": 4670}, {"Word": ".", "OffsetStartMs": 4870, "OffsetEndMs": 5270}, {"Word": "Theblue", "OffsetStartMs": 5560, "OffsetEndMs": 5865}, {"Word": "points", "OffsetStartMs": 5865, "OffsetEndMs": 6135}, {"Word": "and", "OffsetStartMs": 6135, "OffsetEndMs": 6405}, {"Word": "the", "OffsetStartMs": 6405, "OffsetEndMs": 6710}, {"Word": "noisy", "OffsetStartMs": 6790, "OffsetEndMs": 7275}, {"Word": "red", "OffsetStartMs": 7275, "OffsetEndMs": 7470}, {"Word": "points", "OffsetStartMs": 7470, "OffsetEndMs": 7695}, {"Word": "on", "OffsetStartMs": 7695, "OffsetEndMs": 7890}, {"Word": "this", "OffsetStartMs": 7890, "OffsetEndMs": 8115}, {"Word": "image", "OffsetStartMs": 8115, "OffsetEndMs": 8450}, {"Word": "correspond", "OffsetStartMs": 8740, "OffsetEndMs": 9140}, {"Word": "to", "OffsetStartMs": 9190, "OffsetEndMs": 9450}, {"Word": "things", "OffsetStartMs": 9450, "OffsetEndMs": 9645}, {"Word": "like", "OffsetStartMs": 9645, "OffsetEndMs": 9980}, {"Word": "robot", "OffsetStartMs": 10180, "OffsetEndMs": 10485}, {"Word": "sensors", "OffsetStartMs": 10485, "OffsetEndMs": 10970}, {"Word": ".Let's", "OffsetStartMs": 11200, "OffsetEndMs": 11535}, {"Word": "say", "OffsetStartMs": 11535, "OffsetEndMs": 11625}, {"Word": "I", "OffsetStartMs": 11625, "OffsetEndMs": 11870}, {"Word": "I", "OffsetStartMs": 12010, "OffsetEndMs": 12285}, {"Word": "have", "OffsetStartMs": 12285, "OffsetEndMs": 12420}, {"Word": "a", "OffsetStartMs": 12420, "OffsetEndMs": 12630}, {"Word": "robot", "OffsetStartMs": 12630, "OffsetEndMs": 12885}, {"Word": "that's", "OffsetStartMs": 12885, "OffsetEndMs": 13260}, {"Word": "trained", "OffsetStartMs": 13260, "OffsetEndMs": 13605}, {"Word": "to", "OffsetStartMs": 13605, "OffsetEndMs": 14000}, {"Word": "has", "OffsetStartMs": 14380, "OffsetEndMs": 14625}, {"Word": "a", "OffsetStartMs": 14625, "OffsetEndMs": 14730}, {"Word": "sensor", "OffsetStartMs": 14730, "OffsetEndMs": 15200}, {"Word": "that", "OffsetStartMs": 15250, "OffsetEndMs": 15510}, {"Word": "is", "OffsetStartMs": 15510, "OffsetEndMs": 15770}, {"Word": "making", "OffsetStartMs": 15820, "OffsetEndMs": 16220}, {"Word": "measurements", "OffsetStartMs": 16330, "OffsetEndMs": 16920}, {"Word": "of", "OffsetStartMs": 16920, "OffsetEndMs": 17240}, {"Word": "depth", "OffsetStartMs": 17530, "OffsetEndMs": 17930}, {"Word": ".", "OffsetStartMs": 18310, "OffsetEndMs": 18710}, {"Word": "Ifthe", "OffsetStartMs": 19330, "OffsetEndMs": 19605}, {"Word": "sensor", "OffsetStartMs": 19605, "OffsetEndMs": 19935}, {"Word": "has", "OffsetStartMs": 19935, "OffsetEndMs": 20115}, {"Word": "noise", "OffsetStartMs": 20115, "OffsetEndMs": 20340}, {"Word": "in", "OffsetStartMs": 20340, "OffsetEndMs": 20520}, {"Word": "it", "OffsetStartMs": 20520, "OffsetEndMs": 20780}, {"Word": "there's", "OffsetStartMs": 20830, "OffsetEndMs": 21240}, {"Word": "no", "OffsetStartMs": 21240, "OffsetEndMs": 21450}, {"Word": "way", "OffsetStartMs": 21450, "OffsetEndMs": 21720}, {"Word": "that", "OffsetStartMs": 21720, "OffsetEndMs": 21915}, {"Word": "I", "OffsetStartMs": 21915, "OffsetEndMs": 22065}, {"Word": "can", "OffsetStartMs": 22065, "OffsetEndMs": 22305}, {"Word": "add", "OffsetStartMs": 22305, "OffsetEndMs": 22605}, {"Word": "any", "OffsetStartMs": 22605, "OffsetEndMs": 22905}, {"Word": "more", "OffsetStartMs": 22905, "OffsetEndMs": 23175}, {"Word": "data", "OffsetStartMs": 23175, "OffsetEndMs": 23510}, {"Word": "into", "OffsetStartMs": 23710, "OffsetEndMs": 24060}, {"Word": "the", "OffsetStartMs": 24060, "OffsetEndMs": 24315}, {"Word": "system", "OffsetStartMs": 24315, "OffsetEndMs": 24615}, {"Word": "to", "OffsetStartMs": 24615, "OffsetEndMs": 24915}, {"Word": "reduce", "OffsetStartMs": 24915, "OffsetEndMs": 25140}, {"Word": "that", "OffsetStartMs": 25140, "OffsetEndMs": 25350}, {"Word": "noise", "OffsetStartMs": 25350, "OffsetEndMs": 25640}, {"Word": "unless", "OffsetStartMs": 25720, "OffsetEndMs": 25995}, {"Word": "I", "OffsetStartMs": 25995, "OffsetEndMs": 26250}, {"Word": "replace", "OffsetStartMs": 26250, "OffsetEndMs": 26565}, {"Word": "my", "OffsetStartMs": 26565, "OffsetEndMs": 26775}, {"Word": "sensor", "OffsetStartMs": 26775, "OffsetEndMs": 27150}, {"Word": "entirely", "OffsetStartMs": 27150, "OffsetEndMs": 27470}], "SpeechSpeed": 14.4}, {"FinalSentence": "So now let's assign some names to the types of uncertainty that we just talked about. The blue area, or the area of high data uncertainty is known as aloric uncertainty. It is irreducible, as we just mentioned, and it can be directly learned from data, which we'll talk about in a little bit.", "SliceSentence": "So now let's assign some names to the types of uncertainty that we just talked about . Theblue area or the area of high data uncertainty is known as aloric uncertainty . Itis irreducible as we just mentioned and it can be directly learned from data which we'll talk about in a little bit", "StartMs": 1969640, "EndMs": 1987760, "WordsNum": 54, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 390}, {"Word": "now", "OffsetStartMs": 390, "OffsetEndMs": 555}, {"Word": "let's", "OffsetStartMs": 555, "OffsetEndMs": 840}, {"Word": "assign", "OffsetStartMs": 840, "OffsetEndMs": 1130}, {"Word": "some", "OffsetStartMs": 1180, "OffsetEndMs": 1530}, {"Word": "names", "OffsetStartMs": 1530, "OffsetEndMs": 1880}, {"Word": "to", "OffsetStartMs": 2140, "OffsetEndMs": 2520}, {"Word": "the", "OffsetStartMs": 2520, "OffsetEndMs": 2760}, {"Word": "types", "OffsetStartMs": 2760, "OffsetEndMs": 2925}, {"Word": "of", "OffsetStartMs": 2925, "OffsetEndMs": 3165}, {"Word": "uncertainty", "OffsetStartMs": 3165, "OffsetEndMs": 3495}, {"Word": "that", "OffsetStartMs": 3495, "OffsetEndMs": 3765}, {"Word": "we", "OffsetStartMs": 3765, "OffsetEndMs": 3900}, {"Word": "just", "OffsetStartMs": 3900, "OffsetEndMs": 4050}, {"Word": "talked", "OffsetStartMs": 4050, "OffsetEndMs": 4275}, {"Word": "about", "OffsetStartMs": 4275, "OffsetEndMs": 4610}, {"Word": ".", "OffsetStartMs": 4930, "OffsetEndMs": 5250}, {"Word": "Theblue", "OffsetStartMs": 5250, "OffsetEndMs": 5475}, {"Word": "area", "OffsetStartMs": 5475, "OffsetEndMs": 5780}, {"Word": "or", "OffsetStartMs": 5920, "OffsetEndMs": 6225}, {"Word": "the", "OffsetStartMs": 6225, "OffsetEndMs": 6390}, {"Word": "area", "OffsetStartMs": 6390, "OffsetEndMs": 6600}, {"Word": "of", "OffsetStartMs": 6600, "OffsetEndMs": 6855}, {"Word": "high", "OffsetStartMs": 6855, "OffsetEndMs": 7080}, {"Word": "data", "OffsetStartMs": 7080, "OffsetEndMs": 7400}, {"Word": "uncertainty", "OffsetStartMs": 7420, "OffsetEndMs": 7820}, {"Word": "is", "OffsetStartMs": 7960, "OffsetEndMs": 8265}, {"Word": "known", "OffsetStartMs": 8265, "OffsetEndMs": 8505}, {"Word": "as", "OffsetStartMs": 8505, "OffsetEndMs": 8790}, {"Word": "aloric", "OffsetStartMs": 8790, "OffsetEndMs": 9660}, {"Word": "uncertainty", "OffsetStartMs": 9660, "OffsetEndMs": 10040}, {"Word": ".", "OffsetStartMs": 10840, "OffsetEndMs": 11100}, {"Word": "Itis", "OffsetStartMs": 11100, "OffsetEndMs": 11265}, {"Word": "irreducible", "OffsetStartMs": 11265, "OffsetEndMs": 12075}, {"Word": "as", "OffsetStartMs": 12075, "OffsetEndMs": 12300}, {"Word": "we", "OffsetStartMs": 12300, "OffsetEndMs": 12450}, {"Word": "just", "OffsetStartMs": 12450, "OffsetEndMs": 12600}, {"Word": "mentioned", "OffsetStartMs": 12600, "OffsetEndMs": 12890}, {"Word": "and", "OffsetStartMs": 13450, "OffsetEndMs": 13725}, {"Word": "it", "OffsetStartMs": 13725, "OffsetEndMs": 13890}, {"Word": "can", "OffsetStartMs": 13890, "OffsetEndMs": 14040}, {"Word": "be", "OffsetStartMs": 14040, "OffsetEndMs": 14235}, {"Word": "directly", "OffsetStartMs": 14235, "OffsetEndMs": 14570}, {"Word": "learned", "OffsetStartMs": 14650, "OffsetEndMs": 15050}, {"Word": "from", "OffsetStartMs": 15100, "OffsetEndMs": 15405}, {"Word": "data", "OffsetStartMs": 15405, "OffsetEndMs": 15710}, {"Word": "which", "OffsetStartMs": 15730, "OffsetEndMs": 16005}, {"Word": "we'll", "OffsetStartMs": 16005, "OffsetEndMs": 16200}, {"Word": "talk", "OffsetStartMs": 16200, "OffsetEndMs": 16350}, {"Word": "about", "OffsetStartMs": 16350, "OffsetEndMs": 16560}, {"Word": "in", "OffsetStartMs": 16560, "OffsetEndMs": 16695}, {"Word": "a", "OffsetStartMs": 16695, "OffsetEndMs": 16770}, {"Word": "little", "OffsetStartMs": 16770, "OffsetEndMs": 16920}, {"Word": "bit", "OffsetStartMs": 16920, "OffsetEndMs": 17240}], "SpeechSpeed": 15.7}, {"FinalSentence": "The green areas of the green boxes that we talked about, which were model uncertainty, are known as epistemic uncertainty, and this cannot be learned directly from the data. However, it can be reduced by adding more data into our systems into these regions.", "SliceSentence": "The green areas of the green boxes that we talked about which were model uncertainty are known as epistemic uncertainty and this cannot be learned directly from the data . Howeverit can be reduced by adding more data into our systems into these regions", "StartMs": 1988580, "EndMs": 2004680, "WordsNum": 44, "Words": [{"Word": "The", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "green", "OffsetStartMs": 360, "OffsetEndMs": 555}, {"Word": "areas", "OffsetStartMs": 555, "OffsetEndMs": 860}, {"Word": "of", "OffsetStartMs": 1000, "OffsetEndMs": 1400}, {"Word": "the", "OffsetStartMs": 1930, "OffsetEndMs": 2175}, {"Word": "green", "OffsetStartMs": 2175, "OffsetEndMs": 2325}, {"Word": "boxes", "OffsetStartMs": 2325, "OffsetEndMs": 2630}, {"Word": "that", "OffsetStartMs": 2680, "OffsetEndMs": 2940}, {"Word": "we", "OffsetStartMs": 2940, "OffsetEndMs": 3060}, {"Word": "talked", "OffsetStartMs": 3060, "OffsetEndMs": 3270}, {"Word": "about", "OffsetStartMs": 3270, "OffsetEndMs": 3620}, {"Word": "which", "OffsetStartMs": 3700, "OffsetEndMs": 4005}, {"Word": "were", "OffsetStartMs": 4005, "OffsetEndMs": 4215}, {"Word": "model", "OffsetStartMs": 4215, "OffsetEndMs": 4520}, {"Word": "uncertainty", "OffsetStartMs": 4600, "OffsetEndMs": 5000}, {"Word": "are", "OffsetStartMs": 5260, "OffsetEndMs": 5565}, {"Word": "known", "OffsetStartMs": 5565, "OffsetEndMs": 5820}, {"Word": "as", "OffsetStartMs": 5820, "OffsetEndMs": 6120}, {"Word": "epistemic", "OffsetStartMs": 6120, "OffsetEndMs": 6915}, {"Word": "uncertainty", "OffsetStartMs": 6915, "OffsetEndMs": 7280}, {"Word": "and", "OffsetStartMs": 7870, "OffsetEndMs": 8160}, {"Word": "this", "OffsetStartMs": 8160, "OffsetEndMs": 8430}, {"Word": "cannot", "OffsetStartMs": 8430, "OffsetEndMs": 8790}, {"Word": "be", "OffsetStartMs": 8790, "OffsetEndMs": 9045}, {"Word": "learned", "OffsetStartMs": 9045, "OffsetEndMs": 9285}, {"Word": "directly", "OffsetStartMs": 9285, "OffsetEndMs": 9615}, {"Word": "from", "OffsetStartMs": 9615, "OffsetEndMs": 9855}, {"Word": "the", "OffsetStartMs": 9855, "OffsetEndMs": 9990}, {"Word": "data", "OffsetStartMs": 9990, "OffsetEndMs": 10250}, {"Word": ".", "OffsetStartMs": 10750, "OffsetEndMs": 11130}, {"Word": "Howeverit", "OffsetStartMs": 11130, "OffsetEndMs": 11430}, {"Word": "can", "OffsetStartMs": 11430, "OffsetEndMs": 11685}, {"Word": "be", "OffsetStartMs": 11685, "OffsetEndMs": 11970}, {"Word": "reduced", "OffsetStartMs": 11970, "OffsetEndMs": 12285}, {"Word": "by", "OffsetStartMs": 12285, "OffsetEndMs": 12540}, {"Word": "adding", "OffsetStartMs": 12540, "OffsetEndMs": 12795}, {"Word": "more", "OffsetStartMs": 12795, "OffsetEndMs": 13050}, {"Word": "data", "OffsetStartMs": 13050, "OffsetEndMs": 13290}, {"Word": "into", "OffsetStartMs": 13290, "OffsetEndMs": 13560}, {"Word": "our", "OffsetStartMs": 13560, "OffsetEndMs": 13770}, {"Word": "systems", "OffsetStartMs": 13770, "OffsetEndMs": 14060}, {"Word": "into", "OffsetStartMs": 14110, "OffsetEndMs": 14445}, {"Word": "these", "OffsetStartMs": 14445, "OffsetEndMs": 14685}, {"Word": "regions", "OffsetStartMs": 14685, "OffsetEndMs": 14990}], "SpeechSpeed": 15.6}, {"FinalSentence": "Okay, so first let's go through allatoric uncertainty. So the goal of estimating allatoric uncertainty is to learn a set of variances that correspond to the input.", "SliceSentence": "Okay so first let's go through allatoric uncertainty . Sothe goal of estimating allatoric uncertainty is to learn a set of variances that correspond to the input", "StartMs": 2010220, "EndMs": 2022480, "WordsNum": 27, "Words": [{"Word": "Okay", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "so", "OffsetStartMs": 790, "OffsetEndMs": 1050}, {"Word": "first", "OffsetStartMs": 1050, "OffsetEndMs": 1230}, {"Word": "let's", "OffsetStartMs": 1230, "OffsetEndMs": 1500}, {"Word": "go", "OffsetStartMs": 1500, "OffsetEndMs": 1680}, {"Word": "through", "OffsetStartMs": 1680, "OffsetEndMs": 1905}, {"Word": "allatoric", "OffsetStartMs": 1905, "OffsetEndMs": 2640}, {"Word": "uncertainty", "OffsetStartMs": 2640, "OffsetEndMs": 2990}, {"Word": ".", "OffsetStartMs": 4120, "OffsetEndMs": 4520}, {"Word": "Sothe", "OffsetStartMs": 4540, "OffsetEndMs": 4830}, {"Word": "goal", "OffsetStartMs": 4830, "OffsetEndMs": 5070}, {"Word": "of", "OffsetStartMs": 5070, "OffsetEndMs": 5420}, {"Word": "estimating", "OffsetStartMs": 5800, "OffsetEndMs": 6240}, {"Word": "allatoric", "OffsetStartMs": 6240, "OffsetEndMs": 7020}, {"Word": "uncertainty", "OffsetStartMs": 7020, "OffsetEndMs": 7370}, {"Word": "is", "OffsetStartMs": 7570, "OffsetEndMs": 7875}, {"Word": "to", "OffsetStartMs": 7875, "OffsetEndMs": 8055}, {"Word": "learn", "OffsetStartMs": 8055, "OffsetEndMs": 8295}, {"Word": "a", "OffsetStartMs": 8295, "OffsetEndMs": 8580}, {"Word": "set", "OffsetStartMs": 8580, "OffsetEndMs": 8820}, {"Word": "of", "OffsetStartMs": 8820, "OffsetEndMs": 9060}, {"Word": "variances", "OffsetStartMs": 9060, "OffsetEndMs": 9735}, {"Word": "that", "OffsetStartMs": 9735, "OffsetEndMs": 9975}, {"Word": "correspond", "OffsetStartMs": 9975, "OffsetEndMs": 10280}, {"Word": "to", "OffsetStartMs": 10480, "OffsetEndMs": 10740}, {"Word": "the", "OffsetStartMs": 10740, "OffsetEndMs": 10965}, {"Word": "input", "OffsetStartMs": 10965, "OffsetEndMs": 11330}], "SpeechSpeed": 13.1}, {"FinalSentence": "Keep in mind that we are not looking at a data distribution and we are, as humans are not estimating the variance. We're training the model to do this task. And so what that means is typically when we train a model, we give it an input X and we expect an output y hat, which is the prediction of the model.", "SliceSentence": "Keep in mind that we are not looking at a data distribution and we are as humans are not estimating the variance .We're training the model to do this task . Andso what that means is typically when we train a model we give it an input X and we expect an output y hat which is the prediction of the model", "StartMs": 2022480, "EndMs": 2039860, "WordsNum": 62, "Words": [{"Word": "Keep", "OffsetStartMs": 10, "OffsetEndMs": 270}, {"Word": "in", "OffsetStartMs": 270, "OffsetEndMs": 405}, {"Word": "mind", "OffsetStartMs": 405, "OffsetEndMs": 570}, {"Word": "that", "OffsetStartMs": 570, "OffsetEndMs": 720}, {"Word": "we", "OffsetStartMs": 720, "OffsetEndMs": 825}, {"Word": "are", "OffsetStartMs": 825, "OffsetEndMs": 945}, {"Word": "not", "OffsetStartMs": 945, "OffsetEndMs": 1185}, {"Word": "looking", "OffsetStartMs": 1185, "OffsetEndMs": 1530}, {"Word": "at", "OffsetStartMs": 1530, "OffsetEndMs": 1830}, {"Word": "a", "OffsetStartMs": 1830, "OffsetEndMs": 2040}, {"Word": "data", "OffsetStartMs": 2040, "OffsetEndMs": 2310}, {"Word": "distribution", "OffsetStartMs": 2310, "OffsetEndMs": 2690}, {"Word": "and", "OffsetStartMs": 2830, "OffsetEndMs": 3135}, {"Word": "we", "OffsetStartMs": 3135, "OffsetEndMs": 3300}, {"Word": "are", "OffsetStartMs": 3300, "OffsetEndMs": 3465}, {"Word": "as", "OffsetStartMs": 3465, "OffsetEndMs": 3690}, {"Word": "humans", "OffsetStartMs": 3690, "OffsetEndMs": 3960}, {"Word": "are", "OffsetStartMs": 3960, "OffsetEndMs": 4185}, {"Word": "not", "OffsetStartMs": 4185, "OffsetEndMs": 4455}, {"Word": "estimating", "OffsetStartMs": 4455, "OffsetEndMs": 4860}, {"Word": "the", "OffsetStartMs": 4860, "OffsetEndMs": 5025}, {"Word": "variance", "OffsetStartMs": 5025, "OffsetEndMs": 5445}, {"Word": ".We're", "OffsetStartMs": 5445, "OffsetEndMs": 5790}, {"Word": "training", "OffsetStartMs": 5790, "OffsetEndMs": 6045}, {"Word": "the", "OffsetStartMs": 6045, "OffsetEndMs": 6285}, {"Word": "model", "OffsetStartMs": 6285, "OffsetEndMs": 6555}, {"Word": "to", "OffsetStartMs": 6555, "OffsetEndMs": 6810}, {"Word": "do", "OffsetStartMs": 6810, "OffsetEndMs": 6930}, {"Word": "this", "OffsetStartMs": 6930, "OffsetEndMs": 7095}, {"Word": "task", "OffsetStartMs": 7095, "OffsetEndMs": 7400}, {"Word": ".", "OffsetStartMs": 7990, "OffsetEndMs": 8295}, {"Word": "Andso", "OffsetStartMs": 8295, "OffsetEndMs": 8460}, {"Word": "what", "OffsetStartMs": 8460, "OffsetEndMs": 8595}, {"Word": "that", "OffsetStartMs": 8595, "OffsetEndMs": 8760}, {"Word": "means", "OffsetStartMs": 8760, "OffsetEndMs": 9050}, {"Word": "is", "OffsetStartMs": 9130, "OffsetEndMs": 9530}, {"Word": "typically", "OffsetStartMs": 9550, "OffsetEndMs": 9950}, {"Word": "when", "OffsetStartMs": 10390, "OffsetEndMs": 10680}, {"Word": "we", "OffsetStartMs": 10680, "OffsetEndMs": 10860}, {"Word": "train", "OffsetStartMs": 10860, "OffsetEndMs": 11025}, {"Word": "a", "OffsetStartMs": 11025, "OffsetEndMs": 11160}, {"Word": "model", "OffsetStartMs": 11160, "OffsetEndMs": 11420}, {"Word": "we", "OffsetStartMs": 11470, "OffsetEndMs": 11730}, {"Word": "give", "OffsetStartMs": 11730, "OffsetEndMs": 11850}, {"Word": "it", "OffsetStartMs": 11850, "OffsetEndMs": 11955}, {"Word": "an", "OffsetStartMs": 11955, "OffsetEndMs": 12150}, {"Word": "input", "OffsetStartMs": 12150, "OffsetEndMs": 12375}, {"Word": "X", "OffsetStartMs": 12375, "OffsetEndMs": 12650}, {"Word": "and", "OffsetStartMs": 12850, "OffsetEndMs": 13140}, {"Word": "we", "OffsetStartMs": 13140, "OffsetEndMs": 13365}, {"Word": "expect", "OffsetStartMs": 13365, "OffsetEndMs": 13635}, {"Word": "an", "OffsetStartMs": 13635, "OffsetEndMs": 13965}, {"Word": "output", "OffsetStartMs": 13965, "OffsetEndMs": 14280}, {"Word": "y", "OffsetStartMs": 14280, "OffsetEndMs": 14550}, {"Word": "hat", "OffsetStartMs": 14550, "OffsetEndMs": 14900}, {"Word": "which", "OffsetStartMs": 14950, "OffsetEndMs": 15210}, {"Word": "is", "OffsetStartMs": 15210, "OffsetEndMs": 15345}, {"Word": "the", "OffsetStartMs": 15345, "OffsetEndMs": 15480}, {"Word": "prediction", "OffsetStartMs": 15480, "OffsetEndMs": 15825}, {"Word": "of", "OffsetStartMs": 15825, "OffsetEndMs": 16065}, {"Word": "the", "OffsetStartMs": 16065, "OffsetEndMs": 16200}, {"Word": "model", "OffsetStartMs": 16200, "OffsetEndMs": 16460}], "SpeechSpeed": 17.3}, {"FinalSentence": "Now we also predict an additional sigma squared, so we add another layer to our model. We have the same output size that predicts a variance for every output.", "SliceSentence": "Now we also predict an additional sigma squared so we add another layer to our model . Wehave the same output size that predicts a variance for every output", "StartMs": 2039940, "EndMs": 2049940, "WordsNum": 29, "Words": [{"Word": "Now", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "we", "OffsetStartMs": 700, "OffsetEndMs": 1100}, {"Word": "also", "OffsetStartMs": 1120, "OffsetEndMs": 1410}, {"Word": "predict", "OffsetStartMs": 1410, "OffsetEndMs": 1680}, {"Word": "an", "OffsetStartMs": 1680, "OffsetEndMs": 1965}, {"Word": "additional", "OffsetStartMs": 1965, "OffsetEndMs": 2270}, {"Word": "sigma", "OffsetStartMs": 2440, "OffsetEndMs": 2925}, {"Word": "squared", "OffsetStartMs": 2925, "OffsetEndMs": 3260}, {"Word": "so", "OffsetStartMs": 3340, "OffsetEndMs": 3600}, {"Word": "we", "OffsetStartMs": 3600, "OffsetEndMs": 3720}, {"Word": "add", "OffsetStartMs": 3720, "OffsetEndMs": 3945}, {"Word": "another", "OffsetStartMs": 3945, "OffsetEndMs": 4245}, {"Word": "layer", "OffsetStartMs": 4245, "OffsetEndMs": 4560}, {"Word": "to", "OffsetStartMs": 4560, "OffsetEndMs": 4770}, {"Word": "our", "OffsetStartMs": 4770, "OffsetEndMs": 4890}, {"Word": "model", "OffsetStartMs": 4890, "OffsetEndMs": 5180}, {"Word": ".", "OffsetStartMs": 5590, "OffsetEndMs": 5835}, {"Word": "Wehave", "OffsetStartMs": 5835, "OffsetEndMs": 5925}, {"Word": "the", "OffsetStartMs": 5925, "OffsetEndMs": 6060}, {"Word": "same", "OffsetStartMs": 6060, "OffsetEndMs": 6350}, {"Word": "output", "OffsetStartMs": 6370, "OffsetEndMs": 6645}, {"Word": "size", "OffsetStartMs": 6645, "OffsetEndMs": 6920}, {"Word": "that", "OffsetStartMs": 7030, "OffsetEndMs": 7335}, {"Word": "predicts", "OffsetStartMs": 7335, "OffsetEndMs": 7785}, {"Word": "a", "OffsetStartMs": 7785, "OffsetEndMs": 7935}, {"Word": "variance", "OffsetStartMs": 7935, "OffsetEndMs": 8355}, {"Word": "for", "OffsetStartMs": 8355, "OffsetEndMs": 8550}, {"Word": "every", "OffsetStartMs": 8550, "OffsetEndMs": 8810}, {"Word": "output", "OffsetStartMs": 8920, "OffsetEndMs": 9320}], "SpeechSpeed": 15.5}, {"FinalSentence": "So the reason why we do this is that we expect that areas in our data set with high data uncertainty are going to have higher variance.", "SliceSentence": "So the reason why we do this is that we expect that areas in our data set with high data uncertainty are going to have higher variance", "StartMs": 2051100, "EndMs": 2059640, "WordsNum": 27, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 495}, {"Word": "the", "OffsetStartMs": 495, "OffsetEndMs": 705}, {"Word": "reason", "OffsetStartMs": 705, "OffsetEndMs": 900}, {"Word": "why", "OffsetStartMs": 900, "OffsetEndMs": 1125}, {"Word": "we", "OffsetStartMs": 1125, "OffsetEndMs": 1260}, {"Word": "do", "OffsetStartMs": 1260, "OffsetEndMs": 1380}, {"Word": "this", "OffsetStartMs": 1380, "OffsetEndMs": 1590}, {"Word": "is", "OffsetStartMs": 1590, "OffsetEndMs": 1815}, {"Word": "that", "OffsetStartMs": 1815, "OffsetEndMs": 2040}, {"Word": "we", "OffsetStartMs": 2040, "OffsetEndMs": 2370}, {"Word": "expect", "OffsetStartMs": 2370, "OffsetEndMs": 2700}, {"Word": "that", "OffsetStartMs": 2700, "OffsetEndMs": 2955}, {"Word": "areas", "OffsetStartMs": 2955, "OffsetEndMs": 3260}, {"Word": "in", "OffsetStartMs": 3280, "OffsetEndMs": 3525}, {"Word": "our", "OffsetStartMs": 3525, "OffsetEndMs": 3645}, {"Word": "data", "OffsetStartMs": 3645, "OffsetEndMs": 3855}, {"Word": "set", "OffsetStartMs": 3855, "OffsetEndMs": 4110}, {"Word": "with", "OffsetStartMs": 4110, "OffsetEndMs": 4365}, {"Word": "high", "OffsetStartMs": 4365, "OffsetEndMs": 4665}, {"Word": "data", "OffsetStartMs": 4665, "OffsetEndMs": 5030}, {"Word": "uncertainty", "OffsetStartMs": 5080, "OffsetEndMs": 5480}, {"Word": "are", "OffsetStartMs": 5830, "OffsetEndMs": 6120}, {"Word": "going", "OffsetStartMs": 6120, "OffsetEndMs": 6300}, {"Word": "to", "OffsetStartMs": 6300, "OffsetEndMs": 6450}, {"Word": "have", "OffsetStartMs": 6450, "OffsetEndMs": 6660}, {"Word": "higher", "OffsetStartMs": 6660, "OffsetEndMs": 6960}, {"Word": "variance", "OffsetStartMs": 6960, "OffsetEndMs": 7520}], "SpeechSpeed": 15.7}, {"FinalSentence": "And the crucial thing to remember here is that this variance is not constant. It depends on the value of X, we typically tend to think of variance as a single number that parameterizes an entire distribution. However, in this case, we may have areas of our input distribution with really high variance, and we may have areas with very low variance. So our variance cannot be independent of the input, and it depends on our input X.", "SliceSentence": "And the crucial thing to remember here is that this variance is not constant . Itdepends on the value of X we typically tend to think of variance as a single number that parameterizes an entire distribution . Howeverin this case we may have areas of our input distribution with really high variance and we may have areas with very low variance . Soour variance cannot be independent of the input and it depends on our input X", "StartMs": 2060580, "EndMs": 2086220, "WordsNum": 78, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "the", "OffsetStartMs": 360, "OffsetEndMs": 525}, {"Word": "crucial", "OffsetStartMs": 525, "OffsetEndMs": 800}, {"Word": "thing", "OffsetStartMs": 850, "OffsetEndMs": 1140}, {"Word": "to", "OffsetStartMs": 1140, "OffsetEndMs": 1320}, {"Word": "remember", "OffsetStartMs": 1320, "OffsetEndMs": 1560}, {"Word": "here", "OffsetStartMs": 1560, "OffsetEndMs": 1910}, {"Word": "is", "OffsetStartMs": 1990, "OffsetEndMs": 2250}, {"Word": "that", "OffsetStartMs": 2250, "OffsetEndMs": 2400}, {"Word": "this", "OffsetStartMs": 2400, "OffsetEndMs": 2610}, {"Word": "variance", "OffsetStartMs": 2610, "OffsetEndMs": 3075}, {"Word": "is", "OffsetStartMs": 3075, "OffsetEndMs": 3360}, {"Word": "not", "OffsetStartMs": 3360, "OffsetEndMs": 3710}, {"Word": "constant", "OffsetStartMs": 3730, "OffsetEndMs": 4130}, {"Word": ".", "OffsetStartMs": 4690, "OffsetEndMs": 5040}, {"Word": "Itdepends", "OffsetStartMs": 5040, "OffsetEndMs": 5355}, {"Word": "on", "OffsetStartMs": 5355, "OffsetEndMs": 5595}, {"Word": "the", "OffsetStartMs": 5595, "OffsetEndMs": 5745}, {"Word": "value", "OffsetStartMs": 5745, "OffsetEndMs": 6020}, {"Word": "of", "OffsetStartMs": 6040, "OffsetEndMs": 6330}, {"Word": "X", "OffsetStartMs": 6330, "OffsetEndMs": 6620}, {"Word": "we", "OffsetStartMs": 6910, "OffsetEndMs": 7200}, {"Word": "typically", "OffsetStartMs": 7200, "OffsetEndMs": 7490}, {"Word": "tend", "OffsetStartMs": 7540, "OffsetEndMs": 7845}, {"Word": "to", "OffsetStartMs": 7845, "OffsetEndMs": 8010}, {"Word": "think", "OffsetStartMs": 8010, "OffsetEndMs": 8145}, {"Word": "of", "OffsetStartMs": 8145, "OffsetEndMs": 8280}, {"Word": "variance", "OffsetStartMs": 8280, "OffsetEndMs": 8715}, {"Word": "as", "OffsetStartMs": 8715, "OffsetEndMs": 8955}, {"Word": "a", "OffsetStartMs": 8955, "OffsetEndMs": 9150}, {"Word": "single", "OffsetStartMs": 9150, "OffsetEndMs": 9375}, {"Word": "number", "OffsetStartMs": 9375, "OffsetEndMs": 9660}, {"Word": "that", "OffsetStartMs": 9660, "OffsetEndMs": 9915}, {"Word": "parameterizes", "OffsetStartMs": 9915, "OffsetEndMs": 10485}, {"Word": "an", "OffsetStartMs": 10485, "OffsetEndMs": 10800}, {"Word": "entire", "OffsetStartMs": 10800, "OffsetEndMs": 11150}, {"Word": "distribution", "OffsetStartMs": 11260, "OffsetEndMs": 11660}, {"Word": ".", "OffsetStartMs": 12370, "OffsetEndMs": 12770}, {"Word": "Howeverin", "OffsetStartMs": 12820, "OffsetEndMs": 13110}, {"Word": "this", "OffsetStartMs": 13110, "OffsetEndMs": 13320}, {"Word": "case", "OffsetStartMs": 13320, "OffsetEndMs": 13640}, {"Word": "we", "OffsetStartMs": 13840, "OffsetEndMs": 14115}, {"Word": "may", "OffsetStartMs": 14115, "OffsetEndMs": 14280}, {"Word": "have", "OffsetStartMs": 14280, "OffsetEndMs": 14460}, {"Word": "areas", "OffsetStartMs": 14460, "OffsetEndMs": 14730}, {"Word": "of", "OffsetStartMs": 14730, "OffsetEndMs": 14970}, {"Word": "our", "OffsetStartMs": 14970, "OffsetEndMs": 15195}, {"Word": "input", "OffsetStartMs": 15195, "OffsetEndMs": 15510}, {"Word": "distribution", "OffsetStartMs": 15510, "OffsetEndMs": 15855}, {"Word": "with", "OffsetStartMs": 15855, "OffsetEndMs": 16155}, {"Word": "really", "OffsetStartMs": 16155, "OffsetEndMs": 16425}, {"Word": "high", "OffsetStartMs": 16425, "OffsetEndMs": 16725}, {"Word": "variance", "OffsetStartMs": 16725, "OffsetEndMs": 17300}, {"Word": "and", "OffsetStartMs": 17350, "OffsetEndMs": 17610}, {"Word": "we", "OffsetStartMs": 17610, "OffsetEndMs": 17730}, {"Word": "may", "OffsetStartMs": 17730, "OffsetEndMs": 17865}, {"Word": "have", "OffsetStartMs": 17865, "OffsetEndMs": 18000}, {"Word": "areas", "OffsetStartMs": 18000, "OffsetEndMs": 18210}, {"Word": "with", "OffsetStartMs": 18210, "OffsetEndMs": 18435}, {"Word": "very", "OffsetStartMs": 18435, "OffsetEndMs": 18660}, {"Word": "low", "OffsetStartMs": 18660, "OffsetEndMs": 18915}, {"Word": "variance", "OffsetStartMs": 18915, "OffsetEndMs": 19430}, {"Word": ".", "OffsetStartMs": 19690, "OffsetEndMs": 20070}, {"Word": "Soour", "OffsetStartMs": 20070, "OffsetEndMs": 20340}, {"Word": "variance", "OffsetStartMs": 20340, "OffsetEndMs": 20715}, {"Word": "cannot", "OffsetStartMs": 20715, "OffsetEndMs": 21000}, {"Word": "be", "OffsetStartMs": 21000, "OffsetEndMs": 21350}, {"Word": "independent", "OffsetStartMs": 21430, "OffsetEndMs": 21780}, {"Word": "of", "OffsetStartMs": 21780, "OffsetEndMs": 21990}, {"Word": "the", "OffsetStartMs": 21990, "OffsetEndMs": 22200}, {"Word": "input", "OffsetStartMs": 22200, "OffsetEndMs": 22515}, {"Word": "and", "OffsetStartMs": 22515, "OffsetEndMs": 22740}, {"Word": "it", "OffsetStartMs": 22740, "OffsetEndMs": 22980}, {"Word": "depends", "OffsetStartMs": 22980, "OffsetEndMs": 23325}, {"Word": "on", "OffsetStartMs": 23325, "OffsetEndMs": 23550}, {"Word": "our", "OffsetStartMs": 23550, "OffsetEndMs": 23805}, {"Word": "input", "OffsetStartMs": 23805, "OffsetEndMs": 24075}, {"Word": "X", "OffsetStartMs": 24075, "OffsetEndMs": 24350}], "SpeechSpeed": 16.5}, {"FinalSentence": "So now that we have this model, we have an extra layer attached to it. In addition to predicting y hat, we also predict a sigma squared. How do we train this model?", "SliceSentence": "So now that we have this model we have an extra layer attached to it . Inaddition to predicting y hat we also predict a sigma squared . Howdo we train this model", "StartMs": 2087120, "EndMs": 2097220, "WordsNum": 33, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "now", "OffsetStartMs": 550, "OffsetEndMs": 825}, {"Word": "that", "OffsetStartMs": 825, "OffsetEndMs": 945}, {"Word": "we", "OffsetStartMs": 945, "OffsetEndMs": 1065}, {"Word": "have", "OffsetStartMs": 1065, "OffsetEndMs": 1245}, {"Word": "this", "OffsetStartMs": 1245, "OffsetEndMs": 1455}, {"Word": "model", "OffsetStartMs": 1455, "OffsetEndMs": 1710}, {"Word": "we", "OffsetStartMs": 1710, "OffsetEndMs": 1905}, {"Word": "have", "OffsetStartMs": 1905, "OffsetEndMs": 1995}, {"Word": "an", "OffsetStartMs": 1995, "OffsetEndMs": 2100}, {"Word": "extra", "OffsetStartMs": 2100, "OffsetEndMs": 2360}, {"Word": "layer", "OffsetStartMs": 2380, "OffsetEndMs": 2775}, {"Word": "attached", "OffsetStartMs": 2775, "OffsetEndMs": 3135}, {"Word": "to", "OffsetStartMs": 3135, "OffsetEndMs": 3345}, {"Word": "it", "OffsetStartMs": 3345, "OffsetEndMs": 3585}, {"Word": ".", "OffsetStartMs": 3585, "OffsetEndMs": 3885}, {"Word": "Inaddition", "OffsetStartMs": 3885, "OffsetEndMs": 4125}, {"Word": "to", "OffsetStartMs": 4125, "OffsetEndMs": 4320}, {"Word": "predicting", "OffsetStartMs": 4320, "OffsetEndMs": 4770}, {"Word": "y", "OffsetStartMs": 4770, "OffsetEndMs": 5025}, {"Word": "hat", "OffsetStartMs": 5025, "OffsetEndMs": 5295}, {"Word": "we", "OffsetStartMs": 5295, "OffsetEndMs": 5595}, {"Word": "also", "OffsetStartMs": 5595, "OffsetEndMs": 5835}, {"Word": "predict", "OffsetStartMs": 5835, "OffsetEndMs": 6045}, {"Word": "a", "OffsetStartMs": 6045, "OffsetEndMs": 6285}, {"Word": "sigma", "OffsetStartMs": 6285, "OffsetEndMs": 6630}, {"Word": "squared", "OffsetStartMs": 6630, "OffsetEndMs": 6980}, {"Word": ".", "OffsetStartMs": 7690, "OffsetEndMs": 7965}, {"Word": "Howdo", "OffsetStartMs": 7965, "OffsetEndMs": 8070}, {"Word": "we", "OffsetStartMs": 8070, "OffsetEndMs": 8220}, {"Word": "train", "OffsetStartMs": 8220, "OffsetEndMs": 8445}, {"Word": "this", "OffsetStartMs": 8445, "OffsetEndMs": 8640}, {"Word": "model", "OffsetStartMs": 8640, "OffsetEndMs": 8930}], "SpeechSpeed": 15.7}, {"FinalSentence": "Our current loss function does not take into account variance at any point. This is your typical mean squared error loss function that is used to train regression models. And there's no way training from this loss function that we can learn whether or not the variance that we're estimating is accurate.", "SliceSentence": "Our current loss function does not take into account variance at any point . Thisis your typical mean squared error loss function that is used to train regression models . Andthere's no way training from this loss function that we can learn whether or not the variance that we're estimating is accurate", "StartMs": 2098320, "EndMs": 2114240, "WordsNum": 52, "Words": [{"Word": "Our", "OffsetStartMs": 70, "OffsetEndMs": 375}, {"Word": "current", "OffsetStartMs": 375, "OffsetEndMs": 630}, {"Word": "loss", "OffsetStartMs": 630, "OffsetEndMs": 930}, {"Word": "function", "OffsetStartMs": 930, "OffsetEndMs": 1280}, {"Word": "does", "OffsetStartMs": 1510, "OffsetEndMs": 1800}, {"Word": "not", "OffsetStartMs": 1800, "OffsetEndMs": 2010}, {"Word": "take", "OffsetStartMs": 2010, "OffsetEndMs": 2235}, {"Word": "into", "OffsetStartMs": 2235, "OffsetEndMs": 2520}, {"Word": "account", "OffsetStartMs": 2520, "OffsetEndMs": 2820}, {"Word": "variance", "OffsetStartMs": 2820, "OffsetEndMs": 3240}, {"Word": "at", "OffsetStartMs": 3240, "OffsetEndMs": 3405}, {"Word": "any", "OffsetStartMs": 3405, "OffsetEndMs": 3630}, {"Word": "point", "OffsetStartMs": 3630, "OffsetEndMs": 3980}, {"Word": ".", "OffsetStartMs": 4060, "OffsetEndMs": 4350}, {"Word": "Thisis", "OffsetStartMs": 4350, "OffsetEndMs": 4530}, {"Word": "your", "OffsetStartMs": 4530, "OffsetEndMs": 4710}, {"Word": "typical", "OffsetStartMs": 4710, "OffsetEndMs": 5000}, {"Word": "mean", "OffsetStartMs": 5020, "OffsetEndMs": 5355}, {"Word": "squared", "OffsetStartMs": 5355, "OffsetEndMs": 5640}, {"Word": "error", "OffsetStartMs": 5640, "OffsetEndMs": 5910}, {"Word": "loss", "OffsetStartMs": 5910, "OffsetEndMs": 6195}, {"Word": "function", "OffsetStartMs": 6195, "OffsetEndMs": 6510}, {"Word": "that", "OffsetStartMs": 6510, "OffsetEndMs": 6735}, {"Word": "is", "OffsetStartMs": 6735, "OffsetEndMs": 6870}, {"Word": "used", "OffsetStartMs": 6870, "OffsetEndMs": 7110}, {"Word": "to", "OffsetStartMs": 7110, "OffsetEndMs": 7335}, {"Word": "train", "OffsetStartMs": 7335, "OffsetEndMs": 7500}, {"Word": "regression", "OffsetStartMs": 7500, "OffsetEndMs": 7860}, {"Word": "models", "OffsetStartMs": 7860, "OffsetEndMs": 8240}, {"Word": ".", "OffsetStartMs": 8650, "OffsetEndMs": 8910}, {"Word": "Andthere's", "OffsetStartMs": 8910, "OffsetEndMs": 9150}, {"Word": "no", "OffsetStartMs": 9150, "OffsetEndMs": 9330}, {"Word": "way", "OffsetStartMs": 9330, "OffsetEndMs": 9650}, {"Word": "training", "OffsetStartMs": 9820, "OffsetEndMs": 10155}, {"Word": "from", "OffsetStartMs": 10155, "OffsetEndMs": 10365}, {"Word": "this", "OffsetStartMs": 10365, "OffsetEndMs": 10530}, {"Word": "loss", "OffsetStartMs": 10530, "OffsetEndMs": 10740}, {"Word": "function", "OffsetStartMs": 10740, "OffsetEndMs": 11060}, {"Word": "that", "OffsetStartMs": 11350, "OffsetEndMs": 11625}, {"Word": "we", "OffsetStartMs": 11625, "OffsetEndMs": 11790}, {"Word": "can", "OffsetStartMs": 11790, "OffsetEndMs": 11985}, {"Word": "learn", "OffsetStartMs": 11985, "OffsetEndMs": 12270}, {"Word": "whether", "OffsetStartMs": 12270, "OffsetEndMs": 12600}, {"Word": "or", "OffsetStartMs": 12600, "OffsetEndMs": 12810}, {"Word": "not", "OffsetStartMs": 12810, "OffsetEndMs": 12975}, {"Word": "the", "OffsetStartMs": 12975, "OffsetEndMs": 13140}, {"Word": "variance", "OffsetStartMs": 13140, "OffsetEndMs": 13440}, {"Word": "that", "OffsetStartMs": 13440, "OffsetEndMs": 13590}, {"Word": "we're", "OffsetStartMs": 13590, "OffsetEndMs": 13875}, {"Word": "estimating", "OffsetStartMs": 13875, "OffsetEndMs": 14190}, {"Word": "is", "OffsetStartMs": 14190, "OffsetEndMs": 14505}, {"Word": "accurate", "OffsetStartMs": 14505, "OffsetEndMs": 14900}], "SpeechSpeed": 18.8}, {"FinalSentence": "So in addition to adding another layer to estimate allatoric uncertainty correctly, we also have to change our loss function.", "SliceSentence": "So in addition to adding another layer to estimate allatoric uncertainty correctly we also have to change our loss function", "StartMs": 2115020, "EndMs": 2123080, "WordsNum": 20, "Words": [{"Word": "So", "OffsetStartMs": 190, "OffsetEndMs": 555}, {"Word": "in", "OffsetStartMs": 555, "OffsetEndMs": 810}, {"Word": "addition", "OffsetStartMs": 810, "OffsetEndMs": 1035}, {"Word": "to", "OffsetStartMs": 1035, "OffsetEndMs": 1215}, {"Word": "adding", "OffsetStartMs": 1215, "OffsetEndMs": 1425}, {"Word": "another", "OffsetStartMs": 1425, "OffsetEndMs": 1710}, {"Word": "layer", "OffsetStartMs": 1710, "OffsetEndMs": 2030}, {"Word": "to", "OffsetStartMs": 2230, "OffsetEndMs": 2625}, {"Word": "estimate", "OffsetStartMs": 2625, "OffsetEndMs": 2910}, {"Word": "allatoric", "OffsetStartMs": 2910, "OffsetEndMs": 3645}, {"Word": "uncertainty", "OffsetStartMs": 3645, "OffsetEndMs": 3980}, {"Word": "correctly", "OffsetStartMs": 4510, "OffsetEndMs": 4910}, {"Word": "we", "OffsetStartMs": 5080, "OffsetEndMs": 5445}, {"Word": "also", "OffsetStartMs": 5445, "OffsetEndMs": 5670}, {"Word": "have", "OffsetStartMs": 5670, "OffsetEndMs": 5820}, {"Word": "to", "OffsetStartMs": 5820, "OffsetEndMs": 5985}, {"Word": "change", "OffsetStartMs": 5985, "OffsetEndMs": 6210}, {"Word": "our", "OffsetStartMs": 6210, "OffsetEndMs": 6450}, {"Word": "loss", "OffsetStartMs": 6450, "OffsetEndMs": 6675}, {"Word": "function", "OffsetStartMs": 6675, "OffsetEndMs": 7010}], "SpeechSpeed": 15.3}, {"FinalSentence": "So the mean squared error actually learns A A multivariate gaussian with a mean Y I and constant variance, and we want to generalize this loss function to when we don't have constant variance.", "SliceSentence": "So the mean squared error actually learns A A multivariate gaussian with a mean Y I and constant variance and we want to generalize this loss function to when we don 't have constant variance", "StartMs": 2124240, "EndMs": 2138560, "WordsNum": 35, "Words": [{"Word": "So", "OffsetStartMs": 100, "OffsetEndMs": 450}, {"Word": "the", "OffsetStartMs": 450, "OffsetEndMs": 645}, {"Word": "mean", "OffsetStartMs": 645, "OffsetEndMs": 825}, {"Word": "squared", "OffsetStartMs": 825, "OffsetEndMs": 1125}, {"Word": "error", "OffsetStartMs": 1125, "OffsetEndMs": 1460}, {"Word": "actually", "OffsetStartMs": 1750, "OffsetEndMs": 2150}, {"Word": "learns", "OffsetStartMs": 3250, "OffsetEndMs": 3765}, {"Word": "A", "OffsetStartMs": 3765, "OffsetEndMs": 4035}, {"Word": "A", "OffsetStartMs": 4035, "OffsetEndMs": 4400}, {"Word": "multivariate", "OffsetStartMs": 4600, "OffsetEndMs": 5355}, {"Word": "gaussian", "OffsetStartMs": 5355, "OffsetEndMs": 5805}, {"Word": "with", "OffsetStartMs": 5805, "OffsetEndMs": 5985}, {"Word": "a", "OffsetStartMs": 5985, "OffsetEndMs": 6150}, {"Word": "mean", "OffsetStartMs": 6150, "OffsetEndMs": 6440}, {"Word": "Y", "OffsetStartMs": 6640, "OffsetEndMs": 6975}, {"Word": "I", "OffsetStartMs": 6975, "OffsetEndMs": 7310}, {"Word": "and", "OffsetStartMs": 7480, "OffsetEndMs": 7830}, {"Word": "constant", "OffsetStartMs": 7830, "OffsetEndMs": 8180}, {"Word": "variance", "OffsetStartMs": 8260, "OffsetEndMs": 8870}, {"Word": "and", "OffsetStartMs": 8980, "OffsetEndMs": 9240}, {"Word": "we", "OffsetStartMs": 9240, "OffsetEndMs": 9375}, {"Word": "want", "OffsetStartMs": 9375, "OffsetEndMs": 9540}, {"Word": "to", "OffsetStartMs": 9540, "OffsetEndMs": 9720}, {"Word": "generalize", "OffsetStartMs": 9720, "OffsetEndMs": 10215}, {"Word": "this", "OffsetStartMs": 10215, "OffsetEndMs": 10410}, {"Word": "loss", "OffsetStartMs": 10410, "OffsetEndMs": 10635}, {"Word": "function", "OffsetStartMs": 10635, "OffsetEndMs": 10970}, {"Word": "to", "OffsetStartMs": 11140, "OffsetEndMs": 11385}, {"Word": "when", "OffsetStartMs": 11385, "OffsetEndMs": 11505}, {"Word": "we", "OffsetStartMs": 11505, "OffsetEndMs": 11670}, {"Word": "don", "OffsetStartMs": 11670, "OffsetEndMs": 11835}, {"Word": "'t", "OffsetStartMs": 11835, "OffsetEndMs": 11985}, {"Word": "have", "OffsetStartMs": 11985, "OffsetEndMs": 12240}, {"Word": "constant", "OffsetStartMs": 12240, "OffsetEndMs": 12620}, {"Word": "variance", "OffsetStartMs": 12640, "OffsetEndMs": 13220}], "SpeechSpeed": 13.3}, {"FinalSentence": "And the way we do this is by changing the loss function to the negative log likelihood. We can think about this for now as a generalization of the mean squared error loss to non constant variances. So now that we have a sigma squared term in our loss function, we can determine how accurately the sigma and the y that we're predicting parameterize the distribution. That is our input.", "SliceSentence": "And the way we do this is by changing the loss function to the negative log likelihood . Wecan think about this for now as a generalization of the mean squared error loss to non constant variances . Sonow that we have a sigma squared term in our loss function we can determine how accurately the sigma and the y that we're predicting parameterize the distribution . Thatis our input", "StartMs": 2139940, "EndMs": 2161560, "WordsNum": 70, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "the", "OffsetStartMs": 360, "OffsetEndMs": 495}, {"Word": "way", "OffsetStartMs": 495, "OffsetEndMs": 615}, {"Word": "we", "OffsetStartMs": 615, "OffsetEndMs": 765}, {"Word": "do", "OffsetStartMs": 765, "OffsetEndMs": 900}, {"Word": "this", "OffsetStartMs": 900, "OffsetEndMs": 1095}, {"Word": "is", "OffsetStartMs": 1095, "OffsetEndMs": 1320}, {"Word": "by", "OffsetStartMs": 1320, "OffsetEndMs": 1515}, {"Word": "changing", "OffsetStartMs": 1515, "OffsetEndMs": 1770}, {"Word": "the", "OffsetStartMs": 1770, "OffsetEndMs": 1965}, {"Word": "loss", "OffsetStartMs": 1965, "OffsetEndMs": 2145}, {"Word": "function", "OffsetStartMs": 2145, "OffsetEndMs": 2480}, {"Word": "to", "OffsetStartMs": 2590, "OffsetEndMs": 2820}, {"Word": "the", "OffsetStartMs": 2820, "OffsetEndMs": 2925}, {"Word": "negative", "OffsetStartMs": 2925, "OffsetEndMs": 3180}, {"Word": "log", "OffsetStartMs": 3180, "OffsetEndMs": 3480}, {"Word": "likelihood", "OffsetStartMs": 3480, "OffsetEndMs": 4130}, {"Word": ".", "OffsetStartMs": 4390, "OffsetEndMs": 4650}, {"Word": "Wecan", "OffsetStartMs": 4650, "OffsetEndMs": 4770}, {"Word": "think", "OffsetStartMs": 4770, "OffsetEndMs": 4935}, {"Word": "about", "OffsetStartMs": 4935, "OffsetEndMs": 5145}, {"Word": "this", "OffsetStartMs": 5145, "OffsetEndMs": 5355}, {"Word": "for", "OffsetStartMs": 5355, "OffsetEndMs": 5520}, {"Word": "now", "OffsetStartMs": 5520, "OffsetEndMs": 5715}, {"Word": "as", "OffsetStartMs": 5715, "OffsetEndMs": 5910}, {"Word": "a", "OffsetStartMs": 5910, "OffsetEndMs": 6045}, {"Word": "generalization", "OffsetStartMs": 6045, "OffsetEndMs": 6800}, {"Word": "of", "OffsetStartMs": 6820, "OffsetEndMs": 7095}, {"Word": "the", "OffsetStartMs": 7095, "OffsetEndMs": 7245}, {"Word": "mean", "OffsetStartMs": 7245, "OffsetEndMs": 7485}, {"Word": "squared", "OffsetStartMs": 7485, "OffsetEndMs": 7785}, {"Word": "error", "OffsetStartMs": 7785, "OffsetEndMs": 8055}, {"Word": "loss", "OffsetStartMs": 8055, "OffsetEndMs": 8420}, {"Word": "to", "OffsetStartMs": 8560, "OffsetEndMs": 8850}, {"Word": "non", "OffsetStartMs": 8850, "OffsetEndMs": 9120}, {"Word": "constant", "OffsetStartMs": 9120, "OffsetEndMs": 9495}, {"Word": "variances", "OffsetStartMs": 9495, "OffsetEndMs": 10280}, {"Word": ".", "OffsetStartMs": 10780, "OffsetEndMs": 11040}, {"Word": "Sonow", "OffsetStartMs": 11040, "OffsetEndMs": 11175}, {"Word": "that", "OffsetStartMs": 11175, "OffsetEndMs": 11295}, {"Word": "we", "OffsetStartMs": 11295, "OffsetEndMs": 11400}, {"Word": "have", "OffsetStartMs": 11400, "OffsetEndMs": 11520}, {"Word": "a", "OffsetStartMs": 11520, "OffsetEndMs": 11685}, {"Word": "sigma", "OffsetStartMs": 11685, "OffsetEndMs": 12045}, {"Word": "squared", "OffsetStartMs": 12045, "OffsetEndMs": 12345}, {"Word": "term", "OffsetStartMs": 12345, "OffsetEndMs": 12675}, {"Word": "in", "OffsetStartMs": 12675, "OffsetEndMs": 12900}, {"Word": "our", "OffsetStartMs": 12900, "OffsetEndMs": 13065}, {"Word": "loss", "OffsetStartMs": 13065, "OffsetEndMs": 13290}, {"Word": "function", "OffsetStartMs": 13290, "OffsetEndMs": 13610}, {"Word": "we", "OffsetStartMs": 14050, "OffsetEndMs": 14310}, {"Word": "can", "OffsetStartMs": 14310, "OffsetEndMs": 14520}, {"Word": "determine", "OffsetStartMs": 14520, "OffsetEndMs": 14835}, {"Word": "how", "OffsetStartMs": 14835, "OffsetEndMs": 15200}, {"Word": "accurately", "OffsetStartMs": 15340, "OffsetEndMs": 15915}, {"Word": "the", "OffsetStartMs": 15915, "OffsetEndMs": 16140}, {"Word": "sigma", "OffsetStartMs": 16140, "OffsetEndMs": 16515}, {"Word": "and", "OffsetStartMs": 16515, "OffsetEndMs": 16755}, {"Word": "the", "OffsetStartMs": 16755, "OffsetEndMs": 16965}, {"Word": "y", "OffsetStartMs": 16965, "OffsetEndMs": 17145}, {"Word": "that", "OffsetStartMs": 17145, "OffsetEndMs": 17310}, {"Word": "we're", "OffsetStartMs": 17310, "OffsetEndMs": 17505}, {"Word": "predicting", "OffsetStartMs": 17505, "OffsetEndMs": 18050}, {"Word": "parameterize", "OffsetStartMs": 18280, "OffsetEndMs": 18885}, {"Word": "the", "OffsetStartMs": 18885, "OffsetEndMs": 19140}, {"Word": "distribution", "OffsetStartMs": 19140, "OffsetEndMs": 19490}, {"Word": ".", "OffsetStartMs": 19720, "OffsetEndMs": 20025}, {"Word": "Thatis", "OffsetStartMs": 20025, "OffsetEndMs": 20190}, {"Word": "our", "OffsetStartMs": 20190, "OffsetEndMs": 20430}, {"Word": "input", "OffsetStartMs": 20430, "OffsetEndMs": 20810}], "SpeechSpeed": 17.5}, {"FinalSentence": "So now that we know how to estimate alloric uncertainty, let's look at a real world example. For this task, we'll focus on semantic segmentation, which is when we label every pixel of an image with its corresponding class. We do this for scene understanding and because it is more fine grained than a typical object detection algorithm.", "SliceSentence": "So now that we know how to estimate alloric uncertainty let's look at a real world example . Forthis task we'll focus on semantic segmentation which is when we label every pixel of an image with its corresponding class . Wedo this for scene understanding and because it is more fine grained than a typical object detection algorithm", "StartMs": 2164320, "EndMs": 2185120, "WordsNum": 58, "Words": [{"Word": "So", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "now", "OffsetStartMs": 670, "OffsetEndMs": 945}, {"Word": "that", "OffsetStartMs": 945, "OffsetEndMs": 1080}, {"Word": "we", "OffsetStartMs": 1080, "OffsetEndMs": 1215}, {"Word": "know", "OffsetStartMs": 1215, "OffsetEndMs": 1395}, {"Word": "how", "OffsetStartMs": 1395, "OffsetEndMs": 1560}, {"Word": "to", "OffsetStartMs": 1560, "OffsetEndMs": 1820}, {"Word": "estimate", "OffsetStartMs": 1870, "OffsetEndMs": 2130}, {"Word": "alloric", "OffsetStartMs": 2130, "OffsetEndMs": 2730}, {"Word": "uncertainty", "OffsetStartMs": 2730, "OffsetEndMs": 3050}, {"Word": "let's", "OffsetStartMs": 3430, "OffsetEndMs": 3795}, {"Word": "look", "OffsetStartMs": 3795, "OffsetEndMs": 3900}, {"Word": "at", "OffsetStartMs": 3900, "OffsetEndMs": 4005}, {"Word": "a", "OffsetStartMs": 4005, "OffsetEndMs": 4170}, {"Word": "real", "OffsetStartMs": 4170, "OffsetEndMs": 4440}, {"Word": "world", "OffsetStartMs": 4440, "OffsetEndMs": 4770}, {"Word": "example", "OffsetStartMs": 4770, "OffsetEndMs": 5150}, {"Word": ".", "OffsetStartMs": 6100, "OffsetEndMs": 6390}, {"Word": "Forthis", "OffsetStartMs": 6390, "OffsetEndMs": 6585}, {"Word": "task", "OffsetStartMs": 6585, "OffsetEndMs": 6890}, {"Word": "we'll", "OffsetStartMs": 6910, "OffsetEndMs": 7260}, {"Word": "focus", "OffsetStartMs": 7260, "OffsetEndMs": 7500}, {"Word": "on", "OffsetStartMs": 7500, "OffsetEndMs": 7830}, {"Word": "semantic", "OffsetStartMs": 7830, "OffsetEndMs": 8460}, {"Word": "segmentation", "OffsetStartMs": 8460, "OffsetEndMs": 9110}, {"Word": "which", "OffsetStartMs": 9580, "OffsetEndMs": 9840}, {"Word": "is", "OffsetStartMs": 9840, "OffsetEndMs": 9990}, {"Word": "when", "OffsetStartMs": 9990, "OffsetEndMs": 10170}, {"Word": "we", "OffsetStartMs": 10170, "OffsetEndMs": 10350}, {"Word": "label", "OffsetStartMs": 10350, "OffsetEndMs": 10640}, {"Word": "every", "OffsetStartMs": 10660, "OffsetEndMs": 11060}, {"Word": "pixel", "OffsetStartMs": 11080, "OffsetEndMs": 11580}, {"Word": "of", "OffsetStartMs": 11580, "OffsetEndMs": 11730}, {"Word": "an", "OffsetStartMs": 11730, "OffsetEndMs": 11850}, {"Word": "image", "OffsetStartMs": 11850, "OffsetEndMs": 12110}, {"Word": "with", "OffsetStartMs": 12190, "OffsetEndMs": 12450}, {"Word": "its", "OffsetStartMs": 12450, "OffsetEndMs": 12630}, {"Word": "corresponding", "OffsetStartMs": 12630, "OffsetEndMs": 13335}, {"Word": "class", "OffsetStartMs": 13335, "OffsetEndMs": 13640}, {"Word": ".", "OffsetStartMs": 14500, "OffsetEndMs": 14775}, {"Word": "Wedo", "OffsetStartMs": 14775, "OffsetEndMs": 14910}, {"Word": "this", "OffsetStartMs": 14910, "OffsetEndMs": 15150}, {"Word": "for", "OffsetStartMs": 15150, "OffsetEndMs": 15450}, {"Word": "scene", "OffsetStartMs": 15450, "OffsetEndMs": 15770}, {"Word": "understanding", "OffsetStartMs": 15850, "OffsetEndMs": 16245}, {"Word": "and", "OffsetStartMs": 16245, "OffsetEndMs": 16575}, {"Word": "because", "OffsetStartMs": 16575, "OffsetEndMs": 16785}, {"Word": "it", "OffsetStartMs": 16785, "OffsetEndMs": 16905}, {"Word": "is", "OffsetStartMs": 16905, "OffsetEndMs": 17055}, {"Word": "more", "OffsetStartMs": 17055, "OffsetEndMs": 17295}, {"Word": "fine", "OffsetStartMs": 17295, "OffsetEndMs": 17595}, {"Word": "grained", "OffsetStartMs": 17595, "OffsetEndMs": 17910}, {"Word": "than", "OffsetStartMs": 17910, "OffsetEndMs": 18075}, {"Word": "a", "OffsetStartMs": 18075, "OffsetEndMs": 18195}, {"Word": "typical", "OffsetStartMs": 18195, "OffsetEndMs": 18470}, {"Word": "object", "OffsetStartMs": 18640, "OffsetEndMs": 18960}, {"Word": "detection", "OffsetStartMs": 18960, "OffsetEndMs": 19395}, {"Word": "algorithm", "OffsetStartMs": 19395, "OffsetEndMs": 19940}], "SpeechSpeed": 15.9}, {"FinalSentence": "So the inputs of this to this data set are known as it's from a data set called citycapes, and the inputs are rgb images of scenes. The labels are pixel wise annotations of this entire image, of which label every pixel belongs to, and the outputs try to mimic the labels. There are also predicted pixel wise masks, so why would we expect that this data set has high natural allatoric uncertainty? And which parts of this data set do you think would have allatoric uncertainty?", "SliceSentence": "So the inputs of this to this data set are known as it's from a data set called citycapes and the inputs are rgb images of scenes . Thelabels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels . Thereare also predicted pixel wise masks so why would we expect that this data set has high natural allatoric uncertainty And which parts of this data set do you think would have allatoric uncertainty", "StartMs": 2185140, "EndMs": 2215700, "WordsNum": 87, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 480}, {"Word": "the", "OffsetStartMs": 480, "OffsetEndMs": 765}, {"Word": "inputs", "OffsetStartMs": 765, "OffsetEndMs": 1110}, {"Word": "of", "OffsetStartMs": 1110, "OffsetEndMs": 1260}, {"Word": "this", "OffsetStartMs": 1260, "OffsetEndMs": 1520}, {"Word": "to", "OffsetStartMs": 1570, "OffsetEndMs": 1815}, {"Word": "this", "OffsetStartMs": 1815, "OffsetEndMs": 1935}, {"Word": "data", "OffsetStartMs": 1935, "OffsetEndMs": 2145}, {"Word": "set", "OffsetStartMs": 2145, "OffsetEndMs": 2475}, {"Word": "are", "OffsetStartMs": 2475, "OffsetEndMs": 2775}, {"Word": "known", "OffsetStartMs": 2775, "OffsetEndMs": 2985}, {"Word": "as", "OffsetStartMs": 2985, "OffsetEndMs": 3225}, {"Word": "it's", "OffsetStartMs": 3225, "OffsetEndMs": 3510}, {"Word": "from", "OffsetStartMs": 3510, "OffsetEndMs": 3630}, {"Word": "a", "OffsetStartMs": 3630, "OffsetEndMs": 3750}, {"Word": "data", "OffsetStartMs": 3750, "OffsetEndMs": 3930}, {"Word": "set", "OffsetStartMs": 3930, "OffsetEndMs": 4140}, {"Word": "called", "OffsetStartMs": 4140, "OffsetEndMs": 4365}, {"Word": "citycapes", "OffsetStartMs": 4365, "OffsetEndMs": 5030}, {"Word": "and", "OffsetStartMs": 5260, "OffsetEndMs": 5610}, {"Word": "the", "OffsetStartMs": 5610, "OffsetEndMs": 5925}, {"Word": "inputs", "OffsetStartMs": 5925, "OffsetEndMs": 6240}, {"Word": "are", "OffsetStartMs": 6240, "OffsetEndMs": 6480}, {"Word": "rgb", "OffsetStartMs": 6480, "OffsetEndMs": 7095}, {"Word": "images", "OffsetStartMs": 7095, "OffsetEndMs": 7370}, {"Word": "of", "OffsetStartMs": 7390, "OffsetEndMs": 7725}, {"Word": "scenes", "OffsetStartMs": 7725, "OffsetEndMs": 8060}, {"Word": ".", "OffsetStartMs": 8890, "OffsetEndMs": 9150}, {"Word": "Thelabels", "OffsetStartMs": 9150, "OffsetEndMs": 9615}, {"Word": "are", "OffsetStartMs": 9615, "OffsetEndMs": 9945}, {"Word": "pixel", "OffsetStartMs": 9945, "OffsetEndMs": 10395}, {"Word": "wise", "OffsetStartMs": 10395, "OffsetEndMs": 10650}, {"Word": "annotations", "OffsetStartMs": 10650, "OffsetEndMs": 11240}, {"Word": "of", "OffsetStartMs": 11380, "OffsetEndMs": 11685}, {"Word": "this", "OffsetStartMs": 11685, "OffsetEndMs": 11925}, {"Word": "entire", "OffsetStartMs": 11925, "OffsetEndMs": 12210}, {"Word": "image", "OffsetStartMs": 12210, "OffsetEndMs": 12560}, {"Word": "of", "OffsetStartMs": 12640, "OffsetEndMs": 12945}, {"Word": "which", "OffsetStartMs": 12945, "OffsetEndMs": 13200}, {"Word": "label", "OffsetStartMs": 13200, "OffsetEndMs": 13550}, {"Word": "every", "OffsetStartMs": 13570, "OffsetEndMs": 13950}, {"Word": "pixel", "OffsetStartMs": 13950, "OffsetEndMs": 14430}, {"Word": "belongs", "OffsetStartMs": 14430, "OffsetEndMs": 14805}, {"Word": "to", "OffsetStartMs": 14805, "OffsetEndMs": 15110}, {"Word": "and", "OffsetStartMs": 15790, "OffsetEndMs": 16065}, {"Word": "the", "OffsetStartMs": 16065, "OffsetEndMs": 16320}, {"Word": "outputs", "OffsetStartMs": 16320, "OffsetEndMs": 16665}, {"Word": "try", "OffsetStartMs": 16665, "OffsetEndMs": 16845}, {"Word": "to", "OffsetStartMs": 16845, "OffsetEndMs": 16995}, {"Word": "mimic", "OffsetStartMs": 16995, "OffsetEndMs": 17205}, {"Word": "the", "OffsetStartMs": 17205, "OffsetEndMs": 17325}, {"Word": "labels", "OffsetStartMs": 17325, "OffsetEndMs": 17715}, {"Word": ".", "OffsetStartMs": 17715, "OffsetEndMs": 17940}, {"Word": "Thereare", "OffsetStartMs": 17940, "OffsetEndMs": 18120}, {"Word": "also", "OffsetStartMs": 18120, "OffsetEndMs": 18360}, {"Word": "predicted", "OffsetStartMs": 18360, "OffsetEndMs": 18750}, {"Word": "pixel", "OffsetStartMs": 18750, "OffsetEndMs": 19110}, {"Word": "wise", "OffsetStartMs": 19110, "OffsetEndMs": 19320}, {"Word": "masks", "OffsetStartMs": 19320, "OffsetEndMs": 19970}, {"Word": "so", "OffsetStartMs": 20710, "OffsetEndMs": 21110}, {"Word": "why", "OffsetStartMs": 21730, "OffsetEndMs": 22065}, {"Word": "would", "OffsetStartMs": 22065, "OffsetEndMs": 22275}, {"Word": "we", "OffsetStartMs": 22275, "OffsetEndMs": 22485}, {"Word": "expect", "OffsetStartMs": 22485, "OffsetEndMs": 22740}, {"Word": "that", "OffsetStartMs": 22740, "OffsetEndMs": 22965}, {"Word": "this", "OffsetStartMs": 22965, "OffsetEndMs": 23190}, {"Word": "data", "OffsetStartMs": 23190, "OffsetEndMs": 23460}, {"Word": "set", "OffsetStartMs": 23460, "OffsetEndMs": 23745}, {"Word": "has", "OffsetStartMs": 23745, "OffsetEndMs": 24080}, {"Word": "high", "OffsetStartMs": 24100, "OffsetEndMs": 24500}, {"Word": "natural", "OffsetStartMs": 24670, "OffsetEndMs": 24960}, {"Word": "allatoric", "OffsetStartMs": 24960, "OffsetEndMs": 25620}, {"Word": "uncertainty", "OffsetStartMs": 25620, "OffsetEndMs": 25940}, {"Word": "And", "OffsetStartMs": 26320, "OffsetEndMs": 26625}, {"Word": "which", "OffsetStartMs": 26625, "OffsetEndMs": 26865}, {"Word": "parts", "OffsetStartMs": 26865, "OffsetEndMs": 27120}, {"Word": "of", "OffsetStartMs": 27120, "OffsetEndMs": 27285}, {"Word": "this", "OffsetStartMs": 27285, "OffsetEndMs": 27405}, {"Word": "data", "OffsetStartMs": 27405, "OffsetEndMs": 27600}, {"Word": "set", "OffsetStartMs": 27600, "OffsetEndMs": 27920}, {"Word": "do", "OffsetStartMs": 28000, "OffsetEndMs": 28230}, {"Word": "you", "OffsetStartMs": 28230, "OffsetEndMs": 28335}, {"Word": "think", "OffsetStartMs": 28335, "OffsetEndMs": 28500}, {"Word": "would", "OffsetStartMs": 28500, "OffsetEndMs": 28680}, {"Word": "have", "OffsetStartMs": 28680, "OffsetEndMs": 28860}, {"Word": "allatoric", "OffsetStartMs": 28860, "OffsetEndMs": 29445}, {"Word": "uncertainty", "OffsetStartMs": 29445, "OffsetEndMs": 29750}], "SpeechSpeed": 15.3}, {"FinalSentence": "Because labeling every single pixel of an image is such a labor intensive task, and it's also very hard to do accurately, we would expect that the boundaries between EM between objects in this image have high allatoric uncertainty, and that's exactly what we see. If you train a model to predict allatoric uncertainty on this data set, corners and boundaries have the highest allator uncertainty, because even if your pixels are like one row off or one column off, that introduces noise into the model.", "SliceSentence": "Because labeling every single pixel of an image is such a labor intensive task and it's also very hard to do accurately we would expect that the boundaries between EM between objects in this image have high allatoric uncertainty and that's exactly what we see . Ifyou train a model to predict allatoric uncertainty on this data set corners and boundaries have the highest allator uncertainty because even if your pixels are like one row off or one column off that introduces noise into the model", "StartMs": 2218660, "EndMs": 2247620, "WordsNum": 86, "Words": [{"Word": "Because", "OffsetStartMs": 220, "OffsetEndMs": 585}, {"Word": "labeling", "OffsetStartMs": 585, "OffsetEndMs": 1190}, {"Word": "every", "OffsetStartMs": 1240, "OffsetEndMs": 1635}, {"Word": "single", "OffsetStartMs": 1635, "OffsetEndMs": 1980}, {"Word": "pixel", "OffsetStartMs": 1980, "OffsetEndMs": 2370}, {"Word": "of", "OffsetStartMs": 2370, "OffsetEndMs": 2475}, {"Word": "an", "OffsetStartMs": 2475, "OffsetEndMs": 2580}, {"Word": "image", "OffsetStartMs": 2580, "OffsetEndMs": 2805}, {"Word": "is", "OffsetStartMs": 2805, "OffsetEndMs": 3120}, {"Word": "such", "OffsetStartMs": 3120, "OffsetEndMs": 3420}, {"Word": "a", "OffsetStartMs": 3420, "OffsetEndMs": 3630}, {"Word": "labor", "OffsetStartMs": 3630, "OffsetEndMs": 3810}, {"Word": "intensive", "OffsetStartMs": 3810, "OffsetEndMs": 4245}, {"Word": "task", "OffsetStartMs": 4245, "OffsetEndMs": 4610}, {"Word": "and", "OffsetStartMs": 4840, "OffsetEndMs": 5100}, {"Word": "it's", "OffsetStartMs": 5100, "OffsetEndMs": 5340}, {"Word": "also", "OffsetStartMs": 5340, "OffsetEndMs": 5565}, {"Word": "very", "OffsetStartMs": 5565, "OffsetEndMs": 5865}, {"Word": "hard", "OffsetStartMs": 5865, "OffsetEndMs": 6180}, {"Word": "to", "OffsetStartMs": 6180, "OffsetEndMs": 6360}, {"Word": "do", "OffsetStartMs": 6360, "OffsetEndMs": 6620}, {"Word": "accurately", "OffsetStartMs": 6640, "OffsetEndMs": 7220}, {"Word": "we", "OffsetStartMs": 7420, "OffsetEndMs": 7680}, {"Word": "would", "OffsetStartMs": 7680, "OffsetEndMs": 7875}, {"Word": "expect", "OffsetStartMs": 7875, "OffsetEndMs": 8115}, {"Word": "that", "OffsetStartMs": 8115, "OffsetEndMs": 8295}, {"Word": "the", "OffsetStartMs": 8295, "OffsetEndMs": 8445}, {"Word": "boundaries", "OffsetStartMs": 8445, "OffsetEndMs": 9020}, {"Word": "between", "OffsetStartMs": 9100, "OffsetEndMs": 9500}, {"Word": "EM", "OffsetStartMs": 9520, "OffsetEndMs": 9920}, {"Word": "between", "OffsetStartMs": 10480, "OffsetEndMs": 10880}, {"Word": "objects", "OffsetStartMs": 11410, "OffsetEndMs": 11685}, {"Word": "in", "OffsetStartMs": 11685, "OffsetEndMs": 11820}, {"Word": "this", "OffsetStartMs": 11820, "OffsetEndMs": 11955}, {"Word": "image", "OffsetStartMs": 11955, "OffsetEndMs": 12225}, {"Word": "have", "OffsetStartMs": 12225, "OffsetEndMs": 12540}, {"Word": "high", "OffsetStartMs": 12540, "OffsetEndMs": 12795}, {"Word": "allatoric", "OffsetStartMs": 12795, "OffsetEndMs": 13485}, {"Word": "uncertainty", "OffsetStartMs": 13485, "OffsetEndMs": 13820}, {"Word": "and", "OffsetStartMs": 14380, "OffsetEndMs": 14640}, {"Word": "that's", "OffsetStartMs": 14640, "OffsetEndMs": 14895}, {"Word": "exactly", "OffsetStartMs": 14895, "OffsetEndMs": 15135}, {"Word": "what", "OffsetStartMs": 15135, "OffsetEndMs": 15360}, {"Word": "we", "OffsetStartMs": 15360, "OffsetEndMs": 15525}, {"Word": "see", "OffsetStartMs": 15525, "OffsetEndMs": 15830}, {"Word": ".", "OffsetStartMs": 15910, "OffsetEndMs": 16170}, {"Word": "Ifyou", "OffsetStartMs": 16170, "OffsetEndMs": 16320}, {"Word": "train", "OffsetStartMs": 16320, "OffsetEndMs": 16485}, {"Word": "a", "OffsetStartMs": 16485, "OffsetEndMs": 16620}, {"Word": "model", "OffsetStartMs": 16620, "OffsetEndMs": 16875}, {"Word": "to", "OffsetStartMs": 16875, "OffsetEndMs": 17115}, {"Word": "predict", "OffsetStartMs": 17115, "OffsetEndMs": 17295}, {"Word": "allatoric", "OffsetStartMs": 17295, "OffsetEndMs": 17940}, {"Word": "uncertainty", "OffsetStartMs": 17940, "OffsetEndMs": 18260}, {"Word": "on", "OffsetStartMs": 18400, "OffsetEndMs": 18705}, {"Word": "this", "OffsetStartMs": 18705, "OffsetEndMs": 18900}, {"Word": "data", "OffsetStartMs": 18900, "OffsetEndMs": 19110}, {"Word": "set", "OffsetStartMs": 19110, "OffsetEndMs": 19430}, {"Word": "corners", "OffsetStartMs": 19720, "OffsetEndMs": 20220}, {"Word": "and", "OffsetStartMs": 20220, "OffsetEndMs": 20430}, {"Word": "boundaries", "OffsetStartMs": 20430, "OffsetEndMs": 20940}, {"Word": "have", "OffsetStartMs": 20940, "OffsetEndMs": 21165}, {"Word": "the", "OffsetStartMs": 21165, "OffsetEndMs": 21330}, {"Word": "highest", "OffsetStartMs": 21330, "OffsetEndMs": 21600}, {"Word": "allator", "OffsetStartMs": 21600, "OffsetEndMs": 22245}, {"Word": "uncertainty", "OffsetStartMs": 22245, "OffsetEndMs": 22610}, {"Word": "because", "OffsetStartMs": 23050, "OffsetEndMs": 23325}, {"Word": "even", "OffsetStartMs": 23325, "OffsetEndMs": 23520}, {"Word": "if", "OffsetStartMs": 23520, "OffsetEndMs": 23715}, {"Word": "your", "OffsetStartMs": 23715, "OffsetEndMs": 23880}, {"Word": "pixels", "OffsetStartMs": 23880, "OffsetEndMs": 24240}, {"Word": "are", "OffsetStartMs": 24240, "OffsetEndMs": 24450}, {"Word": "like", "OffsetStartMs": 24450, "OffsetEndMs": 24675}, {"Word": "one", "OffsetStartMs": 24675, "OffsetEndMs": 24915}, {"Word": "row", "OffsetStartMs": 24915, "OffsetEndMs": 25155}, {"Word": "off", "OffsetStartMs": 25155, "OffsetEndMs": 25410}, {"Word": "or", "OffsetStartMs": 25410, "OffsetEndMs": 25650}, {"Word": "one", "OffsetStartMs": 25650, "OffsetEndMs": 25875}, {"Word": "column", "OffsetStartMs": 25875, "OffsetEndMs": 26190}, {"Word": "off", "OffsetStartMs": 26190, "OffsetEndMs": 26570}, {"Word": "that", "OffsetStartMs": 26770, "OffsetEndMs": 27060}, {"Word": "introduces", "OffsetStartMs": 27060, "OffsetEndMs": 27525}, {"Word": "noise", "OffsetStartMs": 27525, "OffsetEndMs": 27840}, {"Word": "into", "OffsetStartMs": 27840, "OffsetEndMs": 28080}, {"Word": "the", "OffsetStartMs": 28080, "OffsetEndMs": 28215}, {"Word": "model", "OffsetStartMs": 28215, "OffsetEndMs": 28460}], "SpeechSpeed": 17.1}, {"FinalSentence": "The model can still learn in the face of this noise, but it does exist and it can't be reduced.", "SliceSentence": "The model can still learn in the face of this noise but it does exist and it can't be reduced", "StartMs": 2247620, "EndMs": 2253500, "WordsNum": 20, "Words": [{"Word": "The", "OffsetStartMs": 0, "OffsetEndMs": 210}, {"Word": "model", "OffsetStartMs": 210, "OffsetEndMs": 420}, {"Word": "can", "OffsetStartMs": 420, "OffsetEndMs": 675}, {"Word": "still", "OffsetStartMs": 675, "OffsetEndMs": 915}, {"Word": "learn", "OffsetStartMs": 915, "OffsetEndMs": 1230}, {"Word": "in", "OffsetStartMs": 1230, "OffsetEndMs": 1470}, {"Word": "the", "OffsetStartMs": 1470, "OffsetEndMs": 1590}, {"Word": "face", "OffsetStartMs": 1590, "OffsetEndMs": 1740}, {"Word": "of", "OffsetStartMs": 1740, "OffsetEndMs": 1890}, {"Word": "this", "OffsetStartMs": 1890, "OffsetEndMs": 2055}, {"Word": "noise", "OffsetStartMs": 2055, "OffsetEndMs": 2360}, {"Word": "but", "OffsetStartMs": 2470, "OffsetEndMs": 2715}, {"Word": "it", "OffsetStartMs": 2715, "OffsetEndMs": 2835}, {"Word": "does", "OffsetStartMs": 2835, "OffsetEndMs": 3110}, {"Word": "exist", "OffsetStartMs": 3130, "OffsetEndMs": 3510}, {"Word": "and", "OffsetStartMs": 3510, "OffsetEndMs": 3735}, {"Word": "it", "OffsetStartMs": 3735, "OffsetEndMs": 3885}, {"Word": "can't", "OffsetStartMs": 3885, "OffsetEndMs": 4170}, {"Word": "be", "OffsetStartMs": 4170, "OffsetEndMs": 4365}, {"Word": "reduced", "OffsetStartMs": 4365, "OffsetEndMs": 4700}], "SpeechSpeed": 15.8}, {"FinalSentence": "So now that we know about data uncertainty or allatoric uncertainty, let's move on to learning about epistemic uncertainty. As a recap, epistemic uncertainty can best be described as uncertainty in the model itself, and it is reducible by adding data to the model.", "SliceSentence": "So now that we know about data uncertainty or allatoric uncertainty let's move on to learning about epistemic uncertainty . Asa recap epistemic uncertainty can best be described as uncertainty in the model itself and it is reducible by adding data to the model", "StartMs": 2256080, "EndMs": 2273720, "WordsNum": 44, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "now", "OffsetStartMs": 760, "OffsetEndMs": 1035}, {"Word": "that", "OffsetStartMs": 1035, "OffsetEndMs": 1170}, {"Word": "we", "OffsetStartMs": 1170, "OffsetEndMs": 1305}, {"Word": "know", "OffsetStartMs": 1305, "OffsetEndMs": 1560}, {"Word": "about", "OffsetStartMs": 1560, "OffsetEndMs": 1875}, {"Word": "data", "OffsetStartMs": 1875, "OffsetEndMs": 2210}, {"Word": "uncertainty", "OffsetStartMs": 2260, "OffsetEndMs": 2660}, {"Word": "or", "OffsetStartMs": 2890, "OffsetEndMs": 3150}, {"Word": "allatoric", "OffsetStartMs": 3150, "OffsetEndMs": 3855}, {"Word": "uncertainty", "OffsetStartMs": 3855, "OffsetEndMs": 4190}, {"Word": "let's", "OffsetStartMs": 4870, "OffsetEndMs": 5250}, {"Word": "move", "OffsetStartMs": 5250, "OffsetEndMs": 5400}, {"Word": "on", "OffsetStartMs": 5400, "OffsetEndMs": 5640}, {"Word": "to", "OffsetStartMs": 5640, "OffsetEndMs": 5835}, {"Word": "learning", "OffsetStartMs": 5835, "OffsetEndMs": 6080}, {"Word": "about", "OffsetStartMs": 6100, "OffsetEndMs": 6450}, {"Word": "epistemic", "OffsetStartMs": 6450, "OffsetEndMs": 7200}, {"Word": "uncertainty", "OffsetStartMs": 7200, "OffsetEndMs": 7550}, {"Word": ".", "OffsetStartMs": 8200, "OffsetEndMs": 8475}, {"Word": "Asa", "OffsetStartMs": 8475, "OffsetEndMs": 8625}, {"Word": "recap", "OffsetStartMs": 8625, "OffsetEndMs": 9050}, {"Word": "epistemic", "OffsetStartMs": 9190, "OffsetEndMs": 9960}, {"Word": "uncertainty", "OffsetStartMs": 9960, "OffsetEndMs": 10310}, {"Word": "can", "OffsetStartMs": 10420, "OffsetEndMs": 10680}, {"Word": "best", "OffsetStartMs": 10680, "OffsetEndMs": 10890}, {"Word": "be", "OffsetStartMs": 10890, "OffsetEndMs": 11190}, {"Word": "described", "OffsetStartMs": 11190, "OffsetEndMs": 11520}, {"Word": "as", "OffsetStartMs": 11520, "OffsetEndMs": 11880}, {"Word": "uncertainty", "OffsetStartMs": 11880, "OffsetEndMs": 12260}, {"Word": "in", "OffsetStartMs": 12340, "OffsetEndMs": 12645}, {"Word": "the", "OffsetStartMs": 12645, "OffsetEndMs": 12825}, {"Word": "model", "OffsetStartMs": 12825, "OffsetEndMs": 13100}, {"Word": "itself", "OffsetStartMs": 13240, "OffsetEndMs": 13640}, {"Word": "and", "OffsetStartMs": 14050, "OffsetEndMs": 14295}, {"Word": "it", "OffsetStartMs": 14295, "OffsetEndMs": 14415}, {"Word": "is", "OffsetStartMs": 14415, "OffsetEndMs": 14610}, {"Word": "reducible", "OffsetStartMs": 14610, "OffsetEndMs": 15255}, {"Word": "by", "OffsetStartMs": 15255, "OffsetEndMs": 15435}, {"Word": "adding", "OffsetStartMs": 15435, "OffsetEndMs": 15675}, {"Word": "data", "OffsetStartMs": 15675, "OffsetEndMs": 16020}, {"Word": "to", "OffsetStartMs": 16020, "OffsetEndMs": 16245}, {"Word": "the", "OffsetStartMs": 16245, "OffsetEndMs": 16350}, {"Word": "model", "OffsetStartMs": 16350, "OffsetEndMs": 16610}], "SpeechSpeed": 14.7}, {"FinalSentence": "So with epistemic uncertainty, essentially what we're trying to ask is, is the model uncon confidentf about a prediction? So a really simple and very smart way to do this is let's say I train the same network multiple times with random initializations and I ask it to predict the exact I call it on the same input. So let's say I give model one the exact same input, and the blue X is the output of this model.", "SliceSentence": "So with epistemic uncertainty essentially what we're trying to ask is is the model uncon confidentf about a prediction So a really simple and very smart way to do this is let's say I train the same network multiple times with random initializations and I ask it to predict the exact I call it on the same input . Solet's say I give model one the exact same input and the blue X is the output of this model", "StartMs": 2276200, "EndMs": 2302800, "WordsNum": 79, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 525}, {"Word": "with", "OffsetStartMs": 525, "OffsetEndMs": 780}, {"Word": "epistemic", "OffsetStartMs": 780, "OffsetEndMs": 1380}, {"Word": "uncertainty", "OffsetStartMs": 1380, "OffsetEndMs": 1730}, {"Word": "essentially", "OffsetStartMs": 1840, "OffsetEndMs": 2205}, {"Word": "what", "OffsetStartMs": 2205, "OffsetEndMs": 2430}, {"Word": "we're", "OffsetStartMs": 2430, "OffsetEndMs": 2625}, {"Word": "trying", "OffsetStartMs": 2625, "OffsetEndMs": 2805}, {"Word": "to", "OffsetStartMs": 2805, "OffsetEndMs": 2985}, {"Word": "ask", "OffsetStartMs": 2985, "OffsetEndMs": 3225}, {"Word": "is", "OffsetStartMs": 3225, "OffsetEndMs": 3620}, {"Word": "is", "OffsetStartMs": 3790, "OffsetEndMs": 4110}, {"Word": "the", "OffsetStartMs": 4110, "OffsetEndMs": 4305}, {"Word": "model", "OffsetStartMs": 4305, "OffsetEndMs": 4575}, {"Word": "uncon", "OffsetStartMs": 4575, "OffsetEndMs": 4920}, {"Word": "confidentf", "OffsetStartMs": 4920, "OffsetEndMs": 5480}, {"Word": "about", "OffsetStartMs": 5560, "OffsetEndMs": 5865}, {"Word": "a", "OffsetStartMs": 5865, "OffsetEndMs": 6045}, {"Word": "prediction", "OffsetStartMs": 6045, "OffsetEndMs": 6410}, {"Word": "So", "OffsetStartMs": 7000, "OffsetEndMs": 7400}, {"Word": "a", "OffsetStartMs": 7450, "OffsetEndMs": 7725}, {"Word": "really", "OffsetStartMs": 7725, "OffsetEndMs": 7980}, {"Word": "simple", "OffsetStartMs": 7980, "OffsetEndMs": 8360}, {"Word": "and", "OffsetStartMs": 8380, "OffsetEndMs": 8775}, {"Word": "very", "OffsetStartMs": 8775, "OffsetEndMs": 9170}, {"Word": "smart", "OffsetStartMs": 9310, "OffsetEndMs": 9630}, {"Word": "way", "OffsetStartMs": 9630, "OffsetEndMs": 9810}, {"Word": "to", "OffsetStartMs": 9810, "OffsetEndMs": 9930}, {"Word": "do", "OffsetStartMs": 9930, "OffsetEndMs": 10065}, {"Word": "this", "OffsetStartMs": 10065, "OffsetEndMs": 10340}, {"Word": "is", "OffsetStartMs": 10420, "OffsetEndMs": 10820}, {"Word": "let's", "OffsetStartMs": 11140, "OffsetEndMs": 11520}, {"Word": "say", "OffsetStartMs": 11520, "OffsetEndMs": 11670}, {"Word": "I", "OffsetStartMs": 11670, "OffsetEndMs": 11880}, {"Word": "train", "OffsetStartMs": 11880, "OffsetEndMs": 12120}, {"Word": "the", "OffsetStartMs": 12120, "OffsetEndMs": 12345}, {"Word": "same", "OffsetStartMs": 12345, "OffsetEndMs": 12585}, {"Word": "network", "OffsetStartMs": 12585, "OffsetEndMs": 12920}, {"Word": "multiple", "OffsetStartMs": 13120, "OffsetEndMs": 13520}, {"Word": "times", "OffsetStartMs": 13540, "OffsetEndMs": 13940}, {"Word": "with", "OffsetStartMs": 13960, "OffsetEndMs": 14265}, {"Word": "random", "OffsetStartMs": 14265, "OffsetEndMs": 14535}, {"Word": "initializations", "OffsetStartMs": 14535, "OffsetEndMs": 15260}, {"Word": "and", "OffsetStartMs": 15730, "OffsetEndMs": 16080}, {"Word": "I", "OffsetStartMs": 16080, "OffsetEndMs": 16320}, {"Word": "ask", "OffsetStartMs": 16320, "OffsetEndMs": 16545}, {"Word": "it", "OffsetStartMs": 16545, "OffsetEndMs": 16785}, {"Word": "to", "OffsetStartMs": 16785, "OffsetEndMs": 16950}, {"Word": "predict", "OffsetStartMs": 16950, "OffsetEndMs": 17190}, {"Word": "the", "OffsetStartMs": 17190, "OffsetEndMs": 17490}, {"Word": "exact", "OffsetStartMs": 17490, "OffsetEndMs": 17775}, {"Word": "I", "OffsetStartMs": 17775, "OffsetEndMs": 18140}, {"Word": "call", "OffsetStartMs": 18250, "OffsetEndMs": 18540}, {"Word": "it", "OffsetStartMs": 18540, "OffsetEndMs": 18705}, {"Word": "on", "OffsetStartMs": 18705, "OffsetEndMs": 18855}, {"Word": "the", "OffsetStartMs": 18855, "OffsetEndMs": 19005}, {"Word": "same", "OffsetStartMs": 19005, "OffsetEndMs": 19280}, {"Word": "input", "OffsetStartMs": 19300, "OffsetEndMs": 19700}, {"Word": ".", "OffsetStartMs": 20140, "OffsetEndMs": 20540}, {"Word": "Solet's", "OffsetStartMs": 20770, "OffsetEndMs": 21135}, {"Word": "say", "OffsetStartMs": 21135, "OffsetEndMs": 21225}, {"Word": "I", "OffsetStartMs": 21225, "OffsetEndMs": 21360}, {"Word": "give", "OffsetStartMs": 21360, "OffsetEndMs": 21555}, {"Word": "model", "OffsetStartMs": 21555, "OffsetEndMs": 21825}, {"Word": "one", "OffsetStartMs": 21825, "OffsetEndMs": 22155}, {"Word": "the", "OffsetStartMs": 22155, "OffsetEndMs": 22410}, {"Word": "exact", "OffsetStartMs": 22410, "OffsetEndMs": 22620}, {"Word": "same", "OffsetStartMs": 22620, "OffsetEndMs": 22905}, {"Word": "input", "OffsetStartMs": 22905, "OffsetEndMs": 23205}, {"Word": "and", "OffsetStartMs": 23205, "OffsetEndMs": 23490}, {"Word": "the", "OffsetStartMs": 23490, "OffsetEndMs": 23790}, {"Word": "blue", "OffsetStartMs": 23790, "OffsetEndMs": 24045}, {"Word": "X", "OffsetStartMs": 24045, "OffsetEndMs": 24345}, {"Word": "is", "OffsetStartMs": 24345, "OffsetEndMs": 24615}, {"Word": "the", "OffsetStartMs": 24615, "OffsetEndMs": 24840}, {"Word": "output", "OffsetStartMs": 24840, "OffsetEndMs": 25035}, {"Word": "of", "OffsetStartMs": 25035, "OffsetEndMs": 25140}, {"Word": "this", "OffsetStartMs": 25140, "OffsetEndMs": 25305}, {"Word": "model", "OffsetStartMs": 25305, "OffsetEndMs": 25610}], "SpeechSpeed": 15.2}, {"FinalSentence": "And then I do the same thing again with model two, and then again with model three.", "SliceSentence": "And then I do the same thing again with model two and then again with model three", "StartMs": 2303200, "EndMs": 2309560, "WordsNum": 17, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 345}, {"Word": "then", "OffsetStartMs": 345, "OffsetEndMs": 615}, {"Word": "I", "OffsetStartMs": 615, "OffsetEndMs": 885}, {"Word": "do", "OffsetStartMs": 885, "OffsetEndMs": 1005}, {"Word": "the", "OffsetStartMs": 1005, "OffsetEndMs": 1110}, {"Word": "same", "OffsetStartMs": 1110, "OffsetEndMs": 1245}, {"Word": "thing", "OffsetStartMs": 1245, "OffsetEndMs": 1425}, {"Word": "again", "OffsetStartMs": 1425, "OffsetEndMs": 1725}, {"Word": "with", "OffsetStartMs": 1725, "OffsetEndMs": 2010}, {"Word": "model", "OffsetStartMs": 2010, "OffsetEndMs": 2265}, {"Word": "two", "OffsetStartMs": 2265, "OffsetEndMs": 2630}, {"Word": "and", "OffsetStartMs": 3790, "OffsetEndMs": 4050}, {"Word": "then", "OffsetStartMs": 4050, "OffsetEndMs": 4230}, {"Word": "again", "OffsetStartMs": 4230, "OffsetEndMs": 4470}, {"Word": "with", "OffsetStartMs": 4470, "OffsetEndMs": 4695}, {"Word": "model", "OffsetStartMs": 4695, "OffsetEndMs": 4965}, {"Word": "three", "OffsetStartMs": 4965, "OffsetEndMs": 5330}], "SpeechSpeed": 12.7}, {"FinalSentence": "And again, with model four, these models all have the exact same hyper parameters, the exact same architecture, and their train in the same way. The only difference between them is that their weights are all randomly initialized, so where they start from is different.", "SliceSentence": "And again with model four these models all have the exact same hyper parameters the exact same architecture and their train in the same way . Theonly difference between them is that their weights are all randomly initialized so where they start from is different", "StartMs": 2309560, "EndMs": 2324600, "WordsNum": 45, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 375}, {"Word": "again", "OffsetStartMs": 375, "OffsetEndMs": 570}, {"Word": "with", "OffsetStartMs": 570, "OffsetEndMs": 735}, {"Word": "model", "OffsetStartMs": 735, "OffsetEndMs": 975}, {"Word": "four", "OffsetStartMs": 975, "OffsetEndMs": 1340}, {"Word": "these", "OffsetStartMs": 1540, "OffsetEndMs": 1860}, {"Word": "models", "OffsetStartMs": 1860, "OffsetEndMs": 2180}, {"Word": "all", "OffsetStartMs": 2200, "OffsetEndMs": 2535}, {"Word": "have", "OffsetStartMs": 2535, "OffsetEndMs": 2745}, {"Word": "the", "OffsetStartMs": 2745, "OffsetEndMs": 2955}, {"Word": "exact", "OffsetStartMs": 2955, "OffsetEndMs": 3240}, {"Word": "same", "OffsetStartMs": 3240, "OffsetEndMs": 3510}, {"Word": "hyper", "OffsetStartMs": 3510, "OffsetEndMs": 3900}, {"Word": "parameters", "OffsetStartMs": 3900, "OffsetEndMs": 4400}, {"Word": "the", "OffsetStartMs": 4510, "OffsetEndMs": 4800}, {"Word": "exact", "OffsetStartMs": 4800, "OffsetEndMs": 5025}, {"Word": "same", "OffsetStartMs": 5025, "OffsetEndMs": 5250}, {"Word": "architecture", "OffsetStartMs": 5250, "OffsetEndMs": 5540}, {"Word": "and", "OffsetStartMs": 5890, "OffsetEndMs": 6150}, {"Word": "their", "OffsetStartMs": 6150, "OffsetEndMs": 6330}, {"Word": "train", "OffsetStartMs": 6330, "OffsetEndMs": 6570}, {"Word": "in", "OffsetStartMs": 6570, "OffsetEndMs": 6750}, {"Word": "the", "OffsetStartMs": 6750, "OffsetEndMs": 6915}, {"Word": "same", "OffsetStartMs": 6915, "OffsetEndMs": 7185}, {"Word": "way", "OffsetStartMs": 7185, "OffsetEndMs": 7550}, {"Word": ".", "OffsetStartMs": 7750, "OffsetEndMs": 8025}, {"Word": "Theonly", "OffsetStartMs": 8025, "OffsetEndMs": 8295}, {"Word": "difference", "OffsetStartMs": 8295, "OffsetEndMs": 8690}, {"Word": "between", "OffsetStartMs": 8770, "OffsetEndMs": 9045}, {"Word": "them", "OffsetStartMs": 9045, "OffsetEndMs": 9240}, {"Word": "is", "OffsetStartMs": 9240, "OffsetEndMs": 9450}, {"Word": "that", "OffsetStartMs": 9450, "OffsetEndMs": 9660}, {"Word": "their", "OffsetStartMs": 9660, "OffsetEndMs": 9900}, {"Word": "weights", "OffsetStartMs": 9900, "OffsetEndMs": 10275}, {"Word": "are", "OffsetStartMs": 10275, "OffsetEndMs": 10470}, {"Word": "all", "OffsetStartMs": 10470, "OffsetEndMs": 10710}, {"Word": "randomly", "OffsetStartMs": 10710, "OffsetEndMs": 11250}, {"Word": "initialized", "OffsetStartMs": 11250, "OffsetEndMs": 11810}, {"Word": "so", "OffsetStartMs": 11980, "OffsetEndMs": 12360}, {"Word": "where", "OffsetStartMs": 12360, "OffsetEndMs": 12630}, {"Word": "they", "OffsetStartMs": 12630, "OffsetEndMs": 12840}, {"Word": "start", "OffsetStartMs": 12840, "OffsetEndMs": 13125}, {"Word": "from", "OffsetStartMs": 13125, "OffsetEndMs": 13440}, {"Word": "is", "OffsetStartMs": 13440, "OffsetEndMs": 13680}, {"Word": "different", "OffsetStartMs": 13680, "OffsetEndMs": 13970}], "SpeechSpeed": 17.4}, {"FinalSentence": "And the reason why we can use this to determine epistemic uncertainty is because we would expect that with familiar inputs in our network, our networks should all converge to around the same answer, and we should see very little variance in the the log or the outputs that we're predicting.", "SliceSentence": "And the reason why we can use this to determine epistemic uncertainty is because we would expect that with familiar inputs in our network our networks should all converge to around the same answer and we should see very little variance in the the log or the outputs that we're predicting", "StartMs": 2325260, "EndMs": 2343100, "WordsNum": 51, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 345}, {"Word": "the", "OffsetStartMs": 345, "OffsetEndMs": 465}, {"Word": "reason", "OffsetStartMs": 465, "OffsetEndMs": 645}, {"Word": "why", "OffsetStartMs": 645, "OffsetEndMs": 870}, {"Word": "we", "OffsetStartMs": 870, "OffsetEndMs": 1020}, {"Word": "can", "OffsetStartMs": 1020, "OffsetEndMs": 1155}, {"Word": "use", "OffsetStartMs": 1155, "OffsetEndMs": 1350}, {"Word": "this", "OffsetStartMs": 1350, "OffsetEndMs": 1590}, {"Word": "to", "OffsetStartMs": 1590, "OffsetEndMs": 1845}, {"Word": "determine", "OffsetStartMs": 1845, "OffsetEndMs": 2180}, {"Word": "epistemic", "OffsetStartMs": 2590, "OffsetEndMs": 3360}, {"Word": "uncertainty", "OffsetStartMs": 3360, "OffsetEndMs": 3710}, {"Word": "is", "OffsetStartMs": 4060, "OffsetEndMs": 4380}, {"Word": "because", "OffsetStartMs": 4380, "OffsetEndMs": 4575}, {"Word": "we", "OffsetStartMs": 4575, "OffsetEndMs": 4710}, {"Word": "would", "OffsetStartMs": 4710, "OffsetEndMs": 4935}, {"Word": "expect", "OffsetStartMs": 4935, "OffsetEndMs": 5300}, {"Word": "that", "OffsetStartMs": 5470, "OffsetEndMs": 5790}, {"Word": "with", "OffsetStartMs": 5790, "OffsetEndMs": 6110}, {"Word": "familiar", "OffsetStartMs": 6340, "OffsetEndMs": 6740}, {"Word": "inputs", "OffsetStartMs": 7090, "OffsetEndMs": 7590}, {"Word": "in", "OffsetStartMs": 7590, "OffsetEndMs": 7815}, {"Word": "our", "OffsetStartMs": 7815, "OffsetEndMs": 7950}, {"Word": "network", "OffsetStartMs": 7950, "OffsetEndMs": 8240}, {"Word": "our", "OffsetStartMs": 8620, "OffsetEndMs": 9000}, {"Word": "networks", "OffsetStartMs": 9000, "OffsetEndMs": 9375}, {"Word": "should", "OffsetStartMs": 9375, "OffsetEndMs": 9645}, {"Word": "all", "OffsetStartMs": 9645, "OffsetEndMs": 9920}, {"Word": "converge", "OffsetStartMs": 9970, "OffsetEndMs": 10485}, {"Word": "to", "OffsetStartMs": 10485, "OffsetEndMs": 10680}, {"Word": "around", "OffsetStartMs": 10680, "OffsetEndMs": 10875}, {"Word": "the", "OffsetStartMs": 10875, "OffsetEndMs": 11070}, {"Word": "same", "OffsetStartMs": 11070, "OffsetEndMs": 11280}, {"Word": "answer", "OffsetStartMs": 11280, "OffsetEndMs": 11600}, {"Word": "and", "OffsetStartMs": 11890, "OffsetEndMs": 12150}, {"Word": "we", "OffsetStartMs": 12150, "OffsetEndMs": 12300}, {"Word": "should", "OffsetStartMs": 12300, "OffsetEndMs": 12480}, {"Word": "see", "OffsetStartMs": 12480, "OffsetEndMs": 12690}, {"Word": "very", "OffsetStartMs": 12690, "OffsetEndMs": 13010}, {"Word": "little", "OffsetStartMs": 13060, "OffsetEndMs": 13410}, {"Word": "variance", "OffsetStartMs": 13410, "OffsetEndMs": 13980}, {"Word": "in", "OffsetStartMs": 13980, "OffsetEndMs": 14235}, {"Word": "the", "OffsetStartMs": 14235, "OffsetEndMs": 14540}, {"Word": "the", "OffsetStartMs": 14680, "OffsetEndMs": 14955}, {"Word": "log", "OffsetStartMs": 14955, "OffsetEndMs": 15225}, {"Word": "or", "OffsetStartMs": 15225, "OffsetEndMs": 15480}, {"Word": "the", "OffsetStartMs": 15480, "OffsetEndMs": 15705}, {"Word": "outputs", "OffsetStartMs": 15705, "OffsetEndMs": 16005}, {"Word": "that", "OffsetStartMs": 16005, "OffsetEndMs": 16155}, {"Word": "we're", "OffsetStartMs": 16155, "OffsetEndMs": 16350}, {"Word": "predicting", "OffsetStartMs": 16350, "OffsetEndMs": 16880}], "SpeechSpeed": 16.1}, {"FinalSentence": "However, if a model has never seen a specific input before, or that input is very hard to learn, all of these models should predict slightly different answers, and the variance of them should be higher than if they were predicting a similar input.", "SliceSentence": "However if a model has never seen a specific input before or that input is very hard to learn all of these models should predict slightly different answers and the variance of them should be higher than if they were predicting a similar input", "StartMs": 2343100, "EndMs": 2356640, "WordsNum": 44, "Words": [{"Word": "However", "OffsetStartMs": 0, "OffsetEndMs": 380}, {"Word": "if", "OffsetStartMs": 430, "OffsetEndMs": 690}, {"Word": "a", "OffsetStartMs": 690, "OffsetEndMs": 810}, {"Word": "model", "OffsetStartMs": 810, "OffsetEndMs": 1020}, {"Word": "has", "OffsetStartMs": 1020, "OffsetEndMs": 1260}, {"Word": "never", "OffsetStartMs": 1260, "OffsetEndMs": 1515}, {"Word": "seen", "OffsetStartMs": 1515, "OffsetEndMs": 1755}, {"Word": "a", "OffsetStartMs": 1755, "OffsetEndMs": 1905}, {"Word": "specific", "OffsetStartMs": 1905, "OffsetEndMs": 2180}, {"Word": "input", "OffsetStartMs": 2290, "OffsetEndMs": 2565}, {"Word": "before", "OffsetStartMs": 2565, "OffsetEndMs": 2840}, {"Word": "or", "OffsetStartMs": 3010, "OffsetEndMs": 3285}, {"Word": "that", "OffsetStartMs": 3285, "OffsetEndMs": 3525}, {"Word": "input", "OffsetStartMs": 3525, "OffsetEndMs": 3735}, {"Word": "is", "OffsetStartMs": 3735, "OffsetEndMs": 3885}, {"Word": "very", "OffsetStartMs": 3885, "OffsetEndMs": 4170}, {"Word": "hard", "OffsetStartMs": 4170, "OffsetEndMs": 4500}, {"Word": "to", "OffsetStartMs": 4500, "OffsetEndMs": 4710}, {"Word": "learn", "OffsetStartMs": 4710, "OffsetEndMs": 4970}, {"Word": "all", "OffsetStartMs": 5290, "OffsetEndMs": 5580}, {"Word": "of", "OffsetStartMs": 5580, "OffsetEndMs": 5745}, {"Word": "these", "OffsetStartMs": 5745, "OffsetEndMs": 5925}, {"Word": "models", "OffsetStartMs": 5925, "OffsetEndMs": 6230}, {"Word": "should", "OffsetStartMs": 6280, "OffsetEndMs": 6675}, {"Word": "predict", "OffsetStartMs": 6675, "OffsetEndMs": 7065}, {"Word": "slightly", "OffsetStartMs": 7065, "OffsetEndMs": 7455}, {"Word": "different", "OffsetStartMs": 7455, "OffsetEndMs": 7785}, {"Word": "answers", "OffsetStartMs": 7785, "OffsetEndMs": 8120}, {"Word": "and", "OffsetStartMs": 8470, "OffsetEndMs": 8745}, {"Word": "the", "OffsetStartMs": 8745, "OffsetEndMs": 8880}, {"Word": "variance", "OffsetStartMs": 8880, "OffsetEndMs": 9240}, {"Word": "of", "OffsetStartMs": 9240, "OffsetEndMs": 9405}, {"Word": "them", "OffsetStartMs": 9405, "OffsetEndMs": 9600}, {"Word": "should", "OffsetStartMs": 9600, "OffsetEndMs": 9780}, {"Word": "be", "OffsetStartMs": 9780, "OffsetEndMs": 9930}, {"Word": "higher", "OffsetStartMs": 9930, "OffsetEndMs": 10220}, {"Word": "than", "OffsetStartMs": 10390, "OffsetEndMs": 10650}, {"Word": "if", "OffsetStartMs": 10650, "OffsetEndMs": 10770}, {"Word": "they", "OffsetStartMs": 10770, "OffsetEndMs": 10875}, {"Word": "were", "OffsetStartMs": 10875, "OffsetEndMs": 10995}, {"Word": "predicting", "OffsetStartMs": 10995, "OffsetEndMs": 11415}, {"Word": "a", "OffsetStartMs": 11415, "OffsetEndMs": 11655}, {"Word": "similar", "OffsetStartMs": 11655, "OffsetEndMs": 11990}, {"Word": "input", "OffsetStartMs": 12190, "OffsetEndMs": 12590}], "SpeechSpeed": 17.9}, {"FinalSentence": "So creating an ensemble of networks is quite simple, quite simple. You start out with defining the number of ensembles you want, you create them all the exact same way, and then you fit them all on the same training data and training data.", "SliceSentence": "So creating an ensemble of networks is quite simple quite simple . Youstart out with defining the number of ensembles you want you create them all the exact same way and then you fit them all on the same training data and training data", "StartMs": 2358360, "EndMs": 2372540, "WordsNum": 44, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 540}, {"Word": "creating", "OffsetStartMs": 540, "OffsetEndMs": 870}, {"Word": "an", "OffsetStartMs": 870, "OffsetEndMs": 1065}, {"Word": "ensemble", "OffsetStartMs": 1065, "OffsetEndMs": 1545}, {"Word": "of", "OffsetStartMs": 1545, "OffsetEndMs": 1650}, {"Word": "networks", "OffsetStartMs": 1650, "OffsetEndMs": 1910}, {"Word": "is", "OffsetStartMs": 1930, "OffsetEndMs": 2235}, {"Word": "quite", "OffsetStartMs": 2235, "OffsetEndMs": 2475}, {"Word": "simple", "OffsetStartMs": 2475, "OffsetEndMs": 2810}, {"Word": "quite", "OffsetStartMs": 2950, "OffsetEndMs": 3270}, {"Word": "simple", "OffsetStartMs": 3270, "OffsetEndMs": 3590}, {"Word": ".", "OffsetStartMs": 3850, "OffsetEndMs": 4250}, {"Word": "Youstart", "OffsetStartMs": 4570, "OffsetEndMs": 4845}, {"Word": "out", "OffsetStartMs": 4845, "OffsetEndMs": 5055}, {"Word": "with", "OffsetStartMs": 5055, "OffsetEndMs": 5370}, {"Word": "defining", "OffsetStartMs": 5370, "OffsetEndMs": 5790}, {"Word": "the", "OffsetStartMs": 5790, "OffsetEndMs": 5895}, {"Word": "number", "OffsetStartMs": 5895, "OffsetEndMs": 6045}, {"Word": "of", "OffsetStartMs": 6045, "OffsetEndMs": 6195}, {"Word": "ensembles", "OffsetStartMs": 6195, "OffsetEndMs": 6705}, {"Word": "you", "OffsetStartMs": 6705, "OffsetEndMs": 6870}, {"Word": "want", "OffsetStartMs": 6870, "OffsetEndMs": 7130}, {"Word": "you", "OffsetStartMs": 7150, "OffsetEndMs": 7470}, {"Word": "create", "OffsetStartMs": 7470, "OffsetEndMs": 7710}, {"Word": "them", "OffsetStartMs": 7710, "OffsetEndMs": 7905}, {"Word": "all", "OffsetStartMs": 7905, "OffsetEndMs": 8100}, {"Word": "the", "OffsetStartMs": 8100, "OffsetEndMs": 8340}, {"Word": "exact", "OffsetStartMs": 8340, "OffsetEndMs": 8625}, {"Word": "same", "OffsetStartMs": 8625, "OffsetEndMs": 8895}, {"Word": "way", "OffsetStartMs": 8895, "OffsetEndMs": 9200}, {"Word": "and", "OffsetStartMs": 9370, "OffsetEndMs": 9630}, {"Word": "then", "OffsetStartMs": 9630, "OffsetEndMs": 9765}, {"Word": "you", "OffsetStartMs": 9765, "OffsetEndMs": 9930}, {"Word": "fit", "OffsetStartMs": 9930, "OffsetEndMs": 10110}, {"Word": "them", "OffsetStartMs": 10110, "OffsetEndMs": 10275}, {"Word": "all", "OffsetStartMs": 10275, "OffsetEndMs": 10470}, {"Word": "on", "OffsetStartMs": 10470, "OffsetEndMs": 10665}, {"Word": "the", "OffsetStartMs": 10665, "OffsetEndMs": 10860}, {"Word": "same", "OffsetStartMs": 10860, "OffsetEndMs": 11160}, {"Word": "training", "OffsetStartMs": 11160, "OffsetEndMs": 11490}, {"Word": "data", "OffsetStartMs": 11490, "OffsetEndMs": 11840}, {"Word": "and", "OffsetStartMs": 11860, "OffsetEndMs": 12260}, {"Word": "training", "OffsetStartMs": 12580, "OffsetEndMs": 12900}, {"Word": "data", "OffsetStartMs": 12900, "OffsetEndMs": 13220}], "SpeechSpeed": 16.5}, {"FinalSentence": "And then afterwards, when at inference time we call all of the models, every model in the ensemble on our specific input, and then we can treat our new prediction as the average of all of the ensembles. This results in a usually more robust and accurate prediction, and we can treat the uncertainty as the variance of all of these predictions. Again, remember that if we saw familiar inputs or inputs with low epistemic uncertainty, we should expect to have very little variance. And if we had a very unfamiliar input or something that was out of distribution or something the model hasn't seen before, we should have very high epistemic uncertainty or variance.", "SliceSentence": "And then afterwards when at inference time we call all of the models every model in the ensemble on our specific input and then we can treat our new prediction as the average of all of the ensembles . Thisresults in a usually more robust and accurate prediction and we can treat the uncertainty as the variance of all of these predictions . Againremember that if we saw familiar inputs or inputs with low epistemic uncertainty we should expect to have very little variance . Andif we had a very unfamiliar input or something that was out of distribution or something the model hasn't seen before we should have very high epistemic uncertainty or variance", "StartMs": 2373020, "EndMs": 2413200, "WordsNum": 115, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 345}, {"Word": "then", "OffsetStartMs": 345, "OffsetEndMs": 510}, {"Word": "afterwards", "OffsetStartMs": 510, "OffsetEndMs": 800}, {"Word": "when", "OffsetStartMs": 1090, "OffsetEndMs": 1410}, {"Word": "at", "OffsetStartMs": 1410, "OffsetEndMs": 1730}, {"Word": "inference", "OffsetStartMs": 1750, "OffsetEndMs": 2280}, {"Word": "time", "OffsetStartMs": 2280, "OffsetEndMs": 2630}, {"Word": "we", "OffsetStartMs": 2920, "OffsetEndMs": 3255}, {"Word": "call", "OffsetStartMs": 3255, "OffsetEndMs": 3590}, {"Word": "all", "OffsetStartMs": 3970, "OffsetEndMs": 4260}, {"Word": "of", "OffsetStartMs": 4260, "OffsetEndMs": 4410}, {"Word": "the", "OffsetStartMs": 4410, "OffsetEndMs": 4530}, {"Word": "models", "OffsetStartMs": 4530, "OffsetEndMs": 4790}, {"Word": "every", "OffsetStartMs": 4810, "OffsetEndMs": 5160}, {"Word": "model", "OffsetStartMs": 5160, "OffsetEndMs": 5460}, {"Word": "in", "OffsetStartMs": 5460, "OffsetEndMs": 5700}, {"Word": "the", "OffsetStartMs": 5700, "OffsetEndMs": 5850}, {"Word": "ensemble", "OffsetStartMs": 5850, "OffsetEndMs": 6500}, {"Word": "on", "OffsetStartMs": 6700, "OffsetEndMs": 7080}, {"Word": "our", "OffsetStartMs": 7080, "OffsetEndMs": 7440}, {"Word": "specific", "OffsetStartMs": 7440, "OffsetEndMs": 7820}, {"Word": "input", "OffsetStartMs": 7990, "OffsetEndMs": 8390}, {"Word": "and", "OffsetStartMs": 9160, "OffsetEndMs": 9450}, {"Word": "then", "OffsetStartMs": 9450, "OffsetEndMs": 9740}, {"Word": "we", "OffsetStartMs": 9820, "OffsetEndMs": 10095}, {"Word": "can", "OffsetStartMs": 10095, "OffsetEndMs": 10275}, {"Word": "treat", "OffsetStartMs": 10275, "OffsetEndMs": 10580}, {"Word": "our", "OffsetStartMs": 10840, "OffsetEndMs": 11130}, {"Word": "new", "OffsetStartMs": 11130, "OffsetEndMs": 11310}, {"Word": "prediction", "OffsetStartMs": 11310, "OffsetEndMs": 11690}, {"Word": "as", "OffsetStartMs": 11710, "OffsetEndMs": 12000}, {"Word": "the", "OffsetStartMs": 12000, "OffsetEndMs": 12180}, {"Word": "average", "OffsetStartMs": 12180, "OffsetEndMs": 12470}, {"Word": "of", "OffsetStartMs": 12610, "OffsetEndMs": 12960}, {"Word": "all", "OffsetStartMs": 12960, "OffsetEndMs": 13215}, {"Word": "of", "OffsetStartMs": 13215, "OffsetEndMs": 13395}, {"Word": "the", "OffsetStartMs": 13395, "OffsetEndMs": 13530}, {"Word": "ensembles", "OffsetStartMs": 13530, "OffsetEndMs": 14180}, {"Word": ".", "OffsetStartMs": 14260, "OffsetEndMs": 14580}, {"Word": "Thisresults", "OffsetStartMs": 14580, "OffsetEndMs": 14835}, {"Word": "in", "OffsetStartMs": 14835, "OffsetEndMs": 15015}, {"Word": "a", "OffsetStartMs": 15015, "OffsetEndMs": 15120}, {"Word": "usually", "OffsetStartMs": 15120, "OffsetEndMs": 15375}, {"Word": "more", "OffsetStartMs": 15375, "OffsetEndMs": 15765}, {"Word": "robust", "OffsetStartMs": 15765, "OffsetEndMs": 16125}, {"Word": "and", "OffsetStartMs": 16125, "OffsetEndMs": 16470}, {"Word": "accurate", "OffsetStartMs": 16470, "OffsetEndMs": 16740}, {"Word": "prediction", "OffsetStartMs": 16740, "OffsetEndMs": 17090}, {"Word": "and", "OffsetStartMs": 17830, "OffsetEndMs": 18225}, {"Word": "we", "OffsetStartMs": 18225, "OffsetEndMs": 18495}, {"Word": "can", "OffsetStartMs": 18495, "OffsetEndMs": 18660}, {"Word": "treat", "OffsetStartMs": 18660, "OffsetEndMs": 18840}, {"Word": "the", "OffsetStartMs": 18840, "OffsetEndMs": 19080}, {"Word": "uncertainty", "OffsetStartMs": 19080, "OffsetEndMs": 19430}, {"Word": "as", "OffsetStartMs": 19720, "OffsetEndMs": 20070}, {"Word": "the", "OffsetStartMs": 20070, "OffsetEndMs": 20325}, {"Word": "variance", "OffsetStartMs": 20325, "OffsetEndMs": 20775}, {"Word": "of", "OffsetStartMs": 20775, "OffsetEndMs": 20970}, {"Word": "all", "OffsetStartMs": 20970, "OffsetEndMs": 21135}, {"Word": "of", "OffsetStartMs": 21135, "OffsetEndMs": 21270}, {"Word": "these", "OffsetStartMs": 21270, "OffsetEndMs": 21435}, {"Word": "predictions", "OffsetStartMs": 21435, "OffsetEndMs": 21980}, {"Word": ".", "OffsetStartMs": 23050, "OffsetEndMs": 23430}, {"Word": "Againremember", "OffsetStartMs": 23430, "OffsetEndMs": 23760}, {"Word": "that", "OffsetStartMs": 23760, "OffsetEndMs": 24105}, {"Word": "if", "OffsetStartMs": 24105, "OffsetEndMs": 24390}, {"Word": "we", "OffsetStartMs": 24390, "OffsetEndMs": 24555}, {"Word": "saw", "OffsetStartMs": 24555, "OffsetEndMs": 24750}, {"Word": "familiar", "OffsetStartMs": 24750, "OffsetEndMs": 25070}, {"Word": "inputs", "OffsetStartMs": 25240, "OffsetEndMs": 25665}, {"Word": "or", "OffsetStartMs": 25665, "OffsetEndMs": 25935}, {"Word": "inputs", "OffsetStartMs": 25935, "OffsetEndMs": 26205}, {"Word": "with", "OffsetStartMs": 26205, "OffsetEndMs": 26355}, {"Word": "low", "OffsetStartMs": 26355, "OffsetEndMs": 26550}, {"Word": "epistemic", "OffsetStartMs": 26550, "OffsetEndMs": 27165}, {"Word": "uncertainty", "OffsetStartMs": 27165, "OffsetEndMs": 27500}, {"Word": "we", "OffsetStartMs": 27850, "OffsetEndMs": 28125}, {"Word": "should", "OffsetStartMs": 28125, "OffsetEndMs": 28335}, {"Word": "expect", "OffsetStartMs": 28335, "OffsetEndMs": 28575}, {"Word": "to", "OffsetStartMs": 28575, "OffsetEndMs": 28725}, {"Word": "have", "OffsetStartMs": 28725, "OffsetEndMs": 28890}, {"Word": "very", "OffsetStartMs": 28890, "OffsetEndMs": 29175}, {"Word": "little", "OffsetStartMs": 29175, "OffsetEndMs": 29475}, {"Word": "variance", "OffsetStartMs": 29475, "OffsetEndMs": 30020}, {"Word": ".", "OffsetStartMs": 30190, "OffsetEndMs": 30450}, {"Word": "Andif", "OffsetStartMs": 30450, "OffsetEndMs": 30585}, {"Word": "we", "OffsetStartMs": 30585, "OffsetEndMs": 30735}, {"Word": "had", "OffsetStartMs": 30735, "OffsetEndMs": 30945}, {"Word": "a", "OffsetStartMs": 30945, "OffsetEndMs": 31215}, {"Word": "very", "OffsetStartMs": 31215, "OffsetEndMs": 31470}, {"Word": "unfamiliar", "OffsetStartMs": 31470, "OffsetEndMs": 32220}, {"Word": "input", "OffsetStartMs": 32220, "OffsetEndMs": 32520}, {"Word": "or", "OffsetStartMs": 32520, "OffsetEndMs": 32715}, {"Word": "something", "OffsetStartMs": 32715, "OffsetEndMs": 32940}, {"Word": "that", "OffsetStartMs": 32940, "OffsetEndMs": 33165}, {"Word": "was", "OffsetStartMs": 33165, "OffsetEndMs": 33345}, {"Word": "out", "OffsetStartMs": 33345, "OffsetEndMs": 33540}, {"Word": "of", "OffsetStartMs": 33540, "OffsetEndMs": 33795}, {"Word": "distribution", "OffsetStartMs": 33795, "OffsetEndMs": 34160}, {"Word": "or", "OffsetStartMs": 34270, "OffsetEndMs": 34545}, {"Word": "something", "OffsetStartMs": 34545, "OffsetEndMs": 34740}, {"Word": "the", "OffsetStartMs": 34740, "OffsetEndMs": 34905}, {"Word": "model", "OffsetStartMs": 34905, "OffsetEndMs": 35100}, {"Word": "hasn't", "OffsetStartMs": 35100, "OffsetEndMs": 35490}, {"Word": "seen", "OffsetStartMs": 35490, "OffsetEndMs": 35670}, {"Word": "before", "OffsetStartMs": 35670, "OffsetEndMs": 35960}, {"Word": "we", "OffsetStartMs": 36280, "OffsetEndMs": 36555}, {"Word": "should", "OffsetStartMs": 36555, "OffsetEndMs": 36705}, {"Word": "have", "OffsetStartMs": 36705, "OffsetEndMs": 36885}, {"Word": "very", "OffsetStartMs": 36885, "OffsetEndMs": 37170}, {"Word": "high", "OffsetStartMs": 37170, "OffsetEndMs": 37470}, {"Word": "epistemic", "OffsetStartMs": 37470, "OffsetEndMs": 38130}, {"Word": "uncertainty", "OffsetStartMs": 38130, "OffsetEndMs": 38450}, {"Word": "or", "OffsetStartMs": 38470, "OffsetEndMs": 38745}, {"Word": "variance", "OffsetStartMs": 38745, "OffsetEndMs": 39230}], "SpeechSpeed": 16.2}, {"FinalSentence": "So what's the problem with this? Can anyone raise their hand and tell me what a problem with training an ensemble of networks is?", "SliceSentence": "So what's the problem with this Can anyone raise their hand and tell me what a problem with training an ensemble of networks is", "StartMs": 2415100, "EndMs": 2422420, "WordsNum": 24, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "what's", "OffsetStartMs": 700, "OffsetEndMs": 1065}, {"Word": "the", "OffsetStartMs": 1065, "OffsetEndMs": 1170}, {"Word": "problem", "OffsetStartMs": 1170, "OffsetEndMs": 1395}, {"Word": "with", "OffsetStartMs": 1395, "OffsetEndMs": 1650}, {"Word": "this", "OffsetStartMs": 1650, "OffsetEndMs": 1935}, {"Word": "Can", "OffsetStartMs": 1935, "OffsetEndMs": 2265}, {"Word": "anyone", "OffsetStartMs": 2265, "OffsetEndMs": 2600}, {"Word": "raise", "OffsetStartMs": 2920, "OffsetEndMs": 3210}, {"Word": "their", "OffsetStartMs": 3210, "OffsetEndMs": 3360}, {"Word": "hand", "OffsetStartMs": 3360, "OffsetEndMs": 3495}, {"Word": "and", "OffsetStartMs": 3495, "OffsetEndMs": 3630}, {"Word": "tell", "OffsetStartMs": 3630, "OffsetEndMs": 3750}, {"Word": "me", "OffsetStartMs": 3750, "OffsetEndMs": 3870}, {"Word": "what", "OffsetStartMs": 3870, "OffsetEndMs": 3960}, {"Word": "a", "OffsetStartMs": 3960, "OffsetEndMs": 4050}, {"Word": "problem", "OffsetStartMs": 4050, "OffsetEndMs": 4260}, {"Word": "with", "OffsetStartMs": 4260, "OffsetEndMs": 4485}, {"Word": "training", "OffsetStartMs": 4485, "OffsetEndMs": 4695}, {"Word": "an", "OffsetStartMs": 4695, "OffsetEndMs": 4890}, {"Word": "ensemble", "OffsetStartMs": 4890, "OffsetEndMs": 5355}, {"Word": "of", "OffsetStartMs": 5355, "OffsetEndMs": 5460}, {"Word": "networks", "OffsetStartMs": 5460, "OffsetEndMs": 5715}, {"Word": "is", "OffsetStartMs": 5715, "OffsetEndMs": 6110}], "SpeechSpeed": 17.3}, {"FinalSentence": "So training an ensemble of networks is really compute expensive. Even if your model is not very large, training five copies of it or ten copies of it tends to, it takes up compute and time, and that's just not really feasible when we're training on specific tasks.", "SliceSentence": "So training an ensemble of networks is really compute expensive . Evenif your model is not very large training five copies of it or ten copies of it tends to it takes up compute and time and that's just not really feasible when we're training on specific tasks", "StartMs": 2425220, "EndMs": 2443660, "WordsNum": 48, "Words": [{"Word": "So", "OffsetStartMs": 220, "OffsetEndMs": 510}, {"Word": "training", "OffsetStartMs": 510, "OffsetEndMs": 705}, {"Word": "an", "OffsetStartMs": 705, "OffsetEndMs": 840}, {"Word": "ensemble", "OffsetStartMs": 840, "OffsetEndMs": 1200}, {"Word": "of", "OffsetStartMs": 1200, "OffsetEndMs": 1290}, {"Word": "networks", "OffsetStartMs": 1290, "OffsetEndMs": 1545}, {"Word": "is", "OffsetStartMs": 1545, "OffsetEndMs": 1890}, {"Word": "really", "OffsetStartMs": 1890, "OffsetEndMs": 2205}, {"Word": "compute", "OffsetStartMs": 2205, "OffsetEndMs": 2730}, {"Word": "expensive", "OffsetStartMs": 2730, "OffsetEndMs": 3110}, {"Word": ".", "OffsetStartMs": 4030, "OffsetEndMs": 4365}, {"Word": "Evenif", "OffsetStartMs": 4365, "OffsetEndMs": 4575}, {"Word": "your", "OffsetStartMs": 4575, "OffsetEndMs": 4725}, {"Word": "model", "OffsetStartMs": 4725, "OffsetEndMs": 4965}, {"Word": "is", "OffsetStartMs": 4965, "OffsetEndMs": 5205}, {"Word": "not", "OffsetStartMs": 5205, "OffsetEndMs": 5400}, {"Word": "very", "OffsetStartMs": 5400, "OffsetEndMs": 5640}, {"Word": "large", "OffsetStartMs": 5640, "OffsetEndMs": 5960}, {"Word": "training", "OffsetStartMs": 6190, "OffsetEndMs": 6590}, {"Word": "five", "OffsetStartMs": 6610, "OffsetEndMs": 6975}, {"Word": "copies", "OffsetStartMs": 6975, "OffsetEndMs": 7365}, {"Word": "of", "OffsetStartMs": 7365, "OffsetEndMs": 7500}, {"Word": "it", "OffsetStartMs": 7500, "OffsetEndMs": 7635}, {"Word": "or", "OffsetStartMs": 7635, "OffsetEndMs": 7815}, {"Word": "ten", "OffsetStartMs": 7815, "OffsetEndMs": 8040}, {"Word": "copies", "OffsetStartMs": 8040, "OffsetEndMs": 8400}, {"Word": "of", "OffsetStartMs": 8400, "OffsetEndMs": 8520}, {"Word": "it", "OffsetStartMs": 8520, "OffsetEndMs": 8780}, {"Word": "tends", "OffsetStartMs": 9160, "OffsetEndMs": 9615}, {"Word": "to", "OffsetStartMs": 9615, "OffsetEndMs": 9920}, {"Word": "it", "OffsetStartMs": 10360, "OffsetEndMs": 10755}, {"Word": "takes", "OffsetStartMs": 10755, "OffsetEndMs": 11025}, {"Word": "up", "OffsetStartMs": 11025, "OffsetEndMs": 11190}, {"Word": "compute", "OffsetStartMs": 11190, "OffsetEndMs": 11610}, {"Word": "and", "OffsetStartMs": 11610, "OffsetEndMs": 11880}, {"Word": "time", "OffsetStartMs": 11880, "OffsetEndMs": 12200}, {"Word": "and", "OffsetStartMs": 12490, "OffsetEndMs": 12890}, {"Word": "that's", "OffsetStartMs": 12910, "OffsetEndMs": 13290}, {"Word": "just", "OffsetStartMs": 13290, "OffsetEndMs": 13455}, {"Word": "not", "OffsetStartMs": 13455, "OffsetEndMs": 13695}, {"Word": "really", "OffsetStartMs": 13695, "OffsetEndMs": 13980}, {"Word": "feasible", "OffsetStartMs": 13980, "OffsetEndMs": 14600}, {"Word": "when", "OffsetStartMs": 14650, "OffsetEndMs": 14955}, {"Word": "we're", "OffsetStartMs": 14955, "OffsetEndMs": 15180}, {"Word": "training", "OffsetStartMs": 15180, "OffsetEndMs": 15440}, {"Word": "on", "OffsetStartMs": 16240, "OffsetEndMs": 16560}, {"Word": "specific", "OffsetStartMs": 16560, "OffsetEndMs": 16860}, {"Word": "tasks", "OffsetStartMs": 16860, "OffsetEndMs": 17240}], "SpeechSpeed": 14.0}, {"FinalSentence": "However, the key insight for ensembles is that by introducing some method of randomness or stochasticity into our networks, we're able to estimate epistemic uncertainty. So another way that we've seen about introducing stochasticity into networks is by using dropout layers. We've seen dropout layers as a method of reducing overfitting because we randomly drop out different nodes in our, in our layer, and then we continue to propagate information through them and it prevents models from memorizing data.", "SliceSentence": "However the key insight for ensembles is that by introducing some method of randomness or stochasticity into our networks we're able to estimate epistemic uncertainty . Soanother way that we've seen about introducing stochasticity into networks is by using dropout layers .We've seen dropout layers as a method of reducing overfitting because we randomly drop out different nodes in our in our layer and then we continue to propagate information through them and it prevents models from memorizing data", "StartMs": 2443660, "EndMs": 2475060, "WordsNum": 79, "Words": [{"Word": "However", "OffsetStartMs": 70, "OffsetEndMs": 470}, {"Word": "the", "OffsetStartMs": 520, "OffsetEndMs": 825}, {"Word": "key", "OffsetStartMs": 825, "OffsetEndMs": 1130}, {"Word": "insight", "OffsetStartMs": 1420, "OffsetEndMs": 1755}, {"Word": "for", "OffsetStartMs": 1755, "OffsetEndMs": 1920}, {"Word": "ensembles", "OffsetStartMs": 1920, "OffsetEndMs": 2535}, {"Word": "is", "OffsetStartMs": 2535, "OffsetEndMs": 2790}, {"Word": "that", "OffsetStartMs": 2790, "OffsetEndMs": 2985}, {"Word": "by", "OffsetStartMs": 2985, "OffsetEndMs": 3195}, {"Word": "introducing", "OffsetStartMs": 3195, "OffsetEndMs": 3770}, {"Word": "some", "OffsetStartMs": 3820, "OffsetEndMs": 4140}, {"Word": "method", "OffsetStartMs": 4140, "OffsetEndMs": 4455}, {"Word": "of", "OffsetStartMs": 4455, "OffsetEndMs": 4770}, {"Word": "randomness", "OffsetStartMs": 4770, "OffsetEndMs": 5460}, {"Word": "or", "OffsetStartMs": 5460, "OffsetEndMs": 5715}, {"Word": "stochasticity", "OffsetStartMs": 5715, "OffsetEndMs": 6675}, {"Word": "into", "OffsetStartMs": 6675, "OffsetEndMs": 6960}, {"Word": "our", "OffsetStartMs": 6960, "OffsetEndMs": 7140}, {"Word": "networks", "OffsetStartMs": 7140, "OffsetEndMs": 7430}, {"Word": "we're", "OffsetStartMs": 7870, "OffsetEndMs": 8175}, {"Word": "able", "OffsetStartMs": 8175, "OffsetEndMs": 8370}, {"Word": "to", "OffsetStartMs": 8370, "OffsetEndMs": 8720}, {"Word": "estimate", "OffsetStartMs": 8740, "OffsetEndMs": 9060}, {"Word": "epistemic", "OffsetStartMs": 9060, "OffsetEndMs": 9765}, {"Word": "uncertainty", "OffsetStartMs": 9765, "OffsetEndMs": 10130}, {"Word": ".", "OffsetStartMs": 11080, "OffsetEndMs": 11480}, {"Word": "Soanother", "OffsetStartMs": 11650, "OffsetEndMs": 11955}, {"Word": "way", "OffsetStartMs": 11955, "OffsetEndMs": 12135}, {"Word": "that", "OffsetStartMs": 12135, "OffsetEndMs": 12270}, {"Word": "we've", "OffsetStartMs": 12270, "OffsetEndMs": 12555}, {"Word": "seen", "OffsetStartMs": 12555, "OffsetEndMs": 12860}, {"Word": "about", "OffsetStartMs": 13120, "OffsetEndMs": 13520}, {"Word": "introducing", "OffsetStartMs": 13960, "OffsetEndMs": 14595}, {"Word": "stochasticity", "OffsetStartMs": 14595, "OffsetEndMs": 15495}, {"Word": "into", "OffsetStartMs": 15495, "OffsetEndMs": 15675}, {"Word": "networks", "OffsetStartMs": 15675, "OffsetEndMs": 15980}, {"Word": "is", "OffsetStartMs": 16120, "OffsetEndMs": 16395}, {"Word": "by", "OffsetStartMs": 16395, "OffsetEndMs": 16545}, {"Word": "using", "OffsetStartMs": 16545, "OffsetEndMs": 16820}, {"Word": "dropout", "OffsetStartMs": 16930, "OffsetEndMs": 17430}, {"Word": "layers", "OffsetStartMs": 17430, "OffsetEndMs": 17870}, {"Word": ".We've", "OffsetStartMs": 18370, "OffsetEndMs": 18720}, {"Word": "seen", "OffsetStartMs": 18720, "OffsetEndMs": 18885}, {"Word": "dropout", "OffsetStartMs": 18885, "OffsetEndMs": 19230}, {"Word": "layers", "OffsetStartMs": 19230, "OffsetEndMs": 19575}, {"Word": "as", "OffsetStartMs": 19575, "OffsetEndMs": 19740}, {"Word": "a", "OffsetStartMs": 19740, "OffsetEndMs": 19875}, {"Word": "method", "OffsetStartMs": 19875, "OffsetEndMs": 20130}, {"Word": "of", "OffsetStartMs": 20130, "OffsetEndMs": 20460}, {"Word": "reducing", "OffsetStartMs": 20460, "OffsetEndMs": 20775}, {"Word": "overfitting", "OffsetStartMs": 20775, "OffsetEndMs": 21530}, {"Word": "because", "OffsetStartMs": 21760, "OffsetEndMs": 22035}, {"Word": "we", "OffsetStartMs": 22035, "OffsetEndMs": 22200}, {"Word": "randomly", "OffsetStartMs": 22200, "OffsetEndMs": 22725}, {"Word": "drop", "OffsetStartMs": 22725, "OffsetEndMs": 22965}, {"Word": "out", "OffsetStartMs": 22965, "OffsetEndMs": 23250}, {"Word": "different", "OffsetStartMs": 23250, "OffsetEndMs": 23600}, {"Word": "nodes", "OffsetStartMs": 24100, "OffsetEndMs": 24555}, {"Word": "in", "OffsetStartMs": 24555, "OffsetEndMs": 24690}, {"Word": "our", "OffsetStartMs": 24690, "OffsetEndMs": 24855}, {"Word": "in", "OffsetStartMs": 24855, "OffsetEndMs": 25020}, {"Word": "our", "OffsetStartMs": 25020, "OffsetEndMs": 25170}, {"Word": "layer", "OffsetStartMs": 25170, "OffsetEndMs": 25460}, {"Word": "and", "OffsetStartMs": 25720, "OffsetEndMs": 26010}, {"Word": "then", "OffsetStartMs": 26010, "OffsetEndMs": 26280}, {"Word": "we", "OffsetStartMs": 26280, "OffsetEndMs": 26625}, {"Word": "continue", "OffsetStartMs": 26625, "OffsetEndMs": 26910}, {"Word": "to", "OffsetStartMs": 26910, "OffsetEndMs": 27090}, {"Word": "propagate", "OffsetStartMs": 27090, "OffsetEndMs": 27540}, {"Word": "information", "OffsetStartMs": 27540, "OffsetEndMs": 27870}, {"Word": "through", "OffsetStartMs": 27870, "OffsetEndMs": 28095}, {"Word": "them", "OffsetStartMs": 28095, "OffsetEndMs": 28305}, {"Word": "and", "OffsetStartMs": 28305, "OffsetEndMs": 28500}, {"Word": "it", "OffsetStartMs": 28500, "OffsetEndMs": 28650}, {"Word": "prevents", "OffsetStartMs": 28650, "OffsetEndMs": 28965}, {"Word": "models", "OffsetStartMs": 28965, "OffsetEndMs": 29295}, {"Word": "from", "OffsetStartMs": 29295, "OffsetEndMs": 29580}, {"Word": "memorizing", "OffsetStartMs": 29580, "OffsetEndMs": 30075}, {"Word": "data", "OffsetStartMs": 30075, "OffsetEndMs": 30440}], "SpeechSpeed": 15.9}, {"FinalSentence": "However, in the EM case of epistemic uncertainty, we can add dropout layers after every single layer in our model, and in addition, we can keep these dropout layers enabled at test time. Usually we don't keep dropout layers enabled at test time because we don't want to lose any information about the the network's process or any weights when we're at inference time. However, when we're estimating epistemic uncertainty, we do want to keep dropout enabled at test time, because that's how we can introduce randomness, inference time as well.", "SliceSentence": "However in the EM case of epistemic uncertainty we can add dropout layers after every single layer in our model and in addition we can keep these dropout layers enabled at test time . Usuallywe don't keep dropout layers enabled at test time because we don't want to lose any information about the the network's process or any weights when we're at inference time . Howeverwhen we're estimating epistemic uncertainty we do want to keep dropout enabled at test time because that's how we can introduce randomness inference time as well", "StartMs": 2475060, "EndMs": 2508200, "WordsNum": 91, "Words": [{"Word": "However", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "in", "OffsetStartMs": 1210, "OffsetEndMs": 1530}, {"Word": "the", "OffsetStartMs": 1530, "OffsetEndMs": 1785}, {"Word": "EM", "OffsetStartMs": 1785, "OffsetEndMs": 2120}, {"Word": "case", "OffsetStartMs": 2500, "OffsetEndMs": 2865}, {"Word": "of", "OffsetStartMs": 2865, "OffsetEndMs": 3135}, {"Word": "epistemic", "OffsetStartMs": 3135, "OffsetEndMs": 3795}, {"Word": "uncertainty", "OffsetStartMs": 3795, "OffsetEndMs": 4130}, {"Word": "we", "OffsetStartMs": 4600, "OffsetEndMs": 4845}, {"Word": "can", "OffsetStartMs": 4845, "OffsetEndMs": 4965}, {"Word": "add", "OffsetStartMs": 4965, "OffsetEndMs": 5190}, {"Word": "dropout", "OffsetStartMs": 5190, "OffsetEndMs": 5595}, {"Word": "layers", "OffsetStartMs": 5595, "OffsetEndMs": 5910}, {"Word": "after", "OffsetStartMs": 5910, "OffsetEndMs": 6180}, {"Word": "every", "OffsetStartMs": 6180, "OffsetEndMs": 6560}, {"Word": "single", "OffsetStartMs": 6580, "OffsetEndMs": 6980}, {"Word": "layer", "OffsetStartMs": 7180, "OffsetEndMs": 7545}, {"Word": "in", "OffsetStartMs": 7545, "OffsetEndMs": 7770}, {"Word": "our", "OffsetStartMs": 7770, "OffsetEndMs": 7920}, {"Word": "model", "OffsetStartMs": 7920, "OffsetEndMs": 8210}, {"Word": "and", "OffsetStartMs": 8560, "OffsetEndMs": 8850}, {"Word": "in", "OffsetStartMs": 8850, "OffsetEndMs": 9060}, {"Word": "addition", "OffsetStartMs": 9060, "OffsetEndMs": 9380}, {"Word": "we", "OffsetStartMs": 9580, "OffsetEndMs": 9825}, {"Word": "can", "OffsetStartMs": 9825, "OffsetEndMs": 10020}, {"Word": "keep", "OffsetStartMs": 10020, "OffsetEndMs": 10245}, {"Word": "these", "OffsetStartMs": 10245, "OffsetEndMs": 10440}, {"Word": "dropout", "OffsetStartMs": 10440, "OffsetEndMs": 10770}, {"Word": "layers", "OffsetStartMs": 10770, "OffsetEndMs": 11025}, {"Word": "enabled", "OffsetStartMs": 11025, "OffsetEndMs": 11355}, {"Word": "at", "OffsetStartMs": 11355, "OffsetEndMs": 11655}, {"Word": "test", "OffsetStartMs": 11655, "OffsetEndMs": 11940}, {"Word": "time", "OffsetStartMs": 11940, "OffsetEndMs": 12290}, {"Word": ".", "OffsetStartMs": 12460, "OffsetEndMs": 12860}, {"Word": "Usuallywe", "OffsetStartMs": 13150, "OffsetEndMs": 13410}, {"Word": "don't", "OffsetStartMs": 13410, "OffsetEndMs": 13665}, {"Word": "keep", "OffsetStartMs": 13665, "OffsetEndMs": 13830}, {"Word": "dropout", "OffsetStartMs": 13830, "OffsetEndMs": 14175}, {"Word": "layers", "OffsetStartMs": 14175, "OffsetEndMs": 14430}, {"Word": "enabled", "OffsetStartMs": 14430, "OffsetEndMs": 14685}, {"Word": "at", "OffsetStartMs": 14685, "OffsetEndMs": 14895}, {"Word": "test", "OffsetStartMs": 14895, "OffsetEndMs": 15090}, {"Word": "time", "OffsetStartMs": 15090, "OffsetEndMs": 15315}, {"Word": "because", "OffsetStartMs": 15315, "OffsetEndMs": 15510}, {"Word": "we", "OffsetStartMs": 15510, "OffsetEndMs": 15615}, {"Word": "don't", "OffsetStartMs": 15615, "OffsetEndMs": 15825}, {"Word": "want", "OffsetStartMs": 15825, "OffsetEndMs": 15930}, {"Word": "to", "OffsetStartMs": 15930, "OffsetEndMs": 16050}, {"Word": "lose", "OffsetStartMs": 16050, "OffsetEndMs": 16230}, {"Word": "any", "OffsetStartMs": 16230, "OffsetEndMs": 16550}, {"Word": "information", "OffsetStartMs": 16660, "OffsetEndMs": 17060}, {"Word": "about", "OffsetStartMs": 17140, "OffsetEndMs": 17475}, {"Word": "the", "OffsetStartMs": 17475, "OffsetEndMs": 17810}, {"Word": "the", "OffsetStartMs": 17920, "OffsetEndMs": 18165}, {"Word": "network's", "OffsetStartMs": 18165, "OffsetEndMs": 18740}, {"Word": "process", "OffsetStartMs": 19840, "OffsetEndMs": 20235}, {"Word": "or", "OffsetStartMs": 20235, "OffsetEndMs": 20490}, {"Word": "any", "OffsetStartMs": 20490, "OffsetEndMs": 20685}, {"Word": "weights", "OffsetStartMs": 20685, "OffsetEndMs": 21105}, {"Word": "when", "OffsetStartMs": 21105, "OffsetEndMs": 21330}, {"Word": "we're", "OffsetStartMs": 21330, "OffsetEndMs": 21525}, {"Word": "at", "OffsetStartMs": 21525, "OffsetEndMs": 21675}, {"Word": "inference", "OffsetStartMs": 21675, "OffsetEndMs": 22095}, {"Word": "time", "OffsetStartMs": 22095, "OffsetEndMs": 22430}, {"Word": ".", "OffsetStartMs": 22990, "OffsetEndMs": 23385}, {"Word": "Howeverwhen", "OffsetStartMs": 23385, "OffsetEndMs": 23640}, {"Word": "we're", "OffsetStartMs": 23640, "OffsetEndMs": 23990}, {"Word": "estimating", "OffsetStartMs": 24040, "OffsetEndMs": 24450}, {"Word": "epistemic", "OffsetStartMs": 24450, "OffsetEndMs": 25110}, {"Word": "uncertainty", "OffsetStartMs": 25110, "OffsetEndMs": 25460}, {"Word": "we", "OffsetStartMs": 25780, "OffsetEndMs": 26070}, {"Word": "do", "OffsetStartMs": 26070, "OffsetEndMs": 26265}, {"Word": "want", "OffsetStartMs": 26265, "OffsetEndMs": 26475}, {"Word": "to", "OffsetStartMs": 26475, "OffsetEndMs": 26655}, {"Word": "keep", "OffsetStartMs": 26655, "OffsetEndMs": 26835}, {"Word": "dropout", "OffsetStartMs": 26835, "OffsetEndMs": 27210}, {"Word": "enabled", "OffsetStartMs": 27210, "OffsetEndMs": 27510}, {"Word": "at", "OffsetStartMs": 27510, "OffsetEndMs": 27765}, {"Word": "test", "OffsetStartMs": 27765, "OffsetEndMs": 27975}, {"Word": "time", "OffsetStartMs": 27975, "OffsetEndMs": 28280}, {"Word": "because", "OffsetStartMs": 28420, "OffsetEndMs": 28680}, {"Word": "that's", "OffsetStartMs": 28680, "OffsetEndMs": 28905}, {"Word": "how", "OffsetStartMs": 28905, "OffsetEndMs": 29040}, {"Word": "we", "OffsetStartMs": 29040, "OffsetEndMs": 29220}, {"Word": "can", "OffsetStartMs": 29220, "OffsetEndMs": 29510}, {"Word": "introduce", "OffsetStartMs": 29980, "OffsetEndMs": 30300}, {"Word": "randomness", "OffsetStartMs": 30300, "OffsetEndMs": 30890}, {"Word": "inference", "OffsetStartMs": 30970, "OffsetEndMs": 31470}, {"Word": "time", "OffsetStartMs": 31470, "OffsetEndMs": 31710}, {"Word": "as", "OffsetStartMs": 31710, "OffsetEndMs": 31920}, {"Word": "well", "OffsetStartMs": 31920, "OffsetEndMs": 32210}], "SpeechSpeed": 16.0}, {"FinalSentence": "So what we do here is we have one model. It's the same model the entire way through. We add dropout layers with a specific probability, and then we run multiple forward passes and at every forward passes, different layers get different nodes in a layer, get dropped out. So we have that measure of randomness and stochasticity.", "SliceSentence": "So what we do here is we have one model .It's the same model the entire way through . Weadd dropout layers with a specific probability and then we run multiple forward passes and at every forward passes different layers get different nodes in a layer get dropped out . Sowe have that measure of randomness and stochasticity", "StartMs": 2508200, "EndMs": 2526800, "WordsNum": 58, "Words": [{"Word": "So", "OffsetStartMs": 70, "OffsetEndMs": 315}, {"Word": "what", "OffsetStartMs": 315, "OffsetEndMs": 420}, {"Word": "we", "OffsetStartMs": 420, "OffsetEndMs": 555}, {"Word": "do", "OffsetStartMs": 555, "OffsetEndMs": 735}, {"Word": "here", "OffsetStartMs": 735, "OffsetEndMs": 1005}, {"Word": "is", "OffsetStartMs": 1005, "OffsetEndMs": 1260}, {"Word": "we", "OffsetStartMs": 1260, "OffsetEndMs": 1425}, {"Word": "have", "OffsetStartMs": 1425, "OffsetEndMs": 1635}, {"Word": "one", "OffsetStartMs": 1635, "OffsetEndMs": 1890}, {"Word": "model", "OffsetStartMs": 1890, "OffsetEndMs": 2160}, {"Word": ".It's", "OffsetStartMs": 2160, "OffsetEndMs": 2445}, {"Word": "the", "OffsetStartMs": 2445, "OffsetEndMs": 2565}, {"Word": "same", "OffsetStartMs": 2565, "OffsetEndMs": 2745}, {"Word": "model", "OffsetStartMs": 2745, "OffsetEndMs": 2970}, {"Word": "the", "OffsetStartMs": 2970, "OffsetEndMs": 3195}, {"Word": "entire", "OffsetStartMs": 3195, "OffsetEndMs": 3405}, {"Word": "way", "OffsetStartMs": 3405, "OffsetEndMs": 3615}, {"Word": "through", "OffsetStartMs": 3615, "OffsetEndMs": 3920}, {"Word": ".", "OffsetStartMs": 4330, "OffsetEndMs": 4575}, {"Word": "Weadd", "OffsetStartMs": 4575, "OffsetEndMs": 4740}, {"Word": "dropout", "OffsetStartMs": 4740, "OffsetEndMs": 5145}, {"Word": "layers", "OffsetStartMs": 5145, "OffsetEndMs": 5415}, {"Word": "with", "OffsetStartMs": 5415, "OffsetEndMs": 5535}, {"Word": "a", "OffsetStartMs": 5535, "OffsetEndMs": 5655}, {"Word": "specific", "OffsetStartMs": 5655, "OffsetEndMs": 5925}, {"Word": "probability", "OffsetStartMs": 5925, "OffsetEndMs": 6560}, {"Word": "and", "OffsetStartMs": 6760, "OffsetEndMs": 7050}, {"Word": "then", "OffsetStartMs": 7050, "OffsetEndMs": 7200}, {"Word": "we", "OffsetStartMs": 7200, "OffsetEndMs": 7350}, {"Word": "run", "OffsetStartMs": 7350, "OffsetEndMs": 7640}, {"Word": "multiple", "OffsetStartMs": 7780, "OffsetEndMs": 8180}, {"Word": "forward", "OffsetStartMs": 8230, "OffsetEndMs": 8595}, {"Word": "passes", "OffsetStartMs": 8595, "OffsetEndMs": 8960}, {"Word": "and", "OffsetStartMs": 9190, "OffsetEndMs": 9435}, {"Word": "at", "OffsetStartMs": 9435, "OffsetEndMs": 9570}, {"Word": "every", "OffsetStartMs": 9570, "OffsetEndMs": 9825}, {"Word": "forward", "OffsetStartMs": 9825, "OffsetEndMs": 10125}, {"Word": "passes", "OffsetStartMs": 10125, "OffsetEndMs": 10460}, {"Word": "different", "OffsetStartMs": 10720, "OffsetEndMs": 11085}, {"Word": "layers", "OffsetStartMs": 11085, "OffsetEndMs": 11505}, {"Word": "get", "OffsetStartMs": 11505, "OffsetEndMs": 11810}, {"Word": "different", "OffsetStartMs": 11920, "OffsetEndMs": 12270}, {"Word": "nodes", "OffsetStartMs": 12270, "OffsetEndMs": 12675}, {"Word": "in", "OffsetStartMs": 12675, "OffsetEndMs": 12795}, {"Word": "a", "OffsetStartMs": 12795, "OffsetEndMs": 12900}, {"Word": "layer", "OffsetStartMs": 12900, "OffsetEndMs": 13095}, {"Word": "get", "OffsetStartMs": 13095, "OffsetEndMs": 13320}, {"Word": "dropped", "OffsetStartMs": 13320, "OffsetEndMs": 13530}, {"Word": "out", "OffsetStartMs": 13530, "OffsetEndMs": 13850}, {"Word": ".", "OffsetStartMs": 14080, "OffsetEndMs": 14325}, {"Word": "Sowe", "OffsetStartMs": 14325, "OffsetEndMs": 14445}, {"Word": "have", "OffsetStartMs": 14445, "OffsetEndMs": 14625}, {"Word": "that", "OffsetStartMs": 14625, "OffsetEndMs": 14930}, {"Word": "measure", "OffsetStartMs": 15250, "OffsetEndMs": 15615}, {"Word": "of", "OffsetStartMs": 15615, "OffsetEndMs": 15980}, {"Word": "randomness", "OffsetStartMs": 16060, "OffsetEndMs": 16695}, {"Word": "and", "OffsetStartMs": 16695, "OffsetEndMs": 16905}, {"Word": "stochasticity", "OffsetStartMs": 16905, "OffsetEndMs": 17750}], "SpeechSpeed": 17.2}, {"FinalSentence": "So again, in order to implement this, what we have is a model with the exact one model. And then when we're running our forward passes, we can simply run t forward passes where t is usually a number like twenty EM. We keep dropout enabled at test time, and then we use the mean of these samples as the new prediction and the variance of these samples as a measure of epistemic uncertainty.", "SliceSentence": "So again in order to implement this what we have is a model with the exact one model . Andthen when we're running our forward passes we can simply run t forward passes where t is usually a number like twenty EM . Wekeep dropout enabled at test time and then we use the mean of these samples as the new prediction and the variance of these samples as a measure of epistemic uncertainty", "StartMs": 2527800, "EndMs": 2552340, "WordsNum": 74, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 450}, {"Word": "again", "OffsetStartMs": 450, "OffsetEndMs": 735}, {"Word": "in", "OffsetStartMs": 735, "OffsetEndMs": 960}, {"Word": "order", "OffsetStartMs": 960, "OffsetEndMs": 1140}, {"Word": "to", "OffsetStartMs": 1140, "OffsetEndMs": 1455}, {"Word": "implement", "OffsetStartMs": 1455, "OffsetEndMs": 1800}, {"Word": "this", "OffsetStartMs": 1800, "OffsetEndMs": 2145}, {"Word": "what", "OffsetStartMs": 2145, "OffsetEndMs": 2400}, {"Word": "we", "OffsetStartMs": 2400, "OffsetEndMs": 2535}, {"Word": "have", "OffsetStartMs": 2535, "OffsetEndMs": 2805}, {"Word": "is", "OffsetStartMs": 2805, "OffsetEndMs": 3120}, {"Word": "a", "OffsetStartMs": 3120, "OffsetEndMs": 3360}, {"Word": "model", "OffsetStartMs": 3360, "OffsetEndMs": 3680}, {"Word": "with", "OffsetStartMs": 4030, "OffsetEndMs": 4380}, {"Word": "the", "OffsetStartMs": 4380, "OffsetEndMs": 4605}, {"Word": "exact", "OffsetStartMs": 4605, "OffsetEndMs": 4880}, {"Word": "one", "OffsetStartMs": 4960, "OffsetEndMs": 5265}, {"Word": "model", "OffsetStartMs": 5265, "OffsetEndMs": 5570}, {"Word": ".", "OffsetStartMs": 5680, "OffsetEndMs": 5955}, {"Word": "Andthen", "OffsetStartMs": 5955, "OffsetEndMs": 6105}, {"Word": "when", "OffsetStartMs": 6105, "OffsetEndMs": 6270}, {"Word": "we're", "OffsetStartMs": 6270, "OffsetEndMs": 6675}, {"Word": "running", "OffsetStartMs": 6675, "OffsetEndMs": 6945}, {"Word": "our", "OffsetStartMs": 6945, "OffsetEndMs": 7155}, {"Word": "forward", "OffsetStartMs": 7155, "OffsetEndMs": 7425}, {"Word": "passes", "OffsetStartMs": 7425, "OffsetEndMs": 7790}, {"Word": "we", "OffsetStartMs": 8110, "OffsetEndMs": 8370}, {"Word": "can", "OffsetStartMs": 8370, "OffsetEndMs": 8520}, {"Word": "simply", "OffsetStartMs": 8520, "OffsetEndMs": 8790}, {"Word": "run", "OffsetStartMs": 8790, "OffsetEndMs": 9150}, {"Word": "t", "OffsetStartMs": 9150, "OffsetEndMs": 9450}, {"Word": "forward", "OffsetStartMs": 9450, "OffsetEndMs": 9735}, {"Word": "passes", "OffsetStartMs": 9735, "OffsetEndMs": 10095}, {"Word": "where", "OffsetStartMs": 10095, "OffsetEndMs": 10380}, {"Word": "t", "OffsetStartMs": 10380, "OffsetEndMs": 10530}, {"Word": "is", "OffsetStartMs": 10530, "OffsetEndMs": 10650}, {"Word": "usually", "OffsetStartMs": 10650, "OffsetEndMs": 10845}, {"Word": "a", "OffsetStartMs": 10845, "OffsetEndMs": 11010}, {"Word": "number", "OffsetStartMs": 11010, "OffsetEndMs": 11175}, {"Word": "like", "OffsetStartMs": 11175, "OffsetEndMs": 11415}, {"Word": "twenty", "OffsetStartMs": 11415, "OffsetEndMs": 11720}, {"Word": "EM", "OffsetStartMs": 12040, "OffsetEndMs": 12435}, {"Word": ".", "OffsetStartMs": 12435, "OffsetEndMs": 12735}, {"Word": "Wekeep", "OffsetStartMs": 12735, "OffsetEndMs": 12945}, {"Word": "dropout", "OffsetStartMs": 12945, "OffsetEndMs": 13305}, {"Word": "enabled", "OffsetStartMs": 13305, "OffsetEndMs": 13635}, {"Word": "at", "OffsetStartMs": 13635, "OffsetEndMs": 13890}, {"Word": "test", "OffsetStartMs": 13890, "OffsetEndMs": 14115}, {"Word": "time", "OffsetStartMs": 14115, "OffsetEndMs": 14450}, {"Word": "and", "OffsetStartMs": 14500, "OffsetEndMs": 14775}, {"Word": "then", "OffsetStartMs": 14775, "OffsetEndMs": 14955}, {"Word": "we", "OffsetStartMs": 14955, "OffsetEndMs": 15195}, {"Word": "use", "OffsetStartMs": 15195, "OffsetEndMs": 15530}, {"Word": "the", "OffsetStartMs": 15550, "OffsetEndMs": 15945}, {"Word": "mean", "OffsetStartMs": 15945, "OffsetEndMs": 16305}, {"Word": "of", "OffsetStartMs": 16305, "OffsetEndMs": 16575}, {"Word": "these", "OffsetStartMs": 16575, "OffsetEndMs": 16880}, {"Word": "samples", "OffsetStartMs": 17140, "OffsetEndMs": 17700}, {"Word": "as", "OffsetStartMs": 17700, "OffsetEndMs": 18080}, {"Word": "the", "OffsetStartMs": 18460, "OffsetEndMs": 18860}, {"Word": "new", "OffsetStartMs": 19120, "OffsetEndMs": 19425}, {"Word": "prediction", "OffsetStartMs": 19425, "OffsetEndMs": 19820}, {"Word": "and", "OffsetStartMs": 19840, "OffsetEndMs": 20115}, {"Word": "the", "OffsetStartMs": 20115, "OffsetEndMs": 20235}, {"Word": "variance", "OffsetStartMs": 20235, "OffsetEndMs": 20535}, {"Word": "of", "OffsetStartMs": 20535, "OffsetEndMs": 20760}, {"Word": "these", "OffsetStartMs": 20760, "OffsetEndMs": 21110}, {"Word": "samples", "OffsetStartMs": 21190, "OffsetEndMs": 21705}, {"Word": "as", "OffsetStartMs": 21705, "OffsetEndMs": 22005}, {"Word": "a", "OffsetStartMs": 22005, "OffsetEndMs": 22260}, {"Word": "measure", "OffsetStartMs": 22260, "OffsetEndMs": 22485}, {"Word": "of", "OffsetStartMs": 22485, "OffsetEndMs": 22725}, {"Word": "epistemic", "OffsetStartMs": 22725, "OffsetEndMs": 23340}, {"Word": "uncertainty", "OffsetStartMs": 23340, "OffsetEndMs": 23690}], "SpeechSpeed": 15.6}, {"FinalSentence": "So both of the methods we talked about just now involves sampling, and sampling is expensive. Ense sampling is very expensive, but even if you have a pretty large model.", "SliceSentence": "So both of the methods we talked about just now involves sampling and sampling is expensive . Ensesampling is very expensive but even if you have a pretty large model", "StartMs": 2554140, "EndMs": 2565400, "WordsNum": 30, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "both", "OffsetStartMs": 1240, "OffsetEndMs": 1530}, {"Word": "of", "OffsetStartMs": 1530, "OffsetEndMs": 1680}, {"Word": "the", "OffsetStartMs": 1680, "OffsetEndMs": 1875}, {"Word": "methods", "OffsetStartMs": 1875, "OffsetEndMs": 2190}, {"Word": "we", "OffsetStartMs": 2190, "OffsetEndMs": 2430}, {"Word": "talked", "OffsetStartMs": 2430, "OffsetEndMs": 2625}, {"Word": "about", "OffsetStartMs": 2625, "OffsetEndMs": 2850}, {"Word": "just", "OffsetStartMs": 2850, "OffsetEndMs": 3045}, {"Word": "now", "OffsetStartMs": 3045, "OffsetEndMs": 3350}, {"Word": "involves", "OffsetStartMs": 3430, "OffsetEndMs": 3810}, {"Word": "sampling", "OffsetStartMs": 3810, "OffsetEndMs": 4430}, {"Word": "and", "OffsetStartMs": 4540, "OffsetEndMs": 4830}, {"Word": "sampling", "OffsetStartMs": 4830, "OffsetEndMs": 5220}, {"Word": "is", "OffsetStartMs": 5220, "OffsetEndMs": 5475}, {"Word": "expensive", "OffsetStartMs": 5475, "OffsetEndMs": 5840}, {"Word": ".", "OffsetStartMs": 5920, "OffsetEndMs": 6315}, {"Word": "Ensesampling", "OffsetStartMs": 6315, "OffsetEndMs": 6600}, {"Word": "is", "OffsetStartMs": 6600, "OffsetEndMs": 6795}, {"Word": "very", "OffsetStartMs": 6795, "OffsetEndMs": 7100}, {"Word": "expensive", "OffsetStartMs": 7150, "OffsetEndMs": 7550}, {"Word": "but", "OffsetStartMs": 7840, "OffsetEndMs": 8115}, {"Word": "even", "OffsetStartMs": 8115, "OffsetEndMs": 8295}, {"Word": "if", "OffsetStartMs": 8295, "OffsetEndMs": 8460}, {"Word": "you", "OffsetStartMs": 8460, "OffsetEndMs": 8580}, {"Word": "have", "OffsetStartMs": 8580, "OffsetEndMs": 8700}, {"Word": "a", "OffsetStartMs": 8700, "OffsetEndMs": 8850}, {"Word": "pretty", "OffsetStartMs": 8850, "OffsetEndMs": 9060}, {"Word": "large", "OffsetStartMs": 9060, "OffsetEndMs": 9360}, {"Word": "model", "OffsetStartMs": 9360, "OffsetEndMs": 9740}], "SpeechSpeed": 14.7}, {"FinalSentence": "Having our introducing dropout layers and calling twenty forward passes might also be something that's pretty infeasible. And at themus, we're dedicated to developing innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more industries and people.", "SliceSentence": "Having our introducing dropout layers and calling twenty forward passes might also be something that's pretty infeasible . Andat themus we're dedicated to developing innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more industries and people", "StartMs": 2565400, "EndMs": 2585680, "WordsNum": 50, "Words": [{"Word": "Having", "OffsetStartMs": 0, "OffsetEndMs": 290}, {"Word": "our", "OffsetStartMs": 610, "OffsetEndMs": 915}, {"Word": "introducing", "OffsetStartMs": 915, "OffsetEndMs": 1365}, {"Word": "dropout", "OffsetStartMs": 1365, "OffsetEndMs": 1755}, {"Word": "layers", "OffsetStartMs": 1755, "OffsetEndMs": 2070}, {"Word": "and", "OffsetStartMs": 2070, "OffsetEndMs": 2250}, {"Word": "calling", "OffsetStartMs": 2250, "OffsetEndMs": 2505}, {"Word": "twenty", "OffsetStartMs": 2505, "OffsetEndMs": 2820}, {"Word": "forward", "OffsetStartMs": 2820, "OffsetEndMs": 3135}, {"Word": "passes", "OffsetStartMs": 3135, "OffsetEndMs": 3500}, {"Word": "might", "OffsetStartMs": 3700, "OffsetEndMs": 4065}, {"Word": "also", "OffsetStartMs": 4065, "OffsetEndMs": 4290}, {"Word": "be", "OffsetStartMs": 4290, "OffsetEndMs": 4410}, {"Word": "something", "OffsetStartMs": 4410, "OffsetEndMs": 4635}, {"Word": "that's", "OffsetStartMs": 4635, "OffsetEndMs": 4950}, {"Word": "pretty", "OffsetStartMs": 4950, "OffsetEndMs": 5115}, {"Word": "infeasible", "OffsetStartMs": 5115, "OffsetEndMs": 5840}, {"Word": ".", "OffsetStartMs": 6340, "OffsetEndMs": 6740}, {"Word": "Andat", "OffsetStartMs": 6760, "OffsetEndMs": 7080}, {"Word": "themus", "OffsetStartMs": 7080, "OffsetEndMs": 7580}, {"Word": "we're", "OffsetStartMs": 7750, "OffsetEndMs": 8100}, {"Word": "dedicated", "OffsetStartMs": 8100, "OffsetEndMs": 8360}, {"Word": "to", "OffsetStartMs": 8680, "OffsetEndMs": 9080}, {"Word": "developing", "OffsetStartMs": 9100, "OffsetEndMs": 9500}, {"Word": "innovative", "OffsetStartMs": 9940, "OffsetEndMs": 10275}, {"Word": "methods", "OffsetStartMs": 10275, "OffsetEndMs": 10590}, {"Word": "of", "OffsetStartMs": 10590, "OffsetEndMs": 10970}, {"Word": "estimating", "OffsetStartMs": 10990, "OffsetEndMs": 11400}, {"Word": "epistemic", "OffsetStartMs": 11400, "OffsetEndMs": 12045}, {"Word": "uncertainty", "OffsetStartMs": 12045, "OffsetEndMs": 12380}, {"Word": "that", "OffsetStartMs": 12760, "OffsetEndMs": 13080}, {"Word": "don't", "OffsetStartMs": 13080, "OffsetEndMs": 13470}, {"Word": "rely", "OffsetStartMs": 13470, "OffsetEndMs": 13710}, {"Word": "on", "OffsetStartMs": 13710, "OffsetEndMs": 13935}, {"Word": "things", "OffsetStartMs": 13935, "OffsetEndMs": 14145}, {"Word": "like", "OffsetStartMs": 14145, "OffsetEndMs": 14355}, {"Word": "sampling", "OffsetStartMs": 14355, "OffsetEndMs": 14880}, {"Word": "so", "OffsetStartMs": 14880, "OffsetEndMs": 15105}, {"Word": "that", "OffsetStartMs": 15105, "OffsetEndMs": 15240}, {"Word": "they're", "OffsetStartMs": 15240, "OffsetEndMs": 15465}, {"Word": "more", "OffsetStartMs": 15465, "OffsetEndMs": 15630}, {"Word": "generalizable", "OffsetStartMs": 15630, "OffsetEndMs": 16520}, {"Word": "and", "OffsetStartMs": 16570, "OffsetEndMs": 16875}, {"Word": "they're", "OffsetStartMs": 16875, "OffsetEndMs": 17115}, {"Word": "usable", "OffsetStartMs": 17115, "OffsetEndMs": 17520}, {"Word": "by", "OffsetStartMs": 17520, "OffsetEndMs": 17715}, {"Word": "more", "OffsetStartMs": 17715, "OffsetEndMs": 18020}, {"Word": "industries", "OffsetStartMs": 18550, "OffsetEndMs": 18825}, {"Word": "and", "OffsetStartMs": 18825, "OffsetEndMs": 18990}, {"Word": "people", "OffsetStartMs": 18990, "OffsetEndMs": 19280}], "SpeechSpeed": 16.9}, {"FinalSentence": "So a method that we've developed to estimate a method that we've studied to estimate epistemic uncertainty is by using generative modeling. So we've talked about V A couple times now, but let's say I trained AV on the exact same data set we were talking about earlier, which is only dogs and cats. The latent space of this model would be comprised of features that relate to dogs and cats. And if I give it a prototypical dog, it should be able to generate a pretty good representation of this dog, and it should have pretty low reconstruction loss.", "SliceSentence": "So a method that we've developed to estimate a method that we've studied to estimate epistemic uncertainty is by using generative modeling . Sowe've talked about V A couple times now but let's say I trained AV on the exact same data set we were talking about earlier which is only dogs and cats . Thelatent space of this model would be comprised of features that relate to dogs and cats . Andif I give it a prototypical dog it should be able to generate a pretty good representation of this dog and it should have pretty low reconstruction loss", "StartMs": 2585680, "EndMs": 2619100, "WordsNum": 100, "Words": [{"Word": "So", "OffsetStartMs": 0, "OffsetEndMs": 350}, {"Word": "a", "OffsetStartMs": 490, "OffsetEndMs": 765}, {"Word": "method", "OffsetStartMs": 765, "OffsetEndMs": 975}, {"Word": "that", "OffsetStartMs": 975, "OffsetEndMs": 1170}, {"Word": "we've", "OffsetStartMs": 1170, "OffsetEndMs": 1440}, {"Word": "developed", "OffsetStartMs": 1440, "OffsetEndMs": 1730}, {"Word": "to", "OffsetStartMs": 1840, "OffsetEndMs": 2240}, {"Word": "estimate", "OffsetStartMs": 2290, "OffsetEndMs": 2690}, {"Word": "a", "OffsetStartMs": 2890, "OffsetEndMs": 3150}, {"Word": "method", "OffsetStartMs": 3150, "OffsetEndMs": 3300}, {"Word": "that", "OffsetStartMs": 3300, "OffsetEndMs": 3450}, {"Word": "we've", "OffsetStartMs": 3450, "OffsetEndMs": 3645}, {"Word": "studied", "OffsetStartMs": 3645, "OffsetEndMs": 3920}, {"Word": "to", "OffsetStartMs": 3970, "OffsetEndMs": 4370}, {"Word": "estimate", "OffsetStartMs": 4750, "OffsetEndMs": 5025}, {"Word": "epistemic", "OffsetStartMs": 5025, "OffsetEndMs": 5670}, {"Word": "uncertainty", "OffsetStartMs": 5670, "OffsetEndMs": 6020}, {"Word": "is", "OffsetStartMs": 6400, "OffsetEndMs": 6675}, {"Word": "by", "OffsetStartMs": 6675, "OffsetEndMs": 6825}, {"Word": "using", "OffsetStartMs": 6825, "OffsetEndMs": 7100}, {"Word": "generative", "OffsetStartMs": 7180, "OffsetEndMs": 7695}, {"Word": "modeling", "OffsetStartMs": 7695, "OffsetEndMs": 8210}, {"Word": ".", "OffsetStartMs": 8770, "OffsetEndMs": 9120}, {"Word": "Sowe've", "OffsetStartMs": 9120, "OffsetEndMs": 9405}, {"Word": "talked", "OffsetStartMs": 9405, "OffsetEndMs": 9585}, {"Word": "about", "OffsetStartMs": 9585, "OffsetEndMs": 9810}, {"Word": "V", "OffsetStartMs": 9810, "OffsetEndMs": 10095}, {"Word": "A", "OffsetStartMs": 10095, "OffsetEndMs": 10350}, {"Word": "couple", "OffsetStartMs": 10350, "OffsetEndMs": 10530}, {"Word": "times", "OffsetStartMs": 10530, "OffsetEndMs": 10785}, {"Word": "now", "OffsetStartMs": 10785, "OffsetEndMs": 11120}, {"Word": "but", "OffsetStartMs": 11410, "OffsetEndMs": 11810}, {"Word": "let's", "OffsetStartMs": 11980, "OffsetEndMs": 12375}, {"Word": "say", "OffsetStartMs": 12375, "OffsetEndMs": 12555}, {"Word": "I", "OffsetStartMs": 12555, "OffsetEndMs": 12810}, {"Word": "trained", "OffsetStartMs": 12810, "OffsetEndMs": 13065}, {"Word": "AV", "OffsetStartMs": 13290, "OffsetEndMs": 13580}, {"Word": "on", "OffsetStartMs": 13930, "OffsetEndMs": 14330}, {"Word": "the", "OffsetStartMs": 14380, "OffsetEndMs": 14730}, {"Word": "exact", "OffsetStartMs": 14730, "OffsetEndMs": 15000}, {"Word": "same", "OffsetStartMs": 15000, "OffsetEndMs": 15210}, {"Word": "data", "OffsetStartMs": 15210, "OffsetEndMs": 15405}, {"Word": "set", "OffsetStartMs": 15405, "OffsetEndMs": 15570}, {"Word": "we", "OffsetStartMs": 15570, "OffsetEndMs": 15675}, {"Word": "were", "OffsetStartMs": 15675, "OffsetEndMs": 15810}, {"Word": "talking", "OffsetStartMs": 15810, "OffsetEndMs": 16050}, {"Word": "about", "OffsetStartMs": 16050, "OffsetEndMs": 16275}, {"Word": "earlier", "OffsetStartMs": 16275, "OffsetEndMs": 16550}, {"Word": "which", "OffsetStartMs": 16690, "OffsetEndMs": 16935}, {"Word": "is", "OffsetStartMs": 16935, "OffsetEndMs": 17055}, {"Word": "only", "OffsetStartMs": 17055, "OffsetEndMs": 17295}, {"Word": "dogs", "OffsetStartMs": 17295, "OffsetEndMs": 17640}, {"Word": "and", "OffsetStartMs": 17640, "OffsetEndMs": 17925}, {"Word": "cats", "OffsetStartMs": 17925, "OffsetEndMs": 18230}, {"Word": ".", "OffsetStartMs": 19030, "OffsetEndMs": 19290}, {"Word": "Thelatent", "OffsetStartMs": 19290, "OffsetEndMs": 19620}, {"Word": "space", "OffsetStartMs": 19620, "OffsetEndMs": 19875}, {"Word": "of", "OffsetStartMs": 19875, "OffsetEndMs": 20100}, {"Word": "this", "OffsetStartMs": 20100, "OffsetEndMs": 20250}, {"Word": "model", "OffsetStartMs": 20250, "OffsetEndMs": 20540}, {"Word": "would", "OffsetStartMs": 20620, "OffsetEndMs": 20895}, {"Word": "be", "OffsetStartMs": 20895, "OffsetEndMs": 21045}, {"Word": "comprised", "OffsetStartMs": 21045, "OffsetEndMs": 21500}, {"Word": "of", "OffsetStartMs": 21640, "OffsetEndMs": 21990}, {"Word": "features", "OffsetStartMs": 21990, "OffsetEndMs": 22335}, {"Word": "that", "OffsetStartMs": 22335, "OffsetEndMs": 22650}, {"Word": "relate", "OffsetStartMs": 22650, "OffsetEndMs": 22920}, {"Word": "to", "OffsetStartMs": 22920, "OffsetEndMs": 23130}, {"Word": "dogs", "OffsetStartMs": 23130, "OffsetEndMs": 23355}, {"Word": "and", "OffsetStartMs": 23355, "OffsetEndMs": 23625}, {"Word": "cats", "OffsetStartMs": 23625, "OffsetEndMs": 23930}, {"Word": ".", "OffsetStartMs": 24190, "OffsetEndMs": 24435}, {"Word": "Andif", "OffsetStartMs": 24435, "OffsetEndMs": 24540}, {"Word": "I", "OffsetStartMs": 24540, "OffsetEndMs": 24690}, {"Word": "give", "OffsetStartMs": 24690, "OffsetEndMs": 24840}, {"Word": "it", "OffsetStartMs": 24840, "OffsetEndMs": 24975}, {"Word": "a", "OffsetStartMs": 24975, "OffsetEndMs": 25170}, {"Word": "prototypical", "OffsetStartMs": 25170, "OffsetEndMs": 25830}, {"Word": "dog", "OffsetStartMs": 25830, "OffsetEndMs": 26120}, {"Word": "it", "OffsetStartMs": 26410, "OffsetEndMs": 26715}, {"Word": "should", "OffsetStartMs": 26715, "OffsetEndMs": 26880}, {"Word": "be", "OffsetStartMs": 26880, "OffsetEndMs": 27000}, {"Word": "able", "OffsetStartMs": 27000, "OffsetEndMs": 27195}, {"Word": "to", "OffsetStartMs": 27195, "OffsetEndMs": 27390}, {"Word": "generate", "OffsetStartMs": 27390, "OffsetEndMs": 27645}, {"Word": "a", "OffsetStartMs": 27645, "OffsetEndMs": 27945}, {"Word": "pretty", "OffsetStartMs": 27945, "OffsetEndMs": 28185}, {"Word": "good", "OffsetStartMs": 28185, "OffsetEndMs": 28410}, {"Word": "representation", "OffsetStartMs": 28410, "OffsetEndMs": 29070}, {"Word": "of", "OffsetStartMs": 29070, "OffsetEndMs": 29310}, {"Word": "this", "OffsetStartMs": 29310, "OffsetEndMs": 29490}, {"Word": "dog", "OffsetStartMs": 29490, "OffsetEndMs": 29780}, {"Word": "and", "OffsetStartMs": 30070, "OffsetEndMs": 30315}, {"Word": "it", "OffsetStartMs": 30315, "OffsetEndMs": 30435}, {"Word": "should", "OffsetStartMs": 30435, "OffsetEndMs": 30585}, {"Word": "have", "OffsetStartMs": 30585, "OffsetEndMs": 30795}, {"Word": "pretty", "OffsetStartMs": 30795, "OffsetEndMs": 31080}, {"Word": "low", "OffsetStartMs": 31080, "OffsetEndMs": 31410}, {"Word": "reconstruction", "OffsetStartMs": 31410, "OffsetEndMs": 32085}, {"Word": "loss", "OffsetStartMs": 32085, "OffsetEndMs": 32450}], "SpeechSpeed": 16.2}, {"FinalSentence": "Now, if I gave the same example of the horse to this V A, the latent vector that this horse would be decoded to would be incomprehensible to the decoder of this network. The decoder wouldn't be able to know how to project the latent vector back into the original input space, and therefore we should expect to see a much worse reconstruction here. And we should see that the reconstruction loss is much higher than if we gave the model a familiar input or something that it was used to seeing.", "SliceSentence": "Now if I gave the same example of the horse to this V A the latent vector that this horse would be decoded to would be incomprehensible to the decoder of this network . Thedecoder wouldn't be able to know how to project the latent vector back into the original input space and therefore we should expect to see a much worse reconstruction here . Andwe should see that the reconstruction loss is much higher than if we gave the model a familiar input or something that it was used to seeing", "StartMs": 2620420, "EndMs": 2648880, "WordsNum": 92, "Words": [{"Word": "Now", "OffsetStartMs": 160, "OffsetEndMs": 525}, {"Word": "if", "OffsetStartMs": 525, "OffsetEndMs": 750}, {"Word": "I", "OffsetStartMs": 750, "OffsetEndMs": 900}, {"Word": "gave", "OffsetStartMs": 900, "OffsetEndMs": 1095}, {"Word": "the", "OffsetStartMs": 1095, "OffsetEndMs": 1275}, {"Word": "same", "OffsetStartMs": 1275, "OffsetEndMs": 1500}, {"Word": "example", "OffsetStartMs": 1500, "OffsetEndMs": 1850}, {"Word": "of", "OffsetStartMs": 2050, "OffsetEndMs": 2385}, {"Word": "the", "OffsetStartMs": 2385, "OffsetEndMs": 2565}, {"Word": "horse", "OffsetStartMs": 2565, "OffsetEndMs": 2810}, {"Word": "to", "OffsetStartMs": 2920, "OffsetEndMs": 3180}, {"Word": "this", "OffsetStartMs": 3180, "OffsetEndMs": 3345}, {"Word": "V", "OffsetStartMs": 3345, "OffsetEndMs": 3495}, {"Word": "A", "OffsetStartMs": 3495, "OffsetEndMs": 3740}, {"Word": "the", "OffsetStartMs": 4690, "OffsetEndMs": 5090}, {"Word": "latent", "OffsetStartMs": 5500, "OffsetEndMs": 5910}, {"Word": "vector", "OffsetStartMs": 5910, "OffsetEndMs": 6210}, {"Word": "that", "OffsetStartMs": 6210, "OffsetEndMs": 6465}, {"Word": "this", "OffsetStartMs": 6465, "OffsetEndMs": 6630}, {"Word": "horse", "OffsetStartMs": 6630, "OffsetEndMs": 6840}, {"Word": "would", "OffsetStartMs": 6840, "OffsetEndMs": 7020}, {"Word": "be", "OffsetStartMs": 7020, "OffsetEndMs": 7140}, {"Word": "decoded", "OffsetStartMs": 7140, "OffsetEndMs": 7620}, {"Word": "to", "OffsetStartMs": 7620, "OffsetEndMs": 7910}, {"Word": "would", "OffsetStartMs": 8020, "OffsetEndMs": 8310}, {"Word": "be", "OffsetStartMs": 8310, "OffsetEndMs": 8490}, {"Word": "incomprehensible", "OffsetStartMs": 8490, "OffsetEndMs": 9510}, {"Word": "to", "OffsetStartMs": 9510, "OffsetEndMs": 9690}, {"Word": "the", "OffsetStartMs": 9690, "OffsetEndMs": 9780}, {"Word": "decoder", "OffsetStartMs": 9780, "OffsetEndMs": 10215}, {"Word": "of", "OffsetStartMs": 10215, "OffsetEndMs": 10350}, {"Word": "this", "OffsetStartMs": 10350, "OffsetEndMs": 10515}, {"Word": "network", "OffsetStartMs": 10515, "OffsetEndMs": 10820}, {"Word": ".", "OffsetStartMs": 11200, "OffsetEndMs": 11445}, {"Word": "Thedecoder", "OffsetStartMs": 11445, "OffsetEndMs": 11805}, {"Word": "wouldn't", "OffsetStartMs": 11805, "OffsetEndMs": 12075}, {"Word": "be", "OffsetStartMs": 12075, "OffsetEndMs": 12195}, {"Word": "able", "OffsetStartMs": 12195, "OffsetEndMs": 12405}, {"Word": "to", "OffsetStartMs": 12405, "OffsetEndMs": 12630}, {"Word": "know", "OffsetStartMs": 12630, "OffsetEndMs": 12810}, {"Word": "how", "OffsetStartMs": 12810, "OffsetEndMs": 12990}, {"Word": "to", "OffsetStartMs": 12990, "OffsetEndMs": 13200}, {"Word": "project", "OffsetStartMs": 13200, "OffsetEndMs": 13545}, {"Word": "the", "OffsetStartMs": 13545, "OffsetEndMs": 13815}, {"Word": "latent", "OffsetStartMs": 13815, "OffsetEndMs": 14130}, {"Word": "vector", "OffsetStartMs": 14130, "OffsetEndMs": 14420}, {"Word": "back", "OffsetStartMs": 14620, "OffsetEndMs": 14970}, {"Word": "into", "OffsetStartMs": 14970, "OffsetEndMs": 15225}, {"Word": "the", "OffsetStartMs": 15225, "OffsetEndMs": 15390}, {"Word": "original", "OffsetStartMs": 15390, "OffsetEndMs": 15650}, {"Word": "input", "OffsetStartMs": 15760, "OffsetEndMs": 16095}, {"Word": "space", "OffsetStartMs": 16095, "OffsetEndMs": 16430}, {"Word": "and", "OffsetStartMs": 16780, "OffsetEndMs": 17180}, {"Word": "therefore", "OffsetStartMs": 17200, "OffsetEndMs": 17505}, {"Word": "we", "OffsetStartMs": 17505, "OffsetEndMs": 17685}, {"Word": "should", "OffsetStartMs": 17685, "OffsetEndMs": 17880}, {"Word": "expect", "OffsetStartMs": 17880, "OffsetEndMs": 18075}, {"Word": "to", "OffsetStartMs": 18075, "OffsetEndMs": 18240}, {"Word": "see", "OffsetStartMs": 18240, "OffsetEndMs": 18435}, {"Word": "a", "OffsetStartMs": 18435, "OffsetEndMs": 18660}, {"Word": "much", "OffsetStartMs": 18660, "OffsetEndMs": 18975}, {"Word": "worse", "OffsetStartMs": 18975, "OffsetEndMs": 19350}, {"Word": "reconstruction", "OffsetStartMs": 19350, "OffsetEndMs": 20070}, {"Word": "here", "OffsetStartMs": 20070, "OffsetEndMs": 20450}, {"Word": ".", "OffsetStartMs": 20680, "OffsetEndMs": 20985}, {"Word": "Andwe", "OffsetStartMs": 20985, "OffsetEndMs": 21165}, {"Word": "should", "OffsetStartMs": 21165, "OffsetEndMs": 21330}, {"Word": "see", "OffsetStartMs": 21330, "OffsetEndMs": 21620}, {"Word": "that", "OffsetStartMs": 21850, "OffsetEndMs": 22125}, {"Word": "the", "OffsetStartMs": 22125, "OffsetEndMs": 22245}, {"Word": "reconstruction", "OffsetStartMs": 22245, "OffsetEndMs": 22740}, {"Word": "loss", "OffsetStartMs": 22740, "OffsetEndMs": 23010}, {"Word": "is", "OffsetStartMs": 23010, "OffsetEndMs": 23235}, {"Word": "much", "OffsetStartMs": 23235, "OffsetEndMs": 23460}, {"Word": "higher", "OffsetStartMs": 23460, "OffsetEndMs": 23780}, {"Word": "than", "OffsetStartMs": 23800, "OffsetEndMs": 24045}, {"Word": "if", "OffsetStartMs": 24045, "OffsetEndMs": 24165}, {"Word": "we", "OffsetStartMs": 24165, "OffsetEndMs": 24330}, {"Word": "gave", "OffsetStartMs": 24330, "OffsetEndMs": 24555}, {"Word": "the", "OffsetStartMs": 24555, "OffsetEndMs": 24750}, {"Word": "model", "OffsetStartMs": 24750, "OffsetEndMs": 24975}, {"Word": "a", "OffsetStartMs": 24975, "OffsetEndMs": 25230}, {"Word": "familiar", "OffsetStartMs": 25230, "OffsetEndMs": 25520}, {"Word": "input", "OffsetStartMs": 25780, "OffsetEndMs": 26070}, {"Word": "or", "OffsetStartMs": 26070, "OffsetEndMs": 26235}, {"Word": "something", "OffsetStartMs": 26235, "OffsetEndMs": 26430}, {"Word": "that", "OffsetStartMs": 26430, "OffsetEndMs": 26595}, {"Word": "it", "OffsetStartMs": 26595, "OffsetEndMs": 26700}, {"Word": "was", "OffsetStartMs": 26700, "OffsetEndMs": 26850}, {"Word": "used", "OffsetStartMs": 26850, "OffsetEndMs": 27075}, {"Word": "to", "OffsetStartMs": 27075, "OffsetEndMs": 27270}, {"Word": "seeing", "OffsetStartMs": 27270, "OffsetEndMs": 27530}], "SpeechSpeed": 17.1}, {"FinalSentence": "So now let's move on to what I think is the most exciting method of estimating epistemic uncertainty that we'll talk about today. So in both of the examples before, EM sampling is compute intensive, but generative modeling can also be compute intensive. Let's say you don't actually need a variational auto encoder for your task, then you're training an entire decoder for no reason and other than to estimate the epistemic uncertainty, so what if we had a method that did not rely on generative modeling or sampling in order to estimate the epistemic uncertainty?", "SliceSentence": "So now let's move on to what I think is the most exciting method of estimating epistemic uncertainty that we'll talk about today . Soin both of the examples before EM sampling is compute intensive but generative modeling can also be compute intensive .Let's say you don't actually need a variational auto encoder for your task then you're training an entire decoder for no reason and other than to estimate the epistemic uncertainty so what if we had a method that did not rely on generative modeling or sampling in order to estimate the epistemic uncertainty", "StartMs": 2652840, "EndMs": 2684600, "WordsNum": 96, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 530}, {"Word": "now", "OffsetStartMs": 550, "OffsetEndMs": 825}, {"Word": "let's", "OffsetStartMs": 825, "OffsetEndMs": 1050}, {"Word": "move", "OffsetStartMs": 1050, "OffsetEndMs": 1185}, {"Word": "on", "OffsetStartMs": 1185, "OffsetEndMs": 1440}, {"Word": "to", "OffsetStartMs": 1440, "OffsetEndMs": 1815}, {"Word": "what", "OffsetStartMs": 1815, "OffsetEndMs": 2070}, {"Word": "I", "OffsetStartMs": 2070, "OffsetEndMs": 2235}, {"Word": "think", "OffsetStartMs": 2235, "OffsetEndMs": 2445}, {"Word": "is", "OffsetStartMs": 2445, "OffsetEndMs": 2625}, {"Word": "the", "OffsetStartMs": 2625, "OffsetEndMs": 2745}, {"Word": "most", "OffsetStartMs": 2745, "OffsetEndMs": 2985}, {"Word": "exciting", "OffsetStartMs": 2985, "OffsetEndMs": 3315}, {"Word": "method", "OffsetStartMs": 3315, "OffsetEndMs": 3630}, {"Word": "of", "OffsetStartMs": 3630, "OffsetEndMs": 4010}, {"Word": "estimating", "OffsetStartMs": 4360, "OffsetEndMs": 4820}, {"Word": "epistemic", "OffsetStartMs": 5020, "OffsetEndMs": 5760}, {"Word": "uncertainty", "OffsetStartMs": 5760, "OffsetEndMs": 6110}, {"Word": "that", "OffsetStartMs": 6130, "OffsetEndMs": 6390}, {"Word": "we'll", "OffsetStartMs": 6390, "OffsetEndMs": 6600}, {"Word": "talk", "OffsetStartMs": 6600, "OffsetEndMs": 6765}, {"Word": "about", "OffsetStartMs": 6765, "OffsetEndMs": 6975}, {"Word": "today", "OffsetStartMs": 6975, "OffsetEndMs": 7280}, {"Word": ".", "OffsetStartMs": 8110, "OffsetEndMs": 8510}, {"Word": "Soin", "OffsetStartMs": 8560, "OffsetEndMs": 8850}, {"Word": "both", "OffsetStartMs": 8850, "OffsetEndMs": 9030}, {"Word": "of", "OffsetStartMs": 9030, "OffsetEndMs": 9180}, {"Word": "the", "OffsetStartMs": 9180, "OffsetEndMs": 9330}, {"Word": "examples", "OffsetStartMs": 9330, "OffsetEndMs": 9585}, {"Word": "before", "OffsetStartMs": 9585, "OffsetEndMs": 9950}, {"Word": "EM", "OffsetStartMs": 10240, "OffsetEndMs": 10635}, {"Word": "sampling", "OffsetStartMs": 10635, "OffsetEndMs": 11190}, {"Word": "is", "OffsetStartMs": 11190, "OffsetEndMs": 11415}, {"Word": "compute", "OffsetStartMs": 11415, "OffsetEndMs": 11700}, {"Word": "intensive", "OffsetStartMs": 11700, "OffsetEndMs": 12080}, {"Word": "but", "OffsetStartMs": 12340, "OffsetEndMs": 12615}, {"Word": "generative", "OffsetStartMs": 12615, "OffsetEndMs": 13005}, {"Word": "modeling", "OffsetStartMs": 13005, "OffsetEndMs": 13410}, {"Word": "can", "OffsetStartMs": 13410, "OffsetEndMs": 13665}, {"Word": "also", "OffsetStartMs": 13665, "OffsetEndMs": 13890}, {"Word": "be", "OffsetStartMs": 13890, "OffsetEndMs": 14010}, {"Word": "compute", "OffsetStartMs": 14010, "OffsetEndMs": 14280}, {"Word": "intensive", "OffsetStartMs": 14280, "OffsetEndMs": 14720}, {"Word": ".Let's", "OffsetStartMs": 14830, "OffsetEndMs": 15195}, {"Word": "say", "OffsetStartMs": 15195, "OffsetEndMs": 15330}, {"Word": "you", "OffsetStartMs": 15330, "OffsetEndMs": 15465}, {"Word": "don't", "OffsetStartMs": 15465, "OffsetEndMs": 15795}, {"Word": "actually", "OffsetStartMs": 15795, "OffsetEndMs": 16095}, {"Word": "need", "OffsetStartMs": 16095, "OffsetEndMs": 16365}, {"Word": "a", "OffsetStartMs": 16365, "OffsetEndMs": 16545}, {"Word": "variational", "OffsetStartMs": 16545, "OffsetEndMs": 16950}, {"Word": "auto", "OffsetStartMs": 16950, "OffsetEndMs": 17190}, {"Word": "encoder", "OffsetStartMs": 17190, "OffsetEndMs": 17640}, {"Word": "for", "OffsetStartMs": 17640, "OffsetEndMs": 17775}, {"Word": "your", "OffsetStartMs": 17775, "OffsetEndMs": 17910}, {"Word": "task", "OffsetStartMs": 17910, "OffsetEndMs": 18200}, {"Word": "then", "OffsetStartMs": 18580, "OffsetEndMs": 18855}, {"Word": "you're", "OffsetStartMs": 18855, "OffsetEndMs": 19065}, {"Word": "training", "OffsetStartMs": 19065, "OffsetEndMs": 19260}, {"Word": "an", "OffsetStartMs": 19260, "OffsetEndMs": 19515}, {"Word": "entire", "OffsetStartMs": 19515, "OffsetEndMs": 19800}, {"Word": "decoder", "OffsetStartMs": 19800, "OffsetEndMs": 20445}, {"Word": "for", "OffsetStartMs": 20445, "OffsetEndMs": 20655}, {"Word": "no", "OffsetStartMs": 20655, "OffsetEndMs": 20805}, {"Word": "reason", "OffsetStartMs": 20805, "OffsetEndMs": 21080}, {"Word": "and", "OffsetStartMs": 21340, "OffsetEndMs": 21675}, {"Word": "other", "OffsetStartMs": 21675, "OffsetEndMs": 21945}, {"Word": "than", "OffsetStartMs": 21945, "OffsetEndMs": 22170}, {"Word": "to", "OffsetStartMs": 22170, "OffsetEndMs": 22455}, {"Word": "estimate", "OffsetStartMs": 22455, "OffsetEndMs": 22725}, {"Word": "the", "OffsetStartMs": 22725, "OffsetEndMs": 22845}, {"Word": "epistemic", "OffsetStartMs": 22845, "OffsetEndMs": 23400}, {"Word": "uncertainty", "OffsetStartMs": 23400, "OffsetEndMs": 23750}, {"Word": "so", "OffsetStartMs": 24700, "OffsetEndMs": 25100}, {"Word": "what", "OffsetStartMs": 25210, "OffsetEndMs": 25455}, {"Word": "if", "OffsetStartMs": 25455, "OffsetEndMs": 25590}, {"Word": "we", "OffsetStartMs": 25590, "OffsetEndMs": 25740}, {"Word": "had", "OffsetStartMs": 25740, "OffsetEndMs": 25875}, {"Word": "a", "OffsetStartMs": 25875, "OffsetEndMs": 26010}, {"Word": "method", "OffsetStartMs": 26010, "OffsetEndMs": 26220}, {"Word": "that", "OffsetStartMs": 26220, "OffsetEndMs": 26460}, {"Word": "did", "OffsetStartMs": 26460, "OffsetEndMs": 26610}, {"Word": "not", "OffsetStartMs": 26610, "OffsetEndMs": 26835}, {"Word": "rely", "OffsetStartMs": 26835, "OffsetEndMs": 27135}, {"Word": "on", "OffsetStartMs": 27135, "OffsetEndMs": 27375}, {"Word": "generative", "OffsetStartMs": 27375, "OffsetEndMs": 27795}, {"Word": "modeling", "OffsetStartMs": 27795, "OffsetEndMs": 28275}, {"Word": "or", "OffsetStartMs": 28275, "OffsetEndMs": 28590}, {"Word": "sampling", "OffsetStartMs": 28590, "OffsetEndMs": 29115}, {"Word": "in", "OffsetStartMs": 29115, "OffsetEndMs": 29295}, {"Word": "order", "OffsetStartMs": 29295, "OffsetEndMs": 29445}, {"Word": "to", "OffsetStartMs": 29445, "OffsetEndMs": 29750}, {"Word": "estimate", "OffsetStartMs": 29800, "OffsetEndMs": 30120}, {"Word": "the", "OffsetStartMs": 30120, "OffsetEndMs": 30285}, {"Word": "epistemic", "OffsetStartMs": 30285, "OffsetEndMs": 30810}, {"Word": "uncertainty", "OffsetStartMs": 30810, "OffsetEndMs": 31130}], "SpeechSpeed": 17.5}, {"FinalSentence": "That's exactly what a method that we've developed here at themis does. So we view learning as an evidence based process. So if you remember from earlier when we were training the ensemble and calling multiple ensembles on the same input, we received multiple predictions and we calculated that variance.", "SliceSentence": "That's exactly what a method that we've developed here at themis does . Sowe view learning as an evidence based process . Soif you remember from earlier when we were training the ensemble and calling multiple ensembles on the same input we received multiple predictions and we calculated that variance", "StartMs": 2684600, "EndMs": 2704100, "WordsNum": 50, "Words": [{"Word": "That's", "OffsetStartMs": 130, "OffsetEndMs": 540}, {"Word": "exactly", "OffsetStartMs": 540, "OffsetEndMs": 810}, {"Word": "what", "OffsetStartMs": 810, "OffsetEndMs": 1065}, {"Word": "a", "OffsetStartMs": 1065, "OffsetEndMs": 1215}, {"Word": "method", "OffsetStartMs": 1215, "OffsetEndMs": 1490}, {"Word": "that", "OffsetStartMs": 1810, "OffsetEndMs": 2085}, {"Word": "we've", "OffsetStartMs": 2085, "OffsetEndMs": 2370}, {"Word": "developed", "OffsetStartMs": 2370, "OffsetEndMs": 2660}, {"Word": "here", "OffsetStartMs": 2680, "OffsetEndMs": 2970}, {"Word": "at", "OffsetStartMs": 2970, "OffsetEndMs": 3135}, {"Word": "themis", "OffsetStartMs": 3135, "OffsetEndMs": 3465}, {"Word": "does", "OffsetStartMs": 3465, "OffsetEndMs": 3800}, {"Word": ".", "OffsetStartMs": 4540, "OffsetEndMs": 4940}, {"Word": "Sowe", "OffsetStartMs": 5410, "OffsetEndMs": 5730}, {"Word": "view", "OffsetStartMs": 5730, "OffsetEndMs": 5955}, {"Word": "learning", "OffsetStartMs": 5955, "OffsetEndMs": 6260}, {"Word": "as", "OffsetStartMs": 6310, "OffsetEndMs": 6600}, {"Word": "an", "OffsetStartMs": 6600, "OffsetEndMs": 6795}, {"Word": "evidence", "OffsetStartMs": 6795, "OffsetEndMs": 7100}, {"Word": "based", "OffsetStartMs": 7210, "OffsetEndMs": 7590}, {"Word": "process", "OffsetStartMs": 7590, "OffsetEndMs": 7970}, {"Word": ".", "OffsetStartMs": 8380, "OffsetEndMs": 8625}, {"Word": "Soif", "OffsetStartMs": 8625, "OffsetEndMs": 8730}, {"Word": "you", "OffsetStartMs": 8730, "OffsetEndMs": 8895}, {"Word": "remember", "OffsetStartMs": 8895, "OffsetEndMs": 9135}, {"Word": "from", "OffsetStartMs": 9135, "OffsetEndMs": 9345}, {"Word": "earlier", "OffsetStartMs": 9345, "OffsetEndMs": 9620}, {"Word": "when", "OffsetStartMs": 9730, "OffsetEndMs": 9990}, {"Word": "we", "OffsetStartMs": 9990, "OffsetEndMs": 10125}, {"Word": "were", "OffsetStartMs": 10125, "OffsetEndMs": 10400}, {"Word": "training", "OffsetStartMs": 10840, "OffsetEndMs": 11175}, {"Word": "the", "OffsetStartMs": 11175, "OffsetEndMs": 11355}, {"Word": "ensemble", "OffsetStartMs": 11355, "OffsetEndMs": 11940}, {"Word": "and", "OffsetStartMs": 11940, "OffsetEndMs": 12225}, {"Word": "calling", "OffsetStartMs": 12225, "OffsetEndMs": 12540}, {"Word": "multiple", "OffsetStartMs": 12540, "OffsetEndMs": 12920}, {"Word": "ensembles", "OffsetStartMs": 12940, "OffsetEndMs": 13665}, {"Word": "on", "OffsetStartMs": 13665, "OffsetEndMs": 13905}, {"Word": "the", "OffsetStartMs": 13905, "OffsetEndMs": 14055}, {"Word": "same", "OffsetStartMs": 14055, "OffsetEndMs": 14330}, {"Word": "input", "OffsetStartMs": 14350, "OffsetEndMs": 14750}, {"Word": "we", "OffsetStartMs": 15160, "OffsetEndMs": 15510}, {"Word": "received", "OffsetStartMs": 15510, "OffsetEndMs": 15795}, {"Word": "multiple", "OffsetStartMs": 15795, "OffsetEndMs": 16130}, {"Word": "predictions", "OffsetStartMs": 16180, "OffsetEndMs": 16845}, {"Word": "and", "OffsetStartMs": 16845, "OffsetEndMs": 17085}, {"Word": "we", "OffsetStartMs": 17085, "OffsetEndMs": 17250}, {"Word": "calculated", "OffsetStartMs": 17250, "OffsetEndMs": 17730}, {"Word": "that", "OffsetStartMs": 17730, "OffsetEndMs": 17970}, {"Word": "variance", "OffsetStartMs": 17970, "OffsetEndMs": 18530}], "SpeechSpeed": 15.3}, {"FinalSentence": "Now, the way we frame evidential learning is what if we assume that those data points, those predictions, were actually drawn from a distribution themselves. If we could estimate the parameters of this higher order evidential distribution, we would be able to learn this variance or this measure of epistemic uncertainty automatically without doing any sampling or generative modeling. And that's exactly what evidential uncertainty does.", "SliceSentence": "Now the way we frame evidential learning is what if we assume that those data points those predictions were actually drawn from a distribution themselves . Ifwe could estimate the parameters of this higher order evidential distribution we would be able to learn this variance or this measure of epistemic uncertainty automatically without doing any sampling or generative modeling . Andthat's exactly what evidential uncertainty does", "StartMs": 2704100, "EndMs": 2728620, "WordsNum": 66, "Words": [{"Word": "Now", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "the", "OffsetStartMs": 1090, "OffsetEndMs": 1335}, {"Word": "way", "OffsetStartMs": 1335, "OffsetEndMs": 1470}, {"Word": "we", "OffsetStartMs": 1470, "OffsetEndMs": 1650}, {"Word": "frame", "OffsetStartMs": 1650, "OffsetEndMs": 1860}, {"Word": "evidential", "OffsetStartMs": 1860, "OffsetEndMs": 2460}, {"Word": "learning", "OffsetStartMs": 2460, "OffsetEndMs": 2720}, {"Word": "is", "OffsetStartMs": 2770, "OffsetEndMs": 3090}, {"Word": "what", "OffsetStartMs": 3090, "OffsetEndMs": 3270}, {"Word": "if", "OffsetStartMs": 3270, "OffsetEndMs": 3405}, {"Word": "we", "OffsetStartMs": 3405, "OffsetEndMs": 3630}, {"Word": "assume", "OffsetStartMs": 3630, "OffsetEndMs": 3945}, {"Word": "that", "OffsetStartMs": 3945, "OffsetEndMs": 4260}, {"Word": "those", "OffsetStartMs": 4260, "OffsetEndMs": 4560}, {"Word": "data", "OffsetStartMs": 4560, "OffsetEndMs": 4860}, {"Word": "points", "OffsetStartMs": 4860, "OffsetEndMs": 5175}, {"Word": "those", "OffsetStartMs": 5175, "OffsetEndMs": 5460}, {"Word": "predictions", "OffsetStartMs": 5460, "OffsetEndMs": 6080}, {"Word": "were", "OffsetStartMs": 6190, "OffsetEndMs": 6570}, {"Word": "actually", "OffsetStartMs": 6570, "OffsetEndMs": 6870}, {"Word": "drawn", "OffsetStartMs": 6870, "OffsetEndMs": 7140}, {"Word": "from", "OffsetStartMs": 7140, "OffsetEndMs": 7320}, {"Word": "a", "OffsetStartMs": 7320, "OffsetEndMs": 7500}, {"Word": "distribution", "OffsetStartMs": 7500, "OffsetEndMs": 7850}, {"Word": "themselves", "OffsetStartMs": 8050, "OffsetEndMs": 8450}, {"Word": ".", "OffsetStartMs": 9130, "OffsetEndMs": 9420}, {"Word": "Ifwe", "OffsetStartMs": 9420, "OffsetEndMs": 9585}, {"Word": "could", "OffsetStartMs": 9585, "OffsetEndMs": 9860}, {"Word": "estimate", "OffsetStartMs": 9880, "OffsetEndMs": 10155}, {"Word": "the", "OffsetStartMs": 10155, "OffsetEndMs": 10320}, {"Word": "parameters", "OffsetStartMs": 10320, "OffsetEndMs": 10800}, {"Word": "of", "OffsetStartMs": 10800, "OffsetEndMs": 11040}, {"Word": "this", "OffsetStartMs": 11040, "OffsetEndMs": 11220}, {"Word": "higher", "OffsetStartMs": 11220, "OffsetEndMs": 11535}, {"Word": "order", "OffsetStartMs": 11535, "OffsetEndMs": 11910}, {"Word": "evidential", "OffsetStartMs": 11910, "OffsetEndMs": 12690}, {"Word": "distribution", "OffsetStartMs": 12690, "OffsetEndMs": 13070}, {"Word": "we", "OffsetStartMs": 13600, "OffsetEndMs": 13845}, {"Word": "would", "OffsetStartMs": 13845, "OffsetEndMs": 13950}, {"Word": "be", "OffsetStartMs": 13950, "OffsetEndMs": 14070}, {"Word": "able", "OffsetStartMs": 14070, "OffsetEndMs": 14265}, {"Word": "to", "OffsetStartMs": 14265, "OffsetEndMs": 14475}, {"Word": "learn", "OffsetStartMs": 14475, "OffsetEndMs": 14685}, {"Word": "this", "OffsetStartMs": 14685, "OffsetEndMs": 14925}, {"Word": "variance", "OffsetStartMs": 14925, "OffsetEndMs": 15420}, {"Word": "or", "OffsetStartMs": 15420, "OffsetEndMs": 15660}, {"Word": "this", "OffsetStartMs": 15660, "OffsetEndMs": 15825}, {"Word": "measure", "OffsetStartMs": 15825, "OffsetEndMs": 16050}, {"Word": "of", "OffsetStartMs": 16050, "OffsetEndMs": 16260}, {"Word": "epistemic", "OffsetStartMs": 16260, "OffsetEndMs": 16860}, {"Word": "uncertainty", "OffsetStartMs": 16860, "OffsetEndMs": 17210}, {"Word": "automatically", "OffsetStartMs": 17650, "OffsetEndMs": 18050}, {"Word": "without", "OffsetStartMs": 18460, "OffsetEndMs": 18810}, {"Word": "doing", "OffsetStartMs": 18810, "OffsetEndMs": 19080}, {"Word": "any", "OffsetStartMs": 19080, "OffsetEndMs": 19350}, {"Word": "sampling", "OffsetStartMs": 19350, "OffsetEndMs": 19860}, {"Word": "or", "OffsetStartMs": 19860, "OffsetEndMs": 20085}, {"Word": "generative", "OffsetStartMs": 20085, "OffsetEndMs": 20475}, {"Word": "modeling", "OffsetStartMs": 20475, "OffsetEndMs": 20990}, {"Word": ".", "OffsetStartMs": 21130, "OffsetEndMs": 21390}, {"Word": "Andthat's", "OffsetStartMs": 21390, "OffsetEndMs": 21645}, {"Word": "exactly", "OffsetStartMs": 21645, "OffsetEndMs": 21930}, {"Word": "what", "OffsetStartMs": 21930, "OffsetEndMs": 22155}, {"Word": "evidential", "OffsetStartMs": 22155, "OffsetEndMs": 22680}, {"Word": "uncertainty", "OffsetStartMs": 22680, "OffsetEndMs": 23000}, {"Word": "does", "OffsetStartMs": 23050, "OffsetEndMs": 23450}], "SpeechSpeed": 17.6}, {"FinalSentence": "So now that we have many methods in our toolbox for estimating epistemic uncertainty, let's go back to our real world example.", "SliceSentence": "So now that we have many methods in our toolbox for estimating epistemic uncertainty let's go back to our real world example", "StartMs": 2730760, "EndMs": 2740120, "WordsNum": 22, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "now", "OffsetStartMs": 1030, "OffsetEndMs": 1305}, {"Word": "that", "OffsetStartMs": 1305, "OffsetEndMs": 1425}, {"Word": "we", "OffsetStartMs": 1425, "OffsetEndMs": 1545}, {"Word": "have", "OffsetStartMs": 1545, "OffsetEndMs": 1770}, {"Word": "many", "OffsetStartMs": 1770, "OffsetEndMs": 2055}, {"Word": "methods", "OffsetStartMs": 2055, "OffsetEndMs": 2355}, {"Word": "in", "OffsetStartMs": 2355, "OffsetEndMs": 2565}, {"Word": "our", "OffsetStartMs": 2565, "OffsetEndMs": 2715}, {"Word": "toolbox", "OffsetStartMs": 2715, "OffsetEndMs": 3150}, {"Word": "for", "OffsetStartMs": 3150, "OffsetEndMs": 3530}, {"Word": "estimating", "OffsetStartMs": 4360, "OffsetEndMs": 4770}, {"Word": "epistemic", "OffsetStartMs": 4770, "OffsetEndMs": 5460}, {"Word": "uncertainty", "OffsetStartMs": 5460, "OffsetEndMs": 5810}, {"Word": "let's", "OffsetStartMs": 6190, "OffsetEndMs": 6570}, {"Word": "go", "OffsetStartMs": 6570, "OffsetEndMs": 6720}, {"Word": "back", "OffsetStartMs": 6720, "OffsetEndMs": 6960}, {"Word": "to", "OffsetStartMs": 6960, "OffsetEndMs": 7155}, {"Word": "our", "OffsetStartMs": 7155, "OffsetEndMs": 7305}, {"Word": "real", "OffsetStartMs": 7305, "OffsetEndMs": 7545}, {"Word": "world", "OffsetStartMs": 7545, "OffsetEndMs": 7860}, {"Word": "example", "OffsetStartMs": 7860, "OffsetEndMs": 8240}], "SpeechSpeed": 13.2}, {"FinalSentence": "Let's say again, the input is the same as before, it's an rgb image of some scene in a city, and the output again is a pixel level mask of what every pixel in this image belongs to, which class it belongs to.", "SliceSentence": "Let's say again the input is the same as before it's an rgb image of some scene in a city and the output again is a pixel level mask of what every pixel in this image belongs to which class it belongs to", "StartMs": 2740120, "EndMs": 2753660, "WordsNum": 43, "Words": [{"Word": "Let's", "OffsetStartMs": 0, "OffsetEndMs": 270}, {"Word": "say", "OffsetStartMs": 270, "OffsetEndMs": 530}, {"Word": "again", "OffsetStartMs": 940, "OffsetEndMs": 1335}, {"Word": "the", "OffsetStartMs": 1335, "OffsetEndMs": 1680}, {"Word": "input", "OffsetStartMs": 1680, "OffsetEndMs": 1890}, {"Word": "is", "OffsetStartMs": 1890, "OffsetEndMs": 2040}, {"Word": "the", "OffsetStartMs": 2040, "OffsetEndMs": 2190}, {"Word": "same", "OffsetStartMs": 2190, "OffsetEndMs": 2325}, {"Word": "as", "OffsetStartMs": 2325, "OffsetEndMs": 2475}, {"Word": "before", "OffsetStartMs": 2475, "OffsetEndMs": 2750}, {"Word": "it's", "OffsetStartMs": 2830, "OffsetEndMs": 3150}, {"Word": "an", "OffsetStartMs": 3150, "OffsetEndMs": 3375}, {"Word": "rgb", "OffsetStartMs": 3375, "OffsetEndMs": 3930}, {"Word": "image", "OffsetStartMs": 3930, "OffsetEndMs": 4190}, {"Word": "of", "OffsetStartMs": 4210, "OffsetEndMs": 4560}, {"Word": "some", "OffsetStartMs": 4560, "OffsetEndMs": 4845}, {"Word": "scene", "OffsetStartMs": 4845, "OffsetEndMs": 5085}, {"Word": "in", "OffsetStartMs": 5085, "OffsetEndMs": 5235}, {"Word": "a", "OffsetStartMs": 5235, "OffsetEndMs": 5370}, {"Word": "city", "OffsetStartMs": 5370, "OffsetEndMs": 5660}, {"Word": "and", "OffsetStartMs": 6190, "OffsetEndMs": 6540}, {"Word": "the", "OffsetStartMs": 6540, "OffsetEndMs": 6890}, {"Word": "output", "OffsetStartMs": 6940, "OffsetEndMs": 7275}, {"Word": "again", "OffsetStartMs": 7275, "OffsetEndMs": 7590}, {"Word": "is", "OffsetStartMs": 7590, "OffsetEndMs": 7845}, {"Word": "a", "OffsetStartMs": 7845, "OffsetEndMs": 8025}, {"Word": "pixel", "OffsetStartMs": 8025, "OffsetEndMs": 8415}, {"Word": "level", "OffsetStartMs": 8415, "OffsetEndMs": 8640}, {"Word": "mask", "OffsetStartMs": 8640, "OffsetEndMs": 8970}, {"Word": "of", "OffsetStartMs": 8970, "OffsetEndMs": 9210}, {"Word": "what", "OffsetStartMs": 9210, "OffsetEndMs": 9360}, {"Word": "every", "OffsetStartMs": 9360, "OffsetEndMs": 9645}, {"Word": "pixel", "OffsetStartMs": 9645, "OffsetEndMs": 10110}, {"Word": "in", "OffsetStartMs": 10110, "OffsetEndMs": 10290}, {"Word": "this", "OffsetStartMs": 10290, "OffsetEndMs": 10485}, {"Word": "image", "OffsetStartMs": 10485, "OffsetEndMs": 10790}, {"Word": "belongs", "OffsetStartMs": 10870, "OffsetEndMs": 11310}, {"Word": "to", "OffsetStartMs": 11310, "OffsetEndMs": 11460}, {"Word": "which", "OffsetStartMs": 11460, "OffsetEndMs": 11640}, {"Word": "class", "OffsetStartMs": 11640, "OffsetEndMs": 11910}, {"Word": "it", "OffsetStartMs": 11910, "OffsetEndMs": 12150}, {"Word": "belongs", "OffsetStartMs": 12150, "OffsetEndMs": 12480}, {"Word": "to", "OffsetStartMs": 12480, "OffsetEndMs": 12770}], "SpeechSpeed": 15.0}, {"FinalSentence": "Which parts of the data set would you expect to have high epistemic uncertainty? In this example, take a look at the output of the model itself. The model does mostly well on semantic segmentation. However, it gets the sidewalk wrong. It assigns some of the sidewalk to the road, and other parts of the sidewalk are labeled incorrectly.", "SliceSentence": "Which parts of the data set would you expect to have high epistemic uncertainty In this example take a look at the output of the model itself . Themodel does mostly well on semantic segmentation . Howeverit gets the sidewalk wrong . Itassigns some of the sidewalk to the road and other parts of the sidewalk are labeled incorrectly", "StartMs": 2753660, "EndMs": 2774220, "WordsNum": 59, "Words": [{"Word": "Which", "OffsetStartMs": 40, "OffsetEndMs": 375}, {"Word": "parts", "OffsetStartMs": 375, "OffsetEndMs": 710}, {"Word": "of", "OffsetStartMs": 730, "OffsetEndMs": 1005}, {"Word": "the", "OffsetStartMs": 1005, "OffsetEndMs": 1185}, {"Word": "data", "OffsetStartMs": 1185, "OffsetEndMs": 1410}, {"Word": "set", "OffsetStartMs": 1410, "OffsetEndMs": 1665}, {"Word": "would", "OffsetStartMs": 1665, "OffsetEndMs": 1875}, {"Word": "you", "OffsetStartMs": 1875, "OffsetEndMs": 2070}, {"Word": "expect", "OffsetStartMs": 2070, "OffsetEndMs": 2340}, {"Word": "to", "OffsetStartMs": 2340, "OffsetEndMs": 2550}, {"Word": "have", "OffsetStartMs": 2550, "OffsetEndMs": 2745}, {"Word": "high", "OffsetStartMs": 2745, "OffsetEndMs": 3045}, {"Word": "epistemic", "OffsetStartMs": 3045, "OffsetEndMs": 3780}, {"Word": "uncertainty", "OffsetStartMs": 3780, "OffsetEndMs": 4130}, {"Word": "In", "OffsetStartMs": 4840, "OffsetEndMs": 5100}, {"Word": "this", "OffsetStartMs": 5100, "OffsetEndMs": 5295}, {"Word": "example", "OffsetStartMs": 5295, "OffsetEndMs": 5630}, {"Word": "take", "OffsetStartMs": 5890, "OffsetEndMs": 6165}, {"Word": "a", "OffsetStartMs": 6165, "OffsetEndMs": 6300}, {"Word": "look", "OffsetStartMs": 6300, "OffsetEndMs": 6465}, {"Word": "at", "OffsetStartMs": 6465, "OffsetEndMs": 6645}, {"Word": "the", "OffsetStartMs": 6645, "OffsetEndMs": 6920}, {"Word": "output", "OffsetStartMs": 7000, "OffsetEndMs": 7305}, {"Word": "of", "OffsetStartMs": 7305, "OffsetEndMs": 7485}, {"Word": "the", "OffsetStartMs": 7485, "OffsetEndMs": 7620}, {"Word": "model", "OffsetStartMs": 7620, "OffsetEndMs": 7880}, {"Word": "itself", "OffsetStartMs": 7930, "OffsetEndMs": 8330}, {"Word": ".", "OffsetStartMs": 8680, "OffsetEndMs": 8940}, {"Word": "Themodel", "OffsetStartMs": 8940, "OffsetEndMs": 9150}, {"Word": "does", "OffsetStartMs": 9150, "OffsetEndMs": 9435}, {"Word": "mostly", "OffsetStartMs": 9435, "OffsetEndMs": 9770}, {"Word": "well", "OffsetStartMs": 9790, "OffsetEndMs": 10140}, {"Word": "on", "OffsetStartMs": 10140, "OffsetEndMs": 10395}, {"Word": "semantic", "OffsetStartMs": 10395, "OffsetEndMs": 10815}, {"Word": "segmentation", "OffsetStartMs": 10815, "OffsetEndMs": 11390}, {"Word": ".", "OffsetStartMs": 11890, "OffsetEndMs": 12255}, {"Word": "Howeverit", "OffsetStartMs": 12255, "OffsetEndMs": 12480}, {"Word": "gets", "OffsetStartMs": 12480, "OffsetEndMs": 12675}, {"Word": "the", "OffsetStartMs": 12675, "OffsetEndMs": 12915}, {"Word": "sidewalk", "OffsetStartMs": 12915, "OffsetEndMs": 13440}, {"Word": "wrong", "OffsetStartMs": 13440, "OffsetEndMs": 13790}, {"Word": ".", "OffsetStartMs": 14410, "OffsetEndMs": 14810}, {"Word": "Itassigns", "OffsetStartMs": 15040, "OffsetEndMs": 15450}, {"Word": "some", "OffsetStartMs": 15450, "OffsetEndMs": 15570}, {"Word": "of", "OffsetStartMs": 15570, "OffsetEndMs": 15690}, {"Word": "the", "OffsetStartMs": 15690, "OffsetEndMs": 15810}, {"Word": "sidewalk", "OffsetStartMs": 15810, "OffsetEndMs": 16140}, {"Word": "to", "OffsetStartMs": 16140, "OffsetEndMs": 16260}, {"Word": "the", "OffsetStartMs": 16260, "OffsetEndMs": 16365}, {"Word": "road", "OffsetStartMs": 16365, "OffsetEndMs": 16640}, {"Word": "and", "OffsetStartMs": 16840, "OffsetEndMs": 17130}, {"Word": "other", "OffsetStartMs": 17130, "OffsetEndMs": 17355}, {"Word": "parts", "OffsetStartMs": 17355, "OffsetEndMs": 17690}, {"Word": "of", "OffsetStartMs": 17770, "OffsetEndMs": 18060}, {"Word": "the", "OffsetStartMs": 18060, "OffsetEndMs": 18210}, {"Word": "sidewalk", "OffsetStartMs": 18210, "OffsetEndMs": 18525}, {"Word": "are", "OffsetStartMs": 18525, "OffsetEndMs": 18675}, {"Word": "labeled", "OffsetStartMs": 18675, "OffsetEndMs": 18975}, {"Word": "incorrectly", "OffsetStartMs": 18975, "OffsetEndMs": 19760}], "SpeechSpeed": 16.0}, {"FinalSentence": "And we can, using epistemic uncertainty, we can see why this is. The areas of the sidewalk that are discolored have high levels of epistemic uncertainty. Maybe this is because the model has never seen an example of a sidewalk with multiple different colors in it before, or maybe it hasn't been trained on examples with sidewalks generally. Either way, epistemic uncertainty has isolated this specific area of the image as an area of high uncertainty.", "SliceSentence": "And we can using epistemic uncertainty we can see why this is . Theareas of the sidewalk that are discolored have high levels of epistemic uncertainty . Maybethis is because the model has never seen an example of a sidewalk with multiple different colors in it before or maybe it hasn't been trained on examples with sidewalks generally . Eitherway epistemic uncertainty has isolated this specific area of the image as an area of high uncertainty", "StartMs": 2775540, "EndMs": 2802420, "WordsNum": 76, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 470}, {"Word": "we", "OffsetStartMs": 580, "OffsetEndMs": 855}, {"Word": "can", "OffsetStartMs": 855, "OffsetEndMs": 1130}, {"Word": "using", "OffsetStartMs": 1300, "OffsetEndMs": 1635}, {"Word": "epistemic", "OffsetStartMs": 1635, "OffsetEndMs": 2265}, {"Word": "uncertainty", "OffsetStartMs": 2265, "OffsetEndMs": 2600}, {"Word": "we", "OffsetStartMs": 2920, "OffsetEndMs": 3165}, {"Word": "can", "OffsetStartMs": 3165, "OffsetEndMs": 3300}, {"Word": "see", "OffsetStartMs": 3300, "OffsetEndMs": 3510}, {"Word": "why", "OffsetStartMs": 3510, "OffsetEndMs": 3765}, {"Word": "this", "OffsetStartMs": 3765, "OffsetEndMs": 3975}, {"Word": "is", "OffsetStartMs": 3975, "OffsetEndMs": 4250}, {"Word": ".", "OffsetStartMs": 4630, "OffsetEndMs": 4905}, {"Word": "Theareas", "OffsetStartMs": 4905, "OffsetEndMs": 5145}, {"Word": "of", "OffsetStartMs": 5145, "OffsetEndMs": 5370}, {"Word": "the", "OffsetStartMs": 5370, "OffsetEndMs": 5505}, {"Word": "sidewalk", "OffsetStartMs": 5505, "OffsetEndMs": 5955}, {"Word": "that", "OffsetStartMs": 5955, "OffsetEndMs": 6135}, {"Word": "are", "OffsetStartMs": 6135, "OffsetEndMs": 6285}, {"Word": "discolored", "OffsetStartMs": 6285, "OffsetEndMs": 6980}, {"Word": "have", "OffsetStartMs": 7000, "OffsetEndMs": 7335}, {"Word": "high", "OffsetStartMs": 7335, "OffsetEndMs": 7590}, {"Word": "levels", "OffsetStartMs": 7590, "OffsetEndMs": 7905}, {"Word": "of", "OffsetStartMs": 7905, "OffsetEndMs": 8175}, {"Word": "epistemic", "OffsetStartMs": 8175, "OffsetEndMs": 8790}, {"Word": "uncertainty", "OffsetStartMs": 8790, "OffsetEndMs": 9140}, {"Word": ".", "OffsetStartMs": 9730, "OffsetEndMs": 10065}, {"Word": "Maybethis", "OffsetStartMs": 10065, "OffsetEndMs": 10260}, {"Word": "is", "OffsetStartMs": 10260, "OffsetEndMs": 10455}, {"Word": "because", "OffsetStartMs": 10455, "OffsetEndMs": 10740}, {"Word": "the", "OffsetStartMs": 10740, "OffsetEndMs": 10935}, {"Word": "model", "OffsetStartMs": 10935, "OffsetEndMs": 11130}, {"Word": "has", "OffsetStartMs": 11130, "OffsetEndMs": 11355}, {"Word": "never", "OffsetStartMs": 11355, "OffsetEndMs": 11580}, {"Word": "seen", "OffsetStartMs": 11580, "OffsetEndMs": 11835}, {"Word": "an", "OffsetStartMs": 11835, "OffsetEndMs": 12060}, {"Word": "example", "OffsetStartMs": 12060, "OffsetEndMs": 12345}, {"Word": "of", "OffsetStartMs": 12345, "OffsetEndMs": 12570}, {"Word": "a", "OffsetStartMs": 12570, "OffsetEndMs": 12705}, {"Word": "sidewalk", "OffsetStartMs": 12705, "OffsetEndMs": 13080}, {"Word": "with", "OffsetStartMs": 13080, "OffsetEndMs": 13245}, {"Word": "multiple", "OffsetStartMs": 13245, "OffsetEndMs": 13485}, {"Word": "different", "OffsetStartMs": 13485, "OffsetEndMs": 13770}, {"Word": "colors", "OffsetStartMs": 13770, "OffsetEndMs": 14025}, {"Word": "in", "OffsetStartMs": 14025, "OffsetEndMs": 14205}, {"Word": "it", "OffsetStartMs": 14205, "OffsetEndMs": 14310}, {"Word": "before", "OffsetStartMs": 14310, "OffsetEndMs": 14570}, {"Word": "or", "OffsetStartMs": 14950, "OffsetEndMs": 15225}, {"Word": "maybe", "OffsetStartMs": 15225, "OffsetEndMs": 15405}, {"Word": "it", "OffsetStartMs": 15405, "OffsetEndMs": 15585}, {"Word": "hasn't", "OffsetStartMs": 15585, "OffsetEndMs": 15855}, {"Word": "been", "OffsetStartMs": 15855, "OffsetEndMs": 16005}, {"Word": "trained", "OffsetStartMs": 16005, "OffsetEndMs": 16290}, {"Word": "on", "OffsetStartMs": 16290, "OffsetEndMs": 16670}, {"Word": "examples", "OffsetStartMs": 16690, "OffsetEndMs": 17055}, {"Word": "with", "OffsetStartMs": 17055, "OffsetEndMs": 17310}, {"Word": "sidewalks", "OffsetStartMs": 17310, "OffsetEndMs": 17835}, {"Word": "generally", "OffsetStartMs": 17835, "OffsetEndMs": 18140}, {"Word": ".", "OffsetStartMs": 18850, "OffsetEndMs": 19200}, {"Word": "Eitherway", "OffsetStartMs": 19200, "OffsetEndMs": 19550}, {"Word": "epistemic", "OffsetStartMs": 20380, "OffsetEndMs": 21105}, {"Word": "uncertainty", "OffsetStartMs": 21105, "OffsetEndMs": 21420}, {"Word": "has", "OffsetStartMs": 21420, "OffsetEndMs": 21690}, {"Word": "isolated", "OffsetStartMs": 21690, "OffsetEndMs": 22275}, {"Word": "this", "OffsetStartMs": 22275, "OffsetEndMs": 22665}, {"Word": "specific", "OffsetStartMs": 22665, "OffsetEndMs": 23055}, {"Word": "area", "OffsetStartMs": 23055, "OffsetEndMs": 23445}, {"Word": "of", "OffsetStartMs": 23445, "OffsetEndMs": 23700}, {"Word": "the", "OffsetStartMs": 23700, "OffsetEndMs": 23820}, {"Word": "image", "OffsetStartMs": 23820, "OffsetEndMs": 24080}, {"Word": "as", "OffsetStartMs": 24130, "OffsetEndMs": 24405}, {"Word": "an", "OffsetStartMs": 24405, "OffsetEndMs": 24540}, {"Word": "area", "OffsetStartMs": 24540, "OffsetEndMs": 24795}, {"Word": "of", "OffsetStartMs": 24795, "OffsetEndMs": 25095}, {"Word": "high", "OffsetStartMs": 25095, "OffsetEndMs": 25380}, {"Word": "uncertainty", "OffsetStartMs": 25380, "OffsetEndMs": 25760}], "SpeechSpeed": 16.5}, {"FinalSentence": "So today, we've gone through two major challenges for robust deep learning. We've talked about bias, which is what happens when models are skewed by sensitive feature inputs and uncertainty, which is when we can measure a level of confidence of a certain model. Now we'll talk about how themus uses these concepts to build products that transform models to make them more risk aware, and how we're changing the AI landscape in terms of safe and trustworthy AI.", "SliceSentence": "So today we've gone through two major challenges for robust deep learning .We've talked about bias which is what happens when models are skewed by sensitive feature inputs and uncertainty which is when we can measure a level of confidence of a certain model . Nowwe'll talk about how themus uses these concepts to build products that transform models to make them more risk aware and how we're changing the AI landscape in terms of safe and trustworthy AI", "StartMs": 2804880, "EndMs": 2831980, "WordsNum": 79, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "today", "OffsetStartMs": 580, "OffsetEndMs": 975}, {"Word": "we've", "OffsetStartMs": 975, "OffsetEndMs": 1335}, {"Word": "gone", "OffsetStartMs": 1335, "OffsetEndMs": 1500}, {"Word": "through", "OffsetStartMs": 1500, "OffsetEndMs": 1820}, {"Word": "two", "OffsetStartMs": 2440, "OffsetEndMs": 2835}, {"Word": "major", "OffsetStartMs": 2835, "OffsetEndMs": 3230}, {"Word": "challenges", "OffsetStartMs": 3280, "OffsetEndMs": 3680}, {"Word": "for", "OffsetStartMs": 3850, "OffsetEndMs": 4200}, {"Word": "robust", "OffsetStartMs": 4200, "OffsetEndMs": 4485}, {"Word": "deep", "OffsetStartMs": 4485, "OffsetEndMs": 4695}, {"Word": "learning", "OffsetStartMs": 4695, "OffsetEndMs": 4970}, {"Word": ".We've", "OffsetStartMs": 5290, "OffsetEndMs": 5655}, {"Word": "talked", "OffsetStartMs": 5655, "OffsetEndMs": 5835}, {"Word": "about", "OffsetStartMs": 5835, "OffsetEndMs": 6090}, {"Word": "bias", "OffsetStartMs": 6090, "OffsetEndMs": 6615}, {"Word": "which", "OffsetStartMs": 6615, "OffsetEndMs": 6870}, {"Word": "is", "OffsetStartMs": 6870, "OffsetEndMs": 7005}, {"Word": "what", "OffsetStartMs": 7005, "OffsetEndMs": 7155}, {"Word": "happens", "OffsetStartMs": 7155, "OffsetEndMs": 7410}, {"Word": "when", "OffsetStartMs": 7410, "OffsetEndMs": 7665}, {"Word": "models", "OffsetStartMs": 7665, "OffsetEndMs": 7920}, {"Word": "are", "OffsetStartMs": 7920, "OffsetEndMs": 8250}, {"Word": "skewed", "OffsetStartMs": 8250, "OffsetEndMs": 8640}, {"Word": "by", "OffsetStartMs": 8640, "OffsetEndMs": 8820}, {"Word": "sensitive", "OffsetStartMs": 8820, "OffsetEndMs": 9110}, {"Word": "feature", "OffsetStartMs": 9130, "OffsetEndMs": 9530}, {"Word": "inputs", "OffsetStartMs": 9550, "OffsetEndMs": 10070}, {"Word": "and", "OffsetStartMs": 10210, "OffsetEndMs": 10590}, {"Word": "uncertainty", "OffsetStartMs": 10590, "OffsetEndMs": 10970}, {"Word": "which", "OffsetStartMs": 11260, "OffsetEndMs": 11535}, {"Word": "is", "OffsetStartMs": 11535, "OffsetEndMs": 11805}, {"Word": "when", "OffsetStartMs": 11805, "OffsetEndMs": 12090}, {"Word": "we", "OffsetStartMs": 12090, "OffsetEndMs": 12240}, {"Word": "can", "OffsetStartMs": 12240, "OffsetEndMs": 12375}, {"Word": "measure", "OffsetStartMs": 12375, "OffsetEndMs": 12600}, {"Word": "a", "OffsetStartMs": 12600, "OffsetEndMs": 12915}, {"Word": "level", "OffsetStartMs": 12915, "OffsetEndMs": 13200}, {"Word": "of", "OffsetStartMs": 13200, "OffsetEndMs": 13440}, {"Word": "confidence", "OffsetStartMs": 13440, "OffsetEndMs": 13760}, {"Word": "of", "OffsetStartMs": 13900, "OffsetEndMs": 14160}, {"Word": "a", "OffsetStartMs": 14160, "OffsetEndMs": 14280}, {"Word": "certain", "OffsetStartMs": 14280, "OffsetEndMs": 14475}, {"Word": "model", "OffsetStartMs": 14475, "OffsetEndMs": 14810}, {"Word": ".", "OffsetStartMs": 15820, "OffsetEndMs": 16220}, {"Word": "Nowwe'll", "OffsetStartMs": 16240, "OffsetEndMs": 16575}, {"Word": "talk", "OffsetStartMs": 16575, "OffsetEndMs": 16740}, {"Word": "about", "OffsetStartMs": 16740, "OffsetEndMs": 16950}, {"Word": "how", "OffsetStartMs": 16950, "OffsetEndMs": 17175}, {"Word": "themus", "OffsetStartMs": 17175, "OffsetEndMs": 17550}, {"Word": "uses", "OffsetStartMs": 17550, "OffsetEndMs": 17805}, {"Word": "these", "OffsetStartMs": 17805, "OffsetEndMs": 18075}, {"Word": "concepts", "OffsetStartMs": 18075, "OffsetEndMs": 18645}, {"Word": "to", "OffsetStartMs": 18645, "OffsetEndMs": 18840}, {"Word": "build", "OffsetStartMs": 18840, "OffsetEndMs": 19050}, {"Word": "products", "OffsetStartMs": 19050, "OffsetEndMs": 19395}, {"Word": "that", "OffsetStartMs": 19395, "OffsetEndMs": 19790}, {"Word": "transform", "OffsetStartMs": 19870, "OffsetEndMs": 20175}, {"Word": "models", "OffsetStartMs": 20175, "OffsetEndMs": 20480}, {"Word": "to", "OffsetStartMs": 20560, "OffsetEndMs": 20805}, {"Word": "make", "OffsetStartMs": 20805, "OffsetEndMs": 20940}, {"Word": "them", "OffsetStartMs": 20940, "OffsetEndMs": 21120}, {"Word": "more", "OffsetStartMs": 21120, "OffsetEndMs": 21315}, {"Word": "risk", "OffsetStartMs": 21315, "OffsetEndMs": 21585}, {"Word": "aware", "OffsetStartMs": 21585, "OffsetEndMs": 21950}, {"Word": "and", "OffsetStartMs": 22090, "OffsetEndMs": 22380}, {"Word": "how", "OffsetStartMs": 22380, "OffsetEndMs": 22560}, {"Word": "we're", "OffsetStartMs": 22560, "OffsetEndMs": 22830}, {"Word": "changing", "OffsetStartMs": 22830, "OffsetEndMs": 23070}, {"Word": "the", "OffsetStartMs": 23070, "OffsetEndMs": 23385}, {"Word": "AI", "OffsetStartMs": 23385, "OffsetEndMs": 23720}, {"Word": "landscape", "OffsetStartMs": 23800, "OffsetEndMs": 24120}, {"Word": "in", "OffsetStartMs": 24120, "OffsetEndMs": 24330}, {"Word": "terms", "OffsetStartMs": 24330, "OffsetEndMs": 24540}, {"Word": "of", "OffsetStartMs": 24540, "OffsetEndMs": 24765}, {"Word": "safe", "OffsetStartMs": 24765, "OffsetEndMs": 25035}, {"Word": "and", "OffsetStartMs": 25035, "OffsetEndMs": 25290}, {"Word": "trustworthy", "OffsetStartMs": 25290, "OffsetEndMs": 25875}, {"Word": "AI", "OffsetStartMs": 25875, "OffsetEndMs": 26210}], "SpeechSpeed": 16.7}, {"FinalSentence": "So at themis, we believe that uncertainty and bias mitigation unlock a host of new solutions to solving these problems with safe and responsible AI.", "SliceSentence": "So at themis we believe that uncertainty and bias mitigation unlock a host of new solutions to solving these problems with safe and responsible AI", "StartMs": 2834040, "EndMs": 2844480, "WordsNum": 25, "Words": [{"Word": "So", "OffsetStartMs": 190, "OffsetEndMs": 590}, {"Word": "at", "OffsetStartMs": 610, "OffsetEndMs": 900}, {"Word": "themis", "OffsetStartMs": 900, "OffsetEndMs": 1260}, {"Word": "we", "OffsetStartMs": 1260, "OffsetEndMs": 1500}, {"Word": "believe", "OffsetStartMs": 1500, "OffsetEndMs": 1740}, {"Word": "that", "OffsetStartMs": 1740, "OffsetEndMs": 2025}, {"Word": "uncertainty", "OffsetStartMs": 2025, "OffsetEndMs": 2385}, {"Word": "and", "OffsetStartMs": 2385, "OffsetEndMs": 2640}, {"Word": "bias", "OffsetStartMs": 2640, "OffsetEndMs": 2940}, {"Word": "mitigation", "OffsetStartMs": 2940, "OffsetEndMs": 3440}, {"Word": "unlock", "OffsetStartMs": 3460, "OffsetEndMs": 4065}, {"Word": "a", "OffsetStartMs": 4065, "OffsetEndMs": 4460}, {"Word": "host", "OffsetStartMs": 4960, "OffsetEndMs": 5325}, {"Word": "of", "OffsetStartMs": 5325, "OffsetEndMs": 5690}, {"Word": "new", "OffsetStartMs": 5920, "OffsetEndMs": 6225}, {"Word": "solutions", "OffsetStartMs": 6225, "OffsetEndMs": 6530}, {"Word": "to", "OffsetStartMs": 6610, "OffsetEndMs": 6870}, {"Word": "solving", "OffsetStartMs": 6870, "OffsetEndMs": 7215}, {"Word": "these", "OffsetStartMs": 7215, "OffsetEndMs": 7395}, {"Word": "problems", "OffsetStartMs": 7395, "OffsetEndMs": 7700}, {"Word": "with", "OffsetStartMs": 7840, "OffsetEndMs": 8145}, {"Word": "safe", "OffsetStartMs": 8145, "OffsetEndMs": 8385}, {"Word": "and", "OffsetStartMs": 8385, "OffsetEndMs": 8670}, {"Word": "responsible", "OffsetStartMs": 8670, "OffsetEndMs": 9020}, {"Word": "AI", "OffsetStartMs": 9190, "OffsetEndMs": 9590}], "SpeechSpeed": 14.0}, {"FinalSentence": "We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. Let's start with labeling data. Today, we talked about allatoric uncertainty, which is a method to detect mislabeled samples, to highlight label noise, and to generally maybe tell labelers to relabel images or samples that they've gotten. That may be wrong.", "SliceSentence": "We can use bias and uncertainty to mitigate risk in every part of the AI life cycle .Let's start with labeling data . Todaywe talked about allatoric uncertainty which is a method to detect mislabeled samples to highlight label noise and to generally maybe tell labelers to relabel images or samples that they've gotten . Thatmay be wrong", "StartMs": 2844480, "EndMs": 2865220, "WordsNum": 58, "Words": [{"Word": "We", "OffsetStartMs": 10, "OffsetEndMs": 285}, {"Word": "can", "OffsetStartMs": 285, "OffsetEndMs": 420}, {"Word": "use", "OffsetStartMs": 420, "OffsetEndMs": 615}, {"Word": "bias", "OffsetStartMs": 615, "OffsetEndMs": 1080}, {"Word": "and", "OffsetStartMs": 1080, "OffsetEndMs": 1335}, {"Word": "uncertainty", "OffsetStartMs": 1335, "OffsetEndMs": 1670}, {"Word": "to", "OffsetStartMs": 1840, "OffsetEndMs": 2085}, {"Word": "mitigate", "OffsetStartMs": 2085, "OffsetEndMs": 2505}, {"Word": "risk", "OffsetStartMs": 2505, "OffsetEndMs": 2805}, {"Word": "in", "OffsetStartMs": 2805, "OffsetEndMs": 3090}, {"Word": "every", "OffsetStartMs": 3090, "OffsetEndMs": 3410}, {"Word": "part", "OffsetStartMs": 3430, "OffsetEndMs": 3750}, {"Word": "of", "OffsetStartMs": 3750, "OffsetEndMs": 3945}, {"Word": "the", "OffsetStartMs": 3945, "OffsetEndMs": 4065}, {"Word": "AI", "OffsetStartMs": 4065, "OffsetEndMs": 4245}, {"Word": "life", "OffsetStartMs": 4245, "OffsetEndMs": 4485}, {"Word": "cycle", "OffsetStartMs": 4485, "OffsetEndMs": 4790}, {"Word": ".Let's", "OffsetStartMs": 5290, "OffsetEndMs": 5670}, {"Word": "start", "OffsetStartMs": 5670, "OffsetEndMs": 5835}, {"Word": "with", "OffsetStartMs": 5835, "OffsetEndMs": 6045}, {"Word": "labeling", "OffsetStartMs": 6045, "OffsetEndMs": 6480}, {"Word": "data", "OffsetStartMs": 6480, "OffsetEndMs": 6770}, {"Word": ".", "OffsetStartMs": 7090, "OffsetEndMs": 7490}, {"Word": "Todaywe", "OffsetStartMs": 7540, "OffsetEndMs": 7815}, {"Word": "talked", "OffsetStartMs": 7815, "OffsetEndMs": 8025}, {"Word": "about", "OffsetStartMs": 8025, "OffsetEndMs": 8235}, {"Word": "allatoric", "OffsetStartMs": 8235, "OffsetEndMs": 8895}, {"Word": "uncertainty", "OffsetStartMs": 8895, "OffsetEndMs": 9230}, {"Word": "which", "OffsetStartMs": 9550, "OffsetEndMs": 9810}, {"Word": "is", "OffsetStartMs": 9810, "OffsetEndMs": 9915}, {"Word": "a", "OffsetStartMs": 9915, "OffsetEndMs": 10050}, {"Word": "method", "OffsetStartMs": 10050, "OffsetEndMs": 10305}, {"Word": "to", "OffsetStartMs": 10305, "OffsetEndMs": 10605}, {"Word": "detect", "OffsetStartMs": 10605, "OffsetEndMs": 10875}, {"Word": "mislabeled", "OffsetStartMs": 10875, "OffsetEndMs": 11475}, {"Word": "samples", "OffsetStartMs": 11475, "OffsetEndMs": 11960}, {"Word": "to", "OffsetStartMs": 12070, "OffsetEndMs": 12345}, {"Word": "highlight", "OffsetStartMs": 12345, "OffsetEndMs": 12620}, {"Word": "label", "OffsetStartMs": 12640, "OffsetEndMs": 12990}, {"Word": "noise", "OffsetStartMs": 12990, "OffsetEndMs": 13340}, {"Word": "and", "OffsetStartMs": 13420, "OffsetEndMs": 13680}, {"Word": "to", "OffsetStartMs": 13680, "OffsetEndMs": 13800}, {"Word": "generally", "OffsetStartMs": 13800, "OffsetEndMs": 14060}, {"Word": "maybe", "OffsetStartMs": 14080, "OffsetEndMs": 14445}, {"Word": "tell", "OffsetStartMs": 14445, "OffsetEndMs": 14715}, {"Word": "labelers", "OffsetStartMs": 14715, "OffsetEndMs": 15240}, {"Word": "to", "OffsetStartMs": 15240, "OffsetEndMs": 15620}, {"Word": "relabel", "OffsetStartMs": 16300, "OffsetEndMs": 16890}, {"Word": "images", "OffsetStartMs": 16890, "OffsetEndMs": 17150}, {"Word": "or", "OffsetStartMs": 17260, "OffsetEndMs": 17660}, {"Word": "samples", "OffsetStartMs": 17680, "OffsetEndMs": 18210}, {"Word": "that", "OffsetStartMs": 18210, "OffsetEndMs": 18405}, {"Word": "they've", "OffsetStartMs": 18405, "OffsetEndMs": 18615}, {"Word": "gotten", "OffsetStartMs": 18615, "OffsetEndMs": 18860}, {"Word": ".", "OffsetStartMs": 18910, "OffsetEndMs": 19215}, {"Word": "Thatmay", "OffsetStartMs": 19215, "OffsetEndMs": 19395}, {"Word": "be", "OffsetStartMs": 19395, "OffsetEndMs": 19560}, {"Word": "wrong", "OffsetStartMs": 19560, "OffsetEndMs": 19850}], "SpeechSpeed": 16.1}, {"FinalSentence": "In the second part of this cycle, we have analyzing the data before a model is even trained on any data. We can analyze the bias that is present in this data set and EM tell the creators whether or not they should add more samples, which demographics, which areas of the data set are underrepresented in the current data set before we even train a model on them.", "SliceSentence": "In the second part of this cycle we have analyzing the data before a model is even trained on any data . Wecan analyze the bias that is present in this data set and EM tell the creators whether or not they should add more samples which demographics which areas of the data set are underrepresented in the current data set before we even train a model on them", "StartMs": 2865240, "EndMs": 2884800, "WordsNum": 69, "Words": [{"Word": "In", "OffsetStartMs": 100, "OffsetEndMs": 345}, {"Word": "the", "OffsetStartMs": 345, "OffsetEndMs": 465}, {"Word": "second", "OffsetStartMs": 465, "OffsetEndMs": 675}, {"Word": "part", "OffsetStartMs": 675, "OffsetEndMs": 870}, {"Word": "of", "OffsetStartMs": 870, "OffsetEndMs": 975}, {"Word": "this", "OffsetStartMs": 975, "OffsetEndMs": 1110}, {"Word": "cycle", "OffsetStartMs": 1110, "OffsetEndMs": 1400}, {"Word": "we", "OffsetStartMs": 1540, "OffsetEndMs": 1830}, {"Word": "have", "OffsetStartMs": 1830, "OffsetEndMs": 2115}, {"Word": "analyzing", "OffsetStartMs": 2115, "OffsetEndMs": 2700}, {"Word": "the", "OffsetStartMs": 2700, "OffsetEndMs": 2910}, {"Word": "data", "OffsetStartMs": 2910, "OffsetEndMs": 3170}, {"Word": "before", "OffsetStartMs": 3460, "OffsetEndMs": 3825}, {"Word": "a", "OffsetStartMs": 3825, "OffsetEndMs": 4065}, {"Word": "model", "OffsetStartMs": 4065, "OffsetEndMs": 4290}, {"Word": "is", "OffsetStartMs": 4290, "OffsetEndMs": 4515}, {"Word": "even", "OffsetStartMs": 4515, "OffsetEndMs": 4755}, {"Word": "trained", "OffsetStartMs": 4755, "OffsetEndMs": 5070}, {"Word": "on", "OffsetStartMs": 5070, "OffsetEndMs": 5295}, {"Word": "any", "OffsetStartMs": 5295, "OffsetEndMs": 5490}, {"Word": "data", "OffsetStartMs": 5490, "OffsetEndMs": 5810}, {"Word": ".", "OffsetStartMs": 5860, "OffsetEndMs": 6135}, {"Word": "Wecan", "OffsetStartMs": 6135, "OffsetEndMs": 6270}, {"Word": "analyze", "OffsetStartMs": 6270, "OffsetEndMs": 6675}, {"Word": "the", "OffsetStartMs": 6675, "OffsetEndMs": 6855}, {"Word": "bias", "OffsetStartMs": 6855, "OffsetEndMs": 7200}, {"Word": "that", "OffsetStartMs": 7200, "OffsetEndMs": 7380}, {"Word": "is", "OffsetStartMs": 7380, "OffsetEndMs": 7545}, {"Word": "present", "OffsetStartMs": 7545, "OffsetEndMs": 7770}, {"Word": "in", "OffsetStartMs": 7770, "OffsetEndMs": 7980}, {"Word": "this", "OffsetStartMs": 7980, "OffsetEndMs": 8190}, {"Word": "data", "OffsetStartMs": 8190, "OffsetEndMs": 8505}, {"Word": "set", "OffsetStartMs": 8505, "OffsetEndMs": 8870}, {"Word": "and", "OffsetStartMs": 9010, "OffsetEndMs": 9375}, {"Word": "EM", "OffsetStartMs": 9375, "OffsetEndMs": 9740}, {"Word": "tell", "OffsetStartMs": 9880, "OffsetEndMs": 10140}, {"Word": "the", "OffsetStartMs": 10140, "OffsetEndMs": 10290}, {"Word": "creators", "OffsetStartMs": 10290, "OffsetEndMs": 10635}, {"Word": "whether", "OffsetStartMs": 10635, "OffsetEndMs": 10860}, {"Word": "or", "OffsetStartMs": 10860, "OffsetEndMs": 11040}, {"Word": "not", "OffsetStartMs": 11040, "OffsetEndMs": 11190}, {"Word": "they", "OffsetStartMs": 11190, "OffsetEndMs": 11355}, {"Word": "should", "OffsetStartMs": 11355, "OffsetEndMs": 11505}, {"Word": "add", "OffsetStartMs": 11505, "OffsetEndMs": 11700}, {"Word": "more", "OffsetStartMs": 11700, "OffsetEndMs": 11925}, {"Word": "samples", "OffsetStartMs": 11925, "OffsetEndMs": 12450}, {"Word": "which", "OffsetStartMs": 12450, "OffsetEndMs": 12750}, {"Word": "demographics", "OffsetStartMs": 12750, "OffsetEndMs": 13455}, {"Word": "which", "OffsetStartMs": 13455, "OffsetEndMs": 13680}, {"Word": "areas", "OffsetStartMs": 13680, "OffsetEndMs": 13950}, {"Word": "of", "OffsetStartMs": 13950, "OffsetEndMs": 14175}, {"Word": "the", "OffsetStartMs": 14175, "OffsetEndMs": 14265}, {"Word": "data", "OffsetStartMs": 14265, "OffsetEndMs": 14445}, {"Word": "set", "OffsetStartMs": 14445, "OffsetEndMs": 14655}, {"Word": "are", "OffsetStartMs": 14655, "OffsetEndMs": 14805}, {"Word": "underrepresented", "OffsetStartMs": 14805, "OffsetEndMs": 15570}, {"Word": "in", "OffsetStartMs": 15570, "OffsetEndMs": 15825}, {"Word": "the", "OffsetStartMs": 15825, "OffsetEndMs": 15975}, {"Word": "current", "OffsetStartMs": 15975, "OffsetEndMs": 16155}, {"Word": "data", "OffsetStartMs": 16155, "OffsetEndMs": 16395}, {"Word": "set", "OffsetStartMs": 16395, "OffsetEndMs": 16730}, {"Word": "before", "OffsetStartMs": 16870, "OffsetEndMs": 17220}, {"Word": "we", "OffsetStartMs": 17220, "OffsetEndMs": 17445}, {"Word": "even", "OffsetStartMs": 17445, "OffsetEndMs": 17670}, {"Word": "train", "OffsetStartMs": 17670, "OffsetEndMs": 17865}, {"Word": "a", "OffsetStartMs": 17865, "OffsetEndMs": 17970}, {"Word": "model", "OffsetStartMs": 17970, "OffsetEndMs": 18150}, {"Word": "on", "OffsetStartMs": 18150, "OffsetEndMs": 18345}, {"Word": "them", "OffsetStartMs": 18345, "OffsetEndMs": 18620}], "SpeechSpeed": 18.3}, {"FinalSentence": "And then let's go to training the model. Once we're actually training a model, if it's already been trained on a biased data set, we can de bias it adaptively during training using the methods that we talked about today.", "SliceSentence": "And then let's go to training the model . Oncewe're actually training a model if it's already been trained on a biased data set we can de bias it adaptively during training using the methods that we talked about today", "StartMs": 2885060, "EndMs": 2897160, "WordsNum": 40, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 375}, {"Word": "then", "OffsetStartMs": 375, "OffsetEndMs": 650}, {"Word": "let's", "OffsetStartMs": 730, "OffsetEndMs": 1095}, {"Word": "go", "OffsetStartMs": 1095, "OffsetEndMs": 1185}, {"Word": "to", "OffsetStartMs": 1185, "OffsetEndMs": 1350}, {"Word": "training", "OffsetStartMs": 1350, "OffsetEndMs": 1605}, {"Word": "the", "OffsetStartMs": 1605, "OffsetEndMs": 1800}, {"Word": "model", "OffsetStartMs": 1800, "OffsetEndMs": 2060}, {"Word": ".", "OffsetStartMs": 2650, "OffsetEndMs": 2940}, {"Word": "Oncewe're", "OffsetStartMs": 2940, "OffsetEndMs": 3195}, {"Word": "actually", "OffsetStartMs": 3195, "OffsetEndMs": 3405}, {"Word": "training", "OffsetStartMs": 3405, "OffsetEndMs": 3630}, {"Word": "a", "OffsetStartMs": 3630, "OffsetEndMs": 3810}, {"Word": "model", "OffsetStartMs": 3810, "OffsetEndMs": 3990}, {"Word": "if", "OffsetStartMs": 3990, "OffsetEndMs": 4170}, {"Word": "it's", "OffsetStartMs": 4170, "OffsetEndMs": 4380}, {"Word": "already", "OffsetStartMs": 4380, "OffsetEndMs": 4605}, {"Word": "been", "OffsetStartMs": 4605, "OffsetEndMs": 4875}, {"Word": "trained", "OffsetStartMs": 4875, "OffsetEndMs": 5175}, {"Word": "on", "OffsetStartMs": 5175, "OffsetEndMs": 5475}, {"Word": "a", "OffsetStartMs": 5475, "OffsetEndMs": 5715}, {"Word": "biased", "OffsetStartMs": 5715, "OffsetEndMs": 6060}, {"Word": "data", "OffsetStartMs": 6060, "OffsetEndMs": 6300}, {"Word": "set", "OffsetStartMs": 6300, "OffsetEndMs": 6650}, {"Word": "we", "OffsetStartMs": 6760, "OffsetEndMs": 7020}, {"Word": "can", "OffsetStartMs": 7020, "OffsetEndMs": 7170}, {"Word": "de", "OffsetStartMs": 7170, "OffsetEndMs": 7350}, {"Word": "bias", "OffsetStartMs": 7350, "OffsetEndMs": 7665}, {"Word": "it", "OffsetStartMs": 7665, "OffsetEndMs": 7875}, {"Word": "adaptively", "OffsetStartMs": 7875, "OffsetEndMs": 8505}, {"Word": "during", "OffsetStartMs": 8505, "OffsetEndMs": 8775}, {"Word": "training", "OffsetStartMs": 8775, "OffsetEndMs": 9140}, {"Word": "using", "OffsetStartMs": 9190, "OffsetEndMs": 9525}, {"Word": "the", "OffsetStartMs": 9525, "OffsetEndMs": 9720}, {"Word": "methods", "OffsetStartMs": 9720, "OffsetEndMs": 9915}, {"Word": "that", "OffsetStartMs": 9915, "OffsetEndMs": 10110}, {"Word": "we", "OffsetStartMs": 10110, "OffsetEndMs": 10245}, {"Word": "talked", "OffsetStartMs": 10245, "OffsetEndMs": 10455}, {"Word": "about", "OffsetStartMs": 10455, "OffsetEndMs": 10680}, {"Word": "today", "OffsetStartMs": 10680, "OffsetEndMs": 10970}], "SpeechSpeed": 17.9}, {"FinalSentence": "And afterwards, we can also verify or certify deployed machine learning models, making sure that models that are actually out there are as safe and unbiased as they claim they are. And the way we can do this is by leveraging epistemic uncertainty or bias in order to calculate the samples or data points that the model will do. The worst on the model has the the most trouble learning or data set samples that are the most underrepresented in a model data set.", "SliceSentence": "And afterwards we can also verify or certify deployed machine learning models making sure that models that are actually out there are as safe and unbiased as they claim they are . Andthe way we can do this is by leveraging epistemic uncertainty or bias in order to calculate the samples or data points that the model will do . Theworst on the model has the the most trouble learning or data set samples that are the most underrepresented in a model data set", "StartMs": 2897200, "EndMs": 2925140, "WordsNum": 84, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 375}, {"Word": "afterwards", "OffsetStartMs": 375, "OffsetEndMs": 650}, {"Word": "we", "OffsetStartMs": 1330, "OffsetEndMs": 1590}, {"Word": "can", "OffsetStartMs": 1590, "OffsetEndMs": 1850}, {"Word": "also", "OffsetStartMs": 1870, "OffsetEndMs": 2250}, {"Word": "verify", "OffsetStartMs": 2250, "OffsetEndMs": 2940}, {"Word": "or", "OffsetStartMs": 2940, "OffsetEndMs": 3290}, {"Word": "certify", "OffsetStartMs": 4180, "OffsetEndMs": 4850}, {"Word": "deployed", "OffsetStartMs": 5350, "OffsetEndMs": 5865}, {"Word": "machine", "OffsetStartMs": 5865, "OffsetEndMs": 6105}, {"Word": "learning", "OffsetStartMs": 6105, "OffsetEndMs": 6315}, {"Word": "models", "OffsetStartMs": 6315, "OffsetEndMs": 6650}, {"Word": "making", "OffsetStartMs": 7210, "OffsetEndMs": 7590}, {"Word": "sure", "OffsetStartMs": 7590, "OffsetEndMs": 7875}, {"Word": "that", "OffsetStartMs": 7875, "OffsetEndMs": 8070}, {"Word": "models", "OffsetStartMs": 8070, "OffsetEndMs": 8310}, {"Word": "that", "OffsetStartMs": 8310, "OffsetEndMs": 8505}, {"Word": "are", "OffsetStartMs": 8505, "OffsetEndMs": 8745}, {"Word": "actually", "OffsetStartMs": 8745, "OffsetEndMs": 9030}, {"Word": "out", "OffsetStartMs": 9030, "OffsetEndMs": 9240}, {"Word": "there", "OffsetStartMs": 9240, "OffsetEndMs": 9465}, {"Word": "are", "OffsetStartMs": 9465, "OffsetEndMs": 9645}, {"Word": "as", "OffsetStartMs": 9645, "OffsetEndMs": 9920}, {"Word": "safe", "OffsetStartMs": 10150, "OffsetEndMs": 10485}, {"Word": "and", "OffsetStartMs": 10485, "OffsetEndMs": 10710}, {"Word": "unbiased", "OffsetStartMs": 10710, "OffsetEndMs": 11250}, {"Word": "as", "OffsetStartMs": 11250, "OffsetEndMs": 11415}, {"Word": "they", "OffsetStartMs": 11415, "OffsetEndMs": 11610}, {"Word": "claim", "OffsetStartMs": 11610, "OffsetEndMs": 11835}, {"Word": "they", "OffsetStartMs": 11835, "OffsetEndMs": 12015}, {"Word": "are", "OffsetStartMs": 12015, "OffsetEndMs": 12290}, {"Word": ".", "OffsetStartMs": 12460, "OffsetEndMs": 12720}, {"Word": "Andthe", "OffsetStartMs": 12720, "OffsetEndMs": 12825}, {"Word": "way", "OffsetStartMs": 12825, "OffsetEndMs": 12960}, {"Word": "we", "OffsetStartMs": 12960, "OffsetEndMs": 13095}, {"Word": "can", "OffsetStartMs": 13095, "OffsetEndMs": 13215}, {"Word": "do", "OffsetStartMs": 13215, "OffsetEndMs": 13365}, {"Word": "this", "OffsetStartMs": 13365, "OffsetEndMs": 13545}, {"Word": "is", "OffsetStartMs": 13545, "OffsetEndMs": 13725}, {"Word": "by", "OffsetStartMs": 13725, "OffsetEndMs": 13905}, {"Word": "leveraging", "OffsetStartMs": 13905, "OffsetEndMs": 14445}, {"Word": "epistemic", "OffsetStartMs": 14445, "OffsetEndMs": 15150}, {"Word": "uncertainty", "OffsetStartMs": 15150, "OffsetEndMs": 15530}, {"Word": "or", "OffsetStartMs": 15670, "OffsetEndMs": 15975}, {"Word": "bias", "OffsetStartMs": 15975, "OffsetEndMs": 16460}, {"Word": "in", "OffsetStartMs": 16630, "OffsetEndMs": 16920}, {"Word": "order", "OffsetStartMs": 16920, "OffsetEndMs": 17190}, {"Word": "to", "OffsetStartMs": 17190, "OffsetEndMs": 17445}, {"Word": "calculate", "OffsetStartMs": 17445, "OffsetEndMs": 17865}, {"Word": "the", "OffsetStartMs": 17865, "OffsetEndMs": 18000}, {"Word": "samples", "OffsetStartMs": 18000, "OffsetEndMs": 18420}, {"Word": "or", "OffsetStartMs": 18420, "OffsetEndMs": 18615}, {"Word": "data", "OffsetStartMs": 18615, "OffsetEndMs": 18840}, {"Word": "points", "OffsetStartMs": 18840, "OffsetEndMs": 19125}, {"Word": "that", "OffsetStartMs": 19125, "OffsetEndMs": 19320}, {"Word": "the", "OffsetStartMs": 19320, "OffsetEndMs": 19440}, {"Word": "model", "OffsetStartMs": 19440, "OffsetEndMs": 19635}, {"Word": "will", "OffsetStartMs": 19635, "OffsetEndMs": 19830}, {"Word": "do", "OffsetStartMs": 19830, "OffsetEndMs": 19950}, {"Word": ".", "OffsetStartMs": 19950, "OffsetEndMs": 20055}, {"Word": "Theworst", "OffsetStartMs": 20055, "OffsetEndMs": 20235}, {"Word": "on", "OffsetStartMs": 20235, "OffsetEndMs": 20570}, {"Word": "the", "OffsetStartMs": 20710, "OffsetEndMs": 20970}, {"Word": "model", "OffsetStartMs": 20970, "OffsetEndMs": 21165}, {"Word": "has", "OffsetStartMs": 21165, "OffsetEndMs": 21390}, {"Word": "the", "OffsetStartMs": 21390, "OffsetEndMs": 21675}, {"Word": "the", "OffsetStartMs": 21675, "OffsetEndMs": 22070}, {"Word": "most", "OffsetStartMs": 22120, "OffsetEndMs": 22470}, {"Word": "trouble", "OffsetStartMs": 22470, "OffsetEndMs": 22755}, {"Word": "learning", "OffsetStartMs": 22755, "OffsetEndMs": 23090}, {"Word": "or", "OffsetStartMs": 23140, "OffsetEndMs": 23540}, {"Word": "data", "OffsetStartMs": 23590, "OffsetEndMs": 23925}, {"Word": "set", "OffsetStartMs": 23925, "OffsetEndMs": 24260}, {"Word": "samples", "OffsetStartMs": 24400, "OffsetEndMs": 24840}, {"Word": "that", "OffsetStartMs": 24840, "OffsetEndMs": 24945}, {"Word": "are", "OffsetStartMs": 24945, "OffsetEndMs": 25110}, {"Word": "the", "OffsetStartMs": 25110, "OffsetEndMs": 25290}, {"Word": "most", "OffsetStartMs": 25290, "OffsetEndMs": 25500}, {"Word": "underrepresented", "OffsetStartMs": 25500, "OffsetEndMs": 26385}, {"Word": "in", "OffsetStartMs": 26385, "OffsetEndMs": 26610}, {"Word": "a", "OffsetStartMs": 26610, "OffsetEndMs": 26730}, {"Word": "model", "OffsetStartMs": 26730, "OffsetEndMs": 26955}, {"Word": "data", "OffsetStartMs": 26955, "OffsetEndMs": 27255}, {"Word": "set", "OffsetStartMs": 27255, "OffsetEndMs": 27590}], "SpeechSpeed": 16.3}, {"FinalSentence": "If we can test the model on these samples, specifically the hardest samples for the model, and it does well, then we know that the model has probably been trained in a fair and unbiased manner that mitigates uncertainty.", "SliceSentence": "If we can test the model on these samples specifically the hardest samples for the model and it does well then we know that the model has probably been trained in a fair and unbiased manner that mitigates uncertainty", "StartMs": 2925140, "EndMs": 2937080, "WordsNum": 39, "Words": [{"Word": "If", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "we", "OffsetStartMs": 360, "OffsetEndMs": 495}, {"Word": "can", "OffsetStartMs": 495, "OffsetEndMs": 630}, {"Word": "test", "OffsetStartMs": 630, "OffsetEndMs": 825}, {"Word": "the", "OffsetStartMs": 825, "OffsetEndMs": 990}, {"Word": "model", "OffsetStartMs": 990, "OffsetEndMs": 1185}, {"Word": "on", "OffsetStartMs": 1185, "OffsetEndMs": 1425}, {"Word": "these", "OffsetStartMs": 1425, "OffsetEndMs": 1665}, {"Word": "samples", "OffsetStartMs": 1665, "OffsetEndMs": 2070}, {"Word": "specifically", "OffsetStartMs": 2070, "OffsetEndMs": 2360}, {"Word": "the", "OffsetStartMs": 2560, "OffsetEndMs": 2835}, {"Word": "hardest", "OffsetStartMs": 2835, "OffsetEndMs": 3165}, {"Word": "samples", "OffsetStartMs": 3165, "OffsetEndMs": 3510}, {"Word": "for", "OffsetStartMs": 3510, "OffsetEndMs": 3675}, {"Word": "the", "OffsetStartMs": 3675, "OffsetEndMs": 3795}, {"Word": "model", "OffsetStartMs": 3795, "OffsetEndMs": 4040}, {"Word": "and", "OffsetStartMs": 4330, "OffsetEndMs": 4605}, {"Word": "it", "OffsetStartMs": 4605, "OffsetEndMs": 4755}, {"Word": "does", "OffsetStartMs": 4755, "OffsetEndMs": 4965}, {"Word": "well", "OffsetStartMs": 4965, "OffsetEndMs": 5300}, {"Word": "then", "OffsetStartMs": 5530, "OffsetEndMs": 5805}, {"Word": "we", "OffsetStartMs": 5805, "OffsetEndMs": 5940}, {"Word": "know", "OffsetStartMs": 5940, "OffsetEndMs": 6105}, {"Word": "that", "OffsetStartMs": 6105, "OffsetEndMs": 6270}, {"Word": "the", "OffsetStartMs": 6270, "OffsetEndMs": 6375}, {"Word": "model", "OffsetStartMs": 6375, "OffsetEndMs": 6555}, {"Word": "has", "OffsetStartMs": 6555, "OffsetEndMs": 6795}, {"Word": "probably", "OffsetStartMs": 6795, "OffsetEndMs": 7100}, {"Word": "been", "OffsetStartMs": 7150, "OffsetEndMs": 7550}, {"Word": "trained", "OffsetStartMs": 7600, "OffsetEndMs": 7950}, {"Word": "in", "OffsetStartMs": 7950, "OffsetEndMs": 8145}, {"Word": "a", "OffsetStartMs": 8145, "OffsetEndMs": 8280}, {"Word": "fair", "OffsetStartMs": 8280, "OffsetEndMs": 8505}, {"Word": "and", "OffsetStartMs": 8505, "OffsetEndMs": 8715}, {"Word": "unbiased", "OffsetStartMs": 8715, "OffsetEndMs": 9255}, {"Word": "manner", "OffsetStartMs": 9255, "OffsetEndMs": 9560}, {"Word": "that", "OffsetStartMs": 9670, "OffsetEndMs": 9975}, {"Word": "mitigates", "OffsetStartMs": 9975, "OffsetEndMs": 10470}, {"Word": "uncertainty", "OffsetStartMs": 10470, "OffsetEndMs": 10850}], "SpeechSpeed": 18.1}, {"FinalSentence": "And lastly, we can think about, EM, we, we are developing a product at themis called AI guardian, and that's essentially a layer between the artificial intelligence algorithm and the user. And the way this works is this is a type of algorithm that if you're driving an autonomous vehicle, would say, hey, the model doesn't actually know what is happening in the world around it right now. As the user, you should take control of this autonomous vehicle and we can apply this to spears outside autonomy as well.", "SliceSentence": "And lastly we can think about EM we we are developing a product at themis called AI guardian and that's essentially a layer between the artificial intelligence algorithm and the user . Andthe way this works is this is a type of algorithm that if you're driving an autonomous vehicle would say hey the model doesn't actually know what is happening in the world around it right now . Asthe user you should take control of this autonomous vehicle and we can apply this to spears outside autonomy as well", "StartMs": 2937240, "EndMs": 2965080, "WordsNum": 90, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "lastly", "OffsetStartMs": 360, "OffsetEndMs": 885}, {"Word": "we", "OffsetStartMs": 885, "OffsetEndMs": 1080}, {"Word": "can", "OffsetStartMs": 1080, "OffsetEndMs": 1215}, {"Word": "think", "OffsetStartMs": 1215, "OffsetEndMs": 1410}, {"Word": "about", "OffsetStartMs": 1410, "OffsetEndMs": 1730}, {"Word": "EM", "OffsetStartMs": 1810, "OffsetEndMs": 2210}, {"Word": "we", "OffsetStartMs": 2350, "OffsetEndMs": 2655}, {"Word": "we", "OffsetStartMs": 2655, "OffsetEndMs": 2805}, {"Word": "are", "OffsetStartMs": 2805, "OffsetEndMs": 2955}, {"Word": "developing", "OffsetStartMs": 2955, "OffsetEndMs": 3225}, {"Word": "a", "OffsetStartMs": 3225, "OffsetEndMs": 3480}, {"Word": "product", "OffsetStartMs": 3480, "OffsetEndMs": 3705}, {"Word": "at", "OffsetStartMs": 3705, "OffsetEndMs": 3915}, {"Word": "themis", "OffsetStartMs": 3915, "OffsetEndMs": 4290}, {"Word": "called", "OffsetStartMs": 4290, "OffsetEndMs": 4620}, {"Word": "AI", "OffsetStartMs": 4875, "OffsetEndMs": 5070}, {"Word": "guardian", "OffsetStartMs": 5070, "OffsetEndMs": 5600}, {"Word": "and", "OffsetStartMs": 5740, "OffsetEndMs": 6030}, {"Word": "that's", "OffsetStartMs": 6030, "OffsetEndMs": 6345}, {"Word": "essentially", "OffsetStartMs": 6345, "OffsetEndMs": 6585}, {"Word": "a", "OffsetStartMs": 6585, "OffsetEndMs": 6825}, {"Word": "layer", "OffsetStartMs": 6825, "OffsetEndMs": 7100}, {"Word": "between", "OffsetStartMs": 7180, "OffsetEndMs": 7485}, {"Word": "the", "OffsetStartMs": 7485, "OffsetEndMs": 7740}, {"Word": "artificial", "OffsetStartMs": 7740, "OffsetEndMs": 8055}, {"Word": "intelligence", "OffsetStartMs": 8055, "OffsetEndMs": 8420}, {"Word": "algorithm", "OffsetStartMs": 9040, "OffsetEndMs": 9480}, {"Word": "and", "OffsetStartMs": 9480, "OffsetEndMs": 9660}, {"Word": "the", "OffsetStartMs": 9660, "OffsetEndMs": 9795}, {"Word": "user", "OffsetStartMs": 9795, "OffsetEndMs": 10070}, {"Word": ".", "OffsetStartMs": 10450, "OffsetEndMs": 10710}, {"Word": "Andthe", "OffsetStartMs": 10710, "OffsetEndMs": 10815}, {"Word": "way", "OffsetStartMs": 10815, "OffsetEndMs": 10950}, {"Word": "this", "OffsetStartMs": 10950, "OffsetEndMs": 11130}, {"Word": "works", "OffsetStartMs": 11130, "OffsetEndMs": 11420}, {"Word": "is", "OffsetStartMs": 11440, "OffsetEndMs": 11840}, {"Word": "this", "OffsetStartMs": 11890, "OffsetEndMs": 12165}, {"Word": "is", "OffsetStartMs": 12165, "OffsetEndMs": 12300}, {"Word": "a", "OffsetStartMs": 12300, "OffsetEndMs": 12435}, {"Word": "type", "OffsetStartMs": 12435, "OffsetEndMs": 12570}, {"Word": "of", "OffsetStartMs": 12570, "OffsetEndMs": 12795}, {"Word": "algorithm", "OffsetStartMs": 12795, "OffsetEndMs": 13170}, {"Word": "that", "OffsetStartMs": 13170, "OffsetEndMs": 13425}, {"Word": "if", "OffsetStartMs": 13425, "OffsetEndMs": 13680}, {"Word": "you're", "OffsetStartMs": 13680, "OffsetEndMs": 13935}, {"Word": "driving", "OffsetStartMs": 13935, "OffsetEndMs": 14145}, {"Word": "an", "OffsetStartMs": 14145, "OffsetEndMs": 14310}, {"Word": "autonomous", "OffsetStartMs": 14310, "OffsetEndMs": 14790}, {"Word": "vehicle", "OffsetStartMs": 14790, "OffsetEndMs": 15080}, {"Word": "would", "OffsetStartMs": 15430, "OffsetEndMs": 15735}, {"Word": "say", "OffsetStartMs": 15735, "OffsetEndMs": 16040}, {"Word": "hey", "OffsetStartMs": 16180, "OffsetEndMs": 16580}, {"Word": "the", "OffsetStartMs": 16690, "OffsetEndMs": 17090}, {"Word": "model", "OffsetStartMs": 17230, "OffsetEndMs": 17565}, {"Word": "doesn't", "OffsetStartMs": 17565, "OffsetEndMs": 17970}, {"Word": "actually", "OffsetStartMs": 17970, "OffsetEndMs": 18270}, {"Word": "know", "OffsetStartMs": 18270, "OffsetEndMs": 18590}, {"Word": "what", "OffsetStartMs": 18670, "OffsetEndMs": 18915}, {"Word": "is", "OffsetStartMs": 18915, "OffsetEndMs": 19035}, {"Word": "happening", "OffsetStartMs": 19035, "OffsetEndMs": 19275}, {"Word": "in", "OffsetStartMs": 19275, "OffsetEndMs": 19500}, {"Word": "the", "OffsetStartMs": 19500, "OffsetEndMs": 19605}, {"Word": "world", "OffsetStartMs": 19605, "OffsetEndMs": 19800}, {"Word": "around", "OffsetStartMs": 19800, "OffsetEndMs": 20040}, {"Word": "it", "OffsetStartMs": 20040, "OffsetEndMs": 20190}, {"Word": "right", "OffsetStartMs": 20190, "OffsetEndMs": 20340}, {"Word": "now", "OffsetStartMs": 20340, "OffsetEndMs": 20630}, {"Word": ".", "OffsetStartMs": 20650, "OffsetEndMs": 20985}, {"Word": "Asthe", "OffsetStartMs": 20985, "OffsetEndMs": 21180}, {"Word": "user", "OffsetStartMs": 21180, "OffsetEndMs": 21440}, {"Word": "you", "OffsetStartMs": 21520, "OffsetEndMs": 21810}, {"Word": "should", "OffsetStartMs": 21810, "OffsetEndMs": 21960}, {"Word": "take", "OffsetStartMs": 21960, "OffsetEndMs": 22170}, {"Word": "control", "OffsetStartMs": 22170, "OffsetEndMs": 22395}, {"Word": "of", "OffsetStartMs": 22395, "OffsetEndMs": 22515}, {"Word": "this", "OffsetStartMs": 22515, "OffsetEndMs": 22620}, {"Word": "autonomous", "OffsetStartMs": 22620, "OffsetEndMs": 23100}, {"Word": "vehicle", "OffsetStartMs": 23100, "OffsetEndMs": 23390}, {"Word": "and", "OffsetStartMs": 23740, "OffsetEndMs": 24015}, {"Word": "we", "OffsetStartMs": 24015, "OffsetEndMs": 24135}, {"Word": "can", "OffsetStartMs": 24135, "OffsetEndMs": 24285}, {"Word": "apply", "OffsetStartMs": 24285, "OffsetEndMs": 24495}, {"Word": "this", "OffsetStartMs": 24495, "OffsetEndMs": 24800}, {"Word": "to", "OffsetStartMs": 24850, "OffsetEndMs": 25170}, {"Word": "spears", "OffsetStartMs": 25170, "OffsetEndMs": 25590}, {"Word": "outside", "OffsetStartMs": 25590, "OffsetEndMs": 25860}, {"Word": "autonomy", "OffsetStartMs": 25860, "OffsetEndMs": 26430}, {"Word": "as", "OffsetStartMs": 26430, "OffsetEndMs": 26640}, {"Word": "well", "OffsetStartMs": 26640, "OffsetEndMs": 26930}], "SpeechSpeed": 17.9}, {"FinalSentence": "So you'll notice that I skipped one, uh, part of the cycle. I skipped the part about building the model and that's because today we're going to focus a little bit on EM famous AI's product called capa, which is a model agnostic framework for risk estimation. So capa is an open source library. You all will actually use it in your lab today, EM, that transforms models so that they're risk aware. So this is a typical training pipeline. You've seen this many times in the course. By now we have our data, we have the model, and it's Fed into the training algorithm and we get a trained model at the end that outputs a prediction for every input.", "SliceSentence": "So you'll notice that I skipped one uh part of the cycle . Iskipped the part about building the model and that's because today we're going to focus a little bit on EM famous AI's product called capa which is a model agnostic framework for risk estimation . Socapa is an open source library . Youall will actually use it in your lab today EM that transforms models so that they're risk aware . Sothis is a typical training pipeline .You've seen this many times in the course . Bynow we have our data we have the model and it's Fed into the training algorithm and we get a trained model at the end that outputs a prediction for every input", "StartMs": 2966920, "EndMs": 3002940, "WordsNum": 121, "Words": [{"Word": "So", "OffsetStartMs": 130, "OffsetEndMs": 435}, {"Word": "you'll", "OffsetStartMs": 435, "OffsetEndMs": 660}, {"Word": "notice", "OffsetStartMs": 660, "OffsetEndMs": 810}, {"Word": "that", "OffsetStartMs": 810, "OffsetEndMs": 975}, {"Word": "I", "OffsetStartMs": 975, "OffsetEndMs": 1140}, {"Word": "skipped", "OffsetStartMs": 1140, "OffsetEndMs": 1395}, {"Word": "one", "OffsetStartMs": 1395, "OffsetEndMs": 1680}, {"Word": "uh", "OffsetStartMs": 1680, "OffsetEndMs": 2060}, {"Word": "part", "OffsetStartMs": 2140, "OffsetEndMs": 2490}, {"Word": "of", "OffsetStartMs": 2490, "OffsetEndMs": 2805}, {"Word": "the", "OffsetStartMs": 2805, "OffsetEndMs": 3075}, {"Word": "cycle", "OffsetStartMs": 3075, "OffsetEndMs": 3380}, {"Word": ".", "OffsetStartMs": 3490, "OffsetEndMs": 3840}, {"Word": "Iskipped", "OffsetStartMs": 3840, "OffsetEndMs": 4095}, {"Word": "the", "OffsetStartMs": 4095, "OffsetEndMs": 4230}, {"Word": "part", "OffsetStartMs": 4230, "OffsetEndMs": 4410}, {"Word": "about", "OffsetStartMs": 4410, "OffsetEndMs": 4665}, {"Word": "building", "OffsetStartMs": 4665, "OffsetEndMs": 4965}, {"Word": "the", "OffsetStartMs": 4965, "OffsetEndMs": 5190}, {"Word": "model", "OffsetStartMs": 5190, "OffsetEndMs": 5450}, {"Word": "and", "OffsetStartMs": 5560, "OffsetEndMs": 5820}, {"Word": "that's", "OffsetStartMs": 5820, "OffsetEndMs": 6105}, {"Word": "because", "OffsetStartMs": 6105, "OffsetEndMs": 6315}, {"Word": "today", "OffsetStartMs": 6315, "OffsetEndMs": 6570}, {"Word": "we're", "OffsetStartMs": 6570, "OffsetEndMs": 6840}, {"Word": "going", "OffsetStartMs": 6840, "OffsetEndMs": 6915}, {"Word": "to", "OffsetStartMs": 6915, "OffsetEndMs": 7020}, {"Word": "focus", "OffsetStartMs": 7020, "OffsetEndMs": 7200}, {"Word": "a", "OffsetStartMs": 7200, "OffsetEndMs": 7395}, {"Word": "little", "OffsetStartMs": 7395, "OffsetEndMs": 7575}, {"Word": "bit", "OffsetStartMs": 7575, "OffsetEndMs": 7800}, {"Word": "on", "OffsetStartMs": 7800, "OffsetEndMs": 8120}, {"Word": "EM", "OffsetStartMs": 8440, "OffsetEndMs": 8840}, {"Word": "famous", "OffsetStartMs": 9370, "OffsetEndMs": 9770}, {"Word": "AI's", "OffsetStartMs": 9790, "OffsetEndMs": 10230}, {"Word": "product", "OffsetStartMs": 10230, "OffsetEndMs": 10550}, {"Word": "called", "OffsetStartMs": 10570, "OffsetEndMs": 10935}, {"Word": "capa", "OffsetStartMs": 10935, "OffsetEndMs": 11600}, {"Word": "which", "OffsetStartMs": 11680, "OffsetEndMs": 11955}, {"Word": "is", "OffsetStartMs": 11955, "OffsetEndMs": 12120}, {"Word": "a", "OffsetStartMs": 12120, "OffsetEndMs": 12330}, {"Word": "model", "OffsetStartMs": 12330, "OffsetEndMs": 12600}, {"Word": "agnostic", "OffsetStartMs": 12600, "OffsetEndMs": 13215}, {"Word": "framework", "OffsetStartMs": 13215, "OffsetEndMs": 13520}, {"Word": "for", "OffsetStartMs": 13690, "OffsetEndMs": 13965}, {"Word": "risk", "OffsetStartMs": 13965, "OffsetEndMs": 14240}, {"Word": "estimation", "OffsetStartMs": 14260, "OffsetEndMs": 14750}, {"Word": ".", "OffsetStartMs": 15460, "OffsetEndMs": 15860}, {"Word": "Socapa", "OffsetStartMs": 16030, "OffsetEndMs": 16590}, {"Word": "is", "OffsetStartMs": 16590, "OffsetEndMs": 16740}, {"Word": "an", "OffsetStartMs": 16740, "OffsetEndMs": 16935}, {"Word": "open", "OffsetStartMs": 16935, "OffsetEndMs": 17160}, {"Word": "source", "OffsetStartMs": 17160, "OffsetEndMs": 17355}, {"Word": "library", "OffsetStartMs": 17355, "OffsetEndMs": 17660}, {"Word": ".", "OffsetStartMs": 17830, "OffsetEndMs": 18075}, {"Word": "Youall", "OffsetStartMs": 18075, "OffsetEndMs": 18195}, {"Word": "will", "OffsetStartMs": 18195, "OffsetEndMs": 18435}, {"Word": "actually", "OffsetStartMs": 18435, "OffsetEndMs": 18675}, {"Word": "use", "OffsetStartMs": 18675, "OffsetEndMs": 18840}, {"Word": "it", "OffsetStartMs": 18840, "OffsetEndMs": 18990}, {"Word": "in", "OffsetStartMs": 18990, "OffsetEndMs": 19095}, {"Word": "your", "OffsetStartMs": 19095, "OffsetEndMs": 19215}, {"Word": "lab", "OffsetStartMs": 19215, "OffsetEndMs": 19410}, {"Word": "today", "OffsetStartMs": 19410, "OffsetEndMs": 19730}, {"Word": "EM", "OffsetStartMs": 20050, "OffsetEndMs": 20450}, {"Word": "that", "OffsetStartMs": 20560, "OffsetEndMs": 20960}, {"Word": "transforms", "OffsetStartMs": 21160, "OffsetEndMs": 21600}, {"Word": "models", "OffsetStartMs": 21600, "OffsetEndMs": 21885}, {"Word": "so", "OffsetStartMs": 21885, "OffsetEndMs": 22140}, {"Word": "that", "OffsetStartMs": 22140, "OffsetEndMs": 22275}, {"Word": "they're", "OffsetStartMs": 22275, "OffsetEndMs": 22530}, {"Word": "risk", "OffsetStartMs": 22530, "OffsetEndMs": 22785}, {"Word": "aware", "OffsetStartMs": 22785, "OffsetEndMs": 23180}, {"Word": ".", "OffsetStartMs": 23830, "OffsetEndMs": 24090}, {"Word": "Sothis", "OffsetStartMs": 24090, "OffsetEndMs": 24210}, {"Word": "is", "OffsetStartMs": 24210, "OffsetEndMs": 24390}, {"Word": "a", "OffsetStartMs": 24390, "OffsetEndMs": 24630}, {"Word": "typical", "OffsetStartMs": 24630, "OffsetEndMs": 24930}, {"Word": "training", "OffsetStartMs": 24930, "OffsetEndMs": 25260}, {"Word": "pipeline", "OffsetStartMs": 25260, "OffsetEndMs": 25610}, {"Word": ".You've", "OffsetStartMs": 25630, "OffsetEndMs": 25965}, {"Word": "seen", "OffsetStartMs": 25965, "OffsetEndMs": 26055}, {"Word": "this", "OffsetStartMs": 26055, "OffsetEndMs": 26190}, {"Word": "many", "OffsetStartMs": 26190, "OffsetEndMs": 26400}, {"Word": "times", "OffsetStartMs": 26400, "OffsetEndMs": 26610}, {"Word": "in", "OffsetStartMs": 26610, "OffsetEndMs": 26745}, {"Word": "the", "OffsetStartMs": 26745, "OffsetEndMs": 26865}, {"Word": "course", "OffsetStartMs": 26865, "OffsetEndMs": 27075}, {"Word": ".", "OffsetStartMs": 27075, "OffsetEndMs": 27285}, {"Word": "Bynow", "OffsetStartMs": 27285, "OffsetEndMs": 27560}, {"Word": "we", "OffsetStartMs": 27820, "OffsetEndMs": 28095}, {"Word": "have", "OffsetStartMs": 28095, "OffsetEndMs": 28320}, {"Word": "our", "OffsetStartMs": 28320, "OffsetEndMs": 28560}, {"Word": "data", "OffsetStartMs": 28560, "OffsetEndMs": 28830}, {"Word": "we", "OffsetStartMs": 28830, "OffsetEndMs": 29070}, {"Word": "have", "OffsetStartMs": 29070, "OffsetEndMs": 29205}, {"Word": "the", "OffsetStartMs": 29205, "OffsetEndMs": 29325}, {"Word": "model", "OffsetStartMs": 29325, "OffsetEndMs": 29570}, {"Word": "and", "OffsetStartMs": 29680, "OffsetEndMs": 29955}, {"Word": "it's", "OffsetStartMs": 29955, "OffsetEndMs": 30165}, {"Word": "Fed", "OffsetStartMs": 30165, "OffsetEndMs": 30270}, {"Word": "into", "OffsetStartMs": 30270, "OffsetEndMs": 30435}, {"Word": "the", "OffsetStartMs": 30435, "OffsetEndMs": 30615}, {"Word": "training", "OffsetStartMs": 30615, "OffsetEndMs": 30890}, {"Word": "algorithm", "OffsetStartMs": 30940, "OffsetEndMs": 31395}, {"Word": "and", "OffsetStartMs": 31395, "OffsetEndMs": 31590}, {"Word": "we", "OffsetStartMs": 31590, "OffsetEndMs": 31710}, {"Word": "get", "OffsetStartMs": 31710, "OffsetEndMs": 31815}, {"Word": "a", "OffsetStartMs": 31815, "OffsetEndMs": 31950}, {"Word": "trained", "OffsetStartMs": 31950, "OffsetEndMs": 32160}, {"Word": "model", "OffsetStartMs": 32160, "OffsetEndMs": 32400}, {"Word": "at", "OffsetStartMs": 32400, "OffsetEndMs": 32595}, {"Word": "the", "OffsetStartMs": 32595, "OffsetEndMs": 32730}, {"Word": "end", "OffsetStartMs": 32730, "OffsetEndMs": 32970}, {"Word": "that", "OffsetStartMs": 32970, "OffsetEndMs": 33345}, {"Word": "outputs", "OffsetStartMs": 33345, "OffsetEndMs": 33675}, {"Word": "a", "OffsetStartMs": 33675, "OffsetEndMs": 33840}, {"Word": "prediction", "OffsetStartMs": 33840, "OffsetEndMs": 34220}, {"Word": "for", "OffsetStartMs": 34300, "OffsetEndMs": 34560}, {"Word": "every", "OffsetStartMs": 34560, "OffsetEndMs": 34820}, {"Word": "input", "OffsetStartMs": 34930, "OffsetEndMs": 35330}], "SpeechSpeed": 17.5}, {"FinalSentence": "But with capa, what we can do is by adding a single line into any training workflow, we can turn this model into a risk aware variant that essentially calculates biases, uncertainty and label noise for you. Because today, as you've heard by now, there are so many methods of estimating uncertainty and bias, and sometimes certain methods are better than others. It's really hard to determine what kind of uncertainty you're trying to estimate and how to do so. So capa takes care of this for you. By inserting one line into your training workflow, you can achieve a risk aware model that you can then further analyze.", "SliceSentence": "But with capa what we can do is by adding a single line into any training workflow we can turn this model into a risk aware variant that essentially calculates biases uncertainty and label noise for you . Becausetoday as you've heard by now there are so many methods of estimating uncertainty and bias and sometimes certain methods are better than others .It's really hard to determine what kind of uncertainty you're trying to estimate and how to do so . Socapa takes care of this for you . Byinserting one line into your training workflow you can achieve a risk aware model that you can then further analyze", "StartMs": 3004220, "EndMs": 3040060, "WordsNum": 109, "Words": [{"Word": "But", "OffsetStartMs": 100, "OffsetEndMs": 360}, {"Word": "with", "OffsetStartMs": 360, "OffsetEndMs": 525}, {"Word": "capa", "OffsetStartMs": 525, "OffsetEndMs": 1065}, {"Word": "what", "OffsetStartMs": 1065, "OffsetEndMs": 1290}, {"Word": "we", "OffsetStartMs": 1290, "OffsetEndMs": 1410}, {"Word": "can", "OffsetStartMs": 1410, "OffsetEndMs": 1545}, {"Word": "do", "OffsetStartMs": 1545, "OffsetEndMs": 1740}, {"Word": "is", "OffsetStartMs": 1740, "OffsetEndMs": 1950}, {"Word": "by", "OffsetStartMs": 1950, "OffsetEndMs": 2115}, {"Word": "adding", "OffsetStartMs": 2115, "OffsetEndMs": 2340}, {"Word": "a", "OffsetStartMs": 2340, "OffsetEndMs": 2625}, {"Word": "single", "OffsetStartMs": 2625, "OffsetEndMs": 2925}, {"Word": "line", "OffsetStartMs": 2925, "OffsetEndMs": 3255}, {"Word": "into", "OffsetStartMs": 3255, "OffsetEndMs": 3600}, {"Word": "any", "OffsetStartMs": 3600, "OffsetEndMs": 3945}, {"Word": "training", "OffsetStartMs": 3945, "OffsetEndMs": 4275}, {"Word": "workflow", "OffsetStartMs": 4275, "OffsetEndMs": 4910}, {"Word": "we", "OffsetStartMs": 5260, "OffsetEndMs": 5520}, {"Word": "can", "OffsetStartMs": 5520, "OffsetEndMs": 5685}, {"Word": "turn", "OffsetStartMs": 5685, "OffsetEndMs": 5895}, {"Word": "this", "OffsetStartMs": 5895, "OffsetEndMs": 6090}, {"Word": "model", "OffsetStartMs": 6090, "OffsetEndMs": 6360}, {"Word": "into", "OffsetStartMs": 6360, "OffsetEndMs": 6660}, {"Word": "a", "OffsetStartMs": 6660, "OffsetEndMs": 6870}, {"Word": "risk", "OffsetStartMs": 6870, "OffsetEndMs": 7140}, {"Word": "aware", "OffsetStartMs": 7140, "OffsetEndMs": 7440}, {"Word": "variant", "OffsetStartMs": 7440, "OffsetEndMs": 7970}, {"Word": "that", "OffsetStartMs": 8170, "OffsetEndMs": 8490}, {"Word": "essentially", "OffsetStartMs": 8490, "OffsetEndMs": 8805}, {"Word": "calculates", "OffsetStartMs": 8805, "OffsetEndMs": 9435}, {"Word": "biases", "OffsetStartMs": 9435, "OffsetEndMs": 10100}, {"Word": "uncertainty", "OffsetStartMs": 10300, "OffsetEndMs": 10700}, {"Word": "and", "OffsetStartMs": 10810, "OffsetEndMs": 11085}, {"Word": "label", "OffsetStartMs": 11085, "OffsetEndMs": 11310}, {"Word": "noise", "OffsetStartMs": 11310, "OffsetEndMs": 11625}, {"Word": "for", "OffsetStartMs": 11625, "OffsetEndMs": 11880}, {"Word": "you", "OffsetStartMs": 11880, "OffsetEndMs": 12170}, {"Word": ".", "OffsetStartMs": 12580, "OffsetEndMs": 12980}, {"Word": "Becausetoday", "OffsetStartMs": 13030, "OffsetEndMs": 13430}, {"Word": "as", "OffsetStartMs": 13630, "OffsetEndMs": 14030}, {"Word": "you've", "OffsetStartMs": 14830, "OffsetEndMs": 15135}, {"Word": "heard", "OffsetStartMs": 15135, "OffsetEndMs": 15270}, {"Word": "by", "OffsetStartMs": 15270, "OffsetEndMs": 15450}, {"Word": "now", "OffsetStartMs": 15450, "OffsetEndMs": 15720}, {"Word": "there", "OffsetStartMs": 15720, "OffsetEndMs": 15960}, {"Word": "are", "OffsetStartMs": 15960, "OffsetEndMs": 16155}, {"Word": "so", "OffsetStartMs": 16155, "OffsetEndMs": 16410}, {"Word": "many", "OffsetStartMs": 16410, "OffsetEndMs": 16665}, {"Word": "methods", "OffsetStartMs": 16665, "OffsetEndMs": 16980}, {"Word": "of", "OffsetStartMs": 16980, "OffsetEndMs": 17310}, {"Word": "estimating", "OffsetStartMs": 17310, "OffsetEndMs": 17750}, {"Word": "uncertainty", "OffsetStartMs": 17860, "OffsetEndMs": 18255}, {"Word": "and", "OffsetStartMs": 18255, "OffsetEndMs": 18525}, {"Word": "bias", "OffsetStartMs": 18525, "OffsetEndMs": 18980}, {"Word": "and", "OffsetStartMs": 19150, "OffsetEndMs": 19440}, {"Word": "sometimes", "OffsetStartMs": 19440, "OffsetEndMs": 19730}, {"Word": "certain", "OffsetStartMs": 20020, "OffsetEndMs": 20340}, {"Word": "methods", "OffsetStartMs": 20340, "OffsetEndMs": 20625}, {"Word": "are", "OffsetStartMs": 20625, "OffsetEndMs": 20865}, {"Word": "better", "OffsetStartMs": 20865, "OffsetEndMs": 21045}, {"Word": "than", "OffsetStartMs": 21045, "OffsetEndMs": 21225}, {"Word": "others", "OffsetStartMs": 21225, "OffsetEndMs": 21500}, {"Word": ".It's", "OffsetStartMs": 21760, "OffsetEndMs": 22095}, {"Word": "really", "OffsetStartMs": 22095, "OffsetEndMs": 22275}, {"Word": "hard", "OffsetStartMs": 22275, "OffsetEndMs": 22545}, {"Word": "to", "OffsetStartMs": 22545, "OffsetEndMs": 22785}, {"Word": "determine", "OffsetStartMs": 22785, "OffsetEndMs": 23070}, {"Word": "what", "OffsetStartMs": 23070, "OffsetEndMs": 23385}, {"Word": "kind", "OffsetStartMs": 23385, "OffsetEndMs": 23625}, {"Word": "of", "OffsetStartMs": 23625, "OffsetEndMs": 23910}, {"Word": "uncertainty", "OffsetStartMs": 23910, "OffsetEndMs": 24285}, {"Word": "you're", "OffsetStartMs": 24285, "OffsetEndMs": 24630}, {"Word": "trying", "OffsetStartMs": 24630, "OffsetEndMs": 24825}, {"Word": "to", "OffsetStartMs": 24825, "OffsetEndMs": 25140}, {"Word": "estimate", "OffsetStartMs": 25140, "OffsetEndMs": 25515}, {"Word": "and", "OffsetStartMs": 25515, "OffsetEndMs": 25800}, {"Word": "how", "OffsetStartMs": 25800, "OffsetEndMs": 25980}, {"Word": "to", "OffsetStartMs": 25980, "OffsetEndMs": 26145}, {"Word": "do", "OffsetStartMs": 26145, "OffsetEndMs": 26310}, {"Word": "so", "OffsetStartMs": 26310, "OffsetEndMs": 26600}, {"Word": ".", "OffsetStartMs": 26740, "OffsetEndMs": 27000}, {"Word": "Socapa", "OffsetStartMs": 27000, "OffsetEndMs": 27360}, {"Word": "takes", "OffsetStartMs": 27360, "OffsetEndMs": 27555}, {"Word": "care", "OffsetStartMs": 27555, "OffsetEndMs": 27750}, {"Word": "of", "OffsetStartMs": 27750, "OffsetEndMs": 27870}, {"Word": "this", "OffsetStartMs": 27870, "OffsetEndMs": 28080}, {"Word": "for", "OffsetStartMs": 28080, "OffsetEndMs": 28365}, {"Word": "you", "OffsetStartMs": 28365, "OffsetEndMs": 28700}, {"Word": ".", "OffsetStartMs": 28750, "OffsetEndMs": 29025}, {"Word": "Byinserting", "OffsetStartMs": 29025, "OffsetEndMs": 29490}, {"Word": "one", "OffsetStartMs": 29490, "OffsetEndMs": 29745}, {"Word": "line", "OffsetStartMs": 29745, "OffsetEndMs": 30015}, {"Word": "into", "OffsetStartMs": 30015, "OffsetEndMs": 30240}, {"Word": "your", "OffsetStartMs": 30240, "OffsetEndMs": 30420}, {"Word": "training", "OffsetStartMs": 30420, "OffsetEndMs": 30645}, {"Word": "workflow", "OffsetStartMs": 30645, "OffsetEndMs": 31190}, {"Word": "you", "OffsetStartMs": 31240, "OffsetEndMs": 31530}, {"Word": "can", "OffsetStartMs": 31530, "OffsetEndMs": 31820}, {"Word": "achieve", "OffsetStartMs": 31870, "OffsetEndMs": 32175}, {"Word": "a", "OffsetStartMs": 32175, "OffsetEndMs": 32355}, {"Word": "risk", "OffsetStartMs": 32355, "OffsetEndMs": 32625}, {"Word": "aware", "OffsetStartMs": 32625, "OffsetEndMs": 32940}, {"Word": "model", "OffsetStartMs": 32940, "OffsetEndMs": 33240}, {"Word": "that", "OffsetStartMs": 33240, "OffsetEndMs": 33495}, {"Word": "you", "OffsetStartMs": 33495, "OffsetEndMs": 33630}, {"Word": "can", "OffsetStartMs": 33630, "OffsetEndMs": 33765}, {"Word": "then", "OffsetStartMs": 33765, "OffsetEndMs": 33990}, {"Word": "further", "OffsetStartMs": 33990, "OffsetEndMs": 34290}, {"Word": "analyze", "OffsetStartMs": 34290, "OffsetEndMs": 34850}], "SpeechSpeed": 16.9}, {"FinalSentence": "And so this is the one line that I've been talking about. EM, after you build your model, you can just create a wrapper or you can call a wrapper that capa has an extensive library of. And then, in addition to achieving prediction, receiving predictions from your model, you can also receive whatever bias or uncertainty metric that you're trying to estimate.", "SliceSentence": "And so this is the one line that I've been talking about . EMafter you build your model you can just create a wrapper or you can call a wrapper that capa has an extensive library of . Andthen in addition to achieving prediction receiving predictions from your model you can also receive whatever bias or uncertainty metric that you're trying to estimate", "StartMs": 3041340, "EndMs": 3060920, "WordsNum": 63, "Words": [{"Word": "And", "OffsetStartMs": 160, "OffsetEndMs": 465}, {"Word": "so", "OffsetStartMs": 465, "OffsetEndMs": 750}, {"Word": "this", "OffsetStartMs": 750, "OffsetEndMs": 1005}, {"Word": "is", "OffsetStartMs": 1005, "OffsetEndMs": 1155}, {"Word": "the", "OffsetStartMs": 1155, "OffsetEndMs": 1305}, {"Word": "one", "OffsetStartMs": 1305, "OffsetEndMs": 1470}, {"Word": "line", "OffsetStartMs": 1470, "OffsetEndMs": 1665}, {"Word": "that", "OffsetStartMs": 1665, "OffsetEndMs": 1815}, {"Word": "I've", "OffsetStartMs": 1815, "OffsetEndMs": 2025}, {"Word": "been", "OffsetStartMs": 2025, "OffsetEndMs": 2145}, {"Word": "talking", "OffsetStartMs": 2145, "OffsetEndMs": 2420}, {"Word": "about", "OffsetStartMs": 2470, "OffsetEndMs": 2870}, {"Word": ".", "OffsetStartMs": 2980, "OffsetEndMs": 3330}, {"Word": "EMafter", "OffsetStartMs": 3330, "OffsetEndMs": 3645}, {"Word": "you", "OffsetStartMs": 3645, "OffsetEndMs": 3885}, {"Word": "build", "OffsetStartMs": 3885, "OffsetEndMs": 4065}, {"Word": "your", "OffsetStartMs": 4065, "OffsetEndMs": 4260}, {"Word": "model", "OffsetStartMs": 4260, "OffsetEndMs": 4530}, {"Word": "you", "OffsetStartMs": 4530, "OffsetEndMs": 4770}, {"Word": "can", "OffsetStartMs": 4770, "OffsetEndMs": 4890}, {"Word": "just", "OffsetStartMs": 4890, "OffsetEndMs": 5085}, {"Word": "create", "OffsetStartMs": 5085, "OffsetEndMs": 5340}, {"Word": "a", "OffsetStartMs": 5340, "OffsetEndMs": 5550}, {"Word": "wrapper", "OffsetStartMs": 5550, "OffsetEndMs": 5985}, {"Word": "or", "OffsetStartMs": 5985, "OffsetEndMs": 6165}, {"Word": "you", "OffsetStartMs": 6165, "OffsetEndMs": 6300}, {"Word": "can", "OffsetStartMs": 6300, "OffsetEndMs": 6465}, {"Word": "call", "OffsetStartMs": 6465, "OffsetEndMs": 6675}, {"Word": "a", "OffsetStartMs": 6675, "OffsetEndMs": 6855}, {"Word": "wrapper", "OffsetStartMs": 6855, "OffsetEndMs": 7290}, {"Word": "that", "OffsetStartMs": 7290, "OffsetEndMs": 7500}, {"Word": "capa", "OffsetStartMs": 7500, "OffsetEndMs": 7905}, {"Word": "has", "OffsetStartMs": 7905, "OffsetEndMs": 8180}, {"Word": "an", "OffsetStartMs": 8350, "OffsetEndMs": 8750}, {"Word": "extensive", "OffsetStartMs": 8770, "OffsetEndMs": 9105}, {"Word": "library", "OffsetStartMs": 9105, "OffsetEndMs": 9440}, {"Word": "of", "OffsetStartMs": 9490, "OffsetEndMs": 9890}, {"Word": ".", "OffsetStartMs": 10210, "OffsetEndMs": 10485}, {"Word": "Andthen", "OffsetStartMs": 10485, "OffsetEndMs": 10650}, {"Word": "in", "OffsetStartMs": 10650, "OffsetEndMs": 10860}, {"Word": "addition", "OffsetStartMs": 10860, "OffsetEndMs": 11145}, {"Word": "to", "OffsetStartMs": 11145, "OffsetEndMs": 11510}, {"Word": "achieving", "OffsetStartMs": 11890, "OffsetEndMs": 12315}, {"Word": "prediction", "OffsetStartMs": 12315, "OffsetEndMs": 12740}, {"Word": "receiving", "OffsetStartMs": 13210, "OffsetEndMs": 13545}, {"Word": "predictions", "OffsetStartMs": 13545, "OffsetEndMs": 14010}, {"Word": "from", "OffsetStartMs": 14010, "OffsetEndMs": 14160}, {"Word": "your", "OffsetStartMs": 14160, "OffsetEndMs": 14280}, {"Word": "model", "OffsetStartMs": 14280, "OffsetEndMs": 14540}, {"Word": "you", "OffsetStartMs": 14740, "OffsetEndMs": 14985}, {"Word": "can", "OffsetStartMs": 14985, "OffsetEndMs": 15180}, {"Word": "also", "OffsetStartMs": 15180, "OffsetEndMs": 15465}, {"Word": "receive", "OffsetStartMs": 15465, "OffsetEndMs": 15795}, {"Word": "whatever", "OffsetStartMs": 15795, "OffsetEndMs": 16125}, {"Word": "bias", "OffsetStartMs": 16125, "OffsetEndMs": 16545}, {"Word": "or", "OffsetStartMs": 16545, "OffsetEndMs": 16785}, {"Word": "uncertainty", "OffsetStartMs": 16785, "OffsetEndMs": 17100}, {"Word": "metric", "OffsetStartMs": 17100, "OffsetEndMs": 17505}, {"Word": "that", "OffsetStartMs": 17505, "OffsetEndMs": 17640}, {"Word": "you're", "OffsetStartMs": 17640, "OffsetEndMs": 17835}, {"Word": "trying", "OffsetStartMs": 17835, "OffsetEndMs": 18045}, {"Word": "to", "OffsetStartMs": 18045, "OffsetEndMs": 18360}, {"Word": "estimate", "OffsetStartMs": 18360, "OffsetEndMs": 18740}], "SpeechSpeed": 17.9}, {"FinalSentence": "And the way capsule works is it does this by wrapping models for every uncertainty metric that we want to estimate, we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities. In the case of allatoric uncertainty, this could be adding a new layer. In the case of a variational auto encoder, this could be creating and training the decoder and calculating the reconstruction loss on the fly.", "SliceSentence": "And the way capsule works is it does this by wrapping models for every uncertainty metric that we want to estimate we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities . Inthe case of allatoric uncertainty this could be adding a new layer . Inthe case of a variational auto encoder this could be creating and training the decoder and calculating the reconstruction loss on the fly", "StartMs": 3063080, "EndMs": 3088580, "WordsNum": 77, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 465}, {"Word": "the", "OffsetStartMs": 465, "OffsetEndMs": 675}, {"Word": "way", "OffsetStartMs": 675, "OffsetEndMs": 855}, {"Word": "capsule", "OffsetStartMs": 855, "OffsetEndMs": 1290}, {"Word": "works", "OffsetStartMs": 1290, "OffsetEndMs": 1550}, {"Word": "is", "OffsetStartMs": 1600, "OffsetEndMs": 1875}, {"Word": "it", "OffsetStartMs": 1875, "OffsetEndMs": 1995}, {"Word": "does", "OffsetStartMs": 1995, "OffsetEndMs": 2145}, {"Word": "this", "OffsetStartMs": 2145, "OffsetEndMs": 2385}, {"Word": "by", "OffsetStartMs": 2385, "OffsetEndMs": 2640}, {"Word": "wrapping", "OffsetStartMs": 2640, "OffsetEndMs": 3105}, {"Word": "models", "OffsetStartMs": 3105, "OffsetEndMs": 3410}, {"Word": "for", "OffsetStartMs": 4030, "OffsetEndMs": 4305}, {"Word": "every", "OffsetStartMs": 4305, "OffsetEndMs": 4580}, {"Word": "uncertainty", "OffsetStartMs": 4660, "OffsetEndMs": 5055}, {"Word": "metric", "OffsetStartMs": 5055, "OffsetEndMs": 5550}, {"Word": "that", "OffsetStartMs": 5550, "OffsetEndMs": 5730}, {"Word": "we", "OffsetStartMs": 5730, "OffsetEndMs": 5865}, {"Word": "want", "OffsetStartMs": 5865, "OffsetEndMs": 6045}, {"Word": "to", "OffsetStartMs": 6045, "OffsetEndMs": 6345}, {"Word": "estimate", "OffsetStartMs": 6345, "OffsetEndMs": 6740}, {"Word": "we", "OffsetStartMs": 6910, "OffsetEndMs": 7185}, {"Word": "can", "OffsetStartMs": 7185, "OffsetEndMs": 7410}, {"Word": "apply", "OffsetStartMs": 7410, "OffsetEndMs": 7710}, {"Word": "and", "OffsetStartMs": 7710, "OffsetEndMs": 7980}, {"Word": "create", "OffsetStartMs": 7980, "OffsetEndMs": 8235}, {"Word": "the", "OffsetStartMs": 8235, "OffsetEndMs": 8445}, {"Word": "minimal", "OffsetStartMs": 8445, "OffsetEndMs": 8820}, {"Word": "model", "OffsetStartMs": 8820, "OffsetEndMs": 9075}, {"Word": "modifications", "OffsetStartMs": 9075, "OffsetEndMs": 9750}, {"Word": "as", "OffsetStartMs": 9750, "OffsetEndMs": 10130}, {"Word": "necessary", "OffsetStartMs": 10240, "OffsetEndMs": 10640}, {"Word": "while", "OffsetStartMs": 10870, "OffsetEndMs": 11190}, {"Word": "preserving", "OffsetStartMs": 11190, "OffsetEndMs": 11700}, {"Word": "the", "OffsetStartMs": 11700, "OffsetEndMs": 11865}, {"Word": "initial", "OffsetStartMs": 11865, "OffsetEndMs": 12075}, {"Word": "architecture", "OffsetStartMs": 12075, "OffsetEndMs": 12410}, {"Word": "and", "OffsetStartMs": 12640, "OffsetEndMs": 12930}, {"Word": "predictive", "OffsetStartMs": 12930, "OffsetEndMs": 13320}, {"Word": "capabilities", "OffsetStartMs": 13320, "OffsetEndMs": 13610}, {"Word": ".", "OffsetStartMs": 14620, "OffsetEndMs": 14880}, {"Word": "Inthe", "OffsetStartMs": 14880, "OffsetEndMs": 15000}, {"Word": "case", "OffsetStartMs": 15000, "OffsetEndMs": 15180}, {"Word": "of", "OffsetStartMs": 15180, "OffsetEndMs": 15345}, {"Word": "allatoric", "OffsetStartMs": 15345, "OffsetEndMs": 15915}, {"Word": "uncertainty", "OffsetStartMs": 15915, "OffsetEndMs": 16250}, {"Word": "this", "OffsetStartMs": 16390, "OffsetEndMs": 16680}, {"Word": "could", "OffsetStartMs": 16680, "OffsetEndMs": 16815}, {"Word": "be", "OffsetStartMs": 16815, "OffsetEndMs": 16935}, {"Word": "adding", "OffsetStartMs": 16935, "OffsetEndMs": 17130}, {"Word": "a", "OffsetStartMs": 17130, "OffsetEndMs": 17295}, {"Word": "new", "OffsetStartMs": 17295, "OffsetEndMs": 17430}, {"Word": "layer", "OffsetStartMs": 17430, "OffsetEndMs": 17720}, {"Word": ".", "OffsetStartMs": 18010, "OffsetEndMs": 18270}, {"Word": "Inthe", "OffsetStartMs": 18270, "OffsetEndMs": 18390}, {"Word": "case", "OffsetStartMs": 18390, "OffsetEndMs": 18555}, {"Word": "of", "OffsetStartMs": 18555, "OffsetEndMs": 18705}, {"Word": "a", "OffsetStartMs": 18705, "OffsetEndMs": 18810}, {"Word": "variational", "OffsetStartMs": 18810, "OffsetEndMs": 19185}, {"Word": "auto", "OffsetStartMs": 19185, "OffsetEndMs": 19455}, {"Word": "encoder", "OffsetStartMs": 19455, "OffsetEndMs": 19965}, {"Word": "this", "OffsetStartMs": 19965, "OffsetEndMs": 20160}, {"Word": "could", "OffsetStartMs": 20160, "OffsetEndMs": 20310}, {"Word": "be", "OffsetStartMs": 20310, "OffsetEndMs": 20550}, {"Word": "creating", "OffsetStartMs": 20550, "OffsetEndMs": 20910}, {"Word": "and", "OffsetStartMs": 20910, "OffsetEndMs": 21180}, {"Word": "training", "OffsetStartMs": 21180, "OffsetEndMs": 21405}, {"Word": "the", "OffsetStartMs": 21405, "OffsetEndMs": 21600}, {"Word": "decoder", "OffsetStartMs": 21600, "OffsetEndMs": 22185}, {"Word": "and", "OffsetStartMs": 22185, "OffsetEndMs": 22470}, {"Word": "calculating", "OffsetStartMs": 22470, "OffsetEndMs": 22920}, {"Word": "the", "OffsetStartMs": 22920, "OffsetEndMs": 23055}, {"Word": "reconstruction", "OffsetStartMs": 23055, "OffsetEndMs": 23520}, {"Word": "loss", "OffsetStartMs": 23520, "OffsetEndMs": 23820}, {"Word": "on", "OffsetStartMs": 23820, "OffsetEndMs": 24030}, {"Word": "the", "OffsetStartMs": 24030, "OffsetEndMs": 24165}, {"Word": "fly", "OffsetStartMs": 24165, "OffsetEndMs": 24440}], "SpeechSpeed": 18.2}, {"FinalSentence": "And this is an example of capa working on one of the data sets that we talked about today, which was the cubic data set with added noise in it. And also another simple classification task. And the reason why I wanted to show this image is to show that using capsa, we can achieve all of these uncertainty estimates with very little additional added work.", "SliceSentence": "And this is an example of capa working on one of the data sets that we talked about today which was the cubic data set with added noise in it . Andalso another simple classification task . Andthe reason why I wanted to show this image is to show that using capsa we can achieve all of these uncertainty estimates with very little additional added work", "StartMs": 3089920, "EndMs": 3110440, "WordsNum": 66, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 470}, {"Word": "this", "OffsetStartMs": 520, "OffsetEndMs": 780}, {"Word": "is", "OffsetStartMs": 780, "OffsetEndMs": 900}, {"Word": "an", "OffsetStartMs": 900, "OffsetEndMs": 1080}, {"Word": "example", "OffsetStartMs": 1080, "OffsetEndMs": 1400}, {"Word": "of", "OffsetStartMs": 1420, "OffsetEndMs": 1740}, {"Word": "capa", "OffsetStartMs": 1740, "OffsetEndMs": 2300}, {"Word": "working", "OffsetStartMs": 2620, "OffsetEndMs": 3020}, {"Word": "on", "OffsetStartMs": 3040, "OffsetEndMs": 3440}, {"Word": "one", "OffsetStartMs": 3580, "OffsetEndMs": 3840}, {"Word": "of", "OffsetStartMs": 3840, "OffsetEndMs": 3945}, {"Word": "the", "OffsetStartMs": 3945, "OffsetEndMs": 4035}, {"Word": "data", "OffsetStartMs": 4035, "OffsetEndMs": 4200}, {"Word": "sets", "OffsetStartMs": 4200, "OffsetEndMs": 4410}, {"Word": "that", "OffsetStartMs": 4410, "OffsetEndMs": 4575}, {"Word": "we", "OffsetStartMs": 4575, "OffsetEndMs": 4725}, {"Word": "talked", "OffsetStartMs": 4725, "OffsetEndMs": 4935}, {"Word": "about", "OffsetStartMs": 4935, "OffsetEndMs": 5145}, {"Word": "today", "OffsetStartMs": 5145, "OffsetEndMs": 5420}, {"Word": "which", "OffsetStartMs": 5500, "OffsetEndMs": 5775}, {"Word": "was", "OffsetStartMs": 5775, "OffsetEndMs": 5910}, {"Word": "the", "OffsetStartMs": 5910, "OffsetEndMs": 6060}, {"Word": "cubic", "OffsetStartMs": 6060, "OffsetEndMs": 6345}, {"Word": "data", "OffsetStartMs": 6345, "OffsetEndMs": 6555}, {"Word": "set", "OffsetStartMs": 6555, "OffsetEndMs": 6795}, {"Word": "with", "OffsetStartMs": 6795, "OffsetEndMs": 7095}, {"Word": "added", "OffsetStartMs": 7095, "OffsetEndMs": 7425}, {"Word": "noise", "OffsetStartMs": 7425, "OffsetEndMs": 7680}, {"Word": "in", "OffsetStartMs": 7680, "OffsetEndMs": 7845}, {"Word": "it", "OffsetStartMs": 7845, "OffsetEndMs": 8090}, {"Word": ".", "OffsetStartMs": 8200, "OffsetEndMs": 8565}, {"Word": "Andalso", "OffsetStartMs": 8565, "OffsetEndMs": 8880}, {"Word": "another", "OffsetStartMs": 8880, "OffsetEndMs": 9165}, {"Word": "simple", "OffsetStartMs": 9165, "OffsetEndMs": 9435}, {"Word": "classification", "OffsetStartMs": 9435, "OffsetEndMs": 10010}, {"Word": "task", "OffsetStartMs": 10030, "OffsetEndMs": 10430}, {"Word": ".", "OffsetStartMs": 10660, "OffsetEndMs": 10935}, {"Word": "Andthe", "OffsetStartMs": 10935, "OffsetEndMs": 11055}, {"Word": "reason", "OffsetStartMs": 11055, "OffsetEndMs": 11235}, {"Word": "why", "OffsetStartMs": 11235, "OffsetEndMs": 11570}, {"Word": "I", "OffsetStartMs": 11830, "OffsetEndMs": 12105}, {"Word": "wanted", "OffsetStartMs": 12105, "OffsetEndMs": 12285}, {"Word": "to", "OffsetStartMs": 12285, "OffsetEndMs": 12465}, {"Word": "show", "OffsetStartMs": 12465, "OffsetEndMs": 12600}, {"Word": "this", "OffsetStartMs": 12600, "OffsetEndMs": 12735}, {"Word": "image", "OffsetStartMs": 12735, "OffsetEndMs": 12990}, {"Word": "is", "OffsetStartMs": 12990, "OffsetEndMs": 13260}, {"Word": "to", "OffsetStartMs": 13260, "OffsetEndMs": 13425}, {"Word": "show", "OffsetStartMs": 13425, "OffsetEndMs": 13590}, {"Word": "that", "OffsetStartMs": 13590, "OffsetEndMs": 13845}, {"Word": "using", "OffsetStartMs": 13845, "OffsetEndMs": 14175}, {"Word": "capsa", "OffsetStartMs": 14175, "OffsetEndMs": 14625}, {"Word": "we", "OffsetStartMs": 14625, "OffsetEndMs": 14760}, {"Word": "can", "OffsetStartMs": 14760, "OffsetEndMs": 14955}, {"Word": "achieve", "OffsetStartMs": 14955, "OffsetEndMs": 15255}, {"Word": "all", "OffsetStartMs": 15255, "OffsetEndMs": 15540}, {"Word": "of", "OffsetStartMs": 15540, "OffsetEndMs": 15750}, {"Word": "these", "OffsetStartMs": 15750, "OffsetEndMs": 16020}, {"Word": "uncertainty", "OffsetStartMs": 16020, "OffsetEndMs": 16400}, {"Word": "estimates", "OffsetStartMs": 16630, "OffsetEndMs": 17120}, {"Word": "with", "OffsetStartMs": 17200, "OffsetEndMs": 17550}, {"Word": "very", "OffsetStartMs": 17550, "OffsetEndMs": 17895}, {"Word": "little", "OffsetStartMs": 17895, "OffsetEndMs": 18270}, {"Word": "additional", "OffsetStartMs": 18270, "OffsetEndMs": 18650}, {"Word": "added", "OffsetStartMs": 18700, "OffsetEndMs": 19050}, {"Word": "work", "OffsetStartMs": 19050, "OffsetEndMs": 19400}], "SpeechSpeed": 17.0}, {"FinalSentence": "So using all of the products that I just talked about today and using capa themis is unlocking the key to deploy deep learning models safely across fields. We can now answer a lot of the questions that the headlines were raising earlier, which is when should a human take control of an autonomous vehicle? What types of data are underrepresented in commercial autonomous driving pipelines? We now have educated answers to these questions due to products that themis is developing.", "SliceSentence": "So using all of the products that I just talked about today and using capa themis is unlocking the key to deploy deep learning models safely across fields . Wecan now answer a lot of the questions that the headlines were raising earlier which is when should a human take control of an autonomous vehicle What types of data are underrepresented in commercial autonomous driving pipelines We now have educated answers to these questions due to products that themis is developing", "StartMs": 3113680, "EndMs": 3140380, "WordsNum": 81, "Words": [{"Word": "So", "OffsetStartMs": 160, "OffsetEndMs": 560}, {"Word": "using", "OffsetStartMs": 610, "OffsetEndMs": 975}, {"Word": "all", "OffsetStartMs": 975, "OffsetEndMs": 1215}, {"Word": "of", "OffsetStartMs": 1215, "OffsetEndMs": 1350}, {"Word": "the", "OffsetStartMs": 1350, "OffsetEndMs": 1485}, {"Word": "products", "OffsetStartMs": 1485, "OffsetEndMs": 1710}, {"Word": "that", "OffsetStartMs": 1710, "OffsetEndMs": 1905}, {"Word": "I", "OffsetStartMs": 1905, "OffsetEndMs": 2040}, {"Word": "just", "OffsetStartMs": 2040, "OffsetEndMs": 2205}, {"Word": "talked", "OffsetStartMs": 2205, "OffsetEndMs": 2415}, {"Word": "about", "OffsetStartMs": 2415, "OffsetEndMs": 2640}, {"Word": "today", "OffsetStartMs": 2640, "OffsetEndMs": 2930}, {"Word": "and", "OffsetStartMs": 3070, "OffsetEndMs": 3345}, {"Word": "using", "OffsetStartMs": 3345, "OffsetEndMs": 3585}, {"Word": "capa", "OffsetStartMs": 3585, "OffsetEndMs": 4190}, {"Word": "themis", "OffsetStartMs": 4360, "OffsetEndMs": 4815}, {"Word": "is", "OffsetStartMs": 4815, "OffsetEndMs": 5010}, {"Word": "unlocking", "OffsetStartMs": 5010, "OffsetEndMs": 5610}, {"Word": "the", "OffsetStartMs": 5610, "OffsetEndMs": 5760}, {"Word": "key", "OffsetStartMs": 5760, "OffsetEndMs": 6000}, {"Word": "to", "OffsetStartMs": 6000, "OffsetEndMs": 6270}, {"Word": "deploy", "OffsetStartMs": 6270, "OffsetEndMs": 6510}, {"Word": "deep", "OffsetStartMs": 6510, "OffsetEndMs": 6720}, {"Word": "learning", "OffsetStartMs": 6720, "OffsetEndMs": 6975}, {"Word": "models", "OffsetStartMs": 6975, "OffsetEndMs": 7320}, {"Word": "safely", "OffsetStartMs": 7320, "OffsetEndMs": 7700}, {"Word": "across", "OffsetStartMs": 7840, "OffsetEndMs": 8205}, {"Word": "fields", "OffsetStartMs": 8205, "OffsetEndMs": 8570}, {"Word": ".", "OffsetStartMs": 9250, "OffsetEndMs": 9510}, {"Word": "Wecan", "OffsetStartMs": 9510, "OffsetEndMs": 9645}, {"Word": "now", "OffsetStartMs": 9645, "OffsetEndMs": 9855}, {"Word": "answer", "OffsetStartMs": 9855, "OffsetEndMs": 10185}, {"Word": "a", "OffsetStartMs": 10185, "OffsetEndMs": 10455}, {"Word": "lot", "OffsetStartMs": 10455, "OffsetEndMs": 10620}, {"Word": "of", "OffsetStartMs": 10620, "OffsetEndMs": 10755}, {"Word": "the", "OffsetStartMs": 10755, "OffsetEndMs": 10875}, {"Word": "questions", "OffsetStartMs": 10875, "OffsetEndMs": 11150}, {"Word": "that", "OffsetStartMs": 11200, "OffsetEndMs": 11460}, {"Word": "the", "OffsetStartMs": 11460, "OffsetEndMs": 11580}, {"Word": "headlines", "OffsetStartMs": 11580, "OffsetEndMs": 11895}, {"Word": "were", "OffsetStartMs": 11895, "OffsetEndMs": 12075}, {"Word": "raising", "OffsetStartMs": 12075, "OffsetEndMs": 12300}, {"Word": "earlier", "OffsetStartMs": 12300, "OffsetEndMs": 12650}, {"Word": "which", "OffsetStartMs": 12910, "OffsetEndMs": 13185}, {"Word": "is", "OffsetStartMs": 13185, "OffsetEndMs": 13460}, {"Word": "when", "OffsetStartMs": 13630, "OffsetEndMs": 13995}, {"Word": "should", "OffsetStartMs": 13995, "OffsetEndMs": 14205}, {"Word": "a", "OffsetStartMs": 14205, "OffsetEndMs": 14340}, {"Word": "human", "OffsetStartMs": 14340, "OffsetEndMs": 14595}, {"Word": "take", "OffsetStartMs": 14595, "OffsetEndMs": 14925}, {"Word": "control", "OffsetStartMs": 14925, "OffsetEndMs": 15255}, {"Word": "of", "OffsetStartMs": 15255, "OffsetEndMs": 15465}, {"Word": "an", "OffsetStartMs": 15465, "OffsetEndMs": 15540}, {"Word": "autonomous", "OffsetStartMs": 15540, "OffsetEndMs": 16020}, {"Word": "vehicle", "OffsetStartMs": 16020, "OffsetEndMs": 16310}, {"Word": "What", "OffsetStartMs": 16870, "OffsetEndMs": 17175}, {"Word": "types", "OffsetStartMs": 17175, "OffsetEndMs": 17400}, {"Word": "of", "OffsetStartMs": 17400, "OffsetEndMs": 17595}, {"Word": "data", "OffsetStartMs": 17595, "OffsetEndMs": 17805}, {"Word": "are", "OffsetStartMs": 17805, "OffsetEndMs": 18015}, {"Word": "underrepresented", "OffsetStartMs": 18015, "OffsetEndMs": 18735}, {"Word": "in", "OffsetStartMs": 18735, "OffsetEndMs": 19020}, {"Word": "commercial", "OffsetStartMs": 19020, "OffsetEndMs": 19320}, {"Word": "autonomous", "OffsetStartMs": 19320, "OffsetEndMs": 19860}, {"Word": "driving", "OffsetStartMs": 19860, "OffsetEndMs": 20115}, {"Word": "pipelines", "OffsetStartMs": 20115, "OffsetEndMs": 20660}, {"Word": "We", "OffsetStartMs": 21040, "OffsetEndMs": 21315}, {"Word": "now", "OffsetStartMs": 21315, "OffsetEndMs": 21555}, {"Word": "have", "OffsetStartMs": 21555, "OffsetEndMs": 21885}, {"Word": "educated", "OffsetStartMs": 21885, "OffsetEndMs": 22250}, {"Word": "answers", "OffsetStartMs": 22420, "OffsetEndMs": 22820}, {"Word": "to", "OffsetStartMs": 22870, "OffsetEndMs": 23100}, {"Word": "these", "OffsetStartMs": 23100, "OffsetEndMs": 23250}, {"Word": "questions", "OffsetStartMs": 23250, "OffsetEndMs": 23570}, {"Word": "due", "OffsetStartMs": 23890, "OffsetEndMs": 24165}, {"Word": "to", "OffsetStartMs": 24165, "OffsetEndMs": 24315}, {"Word": "products", "OffsetStartMs": 24315, "OffsetEndMs": 24555}, {"Word": "that", "OffsetStartMs": 24555, "OffsetEndMs": 24810}, {"Word": "themis", "OffsetStartMs": 24810, "OffsetEndMs": 25140}, {"Word": "is", "OffsetStartMs": 25140, "OffsetEndMs": 25350}, {"Word": "developing", "OffsetStartMs": 25350, "OffsetEndMs": 25670}], "SpeechSpeed": 17.8}, {"FinalSentence": "And in spheres such as medicine and health care, we can now answer questions such as when is a model uncertain about a life threatening diagnosis? When should this diagnosis be passed to a medical professional before this information is conveyed to a patient? Or what types of patients might drug discovery algorithms be biased against?", "SliceSentence": "And in spheres such as medicine and health care we can now answer questions such as when is a model uncertain about a life threatening diagnosis When should this diagnosis be passed to a medical professional before this information is conveyed to a patient Or what types of patients might drug discovery algorithms be biased against", "StartMs": 3141340, "EndMs": 3159980, "WordsNum": 56, "Words": [{"Word": "And", "OffsetStartMs": 100, "OffsetEndMs": 420}, {"Word": "in", "OffsetStartMs": 420, "OffsetEndMs": 630}, {"Word": "spheres", "OffsetStartMs": 630, "OffsetEndMs": 960}, {"Word": "such", "OffsetStartMs": 960, "OffsetEndMs": 1110}, {"Word": "as", "OffsetStartMs": 1110, "OffsetEndMs": 1275}, {"Word": "medicine", "OffsetStartMs": 1275, "OffsetEndMs": 1580}, {"Word": "and", "OffsetStartMs": 1660, "OffsetEndMs": 1935}, {"Word": "health", "OffsetStartMs": 1935, "OffsetEndMs": 2145}, {"Word": "care", "OffsetStartMs": 2145, "OffsetEndMs": 2480}, {"Word": "we", "OffsetStartMs": 2590, "OffsetEndMs": 2865}, {"Word": "can", "OffsetStartMs": 2865, "OffsetEndMs": 3000}, {"Word": "now", "OffsetStartMs": 3000, "OffsetEndMs": 3165}, {"Word": "answer", "OffsetStartMs": 3165, "OffsetEndMs": 3450}, {"Word": "questions", "OffsetStartMs": 3450, "OffsetEndMs": 3810}, {"Word": "such", "OffsetStartMs": 3810, "OffsetEndMs": 4065}, {"Word": "as", "OffsetStartMs": 4065, "OffsetEndMs": 4340}, {"Word": "when", "OffsetStartMs": 4480, "OffsetEndMs": 4815}, {"Word": "is", "OffsetStartMs": 4815, "OffsetEndMs": 5025}, {"Word": "a", "OffsetStartMs": 5025, "OffsetEndMs": 5160}, {"Word": "model", "OffsetStartMs": 5160, "OffsetEndMs": 5420}, {"Word": "uncertain", "OffsetStartMs": 5530, "OffsetEndMs": 5930}, {"Word": "about", "OffsetStartMs": 5980, "OffsetEndMs": 6240}, {"Word": "a", "OffsetStartMs": 6240, "OffsetEndMs": 6360}, {"Word": "life", "OffsetStartMs": 6360, "OffsetEndMs": 6540}, {"Word": "threatening", "OffsetStartMs": 6540, "OffsetEndMs": 6900}, {"Word": "diagnosis", "OffsetStartMs": 6900, "OffsetEndMs": 7400}, {"Word": "When", "OffsetStartMs": 7720, "OffsetEndMs": 8010}, {"Word": "should", "OffsetStartMs": 8010, "OffsetEndMs": 8160}, {"Word": "this", "OffsetStartMs": 8160, "OffsetEndMs": 8295}, {"Word": "diagnosis", "OffsetStartMs": 8295, "OffsetEndMs": 8790}, {"Word": "be", "OffsetStartMs": 8790, "OffsetEndMs": 9060}, {"Word": "passed", "OffsetStartMs": 9060, "OffsetEndMs": 9330}, {"Word": "to", "OffsetStartMs": 9330, "OffsetEndMs": 9525}, {"Word": "a", "OffsetStartMs": 9525, "OffsetEndMs": 9600}, {"Word": "medical", "OffsetStartMs": 9600, "OffsetEndMs": 9860}, {"Word": "professional", "OffsetStartMs": 9910, "OffsetEndMs": 10310}, {"Word": "before", "OffsetStartMs": 10630, "OffsetEndMs": 10980}, {"Word": "this", "OffsetStartMs": 10980, "OffsetEndMs": 11310}, {"Word": "information", "OffsetStartMs": 11310, "OffsetEndMs": 11655}, {"Word": "is", "OffsetStartMs": 11655, "OffsetEndMs": 11910}, {"Word": "conveyed", "OffsetStartMs": 11910, "OffsetEndMs": 12300}, {"Word": "to", "OffsetStartMs": 12300, "OffsetEndMs": 12390}, {"Word": "a", "OffsetStartMs": 12390, "OffsetEndMs": 12465}, {"Word": "patient", "OffsetStartMs": 12465, "OffsetEndMs": 12710}, {"Word": "Or", "OffsetStartMs": 13450, "OffsetEndMs": 13850}, {"Word": "what", "OffsetStartMs": 14050, "OffsetEndMs": 14385}, {"Word": "types", "OffsetStartMs": 14385, "OffsetEndMs": 14625}, {"Word": "of", "OffsetStartMs": 14625, "OffsetEndMs": 14805}, {"Word": "patients", "OffsetStartMs": 14805, "OffsetEndMs": 15075}, {"Word": "might", "OffsetStartMs": 15075, "OffsetEndMs": 15390}, {"Word": "drug", "OffsetStartMs": 15390, "OffsetEndMs": 15675}, {"Word": "discovery", "OffsetStartMs": 15675, "OffsetEndMs": 16040}, {"Word": "algorithms", "OffsetStartMs": 16090, "OffsetEndMs": 16560}, {"Word": "be", "OffsetStartMs": 16560, "OffsetEndMs": 16785}, {"Word": "biased", "OffsetStartMs": 16785, "OffsetEndMs": 17160}, {"Word": "against", "OffsetStartMs": 17160, "OffsetEndMs": 17510}], "SpeechSpeed": 17.8}, {"FinalSentence": "And today, the, the application that you guys will focus on is on facial detection. You'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed in some ways for you so that you can discover them on your own. And we, we highly encourage you to compete in the competition, which the details are described in the lab. But basically it's about analyzing this data set, creating risk aware models that mitigate bias and uncertainty in the specific training pipeline.", "SliceSentence": "And today the the application that you guys will focus on is on facial detection .You'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed in some ways for you so that you can discover them on your own . Andwe we highly encourage you to compete in the competition which the details are described in the lab . Butbasically it's about analyzing this data set creating risk aware models that mitigate bias and uncertainty in the specific training pipeline", "StartMs": 3159980, "EndMs": 3190320, "WordsNum": 88, "Words": [{"Word": "And", "OffsetStartMs": 0, "OffsetEndMs": 150}, {"Word": "today", "OffsetStartMs": 150, "OffsetEndMs": 440}, {"Word": "the", "OffsetStartMs": 670, "OffsetEndMs": 1050}, {"Word": "the", "OffsetStartMs": 1050, "OffsetEndMs": 1430}, {"Word": "application", "OffsetStartMs": 1630, "OffsetEndMs": 1935}, {"Word": "that", "OffsetStartMs": 1935, "OffsetEndMs": 2100}, {"Word": "you", "OffsetStartMs": 2100, "OffsetEndMs": 2235}, {"Word": "guys", "OffsetStartMs": 2235, "OffsetEndMs": 2400}, {"Word": "will", "OffsetStartMs": 2400, "OffsetEndMs": 2565}, {"Word": "focus", "OffsetStartMs": 2565, "OffsetEndMs": 2760}, {"Word": "on", "OffsetStartMs": 2760, "OffsetEndMs": 3080}, {"Word": "is", "OffsetStartMs": 3190, "OffsetEndMs": 3480}, {"Word": "on", "OffsetStartMs": 3480, "OffsetEndMs": 3675}, {"Word": "facial", "OffsetStartMs": 3675, "OffsetEndMs": 4080}, {"Word": "detection", "OffsetStartMs": 4080, "OffsetEndMs": 4610}, {"Word": ".You'll", "OffsetStartMs": 4930, "OffsetEndMs": 5280}, {"Word": "use", "OffsetStartMs": 5280, "OffsetEndMs": 5460}, {"Word": "capsa", "OffsetStartMs": 5460, "OffsetEndMs": 5925}, {"Word": "in", "OffsetStartMs": 5925, "OffsetEndMs": 6075}, {"Word": "today's", "OffsetStartMs": 6075, "OffsetEndMs": 6435}, {"Word": "lab", "OffsetStartMs": 6435, "OffsetEndMs": 6710}, {"Word": "to", "OffsetStartMs": 6970, "OffsetEndMs": 7370}, {"Word": "thoroughly", "OffsetStartMs": 7870, "OffsetEndMs": 8400}, {"Word": "analyze", "OffsetStartMs": 8400, "OffsetEndMs": 8895}, {"Word": "a", "OffsetStartMs": 8895, "OffsetEndMs": 9230}, {"Word": "common", "OffsetStartMs": 9460, "OffsetEndMs": 9860}, {"Word": "facial", "OffsetStartMs": 10180, "OffsetEndMs": 10635}, {"Word": "detection", "OffsetStartMs": 10635, "OffsetEndMs": 10995}, {"Word": "data", "OffsetStartMs": 10995, "OffsetEndMs": 11205}, {"Word": "set", "OffsetStartMs": 11205, "OffsetEndMs": 11540}, {"Word": "that", "OffsetStartMs": 11590, "OffsetEndMs": 11850}, {"Word": "we've", "OffsetStartMs": 11850, "OffsetEndMs": 12090}, {"Word": "perturbed", "OffsetStartMs": 12090, "OffsetEndMs": 12495}, {"Word": "in", "OffsetStartMs": 12495, "OffsetEndMs": 12615}, {"Word": "some", "OffsetStartMs": 12615, "OffsetEndMs": 12765}, {"Word": "ways", "OffsetStartMs": 12765, "OffsetEndMs": 12990}, {"Word": "for", "OffsetStartMs": 12990, "OffsetEndMs": 13260}, {"Word": "you", "OffsetStartMs": 13260, "OffsetEndMs": 13500}, {"Word": "so", "OffsetStartMs": 13500, "OffsetEndMs": 13680}, {"Word": "that", "OffsetStartMs": 13680, "OffsetEndMs": 13800}, {"Word": "you", "OffsetStartMs": 13800, "OffsetEndMs": 13920}, {"Word": "can", "OffsetStartMs": 13920, "OffsetEndMs": 14115}, {"Word": "discover", "OffsetStartMs": 14115, "OffsetEndMs": 14355}, {"Word": "them", "OffsetStartMs": 14355, "OffsetEndMs": 14520}, {"Word": "on", "OffsetStartMs": 14520, "OffsetEndMs": 14640}, {"Word": "your", "OffsetStartMs": 14640, "OffsetEndMs": 14775}, {"Word": "own", "OffsetStartMs": 14775, "OffsetEndMs": 15050}, {"Word": ".", "OffsetStartMs": 15490, "OffsetEndMs": 15840}, {"Word": "Andwe", "OffsetStartMs": 15840, "OffsetEndMs": 16190}, {"Word": "we", "OffsetStartMs": 16240, "OffsetEndMs": 16545}, {"Word": "highly", "OffsetStartMs": 16545, "OffsetEndMs": 16850}, {"Word": "encourage", "OffsetStartMs": 16900, "OffsetEndMs": 17250}, {"Word": "you", "OffsetStartMs": 17250, "OffsetEndMs": 17460}, {"Word": "to", "OffsetStartMs": 17460, "OffsetEndMs": 17655}, {"Word": "compete", "OffsetStartMs": 17655, "OffsetEndMs": 17895}, {"Word": "in", "OffsetStartMs": 17895, "OffsetEndMs": 18045}, {"Word": "the", "OffsetStartMs": 18045, "OffsetEndMs": 18290}, {"Word": "competition", "OffsetStartMs": 18310, "OffsetEndMs": 18710}, {"Word": "which", "OffsetStartMs": 19180, "OffsetEndMs": 19580}, {"Word": "the", "OffsetStartMs": 19660, "OffsetEndMs": 19995}, {"Word": "details", "OffsetStartMs": 19995, "OffsetEndMs": 20220}, {"Word": "are", "OffsetStartMs": 20220, "OffsetEndMs": 20445}, {"Word": "described", "OffsetStartMs": 20445, "OffsetEndMs": 20685}, {"Word": "in", "OffsetStartMs": 20685, "OffsetEndMs": 20850}, {"Word": "the", "OffsetStartMs": 20850, "OffsetEndMs": 20970}, {"Word": "lab", "OffsetStartMs": 20970, "OffsetEndMs": 21230}, {"Word": ".", "OffsetStartMs": 21370, "OffsetEndMs": 21615}, {"Word": "Butbasically", "OffsetStartMs": 21615, "OffsetEndMs": 21860}, {"Word": "it's", "OffsetStartMs": 21970, "OffsetEndMs": 22335}, {"Word": "about", "OffsetStartMs": 22335, "OffsetEndMs": 22575}, {"Word": "analyzing", "OffsetStartMs": 22575, "OffsetEndMs": 23160}, {"Word": "this", "OffsetStartMs": 23160, "OffsetEndMs": 23385}, {"Word": "data", "OffsetStartMs": 23385, "OffsetEndMs": 23595}, {"Word": "set", "OffsetStartMs": 23595, "OffsetEndMs": 23925}, {"Word": "creating", "OffsetStartMs": 23925, "OffsetEndMs": 24320}, {"Word": "risk", "OffsetStartMs": 24370, "OffsetEndMs": 24750}, {"Word": "aware", "OffsetStartMs": 24750, "OffsetEndMs": 25125}, {"Word": "models", "OffsetStartMs": 25125, "OffsetEndMs": 25520}, {"Word": "that", "OffsetStartMs": 25870, "OffsetEndMs": 26235}, {"Word": "mitigate", "OffsetStartMs": 26235, "OffsetEndMs": 26730}, {"Word": "bias", "OffsetStartMs": 26730, "OffsetEndMs": 27135}, {"Word": "and", "OffsetStartMs": 27135, "OffsetEndMs": 27405}, {"Word": "uncertainty", "OffsetStartMs": 27405, "OffsetEndMs": 27770}, {"Word": "in", "OffsetStartMs": 27880, "OffsetEndMs": 28155}, {"Word": "the", "OffsetStartMs": 28155, "OffsetEndMs": 28320}, {"Word": "specific", "OffsetStartMs": 28320, "OffsetEndMs": 28610}, {"Word": "training", "OffsetStartMs": 28630, "OffsetEndMs": 28980}, {"Word": "pipeline", "OffsetStartMs": 28980, "OffsetEndMs": 29330}], "SpeechSpeed": 16.5}, {"FinalSentence": "And so at themis our goal is to design, advance, and deploy trustworthy AI across industries and around the world. EM, we're passionate about scientific innovation. We release open source tools like the ones you'll use today, and our products transform AI workflows and make artificial intelligence safer for everyone. We partner with industries around the globe and we're hiring for the upcoming summer and for full time roles. So if you're interested, please send an email to careers at famousus I dot io or apply by submitting your resume to the deep learning resume drop and we'll see those resumes and get back to you. Thank you.", "SliceSentence": "And so at themis our goal is to design advance and deploy trustworthy AI across industries and around the world . EMwe're passionate about scientific innovation . Werelease open source tools like the ones you'll use today and our products transform AI workflows and make artificial intelligence safer for everyone . Wepartner with industries around the globe and we're hiring for the upcoming summer and for full time roles . Soif you're interested please send an email to careers at famousus I dot io or apply by submitting your resume to the deep learning resume drop and we'll see those resumes and get back to you . Thankyou", "StartMs": 3191340, "EndMs": 3226540, "WordsNum": 108, "Words": [{"Word": "And", "OffsetStartMs": 70, "OffsetEndMs": 360}, {"Word": "so", "OffsetStartMs": 360, "OffsetEndMs": 555}, {"Word": "at", "OffsetStartMs": 555, "OffsetEndMs": 750}, {"Word": "themis", "OffsetStartMs": 750, "OffsetEndMs": 1140}, {"Word": "our", "OffsetStartMs": 1140, "OffsetEndMs": 1395}, {"Word": "goal", "OffsetStartMs": 1395, "OffsetEndMs": 1620}, {"Word": "is", "OffsetStartMs": 1620, "OffsetEndMs": 1830}, {"Word": "to", "OffsetStartMs": 1830, "OffsetEndMs": 2025}, {"Word": "design", "OffsetStartMs": 2025, "OffsetEndMs": 2330}, {"Word": "advance", "OffsetStartMs": 2470, "OffsetEndMs": 2835}, {"Word": "and", "OffsetStartMs": 2835, "OffsetEndMs": 3150}, {"Word": "deploy", "OffsetStartMs": 3150, "OffsetEndMs": 3480}, {"Word": "trustworthy", "OffsetStartMs": 3480, "OffsetEndMs": 4125}, {"Word": "AI", "OffsetStartMs": 4125, "OffsetEndMs": 4460}, {"Word": "across", "OffsetStartMs": 4510, "OffsetEndMs": 4910}, {"Word": "industries", "OffsetStartMs": 5050, "OffsetEndMs": 5385}, {"Word": "and", "OffsetStartMs": 5385, "OffsetEndMs": 5625}, {"Word": "around", "OffsetStartMs": 5625, "OffsetEndMs": 5820}, {"Word": "the", "OffsetStartMs": 5820, "OffsetEndMs": 5985}, {"Word": "world", "OffsetStartMs": 5985, "OffsetEndMs": 6260}, {"Word": ".", "OffsetStartMs": 6760, "OffsetEndMs": 7160}, {"Word": "EMwe're", "OffsetStartMs": 7390, "OffsetEndMs": 7815}, {"Word": "passionate", "OffsetStartMs": 7815, "OffsetEndMs": 8325}, {"Word": "about", "OffsetStartMs": 8325, "OffsetEndMs": 8535}, {"Word": "scientific", "OffsetStartMs": 8535, "OffsetEndMs": 8840}, {"Word": "innovation", "OffsetStartMs": 9100, "OffsetEndMs": 9500}, {"Word": ".", "OffsetStartMs": 9820, "OffsetEndMs": 10140}, {"Word": "Werelease", "OffsetStartMs": 10140, "OffsetEndMs": 10440}, {"Word": "open", "OffsetStartMs": 10440, "OffsetEndMs": 10710}, {"Word": "source", "OffsetStartMs": 10710, "OffsetEndMs": 10950}, {"Word": "tools", "OffsetStartMs": 10950, "OffsetEndMs": 11235}, {"Word": "like", "OffsetStartMs": 11235, "OffsetEndMs": 11460}, {"Word": "the", "OffsetStartMs": 11460, "OffsetEndMs": 11595}, {"Word": "ones", "OffsetStartMs": 11595, "OffsetEndMs": 11745}, {"Word": "you'll", "OffsetStartMs": 11745, "OffsetEndMs": 12000}, {"Word": "use", "OffsetStartMs": 12000, "OffsetEndMs": 12180}, {"Word": "today", "OffsetStartMs": 12180, "OffsetEndMs": 12500}, {"Word": "and", "OffsetStartMs": 12910, "OffsetEndMs": 13230}, {"Word": "our", "OffsetStartMs": 13230, "OffsetEndMs": 13455}, {"Word": "products", "OffsetStartMs": 13455, "OffsetEndMs": 13760}, {"Word": "transform", "OffsetStartMs": 14080, "OffsetEndMs": 14445}, {"Word": "AI", "OffsetStartMs": 14445, "OffsetEndMs": 14685}, {"Word": "workflows", "OffsetStartMs": 14685, "OffsetEndMs": 15180}, {"Word": "and", "OffsetStartMs": 15180, "OffsetEndMs": 15390}, {"Word": "make", "OffsetStartMs": 15390, "OffsetEndMs": 15645}, {"Word": "artificial", "OffsetStartMs": 15645, "OffsetEndMs": 16005}, {"Word": "intelligence", "OffsetStartMs": 16005, "OffsetEndMs": 16370}, {"Word": "safer", "OffsetStartMs": 16420, "OffsetEndMs": 16920}, {"Word": "for", "OffsetStartMs": 16920, "OffsetEndMs": 17240}, {"Word": "everyone", "OffsetStartMs": 17260, "OffsetEndMs": 17660}, {"Word": ".", "OffsetStartMs": 18250, "OffsetEndMs": 18525}, {"Word": "Wepartner", "OffsetStartMs": 18525, "OffsetEndMs": 18765}, {"Word": "with", "OffsetStartMs": 18765, "OffsetEndMs": 19125}, {"Word": "industries", "OffsetStartMs": 19125, "OffsetEndMs": 19500}, {"Word": "around", "OffsetStartMs": 19500, "OffsetEndMs": 19800}, {"Word": "the", "OffsetStartMs": 19800, "OffsetEndMs": 20010}, {"Word": "globe", "OffsetStartMs": 20010, "OffsetEndMs": 20300}, {"Word": "and", "OffsetStartMs": 20590, "OffsetEndMs": 20985}, {"Word": "we're", "OffsetStartMs": 20985, "OffsetEndMs": 21315}, {"Word": "hiring", "OffsetStartMs": 21315, "OffsetEndMs": 21540}, {"Word": "for", "OffsetStartMs": 21540, "OffsetEndMs": 21765}, {"Word": "the", "OffsetStartMs": 21765, "OffsetEndMs": 21960}, {"Word": "upcoming", "OffsetStartMs": 21960, "OffsetEndMs": 22245}, {"Word": "summer", "OffsetStartMs": 22245, "OffsetEndMs": 22545}, {"Word": "and", "OffsetStartMs": 22545, "OffsetEndMs": 22845}, {"Word": "for", "OffsetStartMs": 22845, "OffsetEndMs": 23040}, {"Word": "full", "OffsetStartMs": 23040, "OffsetEndMs": 23205}, {"Word": "time", "OffsetStartMs": 23205, "OffsetEndMs": 23430}, {"Word": "roles", "OffsetStartMs": 23430, "OffsetEndMs": 23750}, {"Word": ".", "OffsetStartMs": 24040, "OffsetEndMs": 24270}, {"Word": "Soif", "OffsetStartMs": 24270, "OffsetEndMs": 24345}, {"Word": "you're", "OffsetStartMs": 24345, "OffsetEndMs": 24675}, {"Word": "interested", "OffsetStartMs": 24675, "OffsetEndMs": 25050}, {"Word": "please", "OffsetStartMs": 25050, "OffsetEndMs": 25350}, {"Word": "send", "OffsetStartMs": 25350, "OffsetEndMs": 25530}, {"Word": "an", "OffsetStartMs": 25530, "OffsetEndMs": 25725}, {"Word": "email", "OffsetStartMs": 25725, "OffsetEndMs": 26010}, {"Word": "to", "OffsetStartMs": 26010, "OffsetEndMs": 26265}, {"Word": "careers", "OffsetStartMs": 26265, "OffsetEndMs": 26610}, {"Word": "at", "OffsetStartMs": 26610, "OffsetEndMs": 26775}, {"Word": "famousus", "OffsetStartMs": 26775, "OffsetEndMs": 27105}, {"Word": "I", "OffsetStartMs": 27105, "OffsetEndMs": 27330}, {"Word": "dot", "OffsetStartMs": 27330, "OffsetEndMs": 27495}, {"Word": "io", "OffsetStartMs": 27495, "OffsetEndMs": 27920}, {"Word": "or", "OffsetStartMs": 28150, "OffsetEndMs": 28530}, {"Word": "apply", "OffsetStartMs": 28530, "OffsetEndMs": 28890}, {"Word": "by", "OffsetStartMs": 28890, "OffsetEndMs": 29175}, {"Word": "submitting", "OffsetStartMs": 29175, "OffsetEndMs": 29475}, {"Word": "your", "OffsetStartMs": 29475, "OffsetEndMs": 29655}, {"Word": "resume", "OffsetStartMs": 29655, "OffsetEndMs": 29930}, {"Word": "to", "OffsetStartMs": 30220, "OffsetEndMs": 30465}, {"Word": "the", "OffsetStartMs": 30465, "OffsetEndMs": 30570}, {"Word": "deep", "OffsetStartMs": 30570, "OffsetEndMs": 30735}, {"Word": "learning", "OffsetStartMs": 30735, "OffsetEndMs": 30990}, {"Word": "resume", "OffsetStartMs": 30990, "OffsetEndMs": 31340}, {"Word": "drop", "OffsetStartMs": 31360, "OffsetEndMs": 31710}, {"Word": "and", "OffsetStartMs": 31710, "OffsetEndMs": 31950}, {"Word": "we'll", "OffsetStartMs": 31950, "OffsetEndMs": 32175}, {"Word": "see", "OffsetStartMs": 32175, "OffsetEndMs": 32280}, {"Word": "those", "OffsetStartMs": 32280, "OffsetEndMs": 32430}, {"Word": "resumes", "OffsetStartMs": 32430, "OffsetEndMs": 32835}, {"Word": "and", "OffsetStartMs": 32835, "OffsetEndMs": 33000}, {"Word": "get", "OffsetStartMs": 33000, "OffsetEndMs": 33135}, {"Word": "back", "OffsetStartMs": 33135, "OffsetEndMs": 33330}, {"Word": "to", "OffsetStartMs": 33330, "OffsetEndMs": 33480}, {"Word": "you", "OffsetStartMs": 33480, "OffsetEndMs": 33710}, {"Word": ".", "OffsetStartMs": 33730, "OffsetEndMs": 34035}, {"Word": "Thankyou", "OffsetStartMs": 34035, "OffsetEndMs": 34340}], "SpeechSpeed": 17.7}]}, "RequestId": "bd382961-81ca-41b8-b9a9-1821a11b7c58"}