1
00:00:09,220 --> 00:00:11,025
0,260 260,520 810,1210 1380,1670 1670,1805
Good afternoon, everyone,| {thank,you -}
大家下午好，|感谢大家今天的参与，

2
00:00:11,025 --> 00:00:12,135
0,135 135,300 300,555 555,885 885,1110
all for joining today,| my
|我的名字是 Alexander Amini，

3
00:00:12,135 --> 00:00:14,565
0,150 150,420 420,960 960,1475 2065,2430
name is Alexander Amini| and
|我将是你们今年的课程组织者之一，

4
00:00:14,565 --> 00:00:15,285
0,285 285,375 375,480 480,570 570,720
I'll be one of your

5
00:00:15,285 --> 00:00:17,430
0,305 385,995 1195,1500 1500,1805 1855,2145
course organizers this year| along
|和 Ava 一起，

6
00:00:17,430 --> 00:00:19,830
0,270 270,830 1210,1610 1630,1965 1965,2400
with Ava| and together we're
|我们非常兴奋地

7
00:00:19,830 --> 00:00:21,750
0,380 460,860 1030,1430 1480,1770 1770,1920
super excited to| introduce you
|向你们所有人介绍深度学习导论，

8
00:00:21,750 --> 00:00:23,955
0,240 240,620 1090,1490 1780,2040 2040,2205
all to Introduction to {Deep,Learning

9
00:00:23,955 --> 00:00:26,145
0,305 415,815 1105,1505 1765,2025 2025,2190
-},| now MIT {Introduction,to} Deep
|MIT 的深度学习导论是

10
00:00:26,145 --> 00:00:27,750
0,305 445,780 780,1020 1020,1275 1275,1605
Learning is| a really, really
|一个非常有趣、令人兴奋和快节奏的项目，

11
00:00:27,750 --> 00:00:29,930
0,380 640,1040 1060,1365 1365,1665 1665,2180
fun, exciting and fast paced

12
00:00:30,430 --> 00:00:31,890
0,400 450,725 725,950 950,1250 1250,1460
program here at MIT| and
|让我首先给你们一点背景，

13
00:00:31,890 --> 00:00:32,820
0,105 105,255 255,480 480,720 720,930
let me start by just

14
00:00:32,820 --> 00:00:33,660
0,195 195,330 330,480 480,705 705,840
first of all, giving you

15
00:00:33,660 --> 00:00:34,460
0,90 90,240 240,375 375,510 510,800
a little bit of background|
|

16
00:00:34,900 --> 00:00:36,675
0,400 930,1190 1190,1340 1340,1550 1550,1775
into what we do and
让你们了解我们今年所做的工作和你们将学到的东西。

17
00:00:36,675 --> 00:00:37,485
0,180 180,345 345,465 465,615 615,810
what you're going to learn

18
00:00:37,485 --> 00:00:39,560
0,255 255,465 465,755 1225,1625 1675,2075
{about,this -} year.| So this
|所以，本周的深度学习导论，

19
00:00:39,820 --> 00:00:41,160
0,400 540,800 800,995 995,1190 1190,1340
week of Introduction to Deep

20
00:00:41,160 --> 00:00:42,165
0,285 285,570 570,675 675,810 810,1005
Learning,| we're going to cover
|我们将涉及大量的材料，在短短一周内，

21
00:00:42,165 --> 00:00:43,425
0,315 315,570 570,795 795,1065 1065,1260
{a,ton -} of material in

22
00:00:43,425 --> 00:00:45,060
0,275 385,690 690,995 1195,1515 1515,1635
just one week,| you'll learn
|你将学习这个非常吸引人和令人兴奋的领域的基础知识，

23
00:00:45,060 --> 00:00:46,560
0,135 135,590 670,990 990,1230 1230,1500
the foundations of this really,

24
00:00:46,560 --> 00:00:48,915
0,350 610,1010 1150,1550 1570,1970 1990,2355
really fascinating and exciting field|
|

25
00:00:48,915 --> 00:00:50,550
0,360 360,660 660,930 930,1275 1275,1635
of deep learning and artificial
关于深度学习和人工智能，

26
00:00:50,550 --> 00:00:53,355
0,380 1330,1730 1930,2250 2250,2520 2520,2805
intelligence| and more importantly, you're
|更重要的是，你将亲身体验，

27
00:00:53,355 --> 00:00:54,440
0,135 135,270 270,420 420,705 705,1085
going to get hands on

28
00:00:55,190 --> 00:00:57,115
0,400 660,935 935,1610 1610,1775 1775,1925
experience,| actually reinforcing what you
|巩固了你在课程中学到的内容，

29
00:00:57,115 --> 00:00:58,990
0,275 325,660 660,855 855,1355 1555,1875
learn in the lectures| as
|作为动手软件实验的一部分。

30
00:00:58,990 --> 00:01:00,100
0,195 195,360 360,570 570,810 810,1110
part of hands on software

31
00:01:00,100 --> 00:01:01,560
0,560
labs.|
|

32
00:01:01,570 --> 00:01:02,750
0,305 305,515 515,680 680,860 860,1180
Now, over the past decade,|
现在，在过去的十年里，|

33
00:01:03,100 --> 00:01:04,290
0,275 275,455 455,665 665,935 935,1190
AI and deep learning have
人工智能和深度学习有了巨大的复兴，

34
00:01:04,290 --> 00:01:06,225
0,240 240,590 760,1095 1095,1335 1335,1935
really had a huge resurgence|
|

35
00:01:06,225 --> 00:01:08,715
0,240 240,545 655,1055 1195,1985 2185,2490
and had incredible successes| and
取得了令人难以置信的成功，|许多问题，即使在十年前，

36
00:01:08,715 --> 00:01:09,735
0,165 165,285 285,420 420,695 715,1020
a lot of problems, that

37
00:01:09,735 --> 00:01:11,085
0,270 270,555 555,750 750,1020 1020,1350
even just a decade ago,|
|

38
00:01:11,085 --> 00:01:12,045
0,225 225,420 420,600 600,765 765,960
we thought were not really
我们还认为这些问题在不久的将来甚至无法真正解决，

39
00:01:12,045 --> 00:01:14,010
0,305 595,1295 1375,1650 1650,1785 1785,1965
even solvable in the near

40
00:01:14,010 --> 00:01:15,420
0,320 460,735 735,945 945,1245 1245,1410
future,| {now\,,we're -} solving with
|现在，我们用深度学习很轻松地解决。

41
00:01:15,420 --> 00:01:17,540
0,180 180,470 850,1245 1245,1640 1720,2120
deep learning with incredible ease.|
|

42
00:01:18,220 --> 00:01:19,620
0,380 380,695 695,965 965,1160 1160,1400
Now, this past year in
在过去的这一年，尤其是 2022 年，

43
00:01:19,620 --> 00:01:21,765
0,380 460,780 780,1290 1290,1905 1905,2145
{particular,of -} 2022| has been
|对于深度学习的进步来说，是令人难以置信的一年，

44
00:01:21,765 --> 00:01:23,295
0,240 240,570 570,965 1045,1335 1335,1530
an incredible year for deep

45
00:01:23,295 --> 00:01:25,035
0,285 285,665 1195,1440 1440,1605 1605,1740
learning progress,| and I'd like
|我想说的是，

46
00:01:25,035 --> 00:01:25,935
0,165 165,300 300,510 510,720 720,900
to say,| that actually this
|过去的一年成为生成式深度学习的一年，

47
00:01:25,935 --> 00:01:27,520
0,240 240,435 435,645 645,995
past year in particular

48
00:01:27,520 --> 00:01:28,770
0,285 285,480 480,630 630,870 870,1250
has been the year of

49
00:01:28,850 --> 00:01:30,625
0,545 545,755 755,1060 1080,1475 1475,1775
Generative Deep Learning,| using deep
|使用深度学习来生成全新类型的数据，

50
00:01:30,625 --> 00:01:32,365
0,300 300,600 600,905 1165,1500 1500,1740
learning to generate brand new

51
00:01:32,365 --> 00:01:33,820
0,305 415,720 720,1025 1045,1305 1305,1455
types of data,| that I've
|我以前从未见过，

52
00:01:33,820 --> 00:01:35,280
0,150 150,375 375,585 585,890 1060,1460
never been seen before| and
|在现实中也从未存在。

53
00:01:35,450 --> 00:01:37,885
0,400 960,1535 1535,1790 1790,2080 2160,2435
never existed in reality.| {In,fact,
|事实上，我想开始这门课，

54
00:01:37,885 --> 00:01:39,070
0,275 415,690 690,810 810,960 960,1185
-} I want to start

55
00:01:39,070 --> 00:01:40,255
0,240 240,495 495,765 765,975 975,1185
this class| by actually showing
|通过展示几年前我们是如何开始这门课的，

56
00:01:40,255 --> 00:01:41,245
0,225 225,405 405,585 585,780 780,990
you how we started this

57
00:01:41,245 --> 00:01:43,260
0,210 210,465 465,780 780,1145 1615,2015
class several years ago,| which
|我马上会播放这段视频，

58
00:01:43,460 --> 00:01:44,695
0,305 305,530 530,755 755,965 965,1235
was by playing this video,

59
00:01:44,695 --> 00:01:45,415
0,210 210,390 390,495 495,600 600,720
that I'll play in a

60
00:01:45,415 --> 00:01:47,860
0,275 775,1095 1095,1415 1525,1925 2065,2445
second,| now, this video actually
|这段视频实际上是这门课的介绍性视频，

61
00:01:47,860 --> 00:01:49,870
0,380 520,795 795,1440 1440,1730 1750,2010
was an introductory video for

62
00:01:49,870 --> 00:01:51,900
0,135 135,410 430,705 705,980 1630,2030
{the,class -},| it was and
|在某种程度上证明了我所说的想法。

63
00:01:51,920 --> 00:01:53,905
0,260 260,470 470,1270 1290,1685 1685,1985
kind of exemplifies this idea

64
00:01:53,905 --> 00:01:55,200
0,150 150,345 345,600 600,995
that I'm talking about.|
|

65
00:01:55,210 --> 00:01:56,145
0,230 230,320 320,440 440,665 665,935
So let me just stop
所以，让我先停下来，播放这段视频。

66
00:01:56,145 --> 00:01:57,090
0,195 195,375 375,555 555,720 720,945
there and play this video

67
00:01:57,090 --> 00:01:58,280
0,255 255,405 405,650
first of all.|
|

68
00:02:01,050 --> 00:02:03,515
0,395 395,790 1110,1445 1445,1780 2100,2465
Hi, everybody, and welcome to
大家好，欢迎来到 MIT 6.S191 ，

69
00:02:03,515 --> 00:02:05,810
0,365 805,1155 1155,1505 1705,2025 2025,2295
MIT {6.S191 - - -

70
00:02:05,810 --> 00:02:06,700
0,350
-},|
|

71
00:02:06,740 --> 00:02:09,520
0,380 380,760 900,1835 1835,2200 2430,2780
the official introductory course on
MIT 关于深度学习的官方入门课程，

72
00:02:09,520 --> 00:02:11,970
0,300 300,650 940,1320 1320,1700 2050,2450
deep learning, taught here at

73
00:02:12,020 --> 00:02:13,300
0,400
MIT,|
|

74
00:02:13,950 --> 00:02:16,925
0,350 350,700 960,1310 1310,2230 2580,2975
{deep,learning -} is revolutionizing so
深度学习彻底改变了很多领域，

75
00:02:16,925 --> 00:02:18,300
0,390 390,785
many fields,|
|

76
00:02:18,300 --> 00:02:21,050
0,210 210,800 1180,1455 1455,1730 2350,2750
from robotics to medicine and
从机器人到医学，以及介于两者之间的一切，

77
00:02:21,370 --> 00:02:23,080
0,365 365,730 780,1180
everything in between,|
|

78
00:02:23,760 --> 00:02:26,075
0,455 455,760 870,1145 1145,1985 1985,2315
you'll learn the fundamentals of
你将学习这个领域的基础知识，

79
00:02:26,075 --> 00:02:27,965
0,285 285,635 955,1260 1260,1530 1530,1890
this field| and how you
|以及如何构建一些令人难以置信的算法，

80
00:02:27,965 --> 00:02:29,860
0,330 330,665 985,1335 1335,1590 1590,1895
can build some of these

81
00:02:30,240 --> 00:02:32,420
0,400 990,1540
incredible algorithms,|
|

82
00:02:32,850 --> 00:02:35,560
0,320 320,640 1170,1570 1800,2200 2310,2710
in fact, this entire speech
事实上，这个演讲和视频都不是真的，

83
00:02:35,880 --> 00:02:38,350
0,380 380,760 1230,1610 1610,1990 2070,2470
and video are not real,|
|

84
00:02:39,080 --> 00:02:40,940
0,120 120,345 345,680 1030,1430 1510,1860
and were created using deep
是使用深度学习和人工智能创建的，

85
00:02:40,940 --> 00:02:44,560
0,350 820,1220 1540,1940 2260,2660
learning and artificial intelligence,|
|

86
00:02:45,310 --> 00:02:47,100
0,275 275,425 425,650 650,1000 1290,1790
and in this class, you'll
在这节课上，你将学习如何，

87
00:02:47,100 --> 00:02:48,520
0,320 340,740
learn how,|
|

88
00:02:48,880 --> 00:02:49,830
0,210 210,375 375,525 525,660 660,950
it has been an honor
今天很荣幸能和你们交谈，

89
00:02:50,090 --> 00:02:51,240
0,305 305,485 485,650 650,845 845,1150
to speak with you today,|
|

90
00:02:51,800 --> 00:02:54,580
0,275 275,455 455,760
and I {hope,you,enjoy,the,course}.|
我希望你们喜欢这门课。|

91
00:02:56,770 --> 00:02:58,140
0,400 600,860 860,1010 1010,1160 1160,1370
So in case you couldn't
所以，为了防止你不能区分，

92
00:02:58,140 --> 00:03:00,570
0,260 550,950 1150,1550 1750,2115 2115,2430
tell,| this video and its
|这个视频和它的整个音频都不是真的，

93
00:03:00,570 --> 00:03:02,535
0,270 270,590 700,1100 1540,1800 1800,1965
entire audio was actually not

94
00:03:02,535 --> 00:03:04,220
0,225 225,420 420,695 715,1395 1395,1685
real,| {it,was -} synthetically generated
|它是由深度学习算法综合生成的，

95
00:03:04,360 --> 00:03:06,120
0,260 260,395 395,560 560,850 1380,1760
by a deep learning algorithm,|
|

96
00:03:06,120 --> 00:03:07,620
0,135 135,300 300,590 760,1155 1155,1500
and when we introduced this
当我们几年前介绍这个课程的时候，

97
00:03:07,620 --> 00:03:09,090
0,350 460,735 735,885 885,1125 1125,1470
class a few years ago,|
|

98
00:03:09,090 --> 00:03:10,410
0,270 270,510 510,765 765,1020 1020,1320
this video was created several
这个视频是几年前制作的，

99
00:03:10,410 --> 00:03:12,570
0,300 300,650 1000,1320 1320,1640 1810,2160
{years,ago -},| but even several
|但即使在几年前，

100
00:03:12,570 --> 00:03:14,685
0,315 315,680 1330,1590 1590,1830 1830,2115
years ago,| when we introduced
|当我们将其放到 YouTube 上时，像病毒一样传播开来，

101
00:03:14,685 --> 00:03:15,555
0,240 240,480 480,630 630,735 735,870
this and put it on

102
00:03:15,555 --> 00:03:17,505
0,275 595,900 900,1140 1140,1665 1665,1950
Youtube, went {somewhat,viral -}, right,|
|

103
00:03:17,505 --> 00:03:18,660
0,240 240,495 495,720 720,915 915,1155
people {really,loved -} this video,|
人们很喜欢这段视频，|

104
00:03:18,660 --> 00:03:20,340
0,330 330,645 645,1290 1290,1470 1470,1680
they were intrigued by how
他们对视频和音频的感觉的真实性很感兴趣，

105
00:03:20,340 --> 00:03:22,190
0,320 670,945 945,1220 1270,1560 1560,1850
real the video and audio

106
00:03:22,600 --> 00:03:25,430
0,380 380,695 695,1030 2070,2450 2450,2830
felt and looked| entirely generated
|完全是由计算机通过算法生成的，

107
00:03:25,510 --> 00:03:27,120
0,400 420,815 815,1250 1250,1415 1415,1610
by {an,algorithm -} by a

108
00:03:27,120 --> 00:03:29,160
0,350 850,1215 1215,1485 1485,1695 1695,2040
computer,| and people were shocked
|人们被这些方法的力量和现实主义所震惊，

109
00:03:29,160 --> 00:03:30,030
0,120 120,285 285,540 540,750 750,870
with the power and the

110
00:03:30,030 --> 00:03:31,665
0,645 645,930 930,1185 1185,1410 1410,1635
{realism,of -} these types of

111
00:03:31,665 --> 00:03:32,985
0,335 355,720 720,960 960,1110 1110,1320
approaches,| and this was a
|这是几年前的事了。

112
00:03:32,985 --> 00:03:34,260
0,240 240,510 510,875
few years ago.|
|

113
00:03:34,260 --> 00:03:35,780
0,240 240,465 465,800 970,1245 1245,1520
Now, fast forward to today
现在，快进到今天，深度学习今天的状态，

114
00:03:36,190 --> 00:03:37,305
0,275 275,470 470,680 680,890 890,1115
and the state of deep

115
00:03:37,305 --> 00:03:39,900
0,305 325,725 1045,1350 1350,1655
learning today,| we have,
|我们已经看到深度学习以前所未有的速度加速，

116
00:03:40,090 --> 00:03:42,530
0,400 870,1220 1220,1460 1460,1750 1800,2440
have seen deep learning accelerating

117
00:03:42,580 --> 00:03:44,205
0,260 260,410 410,700 990,1385 1385,1625
at a rate faster than

118
00:03:44,205 --> 00:03:46,320
0,165 165,330 330,665 685,1085 1825,2115
we've ever seen before,| {in,fact
|事实上，我们现在不仅可以使用深度学习来生成人脸图像，

119
00:03:46,320 --> 00:03:47,340
0,270 270,525 525,660 660,825 825,1020
-}, we can use deep

120
00:03:47,340 --> 00:03:49,140
0,240 240,510 510,720 720,1010 1480,1800
learning now to generate not

121
00:03:49,140 --> 00:03:51,780
0,320 640,1040 1330,1635 1635,1940 2320,2640
just images of faces,| but
|还可以生成完全合成的环境，

122
00:03:51,780 --> 00:03:54,075
0,320 340,705 705,1250 1270,1940 2050,2295
generate full synthetic environments,| where
|我们可以完全在模拟中训练自动驾驶车辆，

123
00:03:54,075 --> 00:03:55,550
0,120 120,300 300,600 600,1185 1185,1475
we can train autonomous vehicles

124
00:03:55,900 --> 00:03:57,765
0,335 335,545 545,1030 1230,1595 1595,1865
entirely in simulation| and deploy
|并将它们无缝地部署在真实世界的全尺寸车辆上，

125
00:03:57,765 --> 00:03:59,270
0,180 180,375 375,630 630,965 1105,1505
them on full scale vehicles

126
00:03:59,350 --> 00:04:01,040
0,275 275,395 395,530 530,820 930,1690
in the real world seamlessly,|
|

127
00:04:01,240 --> 00:04:02,490
0,260 260,520 600,905 905,1100 1100,1250
the videos here you see
你在这里看到的视频实际上是来自一个数据驱动的模拟器，

128
00:04:02,490 --> 00:04:03,570
0,255 255,540 540,705 705,855 855,1080
are actually from a data

129
00:04:03,570 --> 00:04:05,210
0,300 300,840 840,1095 1095,1380 1380,1640
driven simulator,| from neural networks
|来自名为 VISTA 神经网络生成的，是我们在 MIT 建造的，

130
00:04:05,290 --> 00:04:07,605
0,400 1230,1550 1550,1955 1955,2105 2105,2315
generated called VISTA, that we

131
00:04:07,605 --> 00:04:09,020
0,210 210,435 435,750 750,1065 1065,1415
actually built here at MIT|
|

132
00:04:09,550 --> 00:04:10,995
0,260 260,485 485,740 740,1030 1200,1445
and have open source to
并向公众开放了源代码，因此，你们所有人实际上都可以培训和打造自动驾驶汽车的未来。当然，它也远远超出了这一点。深度学习可以用来直接从我们的说话方式和我们通过我们所说的提示传达给它的语言生成内容。

133
00:04:10,995 --> 00:04:11,760
0,105 105,330 330,540 540,645 645,765
{the,public -},| so all of
|所以，你们所有人都可以训练和构建自动驾驶汽车的未来。

134
00:04:11,760 --> 00:04:13,245
0,180 180,450 450,720 720,1040 1210,1485
you can actually train and

135
00:04:13,245 --> 00:04:14,505
0,150 150,300 300,510 510,690 690,1260
build the future of autonomy

136
00:04:14,505 --> 00:04:16,500
0,240 240,450 450,755 805,1205 1705,1995
and {self,driving -} cars.| And
|当然，它也远远超出了这一点，

137
00:04:16,500 --> 00:04:17,550
0,180 180,375 375,540 540,750 750,1050
of course it goes far

138
00:04:17,550 --> 00:04:18,615
0,285 285,510 510,675 675,855 855,1065
{beyond,this -} as well,| deep
|深度学习可以直接从我们所说的话生成内容，

139
00:04:18,615 --> 00:04:19,905
0,305 535,795 795,915 915,1095 1095,1290
learning can be used to

140
00:04:19,905 --> 00:04:22,710
0,275 355,755 1105,1505 1945,2345 2515,2805
generate content directly from how

141
00:04:22,710 --> 00:04:23,780
0,225 225,465 465,645 645,795 795,1070
we speak| and the language
|我们传达给它的语言，我们说的提示，

142
00:04:24,010 --> 00:04:25,310
0,275 275,440 440,860 860,1055 1055,1300
that we convey to it,

143
00:04:25,390 --> 00:04:26,930
0,365 365,830 830,1025 1025,1220 1220,1540
from prompts that we say,|
|

144
00:04:27,500 --> 00:04:28,880
0,270 270,510 510,750 750,1040 1060,1380
deep learning can reason about
深度学习可以对自然语言中的提示进行推理，比如英语，

145
00:04:28,880 --> 00:04:30,425
0,225 225,660 660,950 1000,1275 1275,1545
the prompts in natural language,

146
00:04:30,425 --> 00:04:32,345
0,375 375,755 835,1140 1140,1445 1645,1920
in English, for example,| and
|然后根据我们指定的内容，指导和控制生成什么，

147
00:04:32,345 --> 00:04:34,010
0,180 180,485 625,1025 1075,1440 1440,1665
then guide and control what

148
00:04:34,010 --> 00:04:36,515
0,165 165,470 1300,1700 1810,2190 2190,2505
is generated according to what

149
00:04:36,515 --> 00:04:38,020
0,285 285,905
we specify,|
|

150
00:04:38,090 --> 00:04:39,850
0,350 350,560 560,910 990,1390 1440,1760
we've seen examples of where
我们已经看到了我们可以生成的例子，

151
00:04:39,850 --> 00:04:41,430
0,195 195,375 375,680 970,1275 1275,1580
we can generate,| for example,
|例如，在现实中从未有过的东西，

152
00:04:42,050 --> 00:04:43,810
0,365 365,730 870,1235 1235,1490 1490,1760
things that again have never

153
00:04:43,810 --> 00:04:44,995
0,435 435,570 570,840 840,1080 1080,1185
existed {in,reality},| {we -} can
|我们可以让神经网络生成一张宇航员骑马的照片，

154
00:04:44,995 --> 00:04:46,000
0,165 165,330 330,525 525,765 765,1005
ask a neural network to

155
00:04:46,000 --> 00:04:47,370
0,195 195,530 550,900 900,1110 1110,1370
generate a photo of an

156
00:04:47,420 --> 00:04:50,160
0,820 840,1240 1260,1535 1535,1810 2340,2740
astronaut riding a horse| and
|它可以想象这会是什么样子，

157
00:04:50,270 --> 00:04:52,530
0,380 380,680 680,935 935,1270 1410,2260
it actually can imagine hallucinate

158
00:04:52,700 --> 00:04:54,010
0,305 305,530 530,800 800,1055 1055,1310
what this might look like,|
|

159
00:04:54,010 --> 00:04:55,075
0,270 270,435 435,570 570,810 810,1065
even though of course this
即使这张照片，

160
00:04:55,075 --> 00:04:57,100
0,305 835,1215 1215,1530 1530,1770 1770,2025
photo,| not only this photo
|不仅这张照片以前从未发生过，

161
00:04:57,100 --> 00:04:58,360
0,270 270,570 570,825 825,1050 1050,1260
has never occurred before,| but
|而且我认为以前没有发生过宇航员骑马的照片，

162
00:04:58,360 --> 00:04:59,220
0,90 90,210 210,315 315,525 525,860
I don't think any photo

163
00:04:59,240 --> 00:05:00,565
0,260 260,365 365,860 860,1115 1115,1325
of an astronaut riding a

164
00:05:00,565 --> 00:05:01,780
0,240 240,480 480,735 735,990 990,1215
horse has ever {occurred,before -},|
|

165
00:05:01,780 --> 00:05:02,710
0,195 195,360 360,495 495,690 690,930
so there's not really even
所以，在这种情况下，甚至没有训练数据可供参考，

166
00:05:02,710 --> 00:05:04,105
0,300 300,650 850,1110 1110,1245 1245,1395
training data that you could

167
00:05:04,105 --> 00:05:05,050
0,150 150,330 330,510 510,705 705,945
go off {in,this -} case,|
|

168
00:05:05,050 --> 00:05:06,790
0,320 460,780 780,1080 1080,1410 1410,1740
and my personal favorite is
我个人最喜欢的是，

169
00:05:06,790 --> 00:05:07,660
0,270 270,450 450,615 615,750 750,870
actually,| how we can not
|我们如何不仅可以构建能够生成图像和视频的软件，

170
00:05:07,660 --> 00:05:09,550
0,210 210,525 525,890 1420,1710 1710,1890
only build software that can

171
00:05:09,550 --> 00:05:11,320
0,285 285,680 730,1020 1020,1310 1480,1770
generate images and videos,| but
|而且还可以构建能够生成软件的软件，

172
00:05:11,320 --> 00:05:12,810
0,225 225,560 700,990 990,1185 1185,1490
build software that can {generate,software

173
00:05:13,250 --> 00:05:15,240
0,400 750,1070 1070,1390 1410,1700 1700,1990
-} as well,| we can
|我们也可以有能够接受语言提示的算法，

174
00:05:15,290 --> 00:05:16,645
0,260 260,500 500,875 875,1115 1115,1355
also have algorithms that can

175
00:05:16,645 --> 00:05:18,625
0,240 240,575 655,1265 1435,1740 1740,1980
take language prompts,| for example
|例如，像这样的提示，

176
00:05:18,625 --> 00:05:20,200
0,210 210,420 420,645 645,935 1225,1575
a prompt like this,| write
|在 TensorFlow 中编写代码来生成或训练神经网络，

177
00:05:20,200 --> 00:05:21,870
0,315 315,585 585,1185 1185,1395 1395,1670
code in Tensorflow to generate

178
00:05:22,160 --> 00:05:23,200
0,290 290,470 470,665 665,830 830,1040
or {to,train -} a neural

179
00:05:23,200 --> 00:05:24,730
0,260 670,945 945,1095 1095,1320 1320,1530
network,| and not only will
|它不仅会编写代码并创建神经网络，

180
00:05:24,730 --> 00:05:26,370
0,260 280,585 585,780 780,1070 1240,1640
it write the code and

181
00:05:26,420 --> 00:05:28,150
0,400 690,1010 1010,1280 1280,1505 1505,1730
create that neural network,| but
|而且它还会有能力对它生成的代码进行推理，

182
00:05:28,150 --> 00:05:29,820
0,105 105,350 610,975 975,1305 1305,1670
it will have the ability

183
00:05:29,990 --> 00:05:31,615
0,290 290,580 660,1060 1170,1445 1445,1625
to reason about the code

184
00:05:31,615 --> 00:05:32,620
0,165 165,360 360,555 555,795 795,1005
that it's generated| and walk
|并一步步地向你解释过程和程序，

185
00:05:32,620 --> 00:05:33,960
0,255 255,555 555,810 810,1020 1020,1340
you through step by step

186
00:05:34,010 --> 00:05:35,430
0,410 410,635 635,905 905,1160 1160,1420
explaining the process and procedure|
|

187
00:05:35,930 --> 00:05:36,775
0,290 290,425 425,560 560,710 710,845
all the way from the
从头到尾，

188
00:05:36,775 --> 00:05:37,660
0,195 195,435 435,615 615,750 750,885
ground up to you,| so
|这样你也可以学习如何完成这一过程。

189
00:05:37,660 --> 00:05:38,460
0,150 150,330 330,555 555,750
that you can actually

190
00:05:38,460 --> 00:05:39,675
0,290 520,810 810,945 945,1050 1050,1215
learn how to do this

191
00:05:39,675 --> 00:05:41,420
0,305 475,795 795,1115
process as well.|
|

192
00:05:41,520 --> 00:05:42,600
0,400
Now,
我认为这些例子中的一些

193
00:05:42,720 --> 00:05:43,865
0,275 275,550 570,830 830,950 950,1145
I think some of these

194
00:05:43,865 --> 00:05:45,560
0,335 355,660 660,885 885,1205 1345,1695
examples| really just highlight how
|确实突显了深度学习和这些方法，

195
00:05:45,560 --> 00:05:47,000
0,350 370,675 675,980 1000,1260 1260,1440
far deep learning and these

196
00:05:47,000 --> 00:05:48,425
0,300 300,600 600,920 1030,1290 1290,1425
methods have come,| in the
|在我们开始这门课程的过去六年里，

197
00:05:48,425 --> 00:05:49,535
0,210 210,465 465,705 705,915 915,1110
past six years since we

198
00:05:49,535 --> 00:05:50,600
0,255 255,525 525,750 750,915 915,1065
started this course,| {and,you -}
|你在几年前的介绍性视频中看到了这个例子，

199
00:05:50,600 --> 00:05:52,235
0,165 165,375 375,710 1210,1485 1485,1635
saw that example just a

200
00:05:52,235 --> 00:05:53,570
0,150 150,390 390,755 895,1185 1185,1335
few years ago from that

201
00:05:53,570 --> 00:05:55,205
0,555 555,860 880,1185 1185,1395 1395,1635
introductory video,| but now we're
|但现在我们看到了如此令人难以置信的进步，

202
00:05:55,205 --> 00:05:56,830
0,195 195,480 480,845 865,1365 1365,1625
seeing such incredible advances,| and
|在我看来，这门课程最令人惊叹的部分是，

203
00:05:56,850 --> 00:05:58,145
0,290 290,545 545,890 890,1145 1145,1295
the most amazing part of

204
00:05:58,145 --> 00:05:59,200
0,180 180,390 390,555 555,735 735,1055
this course, in my opinion,

205
00:05:59,550 --> 00:06:01,250
0,400 450,740 740,1030 1110,1460 1460,1700
is actually| that within this
|在这一周内，我们将从头开始，从今天开始，

206
00:06:01,250 --> 00:06:02,470
0,195 195,435 435,675 675,870 870,1220
one week, we're going to

207
00:06:02,580 --> 00:06:03,770
0,290 290,500 500,785 785,1040 1040,1190
take you through from the

208
00:06:03,770 --> 00:06:05,110
0,195 195,495 495,810 810,1050 1050,1340
ground up, starting from today,|
|

209
00:06:05,370 --> 00:06:07,325
0,305 305,485 485,665 665,1180 1560,1955
all of that foundational building
所有的基础构件，

210
00:06:07,325 --> 00:06:08,675
0,395 565,840 840,1020 1020,1200 1200,1350
blocks,| that will allow you
|将让你理解，并使所有这些令人惊叹的进步成为可能。

211
00:06:08,675 --> 00:06:10,445
0,270 270,665 1105,1395 1395,1590 1590,1770
to understand and make all

212
00:06:10,445 --> 00:06:12,430
0,150 150,425 805,1185 1185,1695 1695,1985
of this amazing advances possible.|
|

213
00:06:13,730 --> 00:06:15,100
0,365 365,605 605,800 800,1100 1100,1370
So with that, hopefully now
有了这些，希望你们现在对这门课所教的内容感到很兴奋，

214
00:06:15,100 --> 00:06:17,200
0,180 180,410 760,1155 1155,1550 1810,2100
you're all super excited about

215
00:06:17,200 --> 00:06:18,265
0,195 195,405 405,630 630,840 840,1065
what this class will teach,|
|

216
00:06:18,265 --> 00:06:19,920
0,330 330,600 600,765 765,1055 1255,1655
{and,I,want - -} to basically
我想现在从退后一步开始，

217
00:06:19,970 --> 00:06:21,220
0,320 320,575 575,830 830,1055 1055,1250
now just start by taking

218
00:06:21,220 --> 00:06:23,220
0,165 165,330 330,620 1060,1395 1395,2000
a step back| and introducing
|介绍一些术语，

219
00:06:23,270 --> 00:06:24,955
0,275 275,425 425,695 695,1520 1520,1685
some of these terminologies,| that
|到目前为止我抛出的，

220
00:06:24,955 --> 00:06:25,750
0,195 195,300 300,390 390,525 525,795
I've kind of been throwing

221
00:06:25,750 --> 00:06:27,295
0,380 430,735 735,1035 1035,1335 1335,1545
around so far,| with deep
|深度学习，人工智能，

222
00:06:27,295 --> 00:06:29,050
0,305 625,990 990,1355 1375,1635 1635,1755
learning, artificial intelligence,| what do
|这些东西是什么意思。

223
00:06:29,050 --> 00:06:30,600
0,150 150,435 435,720 720,1010
these things actually mean.|
|

224
00:06:30,800 --> 00:06:31,840
0,380 380,665 665,815 815,920 920,1040
So first of all, I
所以首先，我想花点时间，

225
00:06:31,840 --> 00:06:33,520
0,120 120,330 330,680 730,1130 1420,1680
want to maybe just take

226
00:06:33,520 --> 00:06:35,300
0,150 150,440 610,1010
a second to,|
|

227
00:06:35,430 --> 00:06:36,400
0,275 275,380 380,515 515,695 695,970
speak a little bit about
说一下智能，

228
00:06:36,420 --> 00:06:38,470
0,400 420,680 680,920 920,1300 1650,2050
intelligence| and what intelligence means
|以及智能的核心含义。

229
00:06:38,730 --> 00:06:39,875
0,245 245,425 425,710 710,980 980,1145
at its core.| {So,to -}
|对我来说，智能是一种处理信息的能力，

230
00:06:39,875 --> 00:06:42,260
0,245 745,1145 1345,1710 1710,2055 2055,2385
me, intelligence is simply the

231
00:06:42,260 --> 00:06:44,765
0,350 430,750 750,1070 1300,1700 2200,2505
ability to process information,| such
|这样我们就可以用它来指导我们未来的决定或采取的行动，

232
00:06:44,765 --> 00:06:45,560
0,180 180,315 315,450 450,615 615,795
that we can use it

233
00:06:45,560 --> 00:06:47,260
0,255 255,615 615,915 915,1220 1300,1700
to inform some future decision

234
00:06:47,640 --> 00:06:49,120
0,290 290,580 720,995 995,1175 1175,1480
or action that we take,|
|

235
00:06:49,810 --> 00:06:51,405
0,400 420,695 695,875 875,1180 1200,1595
now, the field of artificial
现在，人工智能领域就是我们构建算法的能力，

236
00:06:51,405 --> 00:06:53,430
0,395 445,795 795,1145 1225,1625 1645,2025
intelligence is simply the ability

237
00:06:53,430 --> 00:06:55,250
0,195 195,300 300,450 450,710 1330,1820
for us to build algorithms,|
|

238
00:06:55,480 --> 00:06:57,090
0,400 510,1025 1025,1295 1295,1430 1430,1610
artificial algorithms that can do
人工算法，可以完成这个，

239
00:06:57,090 --> 00:06:59,475
0,270 270,620 820,1220 1390,1790 2050,2385
exactly this,| process information, to
|处理信息，指导未来的决策。

240
00:06:59,475 --> 00:07:01,940
0,285 285,540 540,845 985,1385 2065,2465
inform some future decision.| {Now\,,machine
|机器学习是人工智能的一个子集，

241
00:07:01,960 --> 00:07:03,195
0,275 275,550 570,845 845,1040 1040,1235
-} learning is simply a

242
00:07:03,195 --> 00:07:04,700
0,315 315,525 525,780 780,960 960,1505
subset of AI,| which focuses
|它特别关注，

243
00:07:04,900 --> 00:07:06,380
0,400 510,845 845,1055 1055,1205 1205,1480
specifically on,| how we can
|我们如何构建一台机器或教一台机器，

244
00:07:06,490 --> 00:07:09,870
0,350 350,700 870,1270 1740,2140 3060,3380
build a machine to {or,teach}

245
00:07:09,870 --> 00:07:10,965
0,240 240,525 525,780 780,945 945,1095
a machine,| how to do
|如何根据一些经验或数据来做到这一点。

246
00:07:10,965 --> 00:07:12,920
0,275 355,720 720,1035 1035,1385 1555,1955
this from some experiences or

247
00:07:13,030 --> 00:07:15,405
0,400 480,815 815,1150 1740,2105 2105,2375
data, for example.| Now, deep
|现在，深度学习更进一步，

248
00:07:15,405 --> 00:07:16,800
0,255 255,525 525,795 795,1110 1110,1395
learning goes one step beyond

249
00:07:16,800 --> 00:07:18,105
0,320 400,675 675,810 810,945 945,1305
this| and is a subset
|它是机器学习的一个子集，

250
00:07:18,105 --> 00:07:19,640
0,225 225,450 450,690 690,960 960,1535
of machine learning,| which focuses
|它明确地关注所谓的神经网络，

251
00:07:19,720 --> 00:07:20,775
0,530 530,635 635,755 755,875 875,1055
explicitly on what are called

252
00:07:20,775 --> 00:07:21,900
0,270 270,545 565,840 840,990 990,1125
neural networks| and how we
|以及我们如何构建神经网络，

253
00:07:21,900 --> 00:07:23,325
0,105 105,240 240,510 510,770 1120,1425
can build neural networks,| that
|可以从数据中提取特征，

254
00:07:23,325 --> 00:07:24,690
0,285 285,630 630,990 990,1245 1245,1365
can extract features in {the,data

255
00:07:24,690 --> 00:07:25,935
0,255 255,510 510,675 675,980 1000,1245
-},| these are basically what
|这些你可以认为是在数据中出现的模式，

256
00:07:25,935 --> 00:07:26,580
0,90 90,210 210,345 345,450 450,645
you can think of as

257
00:07:26,580 --> 00:07:28,230
0,350 730,1050 1050,1245 1245,1455 1455,1650
patterns, that occur within the

258
00:07:28,230 --> 00:07:29,205
0,260 340,600 600,720 720,840 840,975
data,| so that it can
|这样它也可以学习完成这些任务。

259
00:07:29,205 --> 00:07:30,440
0,210 210,480 480,705 705,915 915,1235
learn to complete these tasks

260
00:07:30,760 --> 00:07:31,680
0,290 290,580
as well.|
|

261
00:07:32,700 --> 00:07:34,450
0,400 570,995 995,1250 1250,1475 1475,1750
Now that's exactly what this
这就是这门课核心的真正意义所在，

262
00:07:34,620 --> 00:07:35,960
0,320 320,530 530,755 755,1025 1025,1340
class is really all about

263
00:07:35,960 --> 00:07:37,265
0,225 225,420 420,770 850,1185 1185,1305
{at,its -} core,| we're going
|我们将尝试教你，

264
00:07:37,265 --> 00:07:37,940
0,135 135,240 240,375 375,555 555,675
to try and teach you|
|

265
00:07:37,940 --> 00:07:38,770
0,90 90,195 195,285 285,390 390,830
and give you the foundational
给你基本的理解，

266
00:07:38,940 --> 00:07:40,490
0,400 630,950 950,1175 1175,1370 1370,1550
understanding| and how we can
|我们如何建立和教计算机学习任务，

267
00:07:40,490 --> 00:07:43,385
0,290 670,1065 1065,1460 1570,1970 2620,2895
build and teach computers to

268
00:07:43,385 --> 00:07:45,140
0,240 240,605 955,1275 1275,1530 1530,1755
learn tasks,| many different types
|许多不同类型的任务，

269
00:07:45,140 --> 00:07:47,135
0,210 210,530 910,1305 1305,1695 1695,1995
of tasks,| directly from raw
|直接从原始数据，

270
00:07:47,135 --> 00:07:48,530
0,305 565,825 825,1050 1050,1200 1200,1395
data,| and that's really what
|这就是这门课最简单的形式。

271
00:07:48,530 --> 00:07:49,685
0,180 180,420 420,780 780,990 990,1155
this class boils down to

272
00:07:49,685 --> 00:07:51,640
0,245 415,720 720,1025 1225,1590 1590,1955
at its most {simple,form -}.|
|

273
00:07:52,140 --> 00:07:53,240
0,260 260,470 470,695 695,890 890,1100
And we'll provide a very
我们将为你提供一个非常坚实的基础，

274
00:07:53,240 --> 00:07:55,085
0,285 285,620 880,1140 1140,1400 1540,1845
solid foundation for you| both
|通过课程在技术方面，

275
00:07:55,085 --> 00:07:56,780
0,195 195,375 375,665 715,1115 1405,1695
on the technical side, through

276
00:07:56,780 --> 00:07:58,490
0,150 150,650 1030,1305 1305,1455 1455,1710
the lectures,| which will happen
|整个课程分两部分进行，

277
00:07:58,490 --> 00:07:59,915
0,270 270,450 450,740 910,1230 1230,1425
in two parts {throughout,the -}

278
00:07:59,915 --> 00:08:00,950
0,240 240,480 480,660 660,885 885,1035
class,| the first lecture and
|第一节课，第二节课，

279
00:08:00,950 --> 00:08:02,105
0,105 105,315 315,645 645,945 945,1155
the second lecture,| each one
|每节课大约一个小时，

280
00:08:02,105 --> 00:08:03,460
0,210 210,375 375,585 585,935
about one hour long,|
|

281
00:08:03,500 --> 00:08:05,005
0,380 380,695 695,905 905,1175 1175,1505
followed by a software lab,|
随后是软件实验，|

282
00:08:05,005 --> 00:08:06,265
0,225 225,420 420,725 745,1080 1080,1260
which will immediately follow the
紧跟在课程之后，

283
00:08:06,265 --> 00:08:07,675
0,485 685,960 960,1125 1125,1290 1290,1410
lectures,| which will try to
|它将试图加强，

284
00:08:07,675 --> 00:08:08,605
0,435 435,600 600,705 705,810 810,930
reinforce| a lot of what
|很多我们在课程的技术部分中所涉及的内容，

285
00:08:08,605 --> 00:08:10,495
0,150 150,330 330,635 925,1325 1615,1890
we cover in the in

286
00:08:10,495 --> 00:08:11,725
0,240 240,605 625,930 930,1095 1095,1230
the technical part of the

287
00:08:11,725 --> 00:08:13,810
0,275 775,1175 1495,1740 1740,1905 1905,2085
class,| and you know give
|给你实践这些想法的经验。

288
00:08:13,810 --> 00:08:15,280
0,120 120,270 270,510 510,860 940,1470
you hands on experience implementing

289
00:08:15,280 --> 00:08:17,220
0,255 255,650 1210,1455 1455,1620 1620,1940
those ideas.| {So,this -} program
|所以这个项目分为两部分，

290
00:08:17,570 --> 00:08:19,150
0,380 380,740 740,1040 1040,1310 1310,1580
is split between these two

291
00:08:19,150 --> 00:08:20,710
0,320 430,720 720,990 990,1425 1425,1560
pieces,| the technical lectures and
|技术课程和软件实验，

292
00:08:20,710 --> 00:08:22,195
0,135 135,390 390,980 1150,1380 1380,1485
the software labs,| we have
|我们今年有几个具体的新更新，

293
00:08:22,195 --> 00:08:23,395
0,225 225,510 510,765 765,990 990,1200
several new updates this year

294
00:08:23,395 --> 00:08:25,375
0,210 210,515 895,1295 1345,1725 1725,1980
in specific,| especially in many
|特别是在后面的许多课程中。

295
00:08:25,375 --> 00:08:27,295
0,150 150,300 300,525 525,1085 1645,1920
of the {later,lectures -}.| The
|第一节课将涉及深度学习的基础，

296
00:08:27,295 --> 00:08:28,480
0,180 180,435 435,675 675,945 945,1185
first lecture will cover the

297
00:08:28,480 --> 00:08:29,935
0,405 405,645 645,825 825,1110 1110,1455
foundations of deep learning,| which
|这是现在将要进行的。

298
00:08:29,935 --> 00:08:30,775
0,255 255,435 435,555 555,660 660,840
is going to be right

299
00:08:30,775 --> 00:08:31,860
0,305
now.|
|

300
00:08:32,090 --> 00:08:33,715
0,290 290,545 545,950 950,1385 1385,1625
And finally, we'll conclude the
最后，我们将结束这门课程，

301
00:08:33,715 --> 00:08:35,370
0,335 355,660 660,945 945,1290 1290,1655
course| with some very exciting
|一些非常令人兴奋来自学术界和产业界的客座演讲，

302
00:08:35,390 --> 00:08:37,500
0,350 350,910 930,1205 1205,1445 1445,2110
guest lectures from both academia

303
00:08:37,520 --> 00:08:38,880
0,400 570,860 860,980 980,1085 1085,1360
and industry,| who are really
|他们真正引领和推动了人工智能和深度学习的发展。

304
00:08:39,110 --> 00:08:41,140
0,400 420,725 725,1030 1080,1480 1710,2030
leading and driving forward the

305
00:08:41,140 --> 00:08:42,550
0,315 315,675 675,990 990,1230 1230,1410
state of AI and deep

306
00:08:42,550 --> 00:08:44,110
0,290 580,840 840,975 975,1250 1300,1560
learning,| {And,of -} course, we
|当然，我们还有很多令人惊叹的奖品，

307
00:08:44,110 --> 00:08:45,895
0,225 225,590 790,1110 1110,1560 1560,1785
have many awesome prizes,| that
|这些奖品来自所有的软件实验，

308
00:08:45,895 --> 00:08:47,815
0,240 240,545 1375,1650 1650,1785 1785,1920
go with all of the

309
00:08:47,815 --> 00:08:49,560
0,255 255,810 810,1095 1095,1380 1380,1745
software labs| and the project
|和课程结束时的项目竞赛，

310
00:08:49,730 --> 00:08:50,695
0,365 365,590 590,695 695,815 815,965
competition at the end of

311
00:08:50,695 --> 00:08:52,435
0,150 150,425 535,935 985,1380 1380,1740
the course,| so maybe quickly
|所以，也许每天要快速地经历这些，

312
00:08:52,435 --> 00:08:53,515
0,225 225,390 390,585 585,810 810,1080
to go through these {each,day

313
00:08:53,515 --> 00:08:54,670
0,335 385,660 660,825 825,990 990,1155
-},| like I said, we'll
|就像我说的，我们会有专门的软件实验，与课程相结合，

314
00:08:54,670 --> 00:08:56,110
0,105 105,380 490,855 855,1275 1275,1440
have dedicated software labs that

315
00:08:56,110 --> 00:08:58,280
0,240 240,480 480,615 615,1100
couple with the lectures,|
|

316
00:08:58,290 --> 00:08:59,770
0,365 365,650 650,860 860,1115 1115,1480
starting today, with lab one,|
从今天开始，从实验一开始，|

317
00:08:59,970 --> 00:09:01,580
0,520 690,980 980,1205 1205,1400 1400,1610
you'll actually build a neural
你将建立一个神经网络，

318
00:09:01,580 --> 00:09:02,870
0,240 240,585 585,840 840,1050 1050,1290
network,| {keeping,with -} the theme
|遵循生成性人工智能的主题，

319
00:09:02,870 --> 00:09:04,280
0,225 225,735 735,1020 1020,1260 1260,1410
of generative AI,| you'll build
|你将构建一个神经网络，

320
00:09:04,280 --> 00:09:05,270
0,135 135,330 330,555 555,795 795,990
a neural network,| that can
|它可以学习、听很多音乐，

321
00:09:05,270 --> 00:09:06,770
0,320 640,1020 1020,1230 1230,1335 1335,1500
learn, listen to a lot

322
00:09:06,770 --> 00:09:08,360
0,290 310,710 880,1230 1230,1440 1440,1590
of music,| and actually learn
|并学习如何在该音乐流派中生成全新的歌曲。

323
00:09:08,360 --> 00:09:09,515
0,150 150,285 285,560 610,930 930,1155
how to generate brand new

324
00:09:09,515 --> 00:09:11,090
0,305 445,735 735,960 960,1395 1395,1575
songs in that genre of

325
00:09:11,090 --> 00:09:12,160
0,320
music.|
|

326
00:09:12,710 --> 00:09:13,945
0,275 275,395 395,640 810,1085 1085,1235
At the end, at the
最后，在课程的下一阶段，在星期五，

327
00:09:13,945 --> 00:09:14,980
0,210 210,465 465,660 660,825 825,1035
next level of the class,

328
00:09:14,980 --> 00:09:16,225
0,270 270,615 615,915 915,1065 1065,1245
on Friday,| we'll host a
|我们将举办一场项目推介比赛，

329
00:09:16,225 --> 00:09:17,815
0,255 255,635 715,1080 1080,1335 1335,1590
project pitch competition,| where either
|你可以单独或作为小组一员，

330
00:09:17,815 --> 00:09:20,095
0,365 595,1170 1170,1565 1825,2130 2130,2280
you individually or as part

331
00:09:20,095 --> 00:09:21,600
0,90 90,240 240,545 805,1155 1155,1505
of a group| can participate
|参与并向我们所有人展示一个想法，

332
00:09:21,800 --> 00:09:23,875
0,400 420,820 960,1340 1340,1720 1800,2075
and present an idea,| a
|一个新颖的深度学习想法，

333
00:09:23,875 --> 00:09:26,350
0,270 270,570 570,875 895,1295 2125,2475
novel deep learning idea to

334
00:09:26,350 --> 00:09:27,670
0,225 225,360 360,620 1000,1230 1230,1320
all of us,| {it,will -}
|这大约是三分钟的时间，

335
00:09:27,670 --> 00:09:29,035
0,120 120,380 430,720 720,1010 1030,1365
be roughly three minutes in

336
00:09:29,035 --> 00:09:31,410
0,335 745,1145 1555,1845 1845,2055 2055,2375
length| and we will focus
|我们不会关注太多，

337
00:09:31,850 --> 00:09:32,875
0,275 275,455 455,695 695,890 890,1025
not as much| because this
|因为这是一个一周的计划，

338
00:09:32,875 --> 00:09:34,290
0,135 135,395 415,750 750,1050 1050,1415
is a one week program,|
|

339
00:09:34,460 --> 00:09:35,185
0,230 230,320 320,470 470,620 620,725
we are not going to
我们不会太关注你推介的结果，

340
00:09:35,185 --> 00:09:36,100
0,225 225,450 450,570 570,720 720,915
focus so much on the

341
00:09:36,100 --> 00:09:37,480
0,320 400,675 675,855 855,1125 1125,1380
results of your pitch,| but
|而是（关注）你试图提出的东西的创新、想法和新颖性。

342
00:09:37,480 --> 00:09:38,935
0,270 270,585 585,920 970,1245 1245,1455
rather the innovation and the

343
00:09:38,935 --> 00:09:40,375
0,195 195,300 300,405 405,995 1135,1440
idea and the novelty of

344
00:09:40,375 --> 00:09:41,460
0,180 180,390 390,540 540,705 705,1085
what you're trying to propose.|
|

345
00:09:42,080 --> 00:09:43,315
0,275 275,545 545,815 815,1010 1010,1235
The prizes here are are
这里的奖品已经相当可观了，

346
00:09:43,315 --> 00:09:45,130
0,210 210,515 775,1110 1110,1445 1465,1815
quite significant already,| where first
|一等奖将获得 NVIDIA GPU ，

347
00:09:45,130 --> 00:09:45,940
0,255 255,435 435,585 585,705 705,810
prize is going to get

348
00:09:45,940 --> 00:09:46,960
0,120 120,585 585,765 765,900 900,1020
an NVIDIA {GPU - -},|
|

349
00:09:46,960 --> 00:09:48,355
0,150 150,300 300,555 555,920 1060,1395
which is really a key
这是一个关键的硬件，

350
00:09:48,355 --> 00:09:49,770
0,225 225,420 420,725 895,1155 1155,1415
piece of hardware,| that is
|这是工具性的，

351
00:09:50,030 --> 00:09:51,400
0,635 635,800 800,920 920,1100 1100,1370
instrumental,| {if,you -} want to
|如果你想建立一个深度学习项目，

352
00:09:51,400 --> 00:09:52,645
0,300 300,615 615,840 840,990 990,1245
actually build a deep learning

353
00:09:52,645 --> 00:09:54,130
0,360 360,675 675,930 930,1200 1200,1485
project| and train these neural
|并训练这些神经网络，

354
00:09:54,130 --> 00:09:55,030
0,225 225,465 465,600 600,720 720,900
networks,| which can be very
|这些神经网络可能非常大，需要大量的计算，

355
00:09:55,030 --> 00:09:55,990
0,270 270,555 555,750 750,855 855,960
large and require a lot

356
00:09:55,990 --> 00:09:57,670
0,120 120,530 880,1185 1185,1545 1545,1680
of compute,| these prizes will
|这些奖品将给你这样做的计算。

357
00:09:57,670 --> 00:09:58,600
0,120 120,240 240,360 360,735 735,930
give you the compute to

358
00:09:58,600 --> 00:10:00,445
0,165 165,470 910,1200 1200,1490 1570,1845
do so.| And finally this
|最后，今年我们颁发一个大奖，

359
00:10:00,445 --> 00:10:01,570
0,165 165,360 360,510 510,915 915,1125
year we'll be awarding a

360
00:10:01,570 --> 00:10:03,010
0,240 240,590 670,945 945,1275 1275,1440
grand prize| for labs two
|为第二和第三个实验，

361
00:10:03,010 --> 00:10:04,780
0,150 150,420 420,800 1270,1560 1560,1770
and three combined,| which will
|将于周二和周三举行，

362
00:10:04,780 --> 00:10:06,180
0,240 240,495 495,825 825,1110 1110,1400
occur on Tuesday and Wednesday,|
|

363
00:10:06,320 --> 00:10:07,825
0,380 380,760 870,1130 1130,1310 1310,1505
focused on, what I believe
关注于，我认为是，

364
00:10:07,825 --> 00:10:08,860
0,210 210,420 420,795 795,930 930,1035
is| actually solving some of
|解决深度学习领域中一些最令人兴奋的问题，

365
00:10:08,860 --> 00:10:10,975
0,135 135,410 430,830 1180,1580 1840,2115
the most exciting problems in

366
00:10:10,975 --> 00:10:12,085
0,180 180,405 405,615 615,810 810,1110
this field of deep learning|
|

367
00:10:12,085 --> 00:10:13,600
0,285 285,575 685,1085 1105,1380 1380,1515
and how, specifically, how we
以及我们如何具体地建立模型，可以健壮，

368
00:10:13,600 --> 00:10:15,865
0,135 135,405 405,800 1390,1790 1990,2265
can build models that can

369
00:10:15,865 --> 00:10:17,950
0,275 445,845 1105,1380 1380,1655 1765,2085
be robust,| not only accurate,
|不仅准确，而且健壮，值得信赖和安全，

370
00:10:17,950 --> 00:10:19,495
0,285 285,615 615,870 870,1395 1395,1545
but robust and trustworthy and

371
00:10:19,495 --> 00:10:21,400
0,305 655,930 930,1200 1200,1620 1620,1905
safe,| when they're deployed as
|当它们部署时，

372
00:10:21,400 --> 00:10:22,660
0,210 210,480 480,885 885,1065 1065,1260
well,| and you'll actually get
|你将获得开发这些类型的解决方案的经验，

373
00:10:22,660 --> 00:10:24,175
0,350 430,830 850,1140 1140,1320 1320,1515
experience developing those types of

374
00:10:24,175 --> 00:10:26,130
0,305 865,1140 1140,1350 1350,1620 1620,1955
solutions,| that can actually advance
|这些可以促进 AI 中最先进的。

375
00:10:26,180 --> 00:10:27,025
0,290 290,440 440,575 575,680 680,845
the {state-of-the-art - - -}

376
00:10:27,025 --> 00:10:28,460
0,335 355,755
in AI.|
|

377
00:10:28,600 --> 00:10:29,625
0,290 290,455 455,590 590,755 755,1025
Now, all of these labs
我提到的所有这些实验和竞赛，

378
00:10:29,625 --> 00:10:31,340
0,90 90,210 210,485 715,1080 1080,1715
that I mentioned and competitions

379
00:10:31,420 --> 00:10:33,140
0,400 660,995 995,1265 1265,1460 1460,1720
here| are going to be
|将在周四晚上 11 点截止，

380
00:10:33,880 --> 00:10:35,430
0,305 305,515 515,800 800,1180 1230,1550
due on Thursday night, at

381
00:10:35,430 --> 00:10:36,765
0,285 285,660 660,885 885,1125 1125,1335
{11pm -},| right before the
|也就是最后一天上课前。

382
00:10:36,765 --> 00:10:38,390
0,225 225,420 420,555 555,845 1225,1625
last day of class.| {And,we'll
|我们会一路帮你，

383
00:10:38,410 --> 00:10:39,420
0,305 305,380 380,575 575,800 800,1010
-} be helping you all

384
00:10:39,420 --> 00:10:40,680
0,240 240,405 405,585 585,915 915,1260
along the way,| this, this
|这个奖品，特别是这个比赛有很好的奖品，

385
00:10:40,680 --> 00:10:42,675
0,350 550,825 825,1100 1330,1680 1680,1995
prize or this competition in

386
00:10:42,675 --> 00:10:45,195
0,365 775,1175 1405,1710 1710,2015 2065,2520
particular has very {significant,prizes -},|
|

387
00:10:45,195 --> 00:10:45,930
0,105 105,240 240,435 435,600 600,735
so I encourage all of
所以我鼓励你们获取奖品，

388
00:10:45,930 --> 00:10:47,130
0,240 240,465 465,675 675,975 975,1200
you to really enter this

389
00:10:47,130 --> 00:10:48,825
0,225 225,560 580,900 900,1220 1390,1695
prize| and try to, try
|努力有机会赢得奖品。

390
00:10:48,825 --> 00:10:49,660
0,305
to

391
00:10:49,660 --> 00:10:50,515
0,120 120,240 240,435 435,630 630,855
get a chance to win

392
00:10:50,515 --> 00:10:51,720
0,225 225,485
the prize.|
|

393
00:10:52,290 --> 00:10:53,315
0,260 260,395 395,650 650,890 890,1025
And of course, like I
当然，就像我说的，我们会一直帮助你，

394
00:10:53,315 --> 00:10:53,975
0,180 180,405 405,495 495,585 585,660
said, we're going to be

395
00:10:53,975 --> 00:10:54,905
0,180 180,390 390,570 570,780 780,930
helping you all along the

396
00:10:54,905 --> 00:10:56,440
0,210 210,420 420,615 615,965 1135,1535
way,| {here -} many available
|在整个课程中提供了许多可用的资源来帮助你实现这一点，

397
00:10:56,640 --> 00:10:57,980
0,395 395,695 695,905 905,1145 1145,1340
resources throughout this class to

398
00:10:57,980 --> 00:11:00,200
0,165 165,470 730,1065 1065,1400 1900,2220
help you achieve this,| please
|如果你有任何问题，请发布 Piazza ，

399
00:11:00,200 --> 00:11:01,505
0,225 225,405 405,975 975,1200 1200,1305
post a Piazza, if you

400
00:11:01,505 --> 00:11:02,825
0,90 90,270 270,605 835,1125 1125,1320
have any questions,| and of
|当然，这个项目有一个令人难以置信的团队，

401
00:11:02,825 --> 00:11:04,565
0,305 625,990 990,1305 1305,1530 1530,1740
course this program has an

402
00:11:04,565 --> 00:11:06,200
0,335 355,755 1075,1350 1350,1500 1500,1635
incredible team,| that you can
|你可以随时联系，

403
00:11:06,200 --> 00:11:06,995
0,135 135,330 330,495 495,600 600,795
reach out to at any

404
00:11:06,995 --> 00:11:08,075
0,335 445,720 720,870 870,990 990,1080
point,| in case you have
|以防你对材料有任何问题或疑问，

405
00:11:08,075 --> 00:11:09,740
0,180 180,515 865,1155 1155,1425 1425,1665
any issues or questions on

406
00:11:09,740 --> 00:11:12,040
0,180 180,500 1120,1520 1540,1830 1830,2300
{the,materials -},| myself and Ava
|我和 Ava 将是你们的两个主要讲课，

407
00:11:12,060 --> 00:11:12,950
0,260 260,395 395,575 575,740 740,890
will be your two main

408
00:11:12,950 --> 00:11:14,360
0,500 700,960 960,1080 1080,1245 1245,1410
lectures,| for the first part
|在课程的第一部分中，

409
00:11:14,360 --> 00:11:15,935
0,135 135,285 285,560 1000,1365 1365,1575
of the class,| we'll also
|我们也会听到，

410
00:11:15,935 --> 00:11:16,910
0,165 165,450 450,690 690,840 840,975
be hearing,| like I said,
|就像我说的，在课程后面部分，

411
00:11:16,910 --> 00:11:17,900
0,150 150,375 375,660 660,870 870,990
in the later part of

412
00:11:17,900 --> 00:11:18,860
0,120 120,315 315,540 540,735 735,960
the class,| from some guest
|从一些客座讲课中，

413
00:11:18,860 --> 00:11:20,690
0,530 790,1065 1065,1320 1320,1635 1635,1830
lectures,| who will share some
|他们将分享一些深度学习最先进的发展。

414
00:11:20,690 --> 00:11:22,100
0,260 340,645 645,950 1030,1290 1290,1410
really cutting edge {state-of-the-art -

415
00:11:22,100 --> 00:11:23,090
0,90 90,240 240,555 555,825 825,990
- -} development {in,deep -}

416
00:11:23,090 --> 00:11:24,680
0,290 730,1065 1065,1275 1275,1440 1440,1590
learning.| And of course I
|当然，我想要大声说出，

417
00:11:24,680 --> 00:11:25,360
0,120 120,225 225,315 315,420 420,680
want to give a huge

418
00:11:25,530 --> 00:11:26,510
0,290 290,455 455,620 620,830 830,980
shout out| and thanks to
|感谢我们所有的赞助商，

419
00:11:26,510 --> 00:11:27,920
0,90 90,195 195,360 360,890 1150,1410
all of our sponsors,| who
|如果没有他们的支持，

420
00:11:27,920 --> 00:11:29,140
0,195 195,480 480,735 735,930 930,1220
without their support,| this program
|这个项目在另一年是不可能的，

421
00:11:29,280 --> 00:11:31,330
0,320 320,425 425,575 575,850 1650,2050
wouldn't have been possible for

422
00:11:31,410 --> 00:11:32,915
0,305 305,590 590,905 905,1235 1235,1505
{yet,again -} another year,| so
|所以谢谢你们。

423
00:11:32,915 --> 00:11:33,820
0,165 165,300 300,545
thank you all.|
|

424
00:11:35,010 --> 00:11:36,370
0,365 365,590 590,830 830,1085 1085,1360
Okay, so now with that,|
好的，那么现在，|

425
00:11:36,570 --> 00:11:37,880
0,425 425,665 665,905 905,1145 1145,1310
let's really dive into the
让我们深入到今天课程中真正有趣的部分，

426
00:11:37,880 --> 00:11:39,260
0,165 165,450 450,720 720,975 975,1380
really fun stuff of today's

427
00:11:39,260 --> 00:11:41,135
0,260 310,600 600,890 1360,1620 1620,1875
lecture,| which is, you know,
|也就是技术部分，

428
00:11:41,135 --> 00:11:42,650
0,390 390,645 645,905 955,1305 1305,1515
the, the technical part,| {and,I,think
|我想开始这个部分，

429
00:11:42,650 --> 00:11:43,475
0,150 150,360 360,540 540,660 660,825
- -}, I want to

430
00:11:43,475 --> 00:11:44,800
0,225 225,465 465,705 705,975 975,1325
start this part| by asking
|通过问你们，让你们自己问，

431
00:11:46,080 --> 00:11:47,105
0,275 275,425 425,620 620,815 815,1025
all of you and having

432
00:11:47,105 --> 00:11:48,980
0,375 375,665 955,1215 1215,1475 1525,1875
yourselves ask,| you know, having
|你们问自己这个问题，

433
00:11:48,980 --> 00:11:50,450
0,225 225,420 420,930 930,1200 1200,1470
you ask yourselves this question,|
|

434
00:11:50,450 --> 00:11:52,415
0,380 640,870 870,1100 1180,1580 1660,1965
of, you know, why are
首先，为什么你们在这里，

435
00:11:52,415 --> 00:11:53,300
0,165 165,285 285,420 420,645 645,885
all of you here, first

436
00:11:53,300 --> 00:11:53,915
0,120 120,240 240,405 405,510 510,615
of all,| why do you
|你们为什么一开始就关心这个话题。

437
00:11:53,915 --> 00:11:55,520
0,180 180,375 375,570 570,875 1345,1605
care about this topic in

438
00:11:55,520 --> 00:11:57,260
0,120 120,315 315,650
the first place.|
|

439
00:11:57,400 --> 00:11:58,965
0,290 290,580 840,1085 1085,1295 1295,1565
I think to answer this
我认为要回答这个问题，

440
00:11:58,965 --> 00:11:59,850
0,300 300,540 540,645 645,765 765,885
question,| we have to take
|我们必须退后一步，

441
00:11:59,850 --> 00:12:00,915
0,150 150,330 330,585 585,840 840,1065
a step back| and think
|想想机器学习的历史，

442
00:12:00,915 --> 00:12:02,265
0,335 535,780 780,945 945,1110 1110,1350
about, you know, the history

443
00:12:02,265 --> 00:12:03,945
0,375 375,645 645,935 1075,1395 1395,1680
of machine learning| and what
|机器学习是什么，

444
00:12:03,945 --> 00:12:05,325
0,255 255,545 625,945 945,1170 1170,1380
machine learning is| and what
|深度学习在机器学习的基础上带来了什么。

445
00:12:05,325 --> 00:12:06,630
0,195 195,465 465,845 925,1170 1170,1305
deep learning brings to the

446
00:12:06,630 --> 00:12:08,040
0,290 430,735 735,915 915,1155 1155,1410
table on top of machine

447
00:12:08,040 --> 00:12:10,250
0,290 880,1230 1230,1580 1630,1920 1920,2210
learning.| {Now\,,traditional -} machine learning
|传统的机器学习算法通常定义了

448
00:12:10,300 --> 00:12:12,195
0,470 470,850 990,1390 1500,1760 1760,1895
algorithms typically define| what are
|数据中所谓的这些特征集，您可以将这些视为数据中的特定模式，然后通常这些特征都是手工设计的。因此，可能会有一个人进入数据集，并拥有大量的领域知识和经验，可以尝试发现这些特征可能是什么。现在深度学习的关键思想，也是这门课的核心，是不是让人来定义这些特征，如果我们可以呢？

449
00:12:12,195 --> 00:12:13,970
0,240 240,605 1045,1320 1320,1485 1485,1775
called these set of features

450
00:12:14,140 --> 00:12:15,000
0,260 260,380 380,560 560,740 740,860
in the data,| you can
|你可以将这些视为数据中的特定模式，

451
00:12:15,000 --> 00:12:15,825
0,120 120,255 255,405 405,570 570,825
think of these as certain

452
00:12:15,825 --> 00:12:17,445
0,365 385,660 660,795 795,1055 1375,1620
patterns in the data| and
|通常这些特征都是手工设计的，

453
00:12:17,445 --> 00:12:18,705
0,120 120,390 390,705 705,990 990,1260
then usually these features are

454
00:12:18,705 --> 00:12:20,025
0,195 195,660 660,900 900,1125 1125,1320
{hand,engineered -},| so probably a
|所以，可能会有一个人进入数据集，

455
00:12:20,025 --> 00:12:21,120
0,275 295,570 570,720 720,915 915,1095
human will come into the

456
00:12:21,120 --> 00:12:22,140
0,195 195,480 480,735 735,900 900,1020
data set| and with a
|并拥有大量的领域知识和经验，

457
00:12:22,140 --> 00:12:23,550
0,120 120,240 240,500 700,1080 1080,1410
lot of domain knowledge and

458
00:12:23,550 --> 00:12:25,755
0,350 760,1140 1140,1425 1425,1650 1650,2205
experience| can try to uncover
|可以尝试发现这些特征可能是什么，

459
00:12:25,755 --> 00:12:27,110
0,195 195,390 390,695 745,1050 1050,1355
what these {features,might -} be,|
|

460
00:12:27,190 --> 00:12:28,260
0,260 260,425 425,695 695,920 920,1070
now the key idea of
现在深度学习的关键思想，

461
00:12:28,260 --> 00:12:29,295
0,195 195,495 495,750 750,870 870,1035
deep learning,| and this is
|也是这门课的核心是，

462
00:12:29,295 --> 00:12:30,800
0,270 270,635 745,990 990,1170 1170,1505
really central to this class

463
00:12:31,150 --> 00:12:32,295
0,290 290,575 575,830 830,965 965,1145
is that,| instead of having
|不是让人来定义这些特征，

464
00:12:32,295 --> 00:12:33,920
0,210 210,515 775,1095 1095,1320 1320,1625
a human define these features,|
|

465
00:12:34,060 --> 00:12:34,940
0,245 245,350 350,470 470,605 605,880
what if we could have
如果我们可以让机器查看所有这些数据，

466
00:12:35,360 --> 00:12:36,860
0,210 210,530 970,1245 1245,1380 1380,1500
a machine look at all

467
00:12:36,860 --> 00:12:38,240
0,135 135,315 315,620 700,1100 1120,1380
of this data| and actually
|并尝试提取和发现数据中的核心模式，

468
00:12:38,240 --> 00:12:39,790
0,150 150,405 405,705 705,900 900,1550
try to extract and uncover

469
00:12:39,810 --> 00:12:41,020
0,290 290,470 470,650 650,875 875,1210
what are the core patterns

470
00:12:41,100 --> 00:12:42,215
0,260 260,395 395,670 750,995 995,1115
in the data,| so that
|以便在看到新数据时可以使用这些模式来做出一些决策。

471
00:12:42,215 --> 00:12:43,685
0,165 165,455 505,840 840,1175 1195,1470
it can use those, when

472
00:12:43,685 --> 00:12:44,765
0,165 165,375 375,585 585,855 855,1080
it sees new data to

473
00:12:44,765 --> 00:12:46,325
0,135 135,375 375,725 1105,1380 1380,1560
make some decisions.| {So,for -}
|例如，如果我们想要检测图像中的人脸，

474
00:12:46,325 --> 00:12:47,500
0,255 255,465 465,585 585,810 810,1175
example, if we wanted to

475
00:12:47,580 --> 00:12:48,970
0,320 320,640 780,1040 1040,1145 1145,1390
detect faces in an image,|
|

476
00:12:49,500 --> 00:12:51,110
0,275 275,470 470,770 770,1030 1230,1610
a deep neural network algorithm
深度神经网络算法可能会了解到，

477
00:12:51,110 --> 00:12:52,610
0,290 310,630 630,950 1120,1380 1380,1500
might actually learn that,| in
|为了检测人脸，

478
00:12:52,610 --> 00:12:53,650
0,165 165,390 390,600 600,765 765,1040
order to detect a face,|
|

479
00:12:53,700 --> 00:12:54,890
0,290 290,485 485,710 710,950 950,1190
it first has to detect
它首先必须检测图像中的线和边等东西，

480
00:12:54,890 --> 00:12:56,435
0,255 255,590 670,1200 1200,1425 1425,1545
things like edges in the

481
00:12:56,435 --> 00:12:58,295
0,245 325,690 690,930 930,1385 1585,1860
image lines and edges,| and
|当你组合这些线和边时，

482
00:12:58,295 --> 00:12:59,225
0,120 120,225 225,525 525,735 735,930
when you combine those lines

483
00:12:59,225 --> 00:13:00,490
0,180 180,540 540,705 705,915 915,1265
and edges,| you can actually
|你可以创建特征的组合，比如角和曲线，

484
00:13:00,510 --> 00:13:02,740
0,400 600,1220 1220,1445 1445,1720 1830,2230
create compositions of features like

485
00:13:02,820 --> 00:13:04,940
0,530 530,815 815,1390 1620,1940 1940,2120
corners and curves,| which when
|当你创建，当你组合这些特征时，

486
00:13:04,940 --> 00:13:06,335
0,135 135,410 610,885 885,1050 1050,1395
you create, when you combine

487
00:13:06,335 --> 00:13:07,430
0,270 270,465 465,645 645,885 885,1095
those,| you can create more
|你可以创建更高层次的特征，

488
00:13:07,430 --> 00:13:08,750
0,195 195,435 435,770 820,1095 1095,1320
high level features,| for example
|例如眼睛、鼻子和耳朵，

489
00:13:08,750 --> 00:13:10,240
0,330 330,600 600,1065 1065,1215 1215,1490
eyes and noses {and,ears -},|
|

490
00:13:10,710 --> 00:13:11,720
0,275 275,440 440,665 665,875 875,1010
and then those are the
然后，这些特征让你最终能够检测到你关心的东西，

491
00:13:11,720 --> 00:13:13,325
0,260 310,710 910,1215 1215,1395 1395,1605
features that allow you to

492
00:13:13,325 --> 00:13:14,810
0,335 595,945 945,1170 1170,1320 1320,1485
ultimately detect what you care

493
00:13:14,810 --> 00:13:15,830
0,210 210,570 570,705 705,870 870,1020
about,| detecting which {is,the -}
|检测出哪个是人脸，

494
00:13:15,830 --> 00:13:16,685
0,240 240,480 480,600 600,705 705,855
face,| but all of these
|但所有这些都来自于所谓的功能分层学习，

495
00:13:16,685 --> 00:13:17,720
0,210 210,480 480,690 690,810 810,1035
come from what are called

496
00:13:17,720 --> 00:13:19,390
0,195 195,315 315,465 465,1160 1270,1670
kind of a hierarchical learning

497
00:13:19,440 --> 00:13:20,960
0,305 305,610 990,1250 1250,1355 1355,1520
of features| and you can
|你可以看到一些这样的例子，

498
00:13:20,960 --> 00:13:22,040
0,210 210,375 375,570 570,855 855,1080
actually see {some,examples -} of

499
00:13:22,040 --> 00:13:23,530
0,260 580,840 840,975 975,1170 1170,1490
these,| these are real features
|这些都是通过神经网络学习到的真实特征，

500
00:13:23,610 --> 00:13:24,710
0,335 335,530 530,635 635,875 875,1100
learned by a neural network|
|

501
00:13:24,710 --> 00:13:26,480
0,225 225,390 390,660 660,980 1270,1770
and how they're combined defines
它们的组合方式定义了这一信息进程。

502
00:13:26,480 --> 00:13:28,400
0,270 270,810 810,1050 1050,1430
this progression of information.|
|

503
00:13:29,360 --> 00:13:30,295
0,245 245,365 365,590 590,800 800,935
But in fact, what I
但事实上，正如我刚才所描述的，

504
00:13:30,295 --> 00:13:32,185
0,270 270,615 615,965 985,1385 1585,1890
just described,| this underlying and
|这种神经网络和深度学习的基础和基础构件已经存在了几十年，

505
00:13:32,185 --> 00:13:34,150
0,305 475,870 870,1265 1315,1650 1650,1965
fundamental building block of neural

506
00:13:34,150 --> 00:13:36,000
0,260 280,555 555,720 720,1010 1450,1850
networks and deep learning have

507
00:13:36,020 --> 00:13:38,310
0,335 335,770 770,995 995,1330 1890,2290
actually existed for decades,| now.
|为什么我们现在在这门课中学习这个，

508
00:13:38,540 --> 00:13:39,670
0,305 305,470 470,620 620,875 875,1130
{why,are -} we studying all

509
00:13:39,670 --> 00:13:40,840
0,150 150,405 405,720 720,945 945,1170
of this now at today

510
00:13:40,840 --> 00:13:42,100
0,225 225,450 450,770 820,1095 1095,1260
in this class| with all
|怀着如此巨大的热情学习这些。

511
00:13:42,100 --> 00:13:44,065
0,195 195,500 700,1635 1635,1785 1785,1965
this great enthusiasm to learn

512
00:13:44,065 --> 00:13:45,600
0,335 355,755 835,1110 1110,1260 1260,1535
this, right.| Well, for one,
|首先，在过去的十年里，已经取得了几项关键的进展。

513
00:13:46,010 --> 00:13:47,425
0,260 260,380 380,620 620,1000 1020,1415
there have been several key

514
00:13:47,425 --> 00:13:48,940
0,615 615,870 870,1065 1065,1320 1320,1515
advances that have occurred in

515
00:13:48,940 --> 00:13:51,010
0,120 120,380 550,950 1510,1845 1845,2070
the past decade.| Number one
|第一，数据比我们生活中的任何时候都更加普遍，

516
00:13:51,010 --> 00:13:52,630
0,180 180,390 390,710 820,1220 1330,1620
is that data is so

517
00:13:52,630 --> 00:13:54,145
0,195 195,420 420,1125 1125,1365 1365,1515
much more pervasive than it

518
00:13:54,145 --> 00:13:55,750
0,180 180,405 405,690 690,1055 1345,1605
has ever been before in

519
00:13:55,750 --> 00:13:57,745
0,150 150,710 1090,1395 1395,1695 1695,1995
{our,lifetimes -},| these models are
|这些模型渴望更多的数据，

520
00:13:57,745 --> 00:13:59,800
0,285 285,570 570,810 810,1145 1675,2055
hungry for {more -} data,|
|

521
00:13:59,800 --> 00:14:01,300
0,210 210,300 300,560 730,1130 1240,1500
and we're living in the
而我们生活在大数据时代，

522
00:14:01,300 --> 00:14:03,260
0,260 490,825 825,1095 1095,1430
age of big data,|
|

523
00:14:03,260 --> 00:14:04,445
0,240 240,450 450,675 675,960 960,1185
more data is available to
这些模型比以往任何时候都有更多的数据可用，

524
00:14:04,445 --> 00:14:05,480
0,120 120,330 330,525 525,720 720,1035
these models than ever before|
|

525
00:14:05,480 --> 00:14:06,920
0,330 330,660 660,1095 1095,1260 1260,1440
and they thrive off of
它们也因此而蓬勃发展。

526
00:14:06,920 --> 00:14:09,845
0,290 640,1230 1230,1550 1630,2150 2530,2925
that.| {Secondly\,,these -} algorithms are
|第二，这些算法是大规模可并行化的，

527
00:14:09,845 --> 00:14:12,155
0,825 825,1740 1740,1980 1980,2175 2175,2310
massively parallelizable,| they require a
|它们需要大量的计算，

528
00:14:12,155 --> 00:14:13,970
0,135 135,285 285,725 865,1265 1375,1815
lot of compute| and we're
|我们也正处于历史上一个独特的时刻，

529
00:14:13,970 --> 00:14:15,220
0,210 210,345 345,480 480,740 850,1250
also at a unique time

530
00:14:15,450 --> 00:14:17,030
0,290 290,580 780,1130 1130,1370 1370,1580
in history,| where we have
|我们有能力训练这些超大规模算法和技术，

531
00:14:17,030 --> 00:14:18,730
0,225 225,530 760,1080 1080,1350 1350,1700
the ability to train these

532
00:14:18,900 --> 00:14:21,065
0,380 380,725 725,1090 1110,1630 1860,2165
extremely large scale algorithms and

533
00:14:21,065 --> 00:14:22,340
0,305 355,615 615,795 795,1170 1170,1275
techniques,| that have existed for
|它们存在了很长时间，

534
00:14:22,340 --> 00:14:23,180
0,75 75,225 225,450 450,675 675,840
a very {long,time -},| but
|但是我们现在可以训练它们了，

535
00:14:23,180 --> 00:14:23,990
0,120 120,225 225,390 390,615 615,810
we can now train them,|
|

536
00:14:23,990 --> 00:14:25,505
0,180 180,315 315,405 405,650 1030,1515
due to the hardware advances
由于硬件方面的进步。

537
00:14:25,505 --> 00:14:26,705
0,135 135,255 255,375 375,635 895,1200
that {have,been -} made.| And
|最后，由于开源工具箱和软件平台，比如 TensorFlow ，

538
00:14:26,705 --> 00:14:28,010
0,305 355,645 645,840 840,1035 1035,1305
finally, due to open source

539
00:14:28,010 --> 00:14:30,310
0,530 580,900 900,1220 1420,1820 1900,2300
toolbox and software platforms, like

540
00:14:30,570 --> 00:14:32,165
0,680 680,890 890,1175 1175,1460 1460,1595
TensorFlow for example,| which all
|你们在这门课上都会学到很多经验，

541
00:14:32,165 --> 00:14:32,840
0,105 105,240 240,390 390,540 540,675
of you will get a

542
00:14:32,840 --> 00:14:34,085
0,90 90,255 255,510 510,830 970,1245
lot of experience on in

543
00:14:34,085 --> 00:14:36,340
0,195 195,515 1165,1565 1615,1935 1935,2255
this class,| training and building
|训练和构建这些神经网络的代码从来没有像现在这样容易，

544
00:14:36,450 --> 00:14:38,000
0,320 320,640 810,1115 1115,1310 1310,1550
the code for these neural

545
00:14:38,000 --> 00:14:39,130
0,240 240,525 525,735 735,885 885,1130
networks {has,never -} been easier,|
|

546
00:14:39,480 --> 00:14:40,505
0,275 275,425 425,575 575,740 740,1025
so that from the software
所以，从软件的角度来看，

547
00:14:40,505 --> 00:14:41,470
0,270 270,390 390,495 495,660 660,965
point of view as well,|
|

548
00:14:41,520 --> 00:14:42,905
0,230 230,320 320,515 515,830 830,1385
there have been incredible advances
开源已经有了令人难以置信的进步，

549
00:14:42,905 --> 00:14:44,390
0,225 225,435 435,725 1045,1305 1305,1485
to open source,| you know,
|你将学到的基本原理。

550
00:14:44,390 --> 00:14:46,955
0,320 580,980 1150,1940 2140,2415 2415,2565
the underlying fundamentals of what

551
00:14:46,955 --> 00:14:47,960
0,195 195,315 315,450 450,695
you're going to learn.|
|

552
00:14:48,530 --> 00:14:49,675
0,365 365,590 590,740 740,935 935,1145
So let me start now
现在让我从基础开始构建，

553
00:14:49,675 --> 00:14:51,325
0,210 210,515 745,1110 1110,1425 1425,1650
with just building up from

554
00:14:51,325 --> 00:14:52,440
0,150 150,345 345,615 615,840 840,1115
the ground up,| the fundamental
|每个神经网络的基本构件，

555
00:14:52,640 --> 00:14:54,775
0,400 420,820 1020,1370 1370,1720 1770,2135
building block of every single

556
00:14:54,775 --> 00:14:55,960
0,330 330,600 600,870 870,1065 1065,1185
neural network,| that you're going
|你们会在课程中学到的，

557
00:14:55,960 --> 00:14:56,905
0,135 135,285 285,465 465,645 645,945
to learn in this class,|
|

558
00:14:56,905 --> 00:14:57,930
0,255 255,495 495,645 645,780 780,1025
{and,that's -} going to be
这只是单个神经元，

559
00:14:58,130 --> 00:14:59,900
0,290 290,470 470,740 740,1270
just a single neuron,|
|

560
00:15:00,060 --> 00:15:01,570
0,400 480,740 740,950 950,1160 1160,1510
and in neural network language,|
在神经网络语言中，|

561
00:15:01,740 --> 00:15:03,125
0,290 290,515 515,890 890,1100 1100,1385
a single neuron is called
单个神经元被称为感知器。

562
00:15:03,125 --> 00:15:04,640
0,225 225,845
a perceptron.|
|

563
00:15:05,230 --> 00:15:06,600
0,380 380,605 605,710 710,830 830,1370
So what is a perceptron,|
那么什么是感知器，|

564
00:15:06,600 --> 00:15:09,015
0,225 225,920 1210,1610 2020,2280 2280,2415
a perceptron is, like I
就像我说的，感知器是一个单一的神经元，

565
00:15:09,015 --> 00:15:10,725
0,225 225,465 465,705 705,1205 1435,1710
said, a single neuron,| {and,it's
|我要说这是一个非常非常简单的想法，

566
00:15:10,725 --> 00:15:12,465
0,395 505,905 1105,1440 1440,1575 1575,1740
-} actually, I'm going to

567
00:15:12,465 --> 00:15:13,850
0,135 135,420 420,720 720,1035 1035,1385
say it's very, very simple

568
00:15:13,900 --> 00:15:14,640
0,320 320,470 470,560 560,665 665,740
idea,| {so,I -} want to
|我想确保观众中的每个人都准确地理解感知器是什么，

569
00:15:14,640 --> 00:15:15,510
0,120 120,255 255,495 495,750 750,870
make sure that everyone in

570
00:15:15,510 --> 00:15:16,995
0,195 195,530 580,990 990,1275 1275,1485
the audience understands exactly what

571
00:15:16,995 --> 00:15:18,225
0,90 90,510 510,785 805,1080 1080,1230
a perceptron is| and how
|以及它是如何工作的。

572
00:15:18,225 --> 00:15:20,505
0,150 150,425 1465,1815 1815,2115 2115,2280
{it,works -}.| So let's start
|让我们首先定义感知器，

573
00:15:20,505 --> 00:15:22,110
0,210 210,495 495,900 900,1005 1005,1605
by first defining a perceptron|
|

574
00:15:22,110 --> 00:15:23,820
0,350 400,800 850,1245 1245,1530 1530,1710
as taking, as input a
将一组输入作为输入，

575
00:15:23,820 --> 00:15:25,470
0,180 180,470 640,1080 1080,1395 1395,1650
set of inputs, right,| so
|所以在左手边，

576
00:15:25,470 --> 00:15:26,340
0,165 165,300 300,465 465,675 675,870
on the left hand side,|
|

577
00:15:26,340 --> 00:15:28,065
0,150 150,315 315,620 790,1110 1110,1725
you can see this perceptron
你可以看到这个感知器接受 m 个不同的输入， 1 到 m ，

578
00:15:28,065 --> 00:15:30,735
0,335 865,1265 1345,1745 1765,2280 2280,2670
takes m different inputs, 1

579
00:15:30,735 --> 00:15:31,700
0,240 240,485
to m,|
|

580
00:15:31,770 --> 00:15:33,010
0,275 275,395 395,545 545,755 755,1240
these are the blue circles,|
这些是蓝色的圆圈，|

581
00:15:33,330 --> 00:15:34,475
0,215 215,290 290,650 650,875 875,1145
We're denoting these inputs as
我们将这些输入表示为 xs ，

582
00:15:34,475 --> 00:15:35,980
0,135 135,360 360,695
{xs - -},|
|

583
00:15:36,520 --> 00:15:38,595
0,305 305,500 500,790 870,1270 1800,2075
each of these numbers, each
这些数字中的每一个，每个输入，乘以相应的权重，

584
00:15:38,595 --> 00:15:40,155
0,150 150,425 445,995 1105,1380 1380,1560
of these inputs, is then

585
00:15:40,155 --> 00:15:42,290
0,665 745,1020 1020,1200 1200,1845 1845,2135
multiplied by a corresponding weight,|
|

586
00:15:42,340 --> 00:15:43,490
0,290 290,440 440,575 575,800 800,1150
which we can call w,|
我们可以称之为 w ，|

587
00:15:43,750 --> 00:15:44,970
0,350 350,665 665,935 935,1100 1100,1220
so {x1 -} will be
所以 x1 将乘以 w1 ，

588
00:15:44,970 --> 00:15:47,180
0,390 390,615 615,885 885,1250 1810,2210
multiplied by {w1 -},| and
|我们将所有这些乘法的结果加在一起，

589
00:15:47,320 --> 00:15:48,675
0,350 350,610 690,995 995,1190 1190,1355
we'll add the result of

590
00:15:48,675 --> 00:15:50,390
0,135 135,255 255,405 405,995 1315,1715
all of these multiplications together,|
|

591
00:15:51,130 --> 00:15:52,740
0,400 570,875 875,1085 1085,1325 1325,1610
now, we take that single
现在，我们取相加后的单个数字，

592
00:15:52,740 --> 00:15:54,555
0,350 400,765 765,1020 1020,1310 1510,1815
number after the addition,| and
|我们把它通过这个非线性的，

593
00:15:54,555 --> 00:15:55,845
0,305 385,690 690,885 885,1065 1065,1290
we pass it through this

594
00:15:55,845 --> 00:15:57,075
0,660 660,885 885,990 990,1095 1095,1230
nonlinear,| what we call a
|我们称之为非线性激活函数，

595
00:15:57,075 --> 00:15:59,235
0,570 570,1020 1020,1415 1735,1995 1995,2160
nonlinear activation function,| and that
|这产生了我们感知器的最终输出，

596
00:15:59,235 --> 00:16:00,585
0,345 345,555 555,845 925,1200 1200,1350
produces our final output of

597
00:16:00,585 --> 00:16:01,725
0,135 135,515 535,840 840,1005 1005,1140
the perception,| which we can
|我们可以称之为 y 。

598
00:16:01,725 --> 00:16:02,780
0,120 120,255 255,545
call it y.|
|

599
00:16:03,390 --> 00:16:04,680
0,400
Now,
现在，这实际上并不完全准确地描述感知器的图像，

600
00:16:05,160 --> 00:16:06,460
0,275 275,485 485,680 680,920 920,1300
this is actually not entirely

601
00:16:06,660 --> 00:16:08,525
0,305 305,500 500,790 1260,1640 1640,1865
accurate of the picture of

602
00:16:08,525 --> 00:16:09,800
0,120 120,630 630,840 840,1050 1050,1275
a perceptron,| there's one step
|有一个步骤我忘了在这里提了，

603
00:16:09,800 --> 00:16:10,610
0,135 135,285 285,480 480,615 615,810
that I forgot to mention

604
00:16:10,610 --> 00:16:12,880
0,350 520,780 780,975 975,1310 1870,2270
here,| {so,in -} addition to
|所以除了将所有这些输入与其相应的权重相乘，

605
00:16:13,320 --> 00:16:15,005
0,730 750,1025 1025,1160 1160,1385 1385,1685
multiplying all of these inputs

606
00:16:15,005 --> 00:16:16,205
0,105 105,240 240,690 690,930 930,1200
with their corresponding weights,| we're
|我们现在还将添加所谓的偏置项，

607
00:16:16,205 --> 00:16:17,150
0,225 225,420 420,615 615,765 765,945
also now going to add

608
00:16:17,150 --> 00:16:18,340
0,255 255,360 360,495 495,870 870,1190
what's called a bias term,|
|

609
00:16:18,510 --> 00:16:20,680
0,320 320,680 680,1000 1500,1835 1835,2170
here denoted as this {w0
这里表示为 w0 ，

610
00:16:20,700 --> 00:16:21,995
0,400 600,860 860,980 980,1115 1115,1295
-},| which is just a
|这只是一个标量权重，

611
00:16:21,995 --> 00:16:23,165
0,405 405,695 715,960 960,1050 1050,1170
scalar weight,| and you can
|你可以想象它的输入是 1 ，

612
00:16:23,165 --> 00:16:24,220
0,135 135,240 240,375 375,660 660,1055
think of it coming with

613
00:16:24,240 --> 00:16:25,570
0,400 420,680 680,830 830,1025 1025,1330
an input of just 1,|
|

614
00:16:25,860 --> 00:16:26,930
0,245 245,470 470,620 620,815 815,1070
so that's going to allow
所以这将允许网络改变它的非线性激活函数，

615
00:16:26,930 --> 00:16:28,870
0,210 210,470 760,1020 1020,1280 1540,1940
the network to basically shift

616
00:16:29,190 --> 00:16:32,555
0,350 350,935 935,1310 1310,1660 3120,3365
its nonlinear activation function,| you
|当它看到它的输入时，是非线性的。

617
00:16:32,555 --> 00:16:34,570
0,195 195,450 450,1085 1435,1725 1725,2015
know, non linearly as it

618
00:16:34,890 --> 00:16:36,300
0,305 305,605 605,1090
sees its inputs.|
|

619
00:16:36,780 --> 00:16:37,670
0,290 290,440 440,545 545,695 695,890
Now on the right hand
现在在右手边，

620
00:16:37,670 --> 00:16:38,450
0,195 195,345 345,480 480,630 630,780
side,| you can see this
|你可以看到这张图是以数学形式表述的，

621
00:16:38,450 --> 00:16:42,005
0,590 1120,1790 2020,2690 2920,3300 3300,3555
diagram mathematically formulated, right,| as
|一个单独的方程，

622
00:16:42,005 --> 00:16:43,130
0,150 150,420 420,780 780,1005 1005,1125
a single equation,| {we,can -}
|我们现在可以用向量和点积的线性代数项重写这个线性方程，

623
00:16:43,130 --> 00:16:45,280
0,180 180,630 630,870 870,1400 1750,2150
now rewrite this linear, this

624
00:16:45,750 --> 00:16:47,990
0,400 420,725 725,1010 1010,1600 1920,2240
equation with linear algebra terms

625
00:16:47,990 --> 00:16:49,450
0,210 210,645 645,870 870,1110 1110,1460
of vectors and dot products,

626
00:16:49,500 --> 00:16:50,705
0,305 305,485 485,665 665,950 950,1205
right,| so for example, we
|例如，我们可以将整个输入 x1 到 xm 定义为大的向量 X ，

627
00:16:50,705 --> 00:16:52,685
0,240 240,605 655,1050 1050,1445 1495,1980
can define our entire inputs

628
00:16:52,685 --> 00:16:54,370
0,285 285,635 865,1125 1125,1335 1335,1685
{x1 -} to {xm -}

629
00:16:54,780 --> 00:16:57,380
0,400 810,1145 1145,1460 1460,1810
as large vector X,

630
00:16:57,380 --> 00:16:58,670
0,270 270,465 465,705 705,1005 1005,1290
right,| that large vector X
|大的向量 X 可以乘以或者矩阵乘以我们的权重 W ，

631
00:16:58,670 --> 00:17:01,190
0,210 210,375 375,1010 1690,2090 2230,2520
can be multiplied by or

632
00:17:01,190 --> 00:17:02,330
0,180 180,345 345,615 615,900 900,1140
take a dot, excuse me,

633
00:17:02,330 --> 00:17:04,930
0,465 465,1010 1150,1550 1780,2085 2085,2600
matrix multiplied with our weights

634
00:17:05,190 --> 00:17:07,430
0,400 780,1180 1230,1565 1565,1960 1980,2240
W,| again, another vector of
|同样， W 是权重的向量 w1 到 wm ，

635
00:17:07,430 --> 00:17:09,050
0,150 150,480 480,780 780,1160 1360,1620
our weights {w1 -} to

636
00:17:09,050 --> 00:17:10,220
0,225 225,590
{wm -},|
|

637
00:17:10,320 --> 00:17:12,560
0,335 335,545 545,740 740,1060 1950,2240
taking their dot product,| not
取它们的点积，|不仅会使它们相乘，而且还会将得到的项相加，

638
00:17:12,560 --> 00:17:14,075
0,290 490,1050 1050,1215 1215,1335 1335,1515
only multiplies them, but it

639
00:17:14,075 --> 00:17:15,740
0,195 195,635 655,975 975,1290 1290,1665
also adds the resulting terms

640
00:17:15,740 --> 00:17:18,005
0,380 820,1155 1155,1350 1350,1790 1990,2265
together,| adding a bias, like
|像我们前面说过的那样，增加一个偏置，

641
00:17:18,005 --> 00:17:19,490
0,165 165,390 390,725 925,1245 1245,1485
we said before,| and applying
|并应用这个非线性函数。

642
00:17:19,490 --> 00:17:20,960
0,210 210,1100
this non-linearity.|
|

643
00:17:22,710 --> 00:17:23,830
0,365 365,590 590,725 725,860 860,1120
Now you might be wondering,|
现在你可能想知道，|

644
00:17:23,910 --> 00:17:25,145
0,260 260,410 410,665 665,920 920,1235
what is this {non-linear -}
这个非线性函数是什么，

645
00:17:25,145 --> 00:17:26,120
0,225 225,495 495,660 660,855 855,975
function,| I've mentioned it a
|我已经提过好几次了，

646
00:17:26,120 --> 00:17:27,815
0,150 150,440 490,890 1300,1560 1560,1695
few times already,| {well\,,I,said -
|我说过这是一个函数，

647
00:17:27,815 --> 00:17:29,050
0,270 270,525 525,750 750,975 975,1235
-} it is a function,

648
00:17:29,310 --> 00:17:31,355
0,305 305,730 960,1360 1470,1775 1775,2045
right,| that's that we pass
|我们将神经网络的输出传给它，

649
00:17:31,355 --> 00:17:32,720
0,365 625,960 960,1065 1065,1170 1170,1365
the outputs of the neural

650
00:17:32,720 --> 00:17:34,565
0,210 210,555 555,950 970,1370 1570,1845
network through| before we return
|在返回给管道中的下一个神经元之前，

651
00:17:34,565 --> 00:17:35,675
0,275 325,585 585,795 795,990 990,1110
it, you know, to the

652
00:17:35,675 --> 00:17:37,400
0,210 210,665 895,1185 1185,1455 1455,1725
next neuron in the, in

653
00:17:37,400 --> 00:17:39,050
0,270 270,650 880,1155 1155,1350 1350,1650
the pipeline,| so one common
|所以一个非线性函数的常见例子，

654
00:17:39,050 --> 00:17:40,580
0,380 430,705 705,840 840,1290 1290,1530
example of a non-linear function,|
|

655
00:17:40,580 --> 00:17:42,230
0,300 300,560 670,1070 1180,1470 1470,1650
that's very popular in deep
在深度神经网络中非常流行的，

656
00:17:42,230 --> 00:17:43,205
0,210 210,405 405,675 675,810 810,975
neural networks,| it's called the
|叫做 sigmoid 函数，

657
00:17:43,205 --> 00:17:44,705
0,345 345,635 985,1230 1230,1350 1350,1500
{sigmoid,function -},| you can think
|你可以把它看作是阈值函数的一种连续形式，

658
00:17:44,705 --> 00:17:45,395
0,135 135,270 270,435 435,585 585,690
of this as kind of

659
00:17:45,395 --> 00:17:46,700
0,210 210,555 555,935 955,1200 1200,1305
a continuous version of a

660
00:17:46,700 --> 00:17:48,395
0,405 405,710 790,1190 1240,1515 1515,1695
threshold function, right,| it goes
|它从 0 到 1 ，

661
00:17:48,395 --> 00:17:49,840
0,210 210,495 495,735 735,995
from 0 to 1,|
|

662
00:17:49,950 --> 00:17:51,350
0,275 275,620 620,970 990,1265 1265,1400
and it's having, it can
它可以让我们在实数线上输入任何实数，

663
00:17:51,350 --> 00:17:52,280
0,120 120,315 315,525 525,720 720,930
take us, input any real

664
00:17:52,280 --> 00:17:53,750
0,290 370,690 690,975 975,1230 1230,1470
number on the real number

665
00:17:53,750 --> 00:17:54,780
0,350
line,|
|

666
00:17:54,780 --> 00:17:55,515
0,225 225,345 345,480 480,600 600,735
and you can see an
你可以在右下角看到一个例子，

667
00:17:55,515 --> 00:17:56,805
0,255 255,450 450,600 600,1095 1095,1290
example of it illustrated on

668
00:17:56,805 --> 00:17:58,275
0,120 120,300 300,540 540,845 1165,1470
the bottom right hand,| {now\,,in
|事实上，非线性激活函数有很多种，

669
00:17:58,275 --> 00:17:59,240
0,180 180,375 375,540 540,675 675,965
-} fact, there are many

670
00:17:59,350 --> 00:18:01,460
0,350 350,590 590,1180 1320,1745 1745,2110
types of non-linear activation functions,|
|

671
00:18:01,540 --> 00:18:02,955
0,275 275,470 470,790 960,1250 1250,1415
that are popular in deep
在深度神经网络中流行的，

672
00:18:02,955 --> 00:18:03,945
0,240 240,435 435,705 705,870 870,990
neural networks| and here are
|下面是一些常见的，

673
00:18:03,945 --> 00:18:05,505
0,165 165,405 405,755 1135,1395 1395,1560
some common ones,| and throughout
|在整个演示过程中，

674
00:18:05,505 --> 00:18:07,065
0,210 210,515 745,1155 1155,1365 1365,1560
this presentation,| you'll actually see
|你将在幻灯片底部看到这些代码片段的一些示例，

675
00:18:07,065 --> 00:18:08,610
0,305 415,810 810,1065 1065,1260 1260,1545
some examples of these code

676
00:18:08,610 --> 00:18:09,675
0,375 375,540 540,660 660,855 855,1065
snippets on the bottom of

677
00:18:09,675 --> 00:18:10,965
0,180 180,435 435,785 835,1170 1170,1290
the slides,| where we'll try
|我们将尝试将你在课程中学到的一些内容

678
00:18:10,965 --> 00:18:12,660
0,240 240,605 655,975 975,1295 1435,1695
and actually tie in some

679
00:18:12,660 --> 00:18:13,560
0,180 180,375 375,555 555,720 720,900
of what you're learning in

680
00:18:13,560 --> 00:18:15,540
0,105 105,560 880,1280 1390,1710 1710,1980
the lectures| to actual software
|与实际软件以及你如何实现这些部分联系起来，

681
00:18:15,540 --> 00:18:16,410
0,195 195,315 315,450 450,615 615,870
and how you can implement

682
00:18:16,410 --> 00:18:18,195
0,285 285,620 1270,1545 1545,1665 1665,1785
these pieces,| which will help
|这将对你的软件实验有很大帮助，

683
00:18:18,195 --> 00:18:18,780
0,105 105,180 180,315 315,450 450,585
you a lot for your

684
00:18:18,780 --> 00:18:20,625
0,255 255,740 760,1320 1320,1590 1590,1845
software {labs,explicitly -},| so the
|所以左边的 sigmoid 激活函数非常流行，

685
00:18:20,625 --> 00:18:21,915
0,405 405,780 780,990 990,1125 1125,1290
sigmoid activation on the left

686
00:18:21,915 --> 00:18:23,430
0,195 195,390 390,690 690,1085 1195,1515
is very popular,| since it's
|因为它是一个输出在 0 到 1 之间的函数，

687
00:18:23,430 --> 00:18:24,705
0,90 90,315 315,680 700,1125 1125,1275
a function that outputs, you

688
00:18:24,705 --> 00:18:25,800
0,210 210,480 480,675 675,855 855,1095
know, between 0 and 1,|
|

689
00:18:25,800 --> 00:18:26,925
0,330 330,645 645,840 840,960 960,1125
so especially when you want
所以特别是当你想要处理概率分布时，

690
00:18:26,925 --> 00:18:28,580
0,165 165,300 300,480 480,1025 1135,1655
to deal with probabilities distributions,

691
00:18:28,600 --> 00:18:30,090
0,290 290,530 530,755 755,1030 1140,1490
for example,| this is very
|这是非常重要的，因为概率在 0 到 1 之间，

692
00:18:30,090 --> 00:18:32,205
0,350 400,690 690,1160 1450,1830 1830,2115
important because probabilities live between

693
00:18:32,205 --> 00:18:33,800
0,180 180,330 330,605
0 and 1,|
|

694
00:18:33,800 --> 00:18:34,970
0,165 165,420 420,690 690,930 930,1170
in modern deep neural networks
在现代深层神经网络中， ReLU 函数，

695
00:18:34,970 --> 00:18:36,425
0,270 270,435 435,765 765,1040 1180,1455
though, the ReLU function,| which
|你可以在最右边看到，

696
00:18:36,425 --> 00:18:36,995
0,120 120,255 255,375 375,450 450,570
you can see on the

697
00:18:36,995 --> 00:18:38,270
0,195 195,435 435,750 750,1080 1080,1275
far right hand,| is a
|是一个非常流行的激活函数，

698
00:18:38,270 --> 00:18:39,920
0,180 180,500 550,990 990,1340 1390,1650
very popular activation function,| {because,it's
|因为它是分段的，线性的，

699
00:18:39,920 --> 00:18:41,090
0,225 225,390 390,600 600,900 900,1170
-} piece wise, linear,| it's
|所以它的计算非常高效，

700
00:18:41,090 --> 00:18:43,100
0,350 430,780 780,990 990,1400 1690,2010
extremely efficient to compute,| especially
|特别是在计算它的导数时，

701
00:18:43,100 --> 00:18:45,110
0,255 255,540 540,770 880,1550 1720,2010
when computing its derivatives,| its
|它的导数是常量，

702
00:18:45,110 --> 00:18:47,630
0,465 465,770 1360,2030 2110,2385 2385,2520
derivatives are constants,| except for
|除了一个是非线性的， 0 。

703
00:18:47,630 --> 00:18:49,360
0,260 310,600 600,960 960,1310
one {non-linear -} yet

704
00:18:49,600 --> 00:18:50,660
0,400
0,|
|

705
00:18:51,600 --> 00:18:53,060
0,395 395,650 650,910 990,1265 1265,1460
Now I hope actually all
现在，我希望你们所有人都在问自己这个问题，

706
00:18:53,060 --> 00:18:54,095
0,180 180,345 345,555 555,765 765,1035
of you are probably asking

707
00:18:54,095 --> 00:18:55,205
0,240 240,450 450,675 675,840 840,1110
this question to yourself of|
|

708
00:18:55,205 --> 00:18:56,120
0,315 315,465 465,540 540,705 705,915
why do we even need
为什么我们需要这个非线性激活函数，

709
00:18:56,120 --> 00:18:57,620
0,180 180,675 675,1035 1035,1305 1305,1500
this nonlinear activation function,| it
|它似乎让整个情况变得复杂起来，

710
00:18:57,620 --> 00:18:58,400
0,165 165,375 375,555 555,675 675,780
seems like it kind of

711
00:18:58,400 --> 00:18:59,740
0,165 165,660 660,870 870,1050 1050,1340
just complicates this whole picture,|
|

712
00:18:59,850 --> 00:19:00,860
0,290 290,440 440,635 635,815 815,1010
when we didn't really need
当我们一开始并不需要它，

713
00:19:00,860 --> 00:19:01,720
0,105 105,210 210,330 330,525 525,860
it in the first place,|
|

714
00:19:02,250 --> 00:19:03,335
0,320 320,500 500,650 650,815 815,1085
{and,I,want - -} to just
我想花一点时间来回答这个问题，

715
00:19:03,335 --> 00:19:04,670
0,270 270,390 390,635 655,915 915,1335
spend a moment on answering

716
00:19:04,670 --> 00:19:06,065
0,285 285,680 760,1065 1065,1260 1260,1395
this,| because the point of
|因为非线性激活函数的要点是，

717
00:19:06,065 --> 00:19:07,720
0,105 105,600 600,990 990,1305 1305,1655
a non-linear activation function is|
|

718
00:19:08,070 --> 00:19:09,545
0,290 290,580 660,980 980,1235 1235,1475
of course, number one is
当然，第一是引入数据的非线性，

719
00:19:09,545 --> 00:19:11,600
0,300 300,695 805,1755 1755,1920 1920,2055
to introduce non-linearities to our

720
00:19:11,600 --> 00:19:13,100
0,290 640,960 960,1140 1140,1290 1290,1500
data, right,| if we think
|如果我们考虑我们的数据，

721
00:19:13,100 --> 00:19:15,380
0,285 285,585 585,920 1750,2055 2055,2280
about our data,| almost all
|几乎所有我们关心的数据，

722
00:19:15,380 --> 00:19:16,445
0,270 270,495 495,645 645,825 825,1065
data that we care about,|
|

723
00:19:16,445 --> 00:19:17,570
0,255 255,465 465,675 675,900 900,1125
all real world data is
所有现实世界的数据都是高度非线性的，

724
00:19:17,570 --> 00:19:19,730
0,270 270,980 1510,1800 1800,1950 1950,2160
highly non-linear,| now this is
|这一点很重要，

725
00:19:19,730 --> 00:19:21,395
0,350 400,800 1150,1410 1410,1530 1530,1665
important,| because if we want
|因为如果我们想要处理这些类型的数据集，

726
00:19:21,395 --> 00:19:21,995
0,120 120,195 195,300 300,450 450,600
to be able to deal

727
00:19:21,995 --> 00:19:23,120
0,135 135,285 285,465 465,615 615,1125
with those types of datasets,|
|

728
00:19:23,120 --> 00:19:24,260
0,225 225,405 405,705 705,960 960,1140
we need models that are
我们需要同样是非线性的模型，

729
00:19:24,260 --> 00:19:25,445
0,195 195,690 690,900 900,1035 1035,1185
also non-linear,| so they can
|这样它们才能捕捉到相同类型的模式，

730
00:19:25,445 --> 00:19:26,645
0,240 240,510 510,705 705,900 900,1200
capture those same types {of,patterns

731
00:19:26,645 --> 00:19:28,130
0,395 595,870 870,1080 1080,1305 1305,1485
-},| so imagine I told
|想象一下我让你们分开，

732
00:19:28,130 --> 00:19:29,135
0,150 150,270 270,530 550,825 825,1005
{you,to -} separate,| for example,
|例如，我给你这个数据集，红点和绿点，

733
00:19:29,135 --> 00:19:29,990
0,165 165,285 285,405 405,555 555,855
I gave you this {dataset

734
00:19:29,990 --> 00:19:31,415
0,260 490,810 810,1035 1035,1230 1230,1425
-}, red points and green

735
00:19:31,415 --> 00:19:32,045
0,195 195,330 330,405 405,510 510,630
points,| and I ask you
|我请你试着区分这两种类型的数据点，

736
00:19:32,045 --> 00:19:33,020
0,120 120,225 225,330 330,605 655,975
to try and separate those

737
00:19:33,020 --> 00:19:34,720
0,285 285,615 615,885 885,1190 1300,1700
two {types,of -} data points,|
|

738
00:19:35,340 --> 00:19:36,470
0,400 450,710 710,845 845,995 995,1130
now you might think that
现在你可能认为这很容易，

739
00:19:36,470 --> 00:19:37,325
0,120 120,240 240,450 450,705 705,855
this is easy,| but if
|但如果我告诉你，你只能用一条线来做到这一点，

740
00:19:37,325 --> 00:19:38,390
0,120 120,240 240,485 595,870 870,1065
I could, only if I

741
00:19:38,390 --> 00:19:39,215
0,225 225,405 405,540 540,675 675,825
told you that you could

742
00:19:39,215 --> 00:19:40,570
0,240 240,525 525,735 735,990 990,1355
only use a {single,line -}

743
00:19:40,740 --> 00:19:41,795
0,245 245,395 395,590 590,845 845,1055
to do so,| well, now
|那么现在它变成了一个非常复杂的问题，

744
00:19:41,795 --> 00:19:42,700
0,135 135,300 300,420 420,585 585,905
{it,becomes -} a very complicated

745
00:19:42,750 --> 00:19:44,180
0,350 350,575 575,850 990,1250 1250,1430
problem,| in fact, you can't
|事实上，你不可能用一条线有效地解决这个问题，

746
00:19:44,180 --> 00:19:45,650
0,165 165,405 405,690 690,1070 1210,1470
really solve it effectively with

747
00:19:45,650 --> 00:19:46,840
0,135 135,375 375,740
a single line,|
|

748
00:19:47,510 --> 00:19:48,720
0,290 290,455 455,680 680,920 920,1210
and in fact, if you
事实上，如果你在你的解决方案中引入非线性激活函数，

749
00:19:49,130 --> 00:19:51,265
0,335 335,830 830,1190 1190,1540 1860,2135
introduce non-linear activation functions to

750
00:19:51,265 --> 00:19:53,250
0,225 225,575 835,1275 1275,1590 1590,1985
your solution,| that's exactly what
|你就可以来处理这类问题，

751
00:19:53,300 --> 00:19:54,760
0,350 350,635 635,970 990,1235 1235,1460
allows you to, you know,

752
00:19:54,760 --> 00:19:55,980
0,285 285,480 480,675 675,900 900,1220
deal with these types of

753
00:19:56,000 --> 00:19:58,525
0,400 660,1340 1340,1700 1700,2050 2220,2525
problems,| {non-linear,activation -} functions allow
|非线性激活函数允许你处理非线性类型的数据。

754
00:19:58,525 --> 00:19:59,755
0,180 180,315 315,480 480,690 690,1230
you to deal with non-linear

755
00:19:59,755 --> 00:20:01,080
0,255 255,495 495,785
types of data.|
|

756
00:20:01,470 --> 00:20:02,825
0,290 290,455 455,725 725,965 965,1355
Now, and that's what exactly
这正是神经网络的核心如此强大的原因。

757
00:20:02,825 --> 00:20:04,750
0,345 345,630 630,875 1135,1530 1530,1925
makes neural networks so powerful

758
00:20:04,920 --> 00:20:06,380
0,320 320,560 560,880
at their core.|
|

759
00:20:06,690 --> 00:20:07,850
0,275 275,590 590,815 815,965 965,1160
So let's understand this maybe
让我们通过一个非常简单的例子来理解这一点，

760
00:20:07,850 --> 00:20:09,080
0,180 180,345 345,570 570,870 870,1230
with a very simple example,|
|

761
00:20:09,080 --> 00:20:10,460
0,375 375,645 645,825 825,1245 1245,1380
walking through this diagram of
再看一下感知器的图示，

762
00:20:10,460 --> 00:20:11,830
0,135 135,690 690,885 885,1065 1065,1370
a perceptron one more time,|
|

763
00:20:12,540 --> 00:20:13,790
0,400 420,710 710,860 860,995 995,1250
{imagine,I,give - -} you this
想象一下，我给你这个训练有素的神经网络和权重，

764
00:20:13,790 --> 00:20:15,515
0,345 345,675 675,950 1000,1335 1335,1725
trained neural network with weights,|
|

765
00:20:15,515 --> 00:20:16,850
0,240 240,495 495,765 765,1035 1035,1335
now not {w1 -} {w2
现在不是 w1 w2 ，我要给你们这些位置的数字，

766
00:20:16,850 --> 00:20:17,900
0,330 330,585 585,705 705,885 885,1050
-}, I'm going to actually

767
00:20:17,900 --> 00:20:19,205
0,135 135,360 360,710 790,1065 1065,1305
give you numbers at {these,locations

768
00:20:19,205 --> 00:20:20,980
0,365 565,825 825,975 975,1230 1230,1775
-},| so the trained weights
|所以，训练后的 w0 是 1 ，

769
00:20:21,210 --> 00:20:23,500
0,400 1110,1510 1560,1835 1835,2000 2000,2290
{w0 -} will be 1|
|

770
00:20:23,610 --> 00:20:24,590
0,305 305,575 575,800 800,890 890,980
and W will be a
W 是向量 3 和 -2 ，

771
00:20:24,590 --> 00:20:26,050
0,255 255,615 615,945 945,1170 1170,1460
vector of 3 and {-2

772
00:20:26,070 --> 00:20:26,940
0,400
-},|
|

773
00:20:27,210 --> 00:20:28,480
0,275 275,440 440,665 665,890 890,1270
so, this neural network has
所以，这个神经网络有两个输入，

774
00:20:28,560 --> 00:20:29,870
0,350 350,800 800,1040 1040,1175 1175,1310
two inputs,| like we said
|就像我们之前说的，它有输入 x1 和 x2 ，

775
00:20:29,870 --> 00:20:30,815
0,165 165,315 315,510 510,735 735,945
before, it has input {x1

776
00:20:30,815 --> 00:20:32,120
0,335 415,675 675,885 885,1095 1095,1305
-}, it has input {x2

777
00:20:32,120 --> 00:20:33,485
0,350 730,990 990,1110 1110,1245 1245,1365
-},| if we want to
|如果我们想要得到它的输出，

778
00:20:33,485 --> 00:20:34,360
0,105 105,315 315,510 510,615 615,875
get the output of it,|
|

779
00:20:34,800 --> 00:20:36,200
0,275 275,545 545,935 935,1205 1205,1400
{this,is -} also the main
这也是主要内容，

780
00:20:36,200 --> 00:20:36,845
0,165 165,270 270,405 405,540 540,645
thing,| I want all of
|我希望你们从今天的演讲中学到的，

781
00:20:36,845 --> 00:20:37,595
0,105 105,225 225,390 390,585 585,750
you to take away from

782
00:20:37,595 --> 00:20:38,795
0,165 165,420 420,720 720,930 930,1200
this lecture today| is that
|为了得到感知器的输出，

783
00:20:38,795 --> 00:20:39,710
0,255 255,405 405,630 630,810 810,915
to get the output of

784
00:20:39,710 --> 00:20:40,610
0,120 120,570 570,660 660,765 765,900
a perceptron| and there are
|我们需要采取三个步骤，

785
00:20:40,610 --> 00:20:41,450
0,180 180,390 390,570 570,705 705,840
three steps we need to

786
00:20:41,450 --> 00:20:43,490
0,260 430,720 720,930 930,1250 1660,2040
take,| from this stage, we
|在这个阶段，我们首先计算输入和权重的乘法，

787
00:20:43,490 --> 00:20:45,590
0,380 640,1110 1110,1275 1275,1860 1860,2100
first compute the multiplication of

788
00:20:45,590 --> 00:20:46,750
0,225 225,525 525,630 630,765 765,1160
our inputs with our weights,|
|

789
00:20:48,570 --> 00:20:51,220
0,400 750,1040 1040,1505 1505,1810 2250,2650
sorry, {yeah\,,multiply -} them together,
抱歉，是的，把它们相乘，结果相加，并计算非线性函数，

790
00:20:51,390 --> 00:20:53,495
0,395 395,740 740,1090 1500,1850 1850,2105
add their result and compute

791
00:20:53,495 --> 00:20:55,420
0,810 810,1095 1095,1305 1305,1590 1590,1925
non-linearity,| it's these three steps
|这三个步骤定义了信息通过感知器的前向传播。

792
00:20:55,770 --> 00:20:58,220
0,400 690,1090 1230,1505 1505,1780 1890,2450
that define the forward propagation

793
00:20:58,220 --> 00:21:00,130
0,300 300,680 820,1125 1125,1290 1290,1910
of information through a perceptron.|
|

794
00:21:00,980 --> 00:21:01,945
0,245 245,545 545,740 740,845 845,965
So let's take a look
那么，让我们来看看它到底是如何工作的，

795
00:21:01,945 --> 00:21:03,505
0,275 565,825 825,1020 1020,1290 1290,1560
at how that exactly works,

796
00:21:03,505 --> 00:21:04,750
0,255 255,525 525,750 750,990 990,1245
right,| {so,if -} we plug
|所以如果我们把这些数字放入方程中，

797
00:21:04,750 --> 00:21:06,550
0,165 165,330 330,620 1000,1400 1540,1800
in these numbers to to

798
00:21:06,550 --> 00:21:08,440
0,210 210,590 1360,1620 1620,1755 1755,1890
those equations,| we can see
|我们可以看到非线性函数里面的所有东西，

799
00:21:08,440 --> 00:21:10,510
0,260 1000,1400 1420,1740 1740,1905 1905,2070
that everything inside of our

800
00:21:10,510 --> 00:21:11,845
0,210 210,840 840,1035 1035,1185 1185,1335
{non-linearity -},| here the {non-linearity
|这里非线性函数是 g ，

801
00:21:11,845 --> 00:21:13,675
0,495 495,675 675,1025 1255,1590 1590,1830
-} is g, right,| that
|g 函数可能是 sigmoid ，

802
00:21:13,675 --> 00:21:15,130
0,305 385,785 955,1245 1245,1380 1380,1455
function g which could be

803
00:21:15,130 --> 00:21:16,495
0,105 105,560 730,1020 1020,1170 1170,1365
a sigmoid,| we saw in
|我们在之前的一张幻灯片看到，

804
00:21:16,495 --> 00:21:19,800
0,330 330,725 1435,1835 2245,2645 2905,3305
{previous,slide -},| that component inside
|事实上，我们的非线性函数中的那个分量只是一条二维线，

805
00:21:19,820 --> 00:21:22,420
0,400 900,1865 1865,2165 2165,2405 2405,2600
our non-linearity is in fact

806
00:21:22,420 --> 00:21:23,740
0,240 240,450 450,600 600,1080 1080,1320
just a {two,dimensional -} line,|
|

807
00:21:23,740 --> 00:21:24,850
0,210 210,390 390,660 660,990 990,1110
it has two inputs| and
它有两个输入，|如果我们考虑这个神经网络可以看到的所有可能的输入的空间，

808
00:21:24,850 --> 00:21:26,010
0,120 120,330 330,585 585,825 825,1160
if we consider the space

809
00:21:26,300 --> 00:21:27,390
0,290 290,470 470,650 650,815 815,1090
of all of the possible

810
00:21:27,560 --> 00:21:29,110
0,320 320,455 455,730 930,1310 1310,1550
inputs that this neural network

811
00:21:29,110 --> 00:21:30,880
0,315 315,650 970,1245 1245,1485 1485,1770
could see,| we can actually
|我们可以把它画在决策边界上，

812
00:21:30,880 --> 00:21:32,455
0,225 225,530 700,1035 1035,1305 1305,1575
plot this on a decision

813
00:21:32,455 --> 00:21:33,715
0,450 450,645 645,825 825,1005 1005,1260
boundary, right,| we can plot
|我们可以画这条二维线，作为决定边界，

814
00:21:33,715 --> 00:21:35,480
0,285 285,480 480,975 975,1265
this two dimensional line,

815
00:21:35,480 --> 00:21:38,675
0,350 760,1080 1080,1400 2470,2805 2805,3195
as as a decision boundary,|
|

816
00:21:38,675 --> 00:21:40,235
0,105 105,270 270,575 655,1290 1290,1560
as a [plane] separating these
就像分隔我们空间的这两个组成部分的平面，

817
00:21:40,235 --> 00:21:41,710
0,270 270,635 655,915 915,1125 1125,1475
two components of our space,|
|

818
00:21:42,390 --> 00:21:44,300
0,290 290,580 1020,1325 1325,1630 1650,1910
{in,fact, -} not only is
事实上，它不仅是一个单一的平面，

819
00:21:44,300 --> 00:21:45,320
0,105 105,225 225,450 450,750 750,1020
it a single plane,| there's
|还有一个方向性成分，取决于我们在平面的哪一边，

820
00:21:45,320 --> 00:21:47,120
0,150 150,870 870,1220 1330,1620 1620,1800
a directionality component depending on

821
00:21:47,120 --> 00:21:48,095
0,225 225,450 450,600 600,750 750,975
which side of the plane

822
00:21:48,095 --> 00:21:49,385
0,225 225,405 405,555 555,815 1015,1290
that we live on,| if
|如果我们看到一个输入，例如这里的 -1 2 ，

823
00:21:49,385 --> 00:21:50,570
0,165 165,285 285,480 480,845 895,1185
we see an input, for

824
00:21:50,570 --> 00:21:52,450
0,255 255,600 600,975 975,1370 1480,1880
example here, {-1 } 2,|
|

825
00:21:52,830 --> 00:21:54,110
0,400 420,725 725,965 965,1145 1145,1280
we actually know that it
我们知道它位于平面的一侧，

826
00:21:54,110 --> 00:21:55,385
0,225 225,560 640,960 960,1140 1140,1275
lives on one side of

827
00:21:55,385 --> 00:21:56,645
0,150 150,425 715,975 975,1095 1095,1260
the plane| and it will
|它会有某种类型的输出，

828
00:21:56,645 --> 00:21:57,910
0,225 225,435 435,690 690,960 960,1265
have a certain type {of,output

829
00:21:57,990 --> 00:21:59,165
0,245 245,350 350,515 515,800 800,1175
-},| in this case, that
|在这种情况下，输出将是正的，

830
00:21:59,165 --> 00:22:00,250
0,285 285,465 465,660 660,825 825,1085
output is going to be

831
00:22:00,690 --> 00:22:02,720
0,400 720,1085 1085,1450 1560,1835 1835,2030
positive, right,| because in this
|因为在这种情况下，当我们将这些分量放入我们的方程时，

832
00:22:02,720 --> 00:22:03,910
0,300 300,540 540,690 690,885 885,1190
case, when we plug those

833
00:22:04,230 --> 00:22:06,050
0,335 335,560 560,800 800,1150 1470,1820
components into our equation,| we'll
|我们会得到一个正数，通过非线性分量，

834
00:22:06,050 --> 00:22:07,430
0,135 135,300 300,590 760,1110 1110,1380
get a positive number, that

835
00:22:07,430 --> 00:22:08,920
0,320 400,750 750,1100
passes through the

836
00:22:09,490 --> 00:22:10,950
0,635 635,950 950,1160 1160,1295 1295,1460
non-linear component,| and that gets
|它也会通过传播，

837
00:22:10,950 --> 00:22:13,005
0,495 495,860 970,1275 1275,1580 1780,2055
propagated through as well,| {of,course,
|当然，如果你在空间的另一边，

838
00:22:13,005 --> 00:22:13,575
0,150 150,285 285,420 420,480 480,570
-} if you're on the

839
00:22:13,575 --> 00:22:14,630
0,195 195,435 435,600 600,765 765,1055
other side of the space,|
|

840
00:22:14,980 --> 00:22:16,455
0,305 305,425 425,560 560,820 1080,1475
you're going to have the
你会得到相反的结果，

841
00:22:16,455 --> 00:22:18,000
0,375 375,755 775,1110 1110,1335 1335,1545
opposite result, right,| and that
|这个阈值函数就在这个决策边界上，

842
00:22:18,000 --> 00:22:19,530
0,645 645,900 900,1155 1155,1335 1335,1530
thresholding function is going to

843
00:22:19,530 --> 00:22:21,600
0,290 760,1160 1240,1530 1530,1770 1770,2070
essentially live at this decision

844
00:22:21,600 --> 00:22:22,845
0,480 480,690 690,885 885,1050 1050,1245
boundary,| so depending on which
|所以，根据你所在空间的哪一边，

845
00:22:22,845 --> 00:22:23,565
0,165 165,270 270,390 390,555 555,720
side of the space you

846
00:22:23,565 --> 00:22:25,650
0,135 135,395 595,885 885,1625 1705,2085
live on,| that thresholding function,
|阈值函数， sigmoid 函数将控制你移动到一边或另一边。

847
00:22:25,650 --> 00:22:27,540
0,300 300,735 735,1040 1360,1665 1665,1890
that sigmoid function, is going

848
00:22:27,540 --> 00:22:29,270
0,165 165,410 640,1040 1120,1425 1425,1730
to then control how you

849
00:22:29,530 --> 00:22:30,450
0,320 320,485 485,635 635,800 800,920
move to one side or

850
00:22:30,450 --> 00:22:31,160
0,90 90,320
the other.|
|

851
00:22:32,510 --> 00:22:34,645
0,400 660,935 935,1210 1350,1750 1800,2135
Now in this particular example,
现在在这个特定的例子中，这是非常方便的，

852
00:22:34,645 --> 00:22:36,265
0,195 195,330 330,600 600,995 1345,1620
this is very convenient,| because
|因为我们可以可视化，

853
00:22:36,265 --> 00:22:37,615
0,135 135,395 445,750 750,1215 1215,1350
we can actually visualize,| and
|我可以在这张幻灯片上为你们画出这个完整的空间，

854
00:22:37,615 --> 00:22:38,700
0,90 90,240 240,450 450,720 720,1085
I can draw this exact

855
00:22:38,720 --> 00:22:40,660
0,380 380,725 725,995 995,1300 1650,1940
full space for you on

856
00:22:40,660 --> 00:22:41,635
0,225 225,465 465,705 705,810 810,975
this slide,| it's only a
|它只是一个二维空间，

857
00:22:41,635 --> 00:22:42,685
0,135 135,555 555,780 780,900 900,1050
two dimensional space,| so it's
|所以我们很容易可视化，

858
00:22:42,685 --> 00:22:43,660
0,150 150,450 450,660 660,795 795,975
very easy for us to

859
00:22:43,660 --> 00:22:45,595
0,495 495,675 675,810 810,1100 1570,1935
visualize,| {but,of -} course, for
|但是当然，对于我们关心的几乎所有问题，

860
00:22:45,595 --> 00:22:47,050
0,315 315,630 630,990 990,1275 1275,1455
almost all problems that we

861
00:22:47,050 --> 00:22:48,930
0,240 240,590 820,1140 1140,1460 1480,1880
care about,| our data points
|我们的数据点都不会是二维的，

862
00:22:49,010 --> 00:22:49,900
0,290 290,470 470,650 650,770 770,890
are not going to be

863
00:22:49,900 --> 00:22:51,265
0,150 150,770 850,1095 1095,1200 1200,1365
two dimensional,| if you think
|如果你考虑一幅图像，

864
00:22:51,265 --> 00:22:53,425
0,150 150,270 270,545 1195,1500 1500,2160
about an image,| the dimensionality
|图像的维度将是你在图像中拥有的像素数，

865
00:22:53,425 --> 00:22:54,190
0,60 60,195 195,405 405,615 615,765
of an image is going

866
00:22:54,190 --> 00:22:54,790
0,90 90,165 165,270 270,420 420,600
to be the number of

867
00:22:54,790 --> 00:22:56,215
0,480 480,735 735,870 870,1130 1150,1425
pixels, that you have in

868
00:22:56,215 --> 00:22:57,250
0,135 135,395 535,780 780,900 900,1035
{the,image -},| so these are
|所以这些将是数千个维度，数百万个维度，甚至更多，

869
00:22:57,250 --> 00:22:58,440
0,150 150,300 300,465 465,770 790,1190
going to be thousands of

870
00:22:58,700 --> 00:23:00,520
0,490 630,1010 1010,1265 1265,1595 1595,1820
dimensions, millions of dimensions or

871
00:23:00,520 --> 00:23:01,420
0,195 195,530
even more,|
|

872
00:23:01,580 --> 00:23:03,115
0,290 290,580 750,1100 1100,1340 1340,1535
and then drawing these types
然后像你在这里看到的那样绘制这些类型的图是根本不可行的，

873
00:23:03,115 --> 00:23:04,900
0,305 805,1230 1230,1500 1500,1650 1650,1785
of plots like you see

874
00:23:04,900 --> 00:23:06,240
0,120 120,300 300,540 540,765 765,1340
here is simply not feasible,|
|

875
00:23:06,560 --> 00:23:07,465
0,245 245,350 350,590 590,770 770,905
{so,we -} can't always do
所以我们不能总是这样做，

876
00:23:07,465 --> 00:23:08,335
0,150 150,315 315,510 510,720 720,870
this,| but hopefully this gives
|但希望这能给你一些直觉，

877
00:23:08,335 --> 00:23:10,410
0,165 165,455 475,1145 1255,1655 1675,2075
you some intuition| to understand
|来理解我们建立更复杂的模型。

878
00:23:10,640 --> 00:23:11,515
0,275 275,395 395,530 530,695 695,875
kind of as we build

879
00:23:11,515 --> 00:23:13,680
0,305 415,815 1075,1475 1495,1830 1830,2165
up into more complex models.|
|

880
00:23:14,930 --> 00:23:15,655
0,260 260,410 410,545 545,635 635,725
So now that we have
现在我们已经对感知器有了一个概念，

881
00:23:15,655 --> 00:23:17,005
0,180 180,375 375,555 555,735 735,1350
an idea of the perceptron,|
|

882
00:23:17,005 --> 00:23:18,145
0,360 360,555 555,780 780,945 945,1140
let's see how we can
让我们看看我们如何真正地利用这个单一的神经元，

883
00:23:18,145 --> 00:23:19,525
0,225 225,435 435,690 690,975 975,1380
actually take this single neuron|
|

884
00:23:19,525 --> 00:23:20,455
0,210 210,420 420,615 615,795 795,930
and start to build it
并开始将其构建成更复杂的东西，

885
00:23:20,455 --> 00:23:21,720
0,165 165,450 450,765 765,990 990,1265
up into something more complicated,|
|

886
00:23:21,860 --> 00:23:23,050
0,260 260,410 410,665 665,920 920,1190
a full neural network,| and
一个完整的神经网络，|并由此建立一个模型。

887
00:23:23,050 --> 00:23:24,150
0,180 180,330 330,555 555,810 810,1100
build a model from that.|
|

888
00:23:25,060 --> 00:23:26,660
0,335 335,635 635,1085 1085,1295 1295,1600
So let's revisit again this
所以让我们再来回顾一下之前的感知器示意图，

889
00:23:26,740 --> 00:23:28,460
0,350 350,815 815,965 965,1100 1100,1720
previous diagram of the perceptron,|
|

890
00:23:28,840 --> 00:23:30,540
0,400 540,920 920,1175 1175,1310 1310,1700
{if\,,again -} just to reiterate
如果，我再重申一次，

891
00:23:30,540 --> 00:23:32,370
0,195 195,495 495,890 970,1370 1510,1830
one more time,| this core
|这里的核心信息，

892
00:23:32,370 --> 00:23:33,390
0,195 195,435 435,750 750,930 930,1020
piece of information,| that I
|我想让你们所有人从这门课上学到的是，

893
00:23:33,390 --> 00:23:34,245
0,135 135,270 270,390 390,570 570,855
want all of you to

894
00:23:34,245 --> 00:23:35,480
0,285 285,540 540,750 750,930 930,1235
take away from this class

895
00:23:35,950 --> 00:23:37,515
0,365 365,635 635,815 815,1340 1340,1565
is| how a perceptron works
|感知器是如何工作的，

896
00:23:37,515 --> 00:23:39,410
0,210 210,375 375,665 745,1415 1495,1895
and| how it propagates information
|以及它是如何将信息传播到决策中的，

897
00:23:39,700 --> 00:23:41,190
0,245 245,470 470,850 1110,1355 1355,1490
to its decision,| there are
|这里有三个步骤，

898
00:23:41,190 --> 00:23:42,450
0,225 225,540 540,855 855,1095 1095,1260
{three,steps -},| first is the
|第一是点积，第二是偏置，第三是非线性，

899
00:23:42,450 --> 00:23:44,220
0,180 180,500 1090,1410 1410,1620 1620,1770
dot product, second is the

900
00:23:44,220 --> 00:23:45,660
0,440 670,960 960,1140 1140,1305 1305,1440
bias, and third {is,the -}

901
00:23:45,660 --> 00:23:47,700
0,830 1030,1290 1290,1455 1455,1710 1710,2040
non-linearity,| and you keep repeating
|你对你神经网络中的每一个感知器都重复这个过程。

902
00:23:47,700 --> 00:23:48,780
0,150 150,435 435,675 675,840 840,1080
this process for every single

903
00:23:48,780 --> 00:23:50,330
0,645 645,885 885,1050 1050,1290 1290,1550
perceptron in your neural network.|
|

904
00:23:51,450 --> 00:23:52,775
0,335 335,710 710,830 830,1190 1190,1325
Let's simplify the diagram a
让我们稍微简化一下图表，

905
00:23:52,775 --> 00:23:54,035
0,150 150,455 595,915 915,1065 1065,1260
little bit,| I'll get rid
|我将去掉权重，

906
00:23:54,035 --> 00:23:56,030
0,195 195,495 495,1055 1555,1845 1845,1995
of the weights| and you
|你可以假设这里的每一行都有一个关联的权重标量，

907
00:23:56,030 --> 00:23:57,110
0,165 165,345 345,510 510,765 765,1080
can assume that every line

908
00:23:57,110 --> 00:23:58,505
0,330 330,600 600,885 885,1185 1185,1395
here now basically has an

909
00:23:58,505 --> 00:24:00,280
0,305 445,810 810,1245 1245,1500 1500,1775
associated weight scalar, that's associated

910
00:24:00,420 --> 00:24:02,290
0,290 290,580 810,1145 1145,1475 1475,1870
with it,| {every,line -} also
|每一行也对应于输入，

911
00:24:02,790 --> 00:24:04,550
0,605 605,755 755,1030 1110,1460 1460,1760
corresponds to the input that's

912
00:24:04,550 --> 00:24:05,540
0,180 180,480 480,720 720,855 855,990
coming in,| it has a
|它有一个权重输入在行上，

913
00:24:05,540 --> 00:24:06,755
0,165 165,480 480,675 675,975 975,1215
weight that's coming in also

914
00:24:06,755 --> 00:24:07,920
0,150 150,455
at the,

915
00:24:08,200 --> 00:24:09,795
0,320 320,485 485,725 725,1120 1230,1595
on the line itself,| {and,I've
|为了简单起见，我也去掉了偏置，

916
00:24:09,795 --> 00:24:11,310
0,405 405,660 660,930 930,1125 1125,1515
-} also removed the bias

917
00:24:11,310 --> 00:24:12,765
0,285 285,495 495,660 660,840 840,1455
just for sake of simplicity,|
|

918
00:24:12,765 --> 00:24:14,595
0,150 150,345 345,495 495,785 1555,1830
but it's still there,| so
但它仍然存在，|现在结果是 z ，

919
00:24:14,595 --> 00:24:16,670
0,240 240,555 555,905 1285,1680 1680,2075
now the result is that

920
00:24:16,960 --> 00:24:18,540
0,400 480,845 845,1175 1175,1340 1340,1580
z,| which let's call that
|我们称之为点积加上偏置的结果，

921
00:24:18,540 --> 00:24:19,665
0,330 330,600 600,735 735,885 885,1125
the result of our dot

922
00:24:19,665 --> 00:24:21,705
0,335 475,780 780,945 945,1355 1705,2040
product plus the bias is

923
00:24:21,705 --> 00:24:22,995
0,300 300,585 585,930 930,1125 1125,1290
going| and that's what we
|这就是我们传递给非线性函数的，

924
00:24:22,995 --> 00:24:24,290
0,180 180,405 405,585 585,1005 1005,1295
pass into our {non-linear,function -},|
|

925
00:24:24,880 --> 00:24:26,745
0,365 365,730 900,1235 1235,1520 1520,1865
that piece is going to
这一块将应用于激活函数，

926
00:24:26,745 --> 00:24:28,610
0,330 330,665 805,1065 1065,1325 1375,1865
be applied to {that,activation -}

927
00:24:28,660 --> 00:24:30,165
0,400 480,755 755,890 890,1150 1230,1505
function,| now the final output
|现在，这里的最终输出将是 g ，

928
00:24:30,165 --> 00:24:31,935
0,275 745,1065 1065,1335 1335,1605 1605,1770
here is simply going to

929
00:24:31,935 --> 00:24:34,140
0,245 595,995 1615,1890 1890,2010 2010,2205
be g,| which is our
|这是我们的 z 的激活函数，

930
00:24:34,140 --> 00:24:35,560
0,375 375,740
activation function

931
00:24:35,560 --> 00:24:37,675
0,350 460,860 1540,1800 1800,1950 1950,2115
of z,| z is going
|z 就是你可以认为这个神经元的状态，

932
00:24:37,675 --> 00:24:38,620
0,105 105,270 270,585 585,825 825,945
to be basically what you

933
00:24:38,620 --> 00:24:39,445
0,135 135,255 255,375 375,555 555,825
can think of the state

934
00:24:39,445 --> 00:24:40,435
0,225 225,375 375,645 645,840 840,990
of this neuron,| it's the
|它是点积加上偏置的结果。

935
00:24:40,435 --> 00:24:42,055
0,195 195,360 360,635 985,1305 1305,1620
result of that dot product

936
00:24:42,055 --> 00:24:43,540
0,300 300,785
plus bias.|
|

937
00:24:44,060 --> 00:24:44,950
0,290 290,440 440,545 545,680 680,890
Now, if we want to
现在，如果我们想定义和建立一个多层输出神经网络，

938
00:24:44,950 --> 00:24:46,570
0,320 340,630 630,840 840,1160 1330,1620
define and build up a

939
00:24:46,570 --> 00:24:48,450
0,285 285,860 1060,1350 1350,1605 1605,1880
multi layered output neural network,|
|

940
00:24:49,010 --> 00:24:50,140
0,245 245,335 335,485 485,755 755,1130
if we want two outputs
比如，如果我们想要这个函数的两个输出，

941
00:24:50,140 --> 00:24:51,160
0,165 165,300 300,570 570,825 825,1020
to this function, for example,|
|

942
00:24:51,160 --> 00:24:52,410
0,225 225,285 285,435 435,740 850,1250
it's a very simple procedure,|
这是一个非常简单的过程，|

943
00:24:52,490 --> 00:24:53,680
0,275 275,425 425,620 620,905 905,1190
{we,just -} have now two
我们现在只有两个神经元，两个感知器，

944
00:24:53,680 --> 00:24:55,930
0,470 490,795 795,1410 1410,1695 1695,2250
neurons, two perceptons,| each perceptron
|每个感知器将控制其相关部分的输出，

945
00:24:55,930 --> 00:24:57,610
0,210 210,420 420,630 630,980 1390,1680
will control the output for

946
00:24:57,610 --> 00:25:00,370
0,255 255,620 730,1130 1300,1700 2500,2760
its associated piece, right,| so
|所以现在我们有两个输出，

947
00:25:00,370 --> 00:25:01,300
0,120 120,210 210,315 315,525 525,930
now we have two outputs,|
|

948
00:25:01,300 --> 00:25:02,200
0,300 300,495 495,615 615,705 705,900
each one is the normal
每一个都是正常的感知器，

949
00:25:02,200 --> 00:25:03,670
0,630 630,855 855,1050 1050,1275 1275,1470
perceptron,| it takes all of
|它接受所有的输入，

950
00:25:03,670 --> 00:25:04,930
0,210 210,615 615,855 855,1035 1035,1260
the inputs,| so they both
|所以它们都接受相同的输入，

951
00:25:04,930 --> 00:25:06,160
0,195 195,345 345,615 615,1005 1005,1230
take the {same,inputs -},| but
|但令人惊讶的是，

952
00:25:06,160 --> 00:25:08,520
0,650 910,1215 1215,1395 1395,1575 1575,2360
amazingly,| now with this mathematical
|现在有了这样的数学理解，

953
00:25:08,540 --> 00:25:10,450
0,400 870,1145 1145,1420 1470,1760 1760,1910
understanding,| we can start to
|我们可以开始完全从头开始建立我们的第一个神经网络。

954
00:25:10,450 --> 00:25:11,970
0,260 280,630 630,930 930,1260 1260,1520
build our first neural network

955
00:25:12,140 --> 00:25:14,395
0,400 420,755 755,1090 1350,1750 2010,2255
{entirely,from -} scratch.| So what
|那么这是什么样子的呢，

956
00:25:14,395 --> 00:25:15,235
0,105 105,255 255,420 420,630 630,840
does that look like,| so
|我们可以首先初始化这两个组件，

957
00:25:15,235 --> 00:25:16,615
0,150 150,345 345,585 585,870 870,1380
we can start by firstly

958
00:25:16,615 --> 00:25:18,865
0,605 865,1260 1260,1605 1605,1955 1975,2250
{initializing,these -} two components,| The
|我们看到的第一个分量是权重矩阵，抱歉，是权重向量，

959
00:25:18,865 --> 00:25:19,945
0,255 255,570 570,765 765,900 900,1080
first component that we saw

960
00:25:19,945 --> 00:25:22,060
0,225 225,390 390,555 555,1205 1825,2115
was the weight matrix, excuse

961
00:25:22,060 --> 00:25:23,260
0,195 195,345 345,510 510,860 880,1200
me, the weight vector,| It's
|在这种情况下，它是权重的向量，

962
00:25:23,260 --> 00:25:24,595
0,120 120,405 405,660 660,1080 1080,1335
a vector of weights in

963
00:25:24,595 --> 00:25:26,120
0,180 180,485
this case,|
|

964
00:25:26,400 --> 00:25:27,875
0,290 290,455 455,730 750,1130 1130,1475
and the second component is
第二个分量是偏置向量，

965
00:25:27,875 --> 00:25:30,170
0,365 685,1085 1195,1635 1635,1985 2035,2295
the, the bias vector,| that
|我们要乘以所有输入的点积乘以我们的权重，

966
00:25:30,170 --> 00:25:31,310
0,150 150,285 285,435 435,945 945,1140
we're going to multiply with

967
00:25:31,310 --> 00:25:32,270
0,120 120,300 300,585 585,825 825,960
the dot product of all

968
00:25:32,270 --> 00:25:33,440
0,105 105,300 300,740 760,1020 1020,1170
of our inputs by our

969
00:25:33,440 --> 00:25:35,720
0,440 760,1155 1155,1550
weights, right,| so,
|所以，现在剩下的唯一一步，

970
00:25:35,970 --> 00:25:37,445
0,260 260,485 485,830 830,1145 1145,1475
the only remaining step now|
|

971
00:25:37,445 --> 00:25:39,310
0,345 345,705 705,990 990,1305 1305,1865
after we've defined these parameters
在我们定义了层的这些参数之后，

972
00:25:39,390 --> 00:25:41,135
0,275 275,470 470,790 1140,1505 1505,1745
of our layer is to|
|

973
00:25:41,135 --> 00:25:42,995
0,270 270,665 775,1035 1035,1295 1555,1860
now define, you know, how
定义信息的前向传播是如何工作的，

974
00:25:42,995 --> 00:25:44,630
0,210 210,450 450,960 960,1275 1275,1635
this forward propagation of information

975
00:25:44,630 --> 00:25:46,030
0,330 330,525 525,780 780,1035 1035,1400
works,| {and,that's -} exactly those
|这正是我向你们强调的三个主要组成部分，

976
00:25:46,140 --> 00:25:47,555
0,305 305,590 590,950 950,1190 1190,1415
three main components, that I've

977
00:25:47,555 --> 00:25:49,540
0,245 865,1365 1365,1500 1500,1650 1650,1985
been stressing to you,| so
|所以我们可以创建这个 call 函数来实现这一点，

978
00:25:49,680 --> 00:25:50,735
0,260 260,410 410,620 620,845 845,1055
we can create this call

979
00:25:50,735 --> 00:25:52,160
0,305 355,615 615,795 795,1095 1095,1425
function to do exactly that,|
|

980
00:25:52,160 --> 00:25:53,690
0,285 285,555 555,780 780,1020 1020,1530
to define this forward propagation
定义信息的正向传播，

981
00:25:53,690 --> 00:25:55,745
0,315 315,710 1480,1740 1740,1875 1875,2055
of information,| and the story
|这里的故事和我们一直看到的完全一样，

982
00:25:55,745 --> 00:25:56,885
0,165 165,420 420,750 750,960 960,1140
here is exactly the same

983
00:25:56,885 --> 00:25:57,950
0,195 195,405 405,555 555,795 795,1065
as we've been seeing it,

984
00:25:57,950 --> 00:25:59,980
0,350 370,885 885,1305 1305,1560 1560,2030
right,| matrix, multiply our inputs
|矩阵，将我们的输入乘以我们的权重，

985
00:26:00,120 --> 00:26:01,620
0,275 275,440 440,880
with our weights,|
|

986
00:26:02,550 --> 00:26:04,260
0,290 290,440 440,850
add a bias,|
添加偏置，|

987
00:26:04,530 --> 00:26:06,530
0,275 275,550 630,980 980,1205 1205,2000
and then apply a non-linearity|
然后应用非线性函数，|

988
00:26:06,530 --> 00:26:08,270
0,300 300,555 555,765 765,1070 1450,1740
and return the result| and
并返回结果，|这段代码就会运行，

989
00:26:08,270 --> 00:26:09,800
0,285 285,680 760,1095 1095,1335 1335,1530
that literally this code will

990
00:26:09,800 --> 00:26:11,240
0,285 285,570 570,795 795,1130 1150,1440
run,| {this,will -} define a
|这将定义一个完整的神经网络层，

991
00:26:11,240 --> 00:26:12,970
0,290 520,825 825,1110 1110,1470 1470,1730
full, a full neural network

992
00:26:13,020 --> 00:26:15,080
0,400 750,1010 1010,1160 1160,1450 1710,2060
layer,| that you can then
|然后你可以这样做，

993
00:26:15,080 --> 00:26:17,630
0,350 460,765 765,1070 1960,2310 2310,2550
take like this,| and of
|当然，实际上，对你们所有人来说幸运的是，

994
00:26:17,630 --> 00:26:18,770
0,285 285,540 540,915 915,1050 1050,1140
course, actually, luckily for all

995
00:26:18,770 --> 00:26:19,745
0,120 120,380 400,675 675,810 810,975
of you,| all of that
|所有的代码，不是很多代码，

996
00:26:19,745 --> 00:26:21,040
0,240 240,450 450,750 750,960 960,1295
code, which wasn't much code,|
|

997
00:26:21,090 --> 00:26:22,670
0,380 380,640 660,1130 1130,1340 1340,1580
that's been abstracted away by
被这些库抽象出来，比如 TensorFlow ，

998
00:26:22,670 --> 00:26:24,140
0,210 210,615 615,810 810,1320 1320,1470
these libraries, like TensorFlow,| you
|你可以简单地像这样调用函数，

999
00:26:24,140 --> 00:26:25,560
0,135 135,410 430,830
can simply call

1000
00:26:25,560 --> 00:26:27,105
0,290 340,645 645,945 945,1245 1245,1545
functions like this,| which will
|它会复制这段代码，

1001
00:26:27,105 --> 00:26:28,860
0,360 360,570 570,750 750,1380 1380,1755
actually, you know, replicate exactly

1002
00:26:28,860 --> 00:26:30,870
0,285 285,480 480,675 675,980 1720,2010
that piece of code,| {so,you
|所以，你不必将所有代码都复制下来，

1003
00:26:30,870 --> 00:26:31,940
0,135 135,300 300,420 420,690 690,1070
-} don't need to necessarily

1004
00:26:31,960 --> 00:26:32,850
0,305 305,470 470,590 590,740 740,890
copy all of that code

1005
00:26:32,850 --> 00:26:34,815
0,165 165,470 1360,1620 1620,1785 1785,1965
down,| you can just call
|你可以直接调用它。

1006
00:26:34,815 --> 00:26:35,660
0,275
it.|
|

1007
00:26:36,250 --> 00:26:38,745
0,400 540,845 845,1150 1620,2020 2250,2495
And with that understanding,| you
有了这种理解，|我们刚刚看到了如何建立一个单一的层，

1008
00:26:38,745 --> 00:26:39,825
0,210 210,510 510,735 735,915 915,1080
know, we just saw how

1009
00:26:39,825 --> 00:26:40,710
0,135 135,270 270,465 465,660 660,885
you could build a single

1010
00:26:40,710 --> 00:26:42,290
0,315 315,525 525,675 675,980 1180,1580
layer,| {but,of -} course, now
|当然，现在你也可以开始考虑如何堆叠这些层了，

1011
00:26:42,670 --> 00:26:43,740
0,275 275,485 485,710 710,905 905,1070
you can actually start to

1012
00:26:43,740 --> 00:26:44,685
0,165 165,360 360,540 540,705 705,945
think about how you can

1013
00:26:44,685 --> 00:26:46,730
0,365 535,855 855,1355 1435,1740 1740,2045
stack these layers as well,|
|

1014
00:26:47,080 --> 00:26:48,660
0,400 750,1055 1055,1220 1220,1370 1370,1580
so since we now have
既然我们现在有了从输入到隐藏输出的转换，

1015
00:26:48,660 --> 00:26:51,050
0,320 370,770 1390,1790 1870,2130 2130,2390
this transformation essentially from our

1016
00:26:51,070 --> 00:26:53,295
0,520 840,1160 1160,1400 1400,1720 1920,2225
inputs to a hidden output,|
|

1017
00:26:53,295 --> 00:26:54,045
0,165 165,315 315,465 465,600 600,750
you can think of this
你可以认为，

1018
00:26:54,045 --> 00:26:56,330
0,180 180,485 1255,1655 1675,1980 1980,2285
as basically| how we can
|我们如何定义一些方法，

1019
00:26:57,350 --> 00:26:59,800
0,400 750,1085 1085,1385 1385,1750 1920,2450
define some way| of transforming
|将这些输入直接转换到某个新的维度空间，

1020
00:26:59,800 --> 00:27:02,260
0,290 340,890 1240,1640 1810,2190 2190,2460
those inputs, right, into some

1021
00:27:02,260 --> 00:27:04,900
0,165 165,720 720,1040 1450,1850 2290,2640
new dimensional space, right,| perhaps
|也许更接近我们想要预测的值，

1022
00:27:04,900 --> 00:27:06,250
0,350 400,720 720,900 900,1125 1125,1350
closer to the value that

1023
00:27:06,250 --> 00:27:07,420
0,120 120,300 300,480 480,740 790,1170
we want to predict,| and,that
|而这种转换最终将被学习，

1024
00:27:07,420 --> 00:27:08,935
0,360 360,740 910,1185 1185,1365 1365,1515
-} transformation is going to

1025
00:27:08,935 --> 00:27:10,795
0,150 150,455 895,1295 1435,1680 1680,1860
be eventually learned| to know
|知道如何将这些输入转换为我们想要的输出，

1026
00:27:10,795 --> 00:27:12,190
0,285 285,600 600,840 840,1080 1080,1395
how to transform those inputs

1027
00:27:12,190 --> 00:27:13,705
0,195 195,420 420,830 850,1305 1305,1515
into our desired outputs,| and
|我们稍后会讲到这一点，

1028
00:27:13,705 --> 00:27:14,490
0,135 135,240 240,345 345,480 480,785
we'll get to {that,later -},|
|

1029
00:27:14,720 --> 00:27:16,150
0,290 290,440 440,700 1020,1280 1280,1430
but for now, the piece
但现在，我真正想要关注的是，

1030
00:27:16,150 --> 00:27:16,930
0,135 135,255 255,435 435,600 600,780
that I want to really

1031
00:27:16,930 --> 00:27:18,085
0,285 285,555 555,795 795,1005 1005,1155
focus on is,| if we
|如果我们有这些更复杂的神经网络，

1032
00:27:18,085 --> 00:27:19,360
0,180 180,390 390,690 690,1005 1005,1275
have these more complex neural

1033
00:27:19,360 --> 00:27:20,860
0,260 670,930 930,1095 1095,1290 1290,1500
networks,| I want to really
|我想真正地提炼出，

1034
00:27:20,860 --> 00:27:22,000
0,450 450,660 660,840 840,975 975,1140
distill down that,| this is
|这并不比我们已经看到的更复杂，

1035
00:27:22,000 --> 00:27:23,230
0,255 255,620 640,945 945,1095 1095,1230
nothing more complex than what

1036
00:27:23,230 --> 00:27:24,340
0,300 300,540 540,795 795,960 960,1110
we've already seen,| if we
|如果我们只关注这张图中的一个神经元，

1037
00:27:24,340 --> 00:27:25,980
0,240 240,570 570,870 870,1140 1140,1640
focus on just one neuron

1038
00:27:26,270 --> 00:27:27,980
0,320 320,620 620,1330
in this diagram,|
|

1039
00:27:28,480 --> 00:27:30,270
0,400 750,1055 1055,1235 1235,1490 1490,1790
take here, for example, {z2
以这里的 z2 为例，

1040
00:27:30,270 --> 00:27:31,935
0,320 820,1140 1140,1320 1320,1470 1470,1665
-},| {z2 -} is this
|z2 是中间层突出显示的神经元，

1041
00:27:31,935 --> 00:27:33,180
0,270 270,480 480,945 945,1095 1095,1245
neuron that's highlighted in the

1042
00:27:33,180 --> 00:27:35,235
0,165 165,470 1300,1665 1665,1845 1845,2055
middle layer,| it's just the
|这就是我们到目前为止在这门课上看到的感知器，

1043
00:27:35,235 --> 00:27:36,495
0,210 210,750 750,900 900,1110 1110,1260
same perceptron that we've been

1044
00:27:36,495 --> 00:27:38,010
0,285 285,555 555,845 985,1275 1275,1515
seeing so far in this

1045
00:27:38,010 --> 00:27:39,705
0,350 460,705 705,950 970,1370 1420,1695
class,| {it,was -}, its output
|它的输出是

1046
00:27:39,705 --> 00:27:40,935
0,135 135,600 600,855 855,1050 1050,1230
is| obtained by taking a
|通过取一个点积，加上一个偏置，然后在所有输入之间应用非线性函数得到的，

1047
00:27:40,935 --> 00:27:42,600
0,195 195,515 775,1095 1095,1260 1260,1665
dot product, adding a bias,

1048
00:27:42,600 --> 00:27:44,295
0,240 240,420 420,660 660,870 870,1695
and then applying that non-linearity

1049
00:27:44,295 --> 00:27:45,380
0,195 195,315 315,420 420,630 630,1085
between all of its inputs,|
|

1050
00:27:46,280 --> 00:27:47,005
0,260 260,380 380,500 500,605 605,725
if we look at a
如果我们看一个不同的节点，

1051
00:27:47,005 --> 00:27:48,400
0,255 255,705 705,870 870,1110 1110,1395
different node,| for example {z3
|例如 z3 ，就在它的正下方，

1052
00:27:48,400 --> 00:27:49,105
0,210 210,360 360,480 480,585 585,705
-}, which is the one

1053
00:27:49,105 --> 00:27:50,440
0,165 165,390 390,725 835,1185 1185,1335
right below it,| it's the
|它是完全相同的故事，

1054
00:27:50,440 --> 00:27:51,595
0,195 195,405 405,645 645,930 930,1155
exact same story,| {again\,,it -}
|同样，它看到的是所有相同的输入，

1055
00:27:51,595 --> 00:27:52,705
0,165 165,375 375,555 555,780 780,1110
sees all the same inputs,|
|

1056
00:27:52,705 --> 00:27:53,470
0,120 120,240 240,390 390,525 525,765
but it has a different
但它有一组不同的权重矩阵，

1057
00:27:53,470 --> 00:27:55,090
0,240 240,360 360,555 555,1220 1360,1620
set of weight matrix,| that
|将应用于这些输入，

1058
00:27:55,090 --> 00:27:56,380
0,165 165,315 315,585 585,950 1030,1290
it's going to apply to

1059
00:27:56,380 --> 00:27:57,310
0,255 255,615 615,720 720,840 840,930
those inputs,| so we'll have
|所以我们会有不同的输出，

1060
00:27:57,310 --> 00:27:58,840
0,120 120,380 430,830 1060,1335 1335,1530
a different output,| but the
|但数学方程是完全相同的。

1061
00:27:58,840 --> 00:28:00,625
0,690 690,990 990,1275 1275,1560 1560,1785
mathematical equations are exactly the

1062
00:28:00,625 --> 00:28:01,440
0,275
same.|
|

1063
00:28:01,540 --> 00:28:02,385
0,275 275,425 425,575 575,695 695,845
So from now on,| I'm
所以从现在开始，|我将简化所有这些线条和图表，

1064
00:28:02,385 --> 00:28:03,180
0,120 120,285 285,510 510,690 690,795
just going to kind of

1065
00:28:03,180 --> 00:28:04,370
0,420 420,570 570,705 705,885 885,1190
simplify all of these lines

1066
00:28:04,390 --> 00:28:06,450
0,380 380,1145 1145,1430 1430,1775 1775,2060
and diagrams| just to show
|只是展示中间的这些图标，

1067
00:28:06,450 --> 00:28:07,610
0,210 210,660 660,810 810,915 915,1160
these icons in the middle,|
|

1068
00:28:07,660 --> 00:28:08,985
0,305 305,610 720,1010 1010,1160 1160,1325
just to demonstrate that these
只是为了证明，这些意味着，

1069
00:28:08,985 --> 00:28:10,095
0,305 565,795 795,900 900,1035 1035,1110
means,| everything is going to
|一切都将完全与一切联系在一起，

1070
00:28:10,095 --> 00:28:11,280
0,90 90,330 330,630 630,945 945,1185
be fully connected to everything|
|

1071
00:28:11,280 --> 00:28:13,170
0,195 195,450 450,660 660,950 1120,1890
and defined by those mathematical
并由我们一直介绍的那些数学公式定义，

1072
00:28:13,170 --> 00:28:14,390
0,320 340,615 615,825 825,945 945,1220
equations that we've been covering,|
|

1073
00:28:14,920 --> 00:28:16,590
0,260 260,485 485,635 635,910 1020,1670
{but,there's -} no extra complexity
但这些模型并没有额外的复杂性，

1074
00:28:16,590 --> 00:28:18,060
0,255 255,480 480,800 1090,1350 1350,1470
in these models| from what
|从你已经看到的情况来看。

1075
00:28:18,060 --> 00:28:19,240
0,225 225,435 435,770
you've already seen.|
|

1076
00:28:19,580 --> 00:28:20,440
0,275 275,410 410,515 515,650 650,860
Now, if you want to
现在，如果你想把这些类型的解决方案叠加在一起，

1077
00:28:20,440 --> 00:28:22,650
0,320 430,750 750,990 990,1310 1810,2210
stack these types of solutions

1078
00:28:22,970 --> 00:28:23,935
0,305 305,485 485,620 620,755 755,965
on top of each other,|
|

1079
00:28:23,935 --> 00:28:25,000
0,240 240,600 600,765 765,930 930,1065
these layers on top of
这些层叠加在一起，

1080
00:28:25,000 --> 00:28:26,365
0,135 135,410 820,1095 1095,1230 1230,1365
each other,| you can not
|你不仅可以非常容易地定义一个层，

1081
00:28:26,365 --> 00:28:27,505
0,240 240,510 510,705 705,915 915,1140
only define one layer very

1082
00:28:27,505 --> 00:28:28,915
0,305 445,705 705,840 840,1110 1110,1410
easily,| but you can actually
|而且你可以创建所谓的顺序模型，

1083
00:28:28,915 --> 00:28:30,370
0,305 325,585 585,705 705,900 900,1455
create what are called sequential

1084
00:28:30,370 --> 00:28:32,290
0,260 310,615 615,1080 1080,1340 1660,1920
models,| {these,sequential -} models, you
|这些顺序模型，你可以定义一个又一个层，

1085
00:28:32,290 --> 00:28:33,460
0,180 180,435 435,675 675,885 885,1170
can define one layer after

1086
00:28:33,460 --> 00:28:35,580
0,380 640,900 900,1160 1300,1700 1720,2120
another| and they define basically
|它们定义了信息的正向传播，

1087
00:28:35,600 --> 00:28:37,270
0,290 290,515 515,1010 1010,1310 1310,1670
the forward propagation of information,|
|

1088
00:28:37,270 --> 00:28:38,920
0,300 300,620 910,1185 1185,1335 1335,1650
not just from the neuron
不仅是从神经元层面，现在是从层层面，

1089
00:28:38,920 --> 00:28:40,120
0,290 430,705 705,885 885,1065 1065,1200
level, but now from the

1090
00:28:40,120 --> 00:28:41,725
0,210 210,540 540,900 900,1280 1330,1605
layer level,| every layer will
|每一层都将完全连接到下一层，

1091
00:28:41,725 --> 00:28:43,150
0,180 180,465 465,845 1075,1320 1320,1425
be fully connected to the

1092
00:28:43,150 --> 00:28:44,695
0,195 195,530 670,945 945,1185 1185,1545
next layer,| and the inputs
|第二层的输入将是上一层的所有输出。

1093
00:28:44,695 --> 00:28:46,210
0,165 165,450 450,765 765,1085 1255,1515
of the secondary layer will

1094
00:28:46,210 --> 00:28:47,440
0,135 135,315 315,495 495,765 765,1230
be all of the outputs

1095
00:28:47,440 --> 00:28:48,740
0,240 240,405 405,630 630,980
of the prior layer.|
|

1096
00:28:50,030 --> 00:28:50,815
0,260 260,380 380,530 530,680 680,785
Now, of course, if you
现在，当然，如果你想创建一个非常深入的神经网络，

1097
00:28:50,815 --> 00:28:51,655
0,105 105,225 225,375 375,555 555,840
want to create a very

1098
00:28:51,655 --> 00:28:53,275
0,315 315,615 615,875 1105,1425 1425,1620
deep neural network,| all the
|所有的深层神经网络就是，

1099
00:28:53,275 --> 00:28:54,430
0,180 180,435 435,645 645,930 930,1155
deep neural network is,| is
|我们只是把这些层堆叠在一起，

1100
00:28:54,430 --> 00:28:55,675
0,165 165,405 405,690 690,1095 1095,1245
we just keep stacking these

1101
00:28:55,675 --> 00:28:56,530
0,285 285,435 435,600 600,735 735,855
layers on top of each

1102
00:28:56,530 --> 00:28:58,015
0,260 520,855 855,1050 1050,1305 1305,1485
other,| there's nothing else to
|这个故事没有什么不同，

1103
00:28:58,015 --> 00:28:59,155
0,150 150,435 435,765 765,945 945,1140
this story,| that's really as
|和之前一样简单，

1104
00:28:59,155 --> 00:29:00,660
0,195 195,390 390,525 525,785 1105,1505
simple as it is,| once,
|所以，这些层全部只是层，

1105
00:29:01,670 --> 00:29:02,730
0,260 260,410 410,680 680,800 800,1060
{so,these -} layers are basically

1106
00:29:02,840 --> 00:29:03,790
0,290 290,440 440,590 590,755 755,950
all they are is just

1107
00:29:03,790 --> 00:29:05,155
0,360 360,510 510,645 645,920 1060,1365
layers,| where the final output
|最终输出被计算，

1108
00:29:05,155 --> 00:29:06,480
0,285 285,815
is computed,

1109
00:29:06,640 --> 00:29:08,550
0,400 720,1040 1040,1310 1310,1640 1640,1910
right,| by going deeper and
|通过深入不同层的过程，

1110
00:29:08,550 --> 00:29:10,200
0,270 270,570 570,795 795,1410 1410,1650
deeper into this progression of

1111
00:29:10,200 --> 00:29:11,700
0,270 270,800 970,1230 1230,1350 1350,1500
different layers,| and you just
|你只是不断地堆叠它们，

1112
00:29:11,700 --> 00:29:12,810
0,180 180,555 555,765 765,975 975,1110
keep stacking them,| until you
|直到你到达最后一层，也就是你的输出层，

1113
00:29:12,810 --> 00:29:13,760
0,150 150,285 285,375 375,585 585,950
get to the last layer,

1114
00:29:14,080 --> 00:29:15,135
0,275 275,425 425,635 635,830 830,1055
which is your output layer,|
|

1115
00:29:15,135 --> 00:29:16,320
0,285 285,390 390,600 600,975 975,1185
it's your final prediction that
这是你想要输出的最终预测，

1116
00:29:16,320 --> 00:29:17,480
0,120 120,270 270,495 495,830
you want to output,

1117
00:29:18,450 --> 00:29:19,370
0,335 335,515 515,650 650,800 800,920
right,| we can create a
|我们可以创建一个深度神经网络来完成这一切，

1118
00:29:19,370 --> 00:29:20,150
0,135 135,330 330,525 525,705 705,780
deep neural network to do

1119
00:29:20,150 --> 00:29:21,610
0,120 120,285 285,510 510,840 840,1460
all of this| by stacking
|通过堆叠这些层并创建这些更具层次感的模型，

1120
00:29:21,750 --> 00:29:23,120
0,290 290,560 560,725 725,1025 1025,1370
these layers and creating these

1121
00:29:23,120 --> 00:29:24,785
0,315 315,1005 1005,1245 1245,1500 1500,1665
more hierarchical models,| like we
|就像我们在今天课程开始时看到的那样，

1122
00:29:24,785 --> 00:29:25,880
0,240 240,510 510,765 765,960 960,1095
saw very early in the

1123
00:29:25,880 --> 00:29:27,905
0,225 225,560 790,1365 1365,1700 1720,2025
beginning of today's lecture,| one
|最终输出被计算是通过，

1124
00:29:27,905 --> 00:29:29,050
0,165 165,300 300,575 625,885 885,1145
where the final output is

1125
00:29:29,190 --> 00:29:30,635
0,395 395,755 755,1030 1050,1295 1295,1445
really computed by,| you know,
|只是越来越深入地进入这个系统。

1126
00:29:30,635 --> 00:29:31,780
0,210 210,450 450,705 705,885 885,1145
just going deeper and deeper

1127
00:29:32,040 --> 00:29:33,640
0,365 365,635 635,940
into this system.|
|

1128
00:29:34,910 --> 00:29:36,415
0,305 305,590 590,1010 1010,1235 1235,1505
Okay, so that's awesome.| {So,we've
好的，这太棒了。|我们现在已经看到了如何从单个神经元到一个层，

1129
00:29:36,415 --> 00:29:37,855
0,345 345,555 555,905 925,1230 1230,1440
-} now seen how we

1130
00:29:37,855 --> 00:29:39,430
0,305 475,840 840,1125 1125,1335 1335,1575
can go from a single

1131
00:29:39,430 --> 00:29:41,305
0,470 640,870 870,990 990,1280 1600,1875
neuron to a layer| to
|再到一个深层神经网络，

1132
00:29:41,305 --> 00:29:41,950
0,165 165,285 285,420 420,555 555,645
all the way to a

1133
00:29:41,950 --> 00:29:43,480
0,150 150,435 435,710 970,1305 1305,1530
deep neural network| building off
|建立在这些基本原理之上。

1134
00:29:43,480 --> 00:29:46,420
0,165 165,440 820,1400 1450,1850
of these foundational principles.|
|

1135
00:29:46,430 --> 00:29:47,305
0,350 350,470 470,575 575,695 695,875
Let's take a look at
让我们来看看我们如何使用这些原理，

1136
00:29:47,305 --> 00:29:49,200
0,270 270,600 600,825 825,1085 1495,1895
how exactly we can use

1137
00:29:49,460 --> 00:29:52,345
0,400 1110,1355 1355,1600 1950,2350 2610,2885
these, you know, principles,| that
|我们刚刚讨论的，

1138
00:29:52,345 --> 00:29:53,920
0,210 210,455 475,875 1105,1380 1380,1575
we've just discussed| to solve
|来解决一个非常现实的问题，

1139
00:29:53,920 --> 00:29:55,435
0,180 180,390 390,675 675,1010 1270,1515
a very real problem,| that
|我想你们今天早上醒来时可能都非常关注这个问题。

1140
00:29:55,435 --> 00:29:56,125
0,120 120,270 270,405 405,525 525,690
I think all of you

1141
00:29:56,125 --> 00:29:57,630
0,195 195,435 435,785 835,1170 1170,1505
are probably very concerned about

1142
00:29:58,670 --> 00:30:00,310
0,305 305,575 575,830 830,1120 1380,1640
this morning when you, when

1143
00:30:00,310 --> 00:30:01,525
0,260 310,600 600,840 840,1050 1050,1215
you woke up.| {So,that -}
|所以问题是，

1144
00:30:01,525 --> 00:30:03,550
0,270 270,635 1045,1445 1465,1755 1755,2025
problem is,| how we can
|我们如何建立一个神经网络来回答这个问题，

1145
00:30:03,550 --> 00:30:04,765
0,270 270,420 420,630 630,890 970,1215
build a neural network to

1146
00:30:04,765 --> 00:30:05,935
0,195 195,435 435,705 705,945 945,1170
answer this question,| which is
|我将如何通过这门课，

1147
00:30:05,935 --> 00:30:07,345
0,365 415,810 810,1065 1065,1215 1215,1410
{will,I}, how will I pass

1148
00:30:07,345 --> 00:30:08,410
0,210 210,450 450,720 720,915 915,1065
this class,| and if I
|我能不能通过。

1149
00:30:08,410 --> 00:30:10,500
0,285 285,540 540,675 675,950
will, will I not.|
|

1150
00:30:10,500 --> 00:30:12,170
0,380 640,885 885,1095 1095,1365 1365,1670
So to answer this question,|
所以，为了回答这个问题，|

1151
00:30:12,220 --> 00:30:12,915
0,335 335,395 395,470 470,575 575,695
let's see if we can
让我们看看是否可以训练一个神经网络来解决这个问题。

1152
00:30:12,915 --> 00:30:13,785
0,120 120,225 225,435 435,645 645,870
train a neural network to

1153
00:30:13,785 --> 00:30:16,240
0,275 445,765 765,1085
solve this problem.|
|

1154
00:30:16,240 --> 00:30:17,305
0,330 330,525 525,630 630,795 795,1065
So to do this,| let's
要做这个，|让我们从一个非常简单的神经网络开始，

1155
00:30:17,305 --> 00:30:18,310
0,150 150,270 270,405 405,660 660,1005
start with a very simple

1156
00:30:18,310 --> 00:30:20,575
0,360 360,620 1450,1845 1845,2055 2055,2265
neural network,| we'll train this
|我们将用两个输入训练这个模型，

1157
00:30:20,575 --> 00:30:21,955
0,285 285,585 585,840 840,1185 1185,1380
model with two inputs,| just
|只有两个输入，

1158
00:30:21,955 --> 00:30:23,125
0,225 225,510 510,765 765,1020 1020,1170
two inputs,| {one,input -} is
|一个输入是你在这一周中参加的讲座的数量，

1159
00:30:23,125 --> 00:30:23,740
0,150 150,255 255,345 345,465 465,615
going to be the number

1160
00:30:23,740 --> 00:30:25,500
0,180 180,710 940,1215 1215,1425 1425,1760
of lectures that you attend,

1161
00:30:25,580 --> 00:30:26,530
0,305 305,470 470,635 635,800 800,950
over the course of this

1162
00:30:26,530 --> 00:30:28,020
0,195 195,500 820,1080 1080,1215 1215,1490
one week,| and the second
|第二个输入将是你在期末项目或竞争中花费了多少小时。

1163
00:30:28,160 --> 00:30:29,190
0,275 275,440 440,635 635,785 785,1030
input is going to be

1164
00:30:29,450 --> 00:30:30,745
0,275 275,440 440,730 870,1130 1130,1295
how many hours that you

1165
00:30:30,745 --> 00:30:31,765
0,195 195,345 345,480 480,690 690,1020
spend on your final project

1166
00:30:31,765 --> 00:30:33,500
0,255 255,515 685,1085
or your competition.|
|

1167
00:30:34,480 --> 00:30:35,925
0,335 335,670 930,1190 1190,1340 1340,1445
Okay, so what we're going
好的，所以我们要做的是，

1168
00:30:35,925 --> 00:30:37,080
0,120 120,210 210,345 345,875 895,1155
to do is,| firstly go
|首先去收集大量数据，

1169
00:30:37,080 --> 00:30:37,890
0,165 165,375 375,570 570,705 705,810
out and collect a lot

1170
00:30:37,890 --> 00:30:38,640
0,120 120,330 330,540 540,645 645,750
of data| from all of
|从过去几年我们教授这门课程中，

1171
00:30:38,640 --> 00:30:39,615
0,135 135,345 345,585 585,765 765,975
the past years that we've

1172
00:30:39,615 --> 00:30:40,950
0,150 150,390 390,725 985,1230 1230,1335
taught this course,| {and,we -}
|我们可以绘制所有这些数据，

1173
00:30:40,950 --> 00:30:41,760
0,165 165,375 375,540 540,660 660,810
can plot all of this

1174
00:30:41,760 --> 00:30:43,020
0,290 370,630 630,810 810,975 975,1260
data,| because it's only two
|因为它只有两个输入空间，

1175
00:30:43,020 --> 00:30:44,040
0,270 270,540 540,735 735,855 855,1020
input space,| we can plot
|我们可以将这些数据绘制在二维特征空间上，

1176
00:30:44,040 --> 00:30:44,805
0,165 165,360 360,525 525,645 645,765
this data on a two

1177
00:30:44,805 --> 00:30:47,300
0,605 745,1145 1285,1685 1975,2235 2235,2495
dimensional {feature,space -},| we can
|我们可以看看你们前面所有学生，

1178
00:30:47,560 --> 00:30:48,435
0,275 275,410 410,530 530,695 695,875
actually look at all of

1179
00:30:48,435 --> 00:30:49,755
0,165 165,455 475,840 840,1125 1125,1320
the students before you,| that
|通过课程和不及格的，

1180
00:30:49,755 --> 00:30:51,285
0,275 445,845 865,1125 1125,1320 1320,1530
have passed the class and

1181
00:30:51,285 --> 00:30:52,860
0,195 195,375 375,615 615,995 1285,1575
failed the class| and see
|看看他们在这个空间里在哪里，

1182
00:30:52,860 --> 00:30:54,255
0,210 210,435 435,740 850,1155 1155,1395
where they lived in this

1183
00:30:54,255 --> 00:30:55,350
0,335 445,705 705,825 825,975 975,1095
space| for the amount of
|他们花了多少个小时，

1184
00:30:55,350 --> 00:30:56,295
0,165 165,345 345,585 585,780 780,945
hours that they've spent,| the
|听了多少节课，

1185
00:30:56,295 --> 00:30:57,300
0,135 135,285 285,600 600,750 750,1005
number of lectures {that,they've -}

1186
00:30:57,300 --> 00:30:59,025
0,225 225,465 465,645 645,920 1180,1725
attended,| and so on,| {green,point}
|诸若此类，|绿点是通过的人，红点是不及格的，

1187
00:30:59,025 --> 00:30:59,910
0,165 165,270 270,510 510,735 735,885
are the people who have

1188
00:30:59,910 --> 00:31:01,065
0,300 300,570 570,735 735,960 960,1155
passed, red are those who

1189
00:31:01,065 --> 00:31:02,060
0,120 120,395
have failed,|
|

1190
00:31:02,180 --> 00:31:04,615
0,400 1110,1415 1415,1700 1700,1960 2040,2435
now, and here's you, right,|
现在，这是你，|

1191
00:31:04,615 --> 00:31:05,785
0,360 360,525 525,810 810,1035 1035,1170
you're right here,| {4 -}
你在这里，|4 5 是你的坐标空间，

1192
00:31:05,785 --> 00:31:07,200
0,225 225,465 465,675 675,1095 1095,1415
5 is your coordinate space,|
|

1193
00:31:07,910 --> 00:31:09,355
0,305 305,610 690,1025 1025,1265 1265,1445
you fall right there| and
你在那里，|你已经听了四节课，

1194
00:31:09,355 --> 00:31:10,750
0,255 255,495 495,750 750,1140 1140,1395
you've attended four lectures,| you've
|你已经在期末作业上花了五个小时，

1195
00:31:10,750 --> 00:31:11,770
0,195 195,405 405,660 660,885 885,1020
spent five hours on your

1196
00:31:11,770 --> 00:31:13,360
0,195 195,530 1060,1320 1320,1455 1455,1590
{final,project -},| we want to
|我们想建立一个神经网络来回答这样一个问题，

1197
00:31:13,360 --> 00:31:14,290
0,135 135,270 270,450 450,705 705,930
build a neural network to

1198
00:31:14,290 --> 00:31:15,745
0,180 180,390 390,645 645,1040 1150,1455
answer the question| of will
|你会通过这门课还是不及格。

1199
00:31:15,745 --> 00:31:16,765
0,210 210,435 435,615 615,810 810,1020
you pass the class or

1200
00:31:16,765 --> 00:31:17,640
0,135 135,270 270,435 435,600 600,875
will you fail the class.|
|

1201
00:31:18,410 --> 00:31:19,600
0,365 365,725 725,845 845,1010 1010,1190
So {let's,do -} it,| {we,have
那我们开始吧，|我们有两个输入，

1202
00:31:19,600 --> 00:31:20,845
0,225 225,525 525,900 900,1080 1080,1245
-} two inputs,| one is
|一个是 4 ，一个是 5 ，

1203
00:31:20,845 --> 00:31:22,330
0,300 300,570 570,750 750,1055 1225,1485
four, one is five,| these
|这是两个数字，

1204
00:31:22,330 --> 00:31:23,230
0,135 135,285 285,540 540,765 765,900
are two numbers,| we can
|我们可以把它们给神经网络，

1205
00:31:23,230 --> 00:31:24,445
0,180 180,390 390,705 705,975 975,1215
feed them through a neural

1206
00:31:24,445 --> 00:31:25,720
0,255 255,510 510,720 720,965 985,1275
network,| that we've just seen
|我们刚刚看到的如何构建的（神经网络），

1207
00:31:25,720 --> 00:31:26,760
0,180 180,345 345,495 495,705 705,1040
how we can build that,|
|

1208
00:31:27,540 --> 00:31:28,290
0,105 105,240 240,420 420,570 570,750
and we feed that into
我们把它输入到一个单层神经网络中，

1209
00:31:28,290 --> 00:31:29,690
0,225 225,480 480,870 870,1140 1140,1400
a single layered neural network,|
|

1210
00:31:30,430 --> 00:31:31,590
0,305 305,515 515,770 770,980 980,1160
three hidden units in this
在这个例子中是三个隐藏的单元，

1211
00:31:31,590 --> 00:31:32,460
0,270 270,480 480,600 600,735 735,870
example,| {but,we -} could make
|但我们可以把它做得更大，

1212
00:31:32,460 --> 00:31:33,480
0,195 195,510 510,735 735,840 840,1020
it larger,| if we wanted
|如果我们想要更有表现力和更强大，

1213
00:31:33,480 --> 00:31:34,695
0,165 165,300 300,555 555,1020 1020,1215
to be more expressive and

1214
00:31:34,695 --> 00:31:36,140
0,165 165,455
more powerful,|
|

1215
00:31:36,150 --> 00:31:37,070
0,275 275,440 440,605 605,770 770,920
and we see here that,|
我们在这里看到，|

1216
00:31:37,070 --> 00:31:38,120
0,135 135,510 510,690 690,825 825,1050
the probability of you passing
你通过这些课程的概率是 0.1 ，

1217
00:31:38,120 --> 00:31:39,520
0,270 270,510 510,795 795,1080 1080,1400
those classes is {0.1 -},|
|

1218
00:31:40,240 --> 00:31:42,595
0,255 255,920 1360,1760 1900,2190 2190,2355
pretty abysmal.| {So,why -} would
相当糟糕。|那么为什么会出现这种情况，

1219
00:31:42,595 --> 00:31:43,710
0,150 150,270 270,390 390,665 715,1115
this be the case, right,|
|

1220
00:31:43,790 --> 00:31:44,760
0,260 260,380 380,515 515,680 680,970
what did we do wrong,|
我们做错了什么，|

1221
00:31:44,960 --> 00:31:45,960
0,245 245,350 350,545 545,635 635,1000
because I don't think it's
因为我不认为这是正确的，

1222
00:31:45,980 --> 00:31:47,515
0,380 380,760 900,1175 1175,1355 1355,1535
{correct\,,right -},| when we looked
|当我们看这个空间的时候，

1223
00:31:47,515 --> 00:31:49,165
0,165 165,375 375,695 1255,1515 1515,1650
at the space,| it looked
|看起来你是通过这门课的很好的候选人，

1224
00:31:49,165 --> 00:31:50,110
0,255 255,495 495,615 615,720 720,945
like actually you were a

1225
00:31:50,110 --> 00:31:51,325
0,285 285,585 585,855 855,1035 1035,1215
good candidate to pass {the,class

1226
00:31:51,325 --> 00:31:52,270
0,240 240,465 465,615 615,795 795,945
-},| but why is the
|但是为什么神经网络说你通过考试的可能性只有 10% 呢，

1227
00:31:52,270 --> 00:31:54,040
0,195 195,435 435,830 1330,1605 1605,1770
neural network saying that {there's,only

1228
00:31:54,040 --> 00:31:55,420
0,105 105,270 270,675 675,1230 1230,1380
-} a 10% likelihood that

1229
00:31:55,420 --> 00:31:56,770
0,135 135,285 285,560 910,1200 1200,1350
you should pass,| does anyone
|有谁有什么想法吗？

1230
00:31:56,770 --> 00:31:57,940
0,105 105,285 285,620
have any ideas?|
|

1231
00:32:02,680 --> 00:32:05,520
0,400 1260,1625 1625,1990
Exactly, exactly,| so,
完全正确，|所以，这个神经网络它刚刚诞生，

1232
00:32:05,960 --> 00:32:07,350
0,290 290,515 515,710 710,1025 1025,1390
this neural network is just,

1233
00:32:07,760 --> 00:32:08,730
0,275 275,395 395,500 500,665 665,970
like it was just born,

1234
00:32:08,840 --> 00:32:10,105
0,320 320,500 500,665 665,935 935,1265
right,| {it,has -} no information
|它没有关于这个世界或这个课程的信息，

1235
00:32:10,105 --> 00:32:12,175
0,240 240,515 895,1290 1290,1685 1735,2070
about the world or this

1236
00:32:12,175 --> 00:32:13,315
0,255 255,435 435,690 690,885 885,1140
class,| it doesn't know what
|它不知道 4 和 5 是什么意思，

1237
00:32:13,315 --> 00:32:14,880
0,210 210,360 360,635 685,1085 1165,1565
4 and 5 mean,| or
|也不知道及格或不及格的概念是什么意思。

1238
00:32:15,290 --> 00:32:16,450
0,305 305,470 470,680 680,920 920,1160
what the notion of passing

1239
00:32:16,450 --> 00:32:19,360
0,210 210,540 540,830 1360,1760
or failing means, right.|
|

1240
00:32:19,750 --> 00:32:20,940
0,350 350,605 605,800 800,1010 1010,1190
Exactly right,| {this,neural -} network
完全正确，|这个神经网络还没有经过训练，

1241
00:32:20,940 --> 00:32:22,110
0,225 225,420 420,645 645,945 945,1170
has not been trained,| you
|你可以把它想象成一个婴儿，

1242
00:32:22,110 --> 00:32:22,845
0,135 135,285 285,420 420,570 570,735
can think of it kind

1243
00:32:22,845 --> 00:32:24,195
0,180 180,450 450,675 675,935 1075,1350
of as {a,baby -},| it
|它还没有学到任何东西，

1244
00:32:24,195 --> 00:32:25,875
0,330 330,635 895,1215 1215,1455 1455,1680
hasn't {learned,anything -} yet,| so
|所以，我们的首要工作是训练它，

1245
00:32:25,875 --> 00:32:28,050
0,240 240,540 540,1175 1375,1775 1885,2175
our job firstly {is,to -}

1246
00:32:28,050 --> 00:32:29,025
0,180 180,360 360,585 585,810 810,975
train it,| and part of
|这种理解的一部分是，

1247
00:32:29,025 --> 00:32:30,225
0,275 355,645 645,825 825,1005 1005,1200
that understanding is| we first
|当神经网络出错时，我们首先需要告诉它，

1248
00:32:30,225 --> 00:32:31,200
0,165 165,330 330,570 570,765 765,975
need to tell the neural

1249
00:32:31,200 --> 00:32:32,630
0,260 280,540 540,660 660,920 1030,1430
{network\,,when -} it makes mistakes,|
|

1250
00:32:33,550 --> 00:32:34,995
0,260 260,790 900,1160 1160,1280 1280,1445
so mathematically we should now
所以，从数学上讲，我们现在应该考虑如何回答这个问题，

1251
00:32:34,995 --> 00:32:36,075
0,240 240,540 540,810 810,975 975,1080
think about how we can

1252
00:32:36,075 --> 00:32:37,340
0,195 195,450 450,735 735,990 990,1265
answer this question,| which is,
|即我的神经网络是否犯了错误，

1253
00:32:38,350 --> 00:32:39,480
0,290 290,455 455,665 665,890 890,1130
did my neural network make

1254
00:32:39,480 --> 00:32:40,680
0,225 225,590 640,915 915,1065 1065,1200
a mistake,| and if it
|如果它犯了一个错误，

1255
00:32:40,680 --> 00:32:41,955
0,120 120,315 315,650 820,1125 1125,1275
made a mistake,| how can
|我如何告诉它这个错误有多大，

1256
00:32:41,955 --> 00:32:42,705
0,135 135,285 285,435 435,615 615,750
I tell it how big

1257
00:32:42,705 --> 00:32:43,500
0,90 90,270 270,480 480,615 615,795
of a mistake it was,|
|

1258
00:32:43,500 --> 00:32:44,340
0,180 180,315 315,465 465,660 660,840
so that the next time
以便它下一次看到这个数据点时，

1259
00:32:44,340 --> 00:32:46,130
0,165 165,470 880,1200 1200,1455 1455,1790
it sees this data point,|
|

1260
00:32:46,480 --> 00:32:48,060
0,260 260,395 395,560 560,850 1020,1580
can it do better, minimize
可以做得更好，将错误降到最低。

1261
00:32:48,060 --> 00:32:49,240
0,345 345,740
that mistake.|
|

1262
00:32:49,490 --> 00:32:51,240
0,400 660,935 935,1160 1160,1385 1385,1750
So in neural network language,|
所以，在神经网络语言中，|

1263
00:32:51,710 --> 00:32:53,490
0,400 480,860 860,1130 1130,1400 1400,1780
those mistakes are called losses,|
这些错误被称为损失，|

1264
00:32:54,380 --> 00:32:55,600
0,305 305,610 630,875 875,1010 1010,1220
and specifically you want to
具体来说，你想定义所谓的损失函数，

1265
00:32:55,600 --> 00:32:56,635
0,225 225,480 480,630 630,795 795,1035
define what's called a loss

1266
00:32:56,635 --> 00:32:58,195
0,365 805,1080 1080,1230 1230,1395 1395,1560
function,| which is going to
|它将把你的预测和真实的预测作为输入，

1267
00:32:58,195 --> 00:33:00,360
0,180 180,485 505,905 1405,1725 1725,2165
take as input your prediction

1268
00:33:01,040 --> 00:33:02,800
0,395 395,665 665,845 845,1240 1440,1760
and the true prediction, right,|
|

1269
00:33:02,800 --> 00:33:03,850
0,210 210,420 420,660 660,855 855,1050
and how far away your
你的预测与真实预测的距离，

1270
00:33:03,850 --> 00:33:05,605
0,375 375,740 1120,1410 1410,1590 1590,1755
prediction is from the true

1271
00:33:05,605 --> 00:33:06,880
0,365 445,765 765,960 960,1125 1125,1275
prediction| tells you how big
|告诉你有多大的损失，

1272
00:33:06,880 --> 00:33:08,340
0,105 105,225 225,500 880,1170 1170,1460
of a loss there is,

1273
00:33:08,980 --> 00:33:10,800
0,225 225,450 450,645 645,950
right,| {so\,,for -} example,|
|比如，|

1274
00:33:11,190 --> 00:33:12,425
0,365 365,560 560,785 785,995 995,1235
let's say we want to
比如，我们想要建立一个神经网络来进行分类，

1275
00:33:12,425 --> 00:33:14,585
0,305 745,1145 1345,1695 1695,1935 1935,2160
build a neural network to

1276
00:33:14,585 --> 00:33:17,080
0,240 240,845 1255,1655
do classification of,|
|

1277
00:33:17,120 --> 00:33:18,355
0,290 290,575 575,815 815,965 965,1235
or, sorry, actually, even before
抱歉，在此之前，我想给你们一些术语，

1278
00:33:18,355 --> 00:33:19,450
0,285 285,465 465,585 585,795 795,1095
that, I want to maybe

1279
00:33:19,450 --> 00:33:21,090
0,240 240,405 405,630 630,1335 1335,1640
give you some terminology.| {So,there
|有多种不同的方式来表达相同的事情，

1280
00:33:21,200 --> 00:33:22,660
0,245 245,490 510,905 905,1220 1220,1460
-} are multiple different ways

1281
00:33:22,660 --> 00:33:23,760
0,210 210,405 405,585 585,780 780,1100
of saying the same thing,|
|

1282
00:33:24,230 --> 00:33:25,840
0,400 630,980 980,1220 1220,1460 1460,1610
in neural networks and deep
在神经网络和深度学习中，

1283
00:33:25,840 --> 00:33:27,415
0,290 310,710 940,1185 1185,1320 1320,1575
learning,| so what I just
|所以我刚才描述的损失函数，

1284
00:33:27,415 --> 00:33:28,530
0,255 255,420 420,555 555,765 765,1115
described as a loss function|
|

1285
00:33:28,640 --> 00:33:30,160
0,395 395,695 695,1000 1110,1400 1400,1520
is also commonly referred to
也通常被称为目标函数，经验风险，成本函数，

1286
00:33:30,160 --> 00:33:32,140
0,90 90,255 255,525 525,890 1270,1980
as an objective function, empirical

1287
00:33:32,140 --> 00:33:33,580
0,285 285,585 585,825 825,1160 1180,1440
risk, a {cost,function -},| these
|这些都是完全一样的东西，

1288
00:33:33,580 --> 00:33:35,065
0,135 135,410 700,1050 1050,1275 1275,1485
are all exactly the same

1289
00:33:35,065 --> 00:33:36,265
0,335 415,720 720,870 870,1020 1020,1200
thing,| they're all a way
|它们都是我们训练神经网络的一种方式，

1290
00:33:36,265 --> 00:33:37,315
0,165 165,300 300,570 570,870 870,1050
for us to train the

1291
00:33:37,315 --> 00:33:38,410
0,195 195,435 435,705 705,915 915,1095
neural network,| to teach the
|在神经网络出错时教它，

1292
00:33:38,410 --> 00:33:39,780
0,210 210,470 730,990 990,1110 1110,1370
neural network when {it,makes -}

1293
00:33:39,890 --> 00:33:42,355
0,400 1050,1450 1680,1955 1955,2150 2150,2465
mistakes,| and what we really
|我们最终真正想要做的是，

1294
00:33:42,355 --> 00:33:43,680
0,330 330,555 555,705 705,945 945,1325
ultimately want to do is|
|

1295
00:33:43,760 --> 00:33:44,845
0,335 335,560 560,740 740,875 875,1085
over the course of an
在整个数据集的过程中，

1296
00:33:44,845 --> 00:33:46,645
0,330 330,660 660,1025 1315,1605 1605,1800
entire data set,| not just
|而不仅是一个错误数据点，

1297
00:33:46,645 --> 00:33:48,115
0,225 225,495 495,795 795,1125 1125,1470
one data point of mistakes,|
|

1298
00:33:48,115 --> 00:33:48,850
0,225 225,345 345,435 435,540 540,735
we want to say over
我们想要在整个数据集上，

1299
00:33:48,850 --> 00:33:50,710
0,255 255,525 525,810 810,1160 1600,1860
the entire data set,| we
|我们想要将这个神经网络犯下的所有平均错误降到最低。

1300
00:33:50,710 --> 00:33:51,985
0,150 150,315 315,830 850,1125 1125,1275
want to minimize all of

1301
00:33:51,985 --> 00:33:53,755
0,210 210,525 525,825 825,1145 1465,1770
the mistakes on average that

1302
00:33:53,755 --> 00:33:55,800
0,210 210,465 465,725 985,1385
this neural network makes.|
|

1303
00:33:56,730 --> 00:33:57,455
0,260 260,380 380,500 500,620 620,725
So if we look at
所以，如果我们看二元分类问题，

1304
00:33:57,455 --> 00:33:58,460
0,105 105,360 360,630 630,795 795,1005
the problem, like I said,

1305
00:33:58,460 --> 00:34:00,365
0,225 225,630 630,1190 1480,1740 1740,1905
of binary classification,| will I
|我会通过这门课吗，

1306
00:34:00,365 --> 00:34:01,445
0,225 225,435 435,705 705,945 945,1080
pass this class or will

1307
00:34:01,445 --> 00:34:02,375
0,150 150,345 345,585 585,690 690,930
I not,| there's a yes
|这里有一个是或不是的答案，

1308
00:34:02,375 --> 00:34:03,680
0,285 285,570 570,885 885,1110 1110,1305
or no answer,| {that,means -}
|这意味着二元分类，

1309
00:34:03,680 --> 00:34:05,740
0,405 405,920
binary classification,|
|

1310
00:34:05,740 --> 00:34:07,210
0,320 610,885 885,1020 1020,1185 1185,1470
now we can use what's
现在我们可以使用所谓的 softmax 交叉熵损失的损失函数，

1311
00:34:07,210 --> 00:34:08,800
0,260 340,615 615,810 810,1130 1240,1590
called a loss function of

1312
00:34:08,800 --> 00:34:10,795
0,240 240,765 765,1035 1035,1605 1605,1995
the softmax cross entropy loss,|
|

1313
00:34:10,795 --> 00:34:11,740
0,330 330,525 525,660 660,765 765,945
{and,for -} those of you
对于你们中不熟悉的人来说，

1314
00:34:11,740 --> 00:34:13,135
0,195 195,420 420,660 660,1035 1035,1395
who aren't familiar,| this notion
|交叉熵的概念是在 MIT 开发的，

1315
00:34:13,135 --> 00:34:14,860
0,365 415,705 705,1140 1140,1365 1365,1725
of cross entropy is actually

1316
00:34:14,860 --> 00:34:16,555
0,380 460,735 735,930 930,1250 1360,1695
developed here at MIT| by
|由 Claude Shannon ，

1317
00:34:16,555 --> 00:34:21,850
0,365 2215,2505 2505,2795 3085,3485 4825,5295
Shannon, excuse me, yes, Claude

1318
00:34:21,850 --> 00:34:25,195
0,530 1030,1290 1290,1550 2380,3050 3100,3345
Shannon,| who {is,visionary -},| he
|他很有远见，|五十多年前，他在这里攻读硕士学位，

1319
00:34:25,195 --> 00:34:27,190
0,90 90,225 225,785 835,1235 1645,1995
did his masters here over

1320
00:34:27,190 --> 00:34:28,465
0,285 285,570 570,825 825,1065 1065,1275
{fifty,years -} ago,| he introduced
|他引入了交叉熵的概念，

1321
00:34:28,465 --> 00:34:29,935
0,180 180,465 465,795 795,1035 1035,1470
this notion of cross entropy,|
|

1322
00:34:29,935 --> 00:34:31,700
0,120 120,300 300,605
and that was,
这就是我们有能力训练这些类型的神经网络的关键，

1323
00:34:31,700 --> 00:34:33,620
0,165 165,410 430,1035 1035,1400 1630,1920
you know, pivotal and the

1324
00:34:33,620 --> 00:34:35,090
0,285 285,510 510,675 675,1010 1180,1470
ability for us to train

1325
00:34:35,090 --> 00:34:36,130
0,180 180,375 375,555 555,780 780,1040
these types of neural networks,|
|

1326
00:34:36,420 --> 00:34:37,720
0,320 320,590 590,860 860,1040 1040,1300
even now into the future.|
即使是现在也是如此。|

1327
00:34:38,560 --> 00:34:40,945
0,380 670,1050 1050,1185 1185,1460 2110,2385
So let's start by,| instead
所以，让我们开始，|不是预测二元交叉熵输出，

1328
00:34:40,945 --> 00:34:43,270
0,275 415,1115 1255,1590 1590,2100 2100,2325
of predicting a binary cross

1329
00:34:43,270 --> 00:34:44,530
0,540 540,855 855,1035 1035,1140 1140,1260
entropy output,| {what,if -} we
|例如，如果我们想要预测你班级成绩的最终分数，

1330
00:34:44,530 --> 00:34:46,615
0,195 195,420 420,710 1210,1610 1690,2085
wanted to predict a final

1331
00:34:46,615 --> 00:34:48,685
0,395 565,870 870,1095 1095,1415 1705,2070
grade of your class score,

1332
00:34:48,685 --> 00:34:49,750
0,255 255,465 465,735 735,870 870,1065
for example,| that's no longer
|这不再是一个二元输出，是或否，

1333
00:34:49,750 --> 00:34:51,025
0,195 195,675 675,945 945,1110 1110,1275
a binary output, yes or

1334
00:34:51,025 --> 00:34:52,645
0,275 415,855 855,1050 1050,1275 1275,1620
no,| it's actually a continuous
|它是一个连续变量，

1335
00:34:52,645 --> 00:34:54,085
0,365 505,780 780,1005 1005,1155 1155,1440
variable, right,| it's the grade,
|这是分数，满分是 100 分，

1336
00:34:54,085 --> 00:34:55,410
0,315 315,390 390,480 480,570 570,1325
let's say out {of,100} points,|
|

1337
00:34:55,790 --> 00:34:57,115
0,260 260,455 455,785 785,1130 1130,1325
what is the value of
你在班级项目中的分数是多少，

1338
00:34:57,115 --> 00:34:58,410
0,180 180,405 405,570 570,845 895,1295
your score in the class

1339
00:34:58,520 --> 00:35:00,020
0,400
project,x|
|

1340
00:35:00,020 --> 00:35:00,920
0,240 240,390 390,555 555,705 705,900
for this type of loss,
对于这种损失，我们可以使用所谓的均方误差损失，

1341
00:35:00,920 --> 00:35:01,745
0,180 180,300 300,465 465,705 705,825
we can use what's called

1342
00:35:01,745 --> 00:35:02,960
0,150 150,375 375,660 660,915 915,1215
a mean squared error loss,|
|

1343
00:35:02,960 --> 00:35:03,710
0,195 195,315 315,435 435,555 555,750
{you,can -} think of this
你可以从字面上认为，

1344
00:35:03,710 --> 00:35:05,495
0,270 270,510 510,690 690,1395 1395,1785
literally as| just subtracting your
|这只是从真实分数中减去你的预测分数，

1345
00:35:05,495 --> 00:35:07,175
0,555 555,845 925,1245 1245,1470 1470,1680
predicted grade from the true

1346
00:35:07,175 --> 00:35:09,460
0,305 655,1055 1135,1695 1695,1980 1980,2285
grade| and minimizing that distance
|并将两者之间的距离最小化。

1347
00:35:09,840 --> 00:35:10,700
0,400
apart.|
|

1348
00:35:12,450 --> 00:35:13,775
0,365 365,605 605,770 770,1025 1025,1325
So I think now we're
所以，我认为现在我们已经准备好将所有这些信息整合在一起，

1349
00:35:13,775 --> 00:35:14,915
0,245 355,600 600,795 795,1005 1005,1140
ready to really put all

1350
00:35:14,915 --> 00:35:16,900
0,135 135,390 390,785 835,1235 1585,1985
of this information together| and
|解决这个训练神经网络，

1351
00:35:17,220 --> 00:35:19,010
0,485 485,725 725,1060 1110,1475 1475,1790
tackle this problem of training

1352
00:35:19,010 --> 00:35:21,620
0,210 210,450 450,710 1750,2150 2290,2610
a neural network| to not
|不仅仅是识别它是多么错误，

1353
00:35:21,620 --> 00:35:23,560
0,320 850,1250
just identify

1354
00:35:23,630 --> 00:35:25,585
0,400 720,1340 1340,1475 1475,1685 1685,1955
how erroneous it is,| how
|它的损失有多大，

1355
00:35:25,585 --> 00:35:27,235
0,240 240,510 510,795 795,1145 1375,1650
large its loss is,| but
|但更重要的是，将这种损失降至最低，

1356
00:35:27,235 --> 00:35:29,155
0,195 195,515 625,1235 1315,1635 1635,1920
more importantly, minimize that loss,|
|

1357
00:35:29,155 --> 00:35:30,220
0,225 225,360 360,585 585,840 840,1065
as a function of seeing
通过查看它观察到的所有这些训练数据。

1358
00:35:30,220 --> 00:35:31,375
0,195 195,360 360,585 585,855 855,1155
all of this training data

1359
00:35:31,375 --> 00:35:33,080
0,210 210,405 405,905
that it observes.|
|

1360
00:35:33,690 --> 00:35:34,670
0,350 350,575 575,725 725,860 860,980
So we know that we
所以我们想要找到这个神经网络，

1361
00:35:34,670 --> 00:35:35,720
0,135 135,300 300,540 540,795 795,1050
want to find this neural

1362
00:35:35,720 --> 00:35:37,115
0,260 430,720 720,870 870,1080 1080,1395
network,| like we mentioned before,|
|就像我们之前提到的那样，|

1363
00:35:37,115 --> 00:35:40,150
0,315 315,875 1345,1650 1650,2495 2635,3035
that minimizes this empirical risk
它可以最小化整个数据集的经验风险或经验损失的平均值，

1364
00:35:40,200 --> 00:35:42,350
0,275 275,410 410,995 995,1270 1500,2150
or this empirical loss averaged

1365
00:35:42,350 --> 00:35:44,020
0,300 300,600 600,975 975,1320 1320,1670
across our entire data set,|
|

1366
00:35:44,400 --> 00:35:45,335
0,290 290,470 470,650 650,800 800,935
{now,this -} means that we
这意味着我们想要在数学上找到这些 w ，

1367
00:35:45,335 --> 00:35:47,680
0,195 195,510 510,905 1135,1805 1945,2345
want to find mathematically these

1368
00:35:48,090 --> 00:35:50,975
0,790 810,1210 1320,1670 1670,2230 2580,2885
ws, right,| that minimize {J(W)
|最小化 J(W) ，

1369
00:35:50,975 --> 00:35:52,175
0,180 180,455 475,735 735,855 855,1200
- -},| {J(W) - -}
|J(W) 是损失函数平均在整个数据集上，

1370
00:35:52,175 --> 00:35:53,735
0,135 135,345 345,665 835,1365 1365,1560
is loss function averaged over

1371
00:35:53,735 --> 00:35:55,040
0,225 225,480 480,750 750,1035 1035,1305
our entire data set,| and
|W 是我们的权重，

1372
00:35:55,040 --> 00:35:56,360
0,320 460,735 735,900 900,1125 1125,1320
W is {our,weight -},| so
|所以我们想要找到一组权重，

1373
00:35:56,360 --> 00:35:57,125
0,105 105,225 225,375 375,555 555,765
we want to find the

1374
00:35:57,125 --> 00:35:59,150
0,180 180,360 360,845 1315,1715 1735,2025
set of weights,| that on
|平均而言，它会给我们最小的损失，

1375
00:35:59,150 --> 00:36:00,455
0,290 550,840 840,1020 1020,1170 1170,1305
average is going to give

1376
00:36:00,455 --> 00:36:02,340
0,225 225,495 495,815
us the minimum,

1377
00:36:02,650 --> 00:36:05,220
0,560 560,940 1080,1400 1400,1720
smallest loss as possible,|
|

1378
00:36:05,620 --> 00:36:07,035
0,365 365,665 665,890 890,1160 1160,1415
now, remember that W here
现在，记住 W 在这里只是一个列表，

1379
00:36:07,035 --> 00:36:09,075
0,165 165,455 715,1115 1225,1625 1705,2040
is just a list,| {basically\,,it's
|它只是我们神经网络中所有权重的一组，

1380
00:36:09,075 --> 00:36:10,020
0,255 255,360 360,540 540,750 750,945
-} just a group of

1381
00:36:10,020 --> 00:36:10,815
0,195 195,375 375,495 495,705 705,795
all of the weights in

1382
00:36:10,815 --> 00:36:11,760
0,120 120,345 345,570 570,795 795,945
our neural network,| you may
|你可能有数百个权重和一个非常小的神经网络，

1383
00:36:11,760 --> 00:36:14,115
0,290 460,860 1180,1580 1810,2235 2235,2355
have hundreds of weights and

1384
00:36:14,115 --> 00:36:15,180
0,105 105,285 285,570 570,825 825,1065
a very, very small neural

1385
00:36:15,180 --> 00:36:16,665
0,260 280,675 675,960 960,1305 1305,1485
network,| or in today's neural
|或者在今天的神经网络中，你可能有数十亿或万亿的权重，

1386
00:36:16,665 --> 00:36:17,670
0,180 180,360 360,465 465,630 630,1005
networks, you may have billions

1387
00:36:17,670 --> 00:36:19,440
0,195 195,680 940,1245 1245,1605 1605,1770
or trillions {of,weights -},| And
|你想要找出每一个权重的值是多少，

1388
00:36:19,440 --> 00:36:20,505
0,105 105,255 255,435 435,710 790,1065
you want to find what

1389
00:36:20,505 --> 00:36:21,405
0,150 150,300 300,510 510,705 705,900
is the value of every

1390
00:36:21,405 --> 00:36:22,610
0,255 255,435 435,555 555,720 720,1205
single one of these weights,|
|

1391
00:36:22,780 --> 00:36:23,880
0,335 335,455 455,650 650,905 905,1100
that's going to result in
这将导致尽可能小的损失。

1392
00:36:23,880 --> 00:36:25,340
0,195 195,555 555,860 880,1170 1170,1460
the smallest loss as possible.|
|

1393
00:36:26,540 --> 00:36:27,610
0,400 450,710 710,815 815,935 935,1070
Now, how can you do
现在，你如何做到这个呢，

1394
00:36:27,610 --> 00:36:29,095
0,260 370,750 750,1035 1035,1245 1245,1485
this,| remember that our loss
|记住，我们的损失函数 J(W) 只是我们权重的函数，

1395
00:36:29,095 --> 00:36:30,990
0,335 415,705 705,870 870,1145 1495,1895
function {J(W) - -} is

1396
00:36:31,130 --> 00:36:32,725
0,380 380,620 620,880 1080,1385 1385,1595
just a function of our

1397
00:36:32,725 --> 00:36:34,950
0,455 565,855 855,1005 1005,1265 1525,2225
weights,| {so,for -} any instantiation
|所以对于任何权重的实例化，

1398
00:36:34,970 --> 00:36:36,205
0,245 245,410 410,860 860,1085 1085,1235
of our weights,| we can
|我们可以计算一个神经网络有多大错误的标量，

1399
00:36:36,205 --> 00:36:38,460
0,330 330,525 525,990 990,1265 1855,2255
compute a scalar value of

1400
00:36:38,720 --> 00:36:40,930
0,245 245,470 470,850 1260,1535 1535,2210
you know how how erroneous

1401
00:36:40,930 --> 00:36:42,390
0,255 255,435 435,735 735,1010 1060,1460
would our neural network be|
|

1402
00:36:42,650 --> 00:36:44,215
0,320 320,640 690,1265 1265,1445 1445,1565
for this instantiation of our
对于我们的权重实例化。

1403
00:36:44,215 --> 00:36:46,495
0,425 865,1265 1675,2055 2055,2160 2160,2280
weights.| So let's try and
|让我们试着想象一下，

1404
00:36:46,495 --> 00:36:47,590
0,450 450,645 645,840 840,990 990,1095
visualize,| for example, in a
|例如，在一个非常简单的二维空间中，

1405
00:36:47,590 --> 00:36:49,690
0,210 210,560 760,1160 1660,1920 1920,2100
very simple example of a

1406
00:36:49,690 --> 00:36:50,875
0,180 180,720 720,945 945,1095 1095,1185
two dimensional space,| where we
|我们只有两个权重，

1407
00:36:50,875 --> 00:36:52,590
0,135 135,390 390,630 630,1055 1315,1715
have only two weights,| extremely
|这里是非常简单的神经网络，非常小，

1408
00:36:53,240 --> 00:36:55,060
0,400 570,935 935,1175 1175,1490 1490,1820
simple neural network here, very

1409
00:36:55,060 --> 00:36:56,520
0,380 430,720 720,915 915,1185 1185,1460
small,| two weight neural network,|
|两个权重的神经网络，|

1410
00:36:56,840 --> 00:36:57,810
0,260 260,380 380,515 515,680 680,970
and we want to find
我们想找出训练这个神经网络的最佳权重是什么，

1411
00:36:58,280 --> 00:37:00,025
0,320 320,635 635,995 995,1340 1340,1745
what are the optimal weights

1412
00:37:00,025 --> 00:37:01,330
0,210 210,360 360,615 615,945 945,1305
that would train this neural

1413
00:37:01,330 --> 00:37:02,080
0,260
network,|
|

1414
00:37:02,080 --> 00:37:03,820
0,210 210,405 405,740 940,1340 1450,1740
we can plot basically the
我们可以画出损失，

1415
00:37:03,820 --> 00:37:06,430
0,290 1210,1515 1515,2175 2175,2400 2400,2610
loss,| how erroneous the neural
|神经网络的误差有多大，

1416
00:37:06,430 --> 00:37:08,010
0,210 210,525 525,810 810,1130 1180,1580
network is,| for every single
|对于这两个权重的每个实例化，

1417
00:37:08,540 --> 00:37:10,795
0,730 810,1160 1160,1490 1490,1775 1775,2255
instantiation of these two weights,

1418
00:37:10,795 --> 00:37:11,905
0,395 415,675 675,780 780,900 900,1110
right,| this is a huge
|这是一个很大的空间，这是一个无限的空间，

1419
00:37:11,905 --> 00:37:13,230
0,240 240,465 465,555 555,1005 1005,1325
space, it's an infinite space,|
|

1420
00:37:13,490 --> 00:37:14,875
0,335 335,605 605,815 815,1070 1070,1385
but still we can try
但我们仍然可以尝试，

1421
00:37:14,875 --> 00:37:15,985
0,335 475,735 735,855 855,975 975,1110
to,| we can have a
|我们可以有一个函数，

1422
00:37:15,985 --> 00:37:17,725
0,270 270,585 585,1205 1255,1530 1530,1740
function,| that evaluates at every
|在这个空间的每一点求值。

1423
00:37:17,725 --> 00:37:19,500
0,240 240,405 405,585 585,905
point in this space.|
|

1424
00:37:19,570 --> 00:37:20,940
0,350 350,560 560,815 815,1145 1145,1370
Now, what we ultimately want
现在，我们最终想要做的是，

1425
00:37:20,940 --> 00:37:22,350
0,150 150,300 300,590 910,1230 1230,1410
to do is,| again, we
|再次，我们想要找出哪一组 ws

1426
00:37:22,350 --> 00:37:25,050
0,150 150,315 315,590 1360,1760 2380,2700
want to find which set

1427
00:37:25,050 --> 00:37:26,655
0,240 240,945 945,1245 1245,1410 1410,1605
of ws| will give us
|将给我们带来尽可能最小的损失，

1428
00:37:26,655 --> 00:37:29,565
0,285 285,845 1585,1985 2005,2405 2635,2910
the smallest loss possible,| {that,means
|这意味着你可以在这里看到的这个图景上的最低点，

1429
00:37:29,565 --> 00:37:31,200
0,275 295,675 675,975 975,1295 1315,1635
-} basically the lowest point

1430
00:37:31,200 --> 00:37:32,685
0,210 210,500 880,1185 1185,1350 1350,1485
on this landscape that you

1431
00:37:32,685 --> 00:37:33,900
0,165 165,345 345,635
can see here,|
|

1432
00:37:33,910 --> 00:37:35,420
0,350 350,620 620,800 800,1235 1235,1510
where is the ws that
将我们带到最低点的 ws 在哪里，

1433
00:37:35,530 --> 00:37:36,650
0,290 290,485 485,650 650,815 815,1120
bring us to that lowest

1434
00:37:36,670 --> 00:37:37,740
0,400
point,|
|

1435
00:37:39,000 --> 00:37:39,740
0,245 245,350 350,455 455,575 575,740
the way that we do
我们做到这一点的方法是，

1436
00:37:39,740 --> 00:37:41,560
0,285 285,680 910,1260 1260,1515 1515,1820
this is| actually just by
|从一个随机的地方开始，

1437
00:37:41,730 --> 00:37:43,070
0,560 560,815 815,995 995,1100 1100,1340
firstly starting at a random

1438
00:37:43,070 --> 00:37:44,150
0,315 315,495 495,645 645,885 885,1080
place,| {we,have -} no idea
|我们不知道从哪里开始，

1439
00:37:44,150 --> 00:37:45,080
0,135 135,285 285,480 480,735 735,930
where to start,| so pick
|在这个空间中选择一个随机的地方开始，

1440
00:37:45,080 --> 00:37:46,235
0,135 135,405 405,765 765,1005 1005,1155
a random place to start

1441
00:37:46,235 --> 00:37:47,795
0,150 150,360 360,695 1075,1335 1335,1560
in this space| and let's
|让我们从那里开始，

1442
00:37:47,795 --> 00:37:49,570
0,180 180,485 685,1020 1020,1355 1375,1775
{start,there -},| at this location,
|在这个位置，让我们评估一下我们的神经网络，

1443
00:37:49,710 --> 00:37:51,230
0,470 470,790 810,1100 1100,1310 1310,1520
let's evaluate {our,neural -} network,|
|

1444
00:37:51,230 --> 00:37:52,900
0,225 225,360 360,800 1120,1395 1395,1670
we can compute the loss
我们可以计算这个特定位置的损失，

1445
00:37:52,920 --> 00:37:55,160
0,320 320,560 560,880 1020,1420 1980,2240
at this specific location| and
|在此基础上，我们可以计算损失是如何变化的，

1446
00:37:55,160 --> 00:37:55,910
0,150 150,315 315,450 450,600 600,750
on top of that, we

1447
00:37:55,910 --> 00:37:58,160
0,210 210,450 450,920 1750,2070 2070,2250
can actually compute how {the,loss

1448
00:37:58,160 --> 00:37:59,510
0,240 240,555 555,890 940,1215 1215,1350
-} is changing,| we can
|我们可以计算损失的梯度，

1449
00:37:59,510 --> 00:38:00,785
0,285 285,450 450,915 915,1155 1155,1275
compute the gradient of the

1450
00:38:00,785 --> 00:38:02,320
0,225 225,495 495,785 835,1185 1185,1535
loss,| because our loss {function,is
|因为我们的损失函数是连续函数，

1451
00:38:02,790 --> 00:38:05,045
0,380 380,755 755,1150 1410,1810 2010,2255
-} a continuous function,| so
|所以我们可以计算函数的导数，

1452
00:38:05,045 --> 00:38:06,370
0,105 105,300 300,495 495,780 780,1325
we can actually compute derivatives

1453
00:38:06,540 --> 00:38:08,150
0,260 260,410 410,700 1050,1385 1385,1610
of our function| across the
|通过空间中的权重，

1454
00:38:08,150 --> 00:38:09,220
0,210 210,405 405,680
space of our

1455
00:38:09,220 --> 00:38:11,035
0,530 760,1020 1020,1170 1170,1575 1575,1815
weights,| and the gradient tells
|梯度告诉我们最高点的方向，

1456
00:38:11,035 --> 00:38:13,480
0,330 330,660 660,995 1255,1655 2155,2445
us the direction of the

1457
00:38:13,480 --> 00:38:15,040
0,290 340,740 910,1185 1185,1350 1350,1560
highest point,| {so,from -} where
|所以，从我们所处的位置来看，

1458
00:38:15,040 --> 00:38:16,495
0,240 240,560 640,915 915,1260 1260,1455
we stand,| the gradient tells
|梯度告诉我们应该去哪里增加我们的损失，

1459
00:38:16,495 --> 00:38:17,580
0,225 225,435 435,615 615,795 795,1085
us where we should go

1460
00:38:18,050 --> 00:38:20,160
0,380 380,740 740,1025 1025,1330
to increase our loss,|
|

1461
00:38:20,380 --> 00:38:21,300
0,290 290,455 455,620 620,770 770,920
now, of course, we don't
现在，当然，我们不想增加我们的损失，

1462
00:38:21,300 --> 00:38:22,125
0,105 105,285 285,480 480,645 645,825
want to increase our loss,|
|

1463
00:38:22,125 --> 00:38:23,055
0,165 165,315 315,555 555,780 780,930
we want to decrease our
我们想减少我们的损失，

1464
00:38:23,055 --> 00:38:24,930
0,180 180,485 565,965 1045,1565 1585,1875
loss,| so we negate our
|所以我们对梯度取反，

1465
00:38:24,930 --> 00:38:25,875
0,390 390,525 525,645 645,780 780,945
gradient| and we take a
|我们朝着与梯度相反的方向迈出一步，

1466
00:38:25,875 --> 00:38:27,440
0,305 355,615 615,875 925,1245 1245,1565
step in the opposite direction

1467
00:38:27,550 --> 00:38:28,650
0,290 290,455 455,755 755,920 920,1100
of the gradient,| {that,brings -}
|这让我们离图景的底部又近了一步，

1468
00:38:28,650 --> 00:38:30,300
0,210 210,465 465,720 720,1040 1390,1650
us one step closer to

1469
00:38:30,300 --> 00:38:31,790
0,120 120,380 490,795 795,1095 1095,1490
the bottom of the landscape,|
|

1470
00:38:32,230 --> 00:38:33,510
0,380 380,635 635,785 785,995 995,1280
and we just keep repeating
我们只是一遍又一遍地重复这个过程，

1471
00:38:33,510 --> 00:38:35,520
0,135 135,410 1300,1620 1620,1785 1785,2010
this process over and {over,again

1472
00:38:35,520 --> 00:38:36,990
0,375 375,705 705,1040 1060,1305 1305,1470
-},| we evaluate the neural
|我们在这个新位置评估神经网络，

1473
00:38:36,990 --> 00:38:38,240
0,180 180,390 390,585 585,870 870,1250
network at this new location,|
|

1474
00:38:38,350 --> 00:38:40,110
0,440 440,650 650,1180 1230,1550 1550,1760
compute its gradient, and step
计算它的梯度，并朝着这个新方向前进，

1475
00:38:40,110 --> 00:38:41,280
0,150 150,315 315,540 540,860 880,1170
in {that,new -} direction,| we
|我们一直在穿过这个图景，直到我们汇聚到最小。

1476
00:38:41,280 --> 00:38:43,890
0,290 400,1160 1270,1670 1780,2180 2230,2610
keep traversing this landscape until

1477
00:38:43,890 --> 00:38:45,140
0,345 345,750 750,900 900,1005 1005,1250
we converge to the minimum.|
|

1478
00:38:47,260 --> 00:38:48,770
0,260 260,440 440,695 695,1175 1175,1510
We can really summarize this
我们可以总结这个算法，

1479
00:38:48,790 --> 00:38:50,535
0,395 395,545 545,820 870,1235 1235,1745
algorithm,| which is known formally
|它的正式名称是梯度下降，

1480
00:38:50,535 --> 00:38:52,530
0,270 270,810 810,1295 1375,1710 1710,1995
as gradient descent, right,| {so,gradient
|所以梯度下降可以简单地写成这样，

1481
00:38:52,530 --> 00:38:54,465
0,390 390,690 690,1010 1480,1770 1770,1935
-} descent simply can be

1482
00:38:54,465 --> 00:38:55,980
0,240 240,510 510,815 835,1110 1110,1515
written like this,| we initialize
|我们初始化所有的权重，

1483
00:38:55,980 --> 00:38:57,810
0,150 150,255 255,390 390,830 1540,1830
all of {our,weights -},| this
|这可以两个权重，

1484
00:38:57,810 --> 00:38:59,340
0,165 165,435 435,830 850,1350 1350,1530
can be two weights,| like
|如你在上一个例子中看到的，

1485
00:38:59,340 --> 00:38:59,925
0,120 120,210 210,270 270,360 360,585
you saw in {the,previous -}

1486
00:38:59,925 --> 00:39:01,305
0,315 315,510 510,615 615,875 895,1380
example,| it can be billions
|它可以是数十亿个权重，

1487
00:39:01,305 --> 00:39:03,075
0,180 180,605 655,930 930,1205 1465,1770
of weights,| like {in,real -}
|就像真正的神经网络，

1488
00:39:03,075 --> 00:39:05,270
0,270 270,545 1225,1500 1500,1875 1875,2195
neural networks,| we compute this
|我们计算损失的偏导数相对于权重的这个梯度，

1489
00:39:05,440 --> 00:39:08,120
0,610 630,1030 1470,1745 1745,2105 2105,2680
gradient of the partial derivative

1490
00:39:08,860 --> 00:39:10,245
0,290 290,515 515,815 815,1130 1130,1385
of our loss with respect

1491
00:39:10,245 --> 00:39:11,070
0,135 135,225 225,555 555,720 720,825
to the weights| and then
|然后我们可以在这个梯度的相反方向上更新我们的权重，

1492
00:39:11,070 --> 00:39:11,955
0,105 105,315 315,540 540,675 675,885
we can update our weights

1493
00:39:11,955 --> 00:39:13,875
0,105 105,365 445,810 810,1175 1555,1920
in the opposite direction of

1494
00:39:13,875 --> 00:39:15,160
0,345 345,905
this gradient,|
|

1495
00:39:15,810 --> 00:39:17,450
0,400 690,1055 1055,1280 1280,1445 1445,1640
so essentially we just take
所以我们只是采取这个很小的量，

1496
00:39:17,450 --> 00:39:19,460
0,240 240,540 540,890 1300,1665 1665,2010
this small amount,| small step,
|很小的一步，你可以想象它，

1497
00:39:19,460 --> 00:39:20,380
0,240 240,375 375,540 540,675 675,920
you can think of it,|
|

1498
00:39:20,490 --> 00:39:22,000
0,350 350,620 620,830 830,1175 1175,1510
which here is denoted as
在这里被表示为 η ，

1499
00:39:22,260 --> 00:39:23,340
0,550
η,|
|

1500
00:39:23,350 --> 00:39:25,050
0,350 350,700 810,1210 1230,1490 1490,1700
and we refer to this
我们指的是这一小步，

1501
00:39:25,050 --> 00:39:27,230
0,345 345,740 1240,1515 1515,1785 1785,2180
small step,| {this,is -} commonly
|这通常被称为学习率，

1502
00:39:27,430 --> 00:39:28,800
0,335 335,560 560,845 845,1220 1220,1370
referred to as what's known

1503
00:39:28,800 --> 00:39:30,225
0,210 210,420 420,705 705,1100 1120,1425
as the learning rate,| it's
|这就像我们多么相信这个梯度，

1504
00:39:30,225 --> 00:39:31,050
0,120 120,315 315,510 510,675 675,825
like how much we want

1505
00:39:31,050 --> 00:39:32,805
0,180 180,470 790,1110 1110,1530 1530,1755
to trust that gradient| and
|并朝着这个梯度的方向前进，

1506
00:39:32,805 --> 00:39:33,855
0,225 225,390 390,555 555,810 810,1050
step in the direction of

1507
00:39:33,855 --> 00:39:35,130
0,195 195,630 630,900 900,1065 1065,1275
that gradient,| we'll talk more
|我们稍后将详细讨论这个问题。

1508
00:39:35,130 --> 00:39:36,540
0,180 180,360 360,650
about this later.|
|

1509
00:39:36,540 --> 00:39:37,760
0,350 370,660 660,795 795,930 930,1220
But just to give you
但为了给你一些代码的感觉，

1510
00:39:37,810 --> 00:39:39,620
0,400 480,785 785,1025 1025,1360 1410,1810
some sense of code,| this
|这个算法也可以很好地翻译成真实的代码，

1511
00:39:40,000 --> 00:39:41,780
0,350 350,515 515,770 770,1025 1025,1780
algorithm is very well translatable

1512
00:39:41,800 --> 00:39:43,020
0,260 260,455 455,740 740,995 995,1220
to real code as well,|
|

1513
00:39:43,020 --> 00:39:44,025
0,180 180,405 405,690 690,885 885,1005
{for,every -} line on the
对于你可以在左侧看到的伪代码上的每一行，

1514
00:39:44,025 --> 00:39:44,790
0,270 270,420 420,540 540,660 660,765
pseudo code you can see

1515
00:39:44,790 --> 00:39:45,765
0,90 90,195 195,440 610,855 855,975
on the left,| you can
|你都可以在右侧看到相应的真实代码，

1516
00:39:45,765 --> 00:39:47,520
0,195 195,1055 1075,1410 1410,1620 1620,1755
see corresponding real code on

1517
00:39:47,520 --> 00:39:48,570
0,120 120,315 315,510 510,660 660,1050
the right,| that is runable|
|这些代码是可以运行的，|

1518
00:39:48,570 --> 00:39:50,085
0,195 195,530 550,1200 1200,1395 1395,1515
and directly implementable by all
并且可以由你们在实验中直接实现。

1519
00:39:50,085 --> 00:39:51,020
0,120 120,240 240,345 345,480 480,935
of you in your labs.|
|

1520
00:39:51,860 --> 00:39:52,505
0,120 120,240 240,435 435,540 540,645
But now let's take a
但现在让我们来具体看看这个术语，

1521
00:39:52,505 --> 00:39:54,155
0,245 265,665 835,1140 1140,1380 1380,1650
look specifically at this term

1522
00:39:54,155 --> 00:39:55,445
0,300 300,540 540,720 720,900 900,1290
here,| {this,is -} the gradient,|
|这就是梯度，|

1523
00:39:55,445 --> 00:39:56,915
0,285 285,615 615,900 900,1200 1200,1470
we touch very briefly on
我们在可视的例子中非常简要地谈到这一点，

1524
00:39:56,915 --> 00:39:58,150
0,240 240,435 435,555 555,815 835,1235
this at the {visual,example -},|
|

1525
00:39:58,560 --> 00:40:00,035
0,395 395,785 785,1010 1010,1175 1175,1475
this explains, like I said,|
这就解释了，就像我说的，|

1526
00:40:00,035 --> 00:40:01,780
0,395 505,810 810,1095 1095,1410 1410,1745
how the loss is changing
损失是如何随着权重的变化而变化的，

1527
00:40:01,830 --> 00:40:02,945
0,275 275,440 440,695 695,965 965,1115
as a function {of,the -}

1528
00:40:02,945 --> 00:40:04,570
0,425 535,855 855,1080 1080,1245 1245,1625
weights,| so as the weights
|所以，随着权重的移动，

1529
00:40:04,650 --> 00:40:06,280
0,365 365,730 930,1205 1205,1355 1355,1630
move around,| will my loss
|我的损失是增加还是减少，

1530
00:40:06,300 --> 00:40:07,445
0,275 275,530 530,830 830,995 995,1145
increase or decrease,| and that
|这将告诉神经网络，

1531
00:40:07,445 --> 00:40:08,350
0,180 180,345 345,480 480,645 645,905
will tell the neural network,|
|

1532
00:40:08,610 --> 00:40:09,850
0,260 260,380 380,560 560,860 860,1240
if it needs to move
它是否需要将权重向某个方向移动。

1533
00:40:09,870 --> 00:40:10,790
0,260 260,500 500,605 605,710 710,920
the weights in a certain

1534
00:40:10,790 --> 00:40:12,240
0,315 315,540 540,800
direction or not.|
|

1535
00:40:12,490 --> 00:40:13,470
0,245 245,350 350,560 560,785 785,980
But I never actually told
但我从来没有告诉过你如何计算这个，

1536
00:40:13,470 --> 00:40:14,895
0,320 430,720 720,855 855,1140 1140,1425
you how to compute this,

1537
00:40:14,895 --> 00:40:16,125
0,395 535,780 780,900 900,1035 1035,1230
right,| and I think that's
|我认为这是一个非常重要的部分，

1538
00:40:16,125 --> 00:40:17,430
0,210 210,570 570,870 870,1125 1125,1305
an extremely important part,| because
|因为如果你不知道这一点，

1539
00:40:17,430 --> 00:40:18,210
0,120 120,225 225,405 405,525 525,780
if you don't know that,|
|

1540
00:40:18,210 --> 00:40:20,295
0,255 255,480 480,980 1660,1935 1935,2085
then you can't, well, you
那么你就不能训练你的神经网络，

1541
00:40:20,295 --> 00:40:21,290
0,225 225,390 390,555 555,735 735,995
can't train your neural network,|
|

1542
00:40:21,490 --> 00:40:22,730
0,260 260,380 380,560 560,860 860,1240
{this,is -} a critical part
这是训练神经网络的关键部分，

1543
00:40:23,050 --> 00:40:24,795
0,400 570,905 905,1220 1220,1475 1475,1745
of training neural networks| and
|计算这条梯度线的过程被称为反向传播。

1544
00:40:24,795 --> 00:40:26,430
0,195 195,515 595,975 975,1395 1395,1635
that process of computing this

1545
00:40:26,430 --> 00:40:28,485
0,350 460,780 780,1200 1200,1490 1750,2055
line, this gradient line is

1546
00:40:28,485 --> 00:40:30,405
0,210 210,515 655,990 990,1535 1675,1920
known as back propagation.| So
|所以，让我们简要介绍一下反向传播，

1547
00:40:30,405 --> 00:40:31,460
0,225 225,345 345,480 480,705 705,1055
let's do a very quick

1548
00:40:32,440 --> 00:40:34,275
0,640 690,950 950,1130 1130,1610 1610,1835
intro to back propagation| and
|以及它是如何工作的，

1549
00:40:34,275 --> 00:40:35,780
0,150 150,315 315,605
how it works.|
|

1550
00:40:36,190 --> 00:40:37,335
0,275 275,470 470,770 770,935 935,1145
So again, let's start with
再次，我们从现存的最简单的神经网络开始，

1551
00:40:37,335 --> 00:40:39,030
0,270 270,750 750,1005 1005,1265 1345,1695
the simplest neural network in

1552
00:40:39,030 --> 00:40:40,610
0,350 460,765 765,1020 1020,1230 1230,1580
existence,| {this,neural -} network has
|这个神经网络有一个输入，一个输出，并且只有一个神经元，

1553
00:40:40,810 --> 00:40:42,360
0,365 365,710 710,1085 1085,1385 1385,1550
one input, one output, and

1554
00:40:42,360 --> 00:40:43,905
0,180 180,405 405,830 1150,1410 1410,1545
only one neuron,| this is
|这就是最简单的事情，

1555
00:40:43,905 --> 00:40:44,930
0,165 165,360 360,555 555,735 735,1025
as simple as {it,gets -},|
|

1556
00:40:45,580 --> 00:40:46,665
0,260 260,410 410,560 560,905 905,1085
we want to compute the
我们想要计算损失相对于我们权重的梯度，

1557
00:40:46,665 --> 00:40:48,530
0,450 450,675 675,855 855,1175 1465,1865
gradient of our loss with

1558
00:40:48,610 --> 00:40:49,980
0,335 335,560 560,770 770,1090 1110,1370
respect {to,our -} weight,| in
|在这种情况下，让我们相对于 w2 ，第二个权重来计算它，

1559
00:40:49,980 --> 00:40:50,910
0,150 150,330 330,585 585,825 825,930
this case, let's compute it

1560
00:40:50,910 --> 00:40:52,340
0,210 210,480 480,720 720,1035 1035,1430
with respect to {w2 -},

1561
00:40:52,360 --> 00:40:53,940
0,290 290,545 545,910
the second weight,|
|

1562
00:40:54,070 --> 00:40:56,025
0,400 660,980 980,1550 1550,1790 1790,1955
so this derivative is going
所以这个导数会告诉我们，

1563
00:40:56,025 --> 00:40:57,105
0,150 150,285 285,495 495,780 780,1080
to tell us| how much
|权重的微小变化会对损失有多大影响，

1564
00:40:57,105 --> 00:40:59,520
0,330 330,675 675,1055 1615,2015 2095,2415
a small change in this

1565
00:40:59,520 --> 00:41:01,050
0,315 315,675 675,1005 1005,1260 1260,1530
weight will affect our loss,|
|

1566
00:41:01,050 --> 00:41:02,280
0,375 375,630 630,795 795,1020 1020,1230
if if a small change,
如果一个小的变化，如果我们在一个方向上稍微改变一下权重，

1567
00:41:02,280 --> 00:41:03,075
0,150 150,300 300,480 480,645 645,795
if we change our weight

1568
00:41:03,075 --> 00:41:03,915
0,165 165,360 360,525 525,645 645,840
a little bit in one

1569
00:41:03,915 --> 00:41:05,430
0,335 505,885 885,1155 1155,1320 1320,1515
direction,| will increase our loss
|会增加我们的损失或减少我们的损失。

1570
00:41:05,430 --> 00:41:07,360
0,285 285,540 540,705 705,980
or decrease our loss.|
|

1571
00:41:07,530 --> 00:41:08,750
0,350 350,545 545,830 830,1040 1040,1220
So to compute that, we
所以要计算它，我们可以写出这个导数，

1572
00:41:08,750 --> 00:41:09,860
0,120 120,255 255,420 420,615 615,1110
can write out this derivative,|
|

1573
00:41:09,860 --> 00:41:11,585
0,195 195,420 420,770 850,1250 1390,1725
{we,can -} start with applying
我们可以从应用链式规则开始，

1574
00:41:11,585 --> 00:41:14,015
0,225 225,450 450,785 1225,1895 2125,2430
the chain rule,| backwards from
|从损失函数到输出的反向，

1575
00:41:14,015 --> 00:41:15,905
0,165 165,345 345,665 1105,1500 1500,1890
the loss function through the

1576
00:41:15,905 --> 00:41:17,780
0,395 685,1085 1345,1620 1620,1755 1755,1875
output,| specifically, what we can
|具体地说，我们可以做的是，

1577
00:41:17,780 --> 00:41:18,545
0,120 120,255 255,390 390,570 570,765
do is,| we can actually
|我们可以把这个导数分解成两个分量，

1578
00:41:18,545 --> 00:41:21,130
0,165 165,815 925,1230 1230,1895 2185,2585
just decompose this derivative into

1579
00:41:21,330 --> 00:41:23,090
0,400 480,880 900,1160 1160,1415 1415,1760
{two,components -},| the first component
|第一个分量是我们的损失对输出的导数，

1580
00:41:23,090 --> 00:41:24,410
0,255 255,435 435,975 975,1170 1170,1320
is the derivative of our

1581
00:41:24,410 --> 00:41:25,655
0,285 285,630 630,870 870,1005 1005,1245
loss with respect to our

1582
00:41:25,655 --> 00:41:27,530
0,395 595,1215 1215,1380 1380,1500 1500,1875
output| multiplied by the derivative
|乘以我们的输出对 w2 的导数，

1583
00:41:27,530 --> 00:41:29,060
0,150 150,410 520,885 885,1230 1230,1530
{of,our -} output with respect

1584
00:41:29,060 --> 00:41:30,260
0,210 210,495 495,840 840,1065 1065,1200
to {w2 -}, right,| this
|这只是一个标准的链式规则的实例化，

1585
00:41:30,260 --> 00:41:32,940
0,150 150,315 315,590 1030,1430
is just a standard

1586
00:41:33,730 --> 00:41:35,775
0,640 750,1100 1100,1445 1445,1760 1760,2045
instantiation of the chain rule|
|

1587
00:41:35,775 --> 00:41:37,410
0,270 270,510 510,845 955,1500 1500,1635
with this original derivative that
用左侧的原始导数。

1588
00:41:37,410 --> 00:41:38,085
0,120 120,255 255,405 405,525 525,675
we had on the left

1589
00:41:38,085 --> 00:41:39,520
0,210 210,515
hand side.|
|

1590
00:41:39,770 --> 00:41:40,735
0,380 380,560 560,710 710,845 845,965
Let's suppose we wanted to
让我们假设我们想要计算权重的梯度，

1591
00:41:40,735 --> 00:41:42,130
0,315 315,465 465,900 900,1215 1215,1395
compute the gradients of the

1592
00:41:42,130 --> 00:41:43,405
0,210 210,510 510,765 765,1035 1035,1275
weight,| before that, which in
|在此之前，在这种情况下，不是 w1 ，而是 w ，

1593
00:41:43,405 --> 00:41:44,560
0,165 165,375 375,660 660,915 915,1155
this case are not {w1

1594
00:41:44,560 --> 00:41:46,195
0,350 370,675 675,980 1210,1485 1485,1635
-}, but w,| excuse me
|抱歉，不是 w2 ，而是 w1 ，

1595
00:41:46,195 --> 00:41:47,580
0,180 180,465 465,825 825,1095 1095,1385
not {w2 -}, but {w1

1596
00:41:47,600 --> 00:41:49,735
0,400 1230,1625 1625,1880 1880,2015 2015,2135
-},| well, all we do
|我们所做的就是用 w1 替换 w2 ，

1597
00:41:49,735 --> 00:41:50,740
0,180 180,420 420,675 675,885 885,1005
is replace {w2 -} with

1598
00:41:50,740 --> 00:41:52,225
0,225 225,590 760,1035 1035,1245 1245,1485
{w1 -}| and that chain
|链式法则仍然有效，

1599
00:41:52,225 --> 00:41:53,800
0,255 255,540 540,875 1015,1335 1335,1575
rule still holds, right,| {that,same
|同样的方程成立，

1600
00:41:53,800 --> 00:41:55,410
0,270 270,620 640,1040 1060,1335 1335,1610
-} equation holds,| but now
|但现在你可以在红色部分看到，

1601
00:41:55,640 --> 00:41:56,935
0,260 260,425 425,730 840,1130 1130,1295
you can see on the

1602
00:41:56,935 --> 00:41:58,450
0,270 270,615 615,870 870,1175 1195,1515
red component,| that last component
|链式规则的最后一个部分，

1603
00:41:58,450 --> 00:41:59,545
0,180 180,315 315,480 480,770 850,1095
of the chain rule,| we
|我们必须再次递归地应用另一个链式规则，

1604
00:41:59,545 --> 00:42:01,615
0,150 150,455 565,945 945,1275 1275,2070
have to once again recursively

1605
00:42:01,615 --> 00:42:02,650
0,240 240,420 420,615 615,825 825,1035
apply one more chain rule,|
|

1606
00:42:02,650 --> 00:42:05,050
0,180 180,450 450,740 910,1310 1750,2400
because that's again another derivative
因为这又是另一个我们不能直接求值的导数，

1607
00:42:05,050 --> 00:42:06,310
0,270 270,465 465,690 690,915 915,1260
that we can't directly evaluate,|
|

1608
00:42:06,310 --> 00:42:07,555
0,240 240,480 480,780 780,990 990,1245
we can expand that once
我们可以用链式规则的另一个实例化再次扩展，

1609
00:42:07,555 --> 00:42:09,355
0,360 360,690 690,1025 1045,1590 1590,1800
more with another instantiation of

1610
00:42:09,355 --> 00:42:10,705
0,135 135,315 315,635 835,1125 1125,1350
the chain rule,| and now
|现在所有这些组件，

1611
00:42:10,705 --> 00:42:12,595
0,210 210,345 345,600 600,995 1615,1890
all of these components,| we
|都可以通过神经网络中的隐藏单元直接传播这些梯度，

1612
00:42:12,595 --> 00:42:14,640
0,275 385,765 765,1305 1305,1500 1500,2045
can directly propagate these gradients

1613
00:42:14,810 --> 00:42:16,705
0,400 420,695 695,890 890,1210 1590,1895
through the hidden units, right,|
|

1614
00:42:16,705 --> 00:42:17,875
0,165 165,330 330,600 600,870 870,1170
in our neural network,| all
在我们的神经网络中，|一直回到这个例子中感兴趣的权重，

1615
00:42:17,875 --> 00:42:19,105
0,150 150,285 285,575 865,1110 1110,1230
the way back to the

1616
00:42:19,105 --> 00:42:20,530
0,210 210,845 895,1155 1155,1275 1275,1425
weight that're interested in in

1617
00:42:20,530 --> 00:42:21,850
0,225 225,560 640,945 945,1140 1140,1320
this {example\,,right -},| so we
|所以我们首先计算对 w2 的导数，

1618
00:42:21,850 --> 00:42:23,200
0,255 255,615 615,750 750,1125 1125,1350
first computed the derivative with

1619
00:42:23,200 --> 00:42:24,680
0,225 225,360 360,585 585,950
respect to {w2 -},|
|

1620
00:42:24,680 --> 00:42:25,715
0,210 210,315 315,420 420,615 615,1035
then we can back propagate
然后我们可以反向传播这些信息，并使用这些信息，

1621
00:42:25,715 --> 00:42:26,770
0,120 120,225 225,405 405,690 690,1055
that and use that information,|
|

1622
00:42:27,180 --> 00:42:28,265
0,230 230,335 335,575 575,830 830,1085
{also,with -} {w1 -},| that's
也对于 w1 ，|这就是为什么我们称之为反向传播，

1623
00:42:28,265 --> 00:42:29,030
0,120 120,270 270,465 465,645 645,765
why we really call it

1624
00:42:29,030 --> 00:42:30,520
0,180 180,720 720,990 990,1185 1185,1490
back propagation,| because this process
|因为这个过程发生从输出一直到输入。

1625
00:42:30,630 --> 00:42:32,105
0,320 320,515 515,740 740,1090 1200,1475
occurs from the output all

1626
00:42:32,105 --> 00:42:32,870
0,120 120,240 240,435 435,600 600,765
the way back to the

1627
00:42:32,870 --> 00:42:33,600
0,320
input.|
|

1628
00:42:34,650 --> 00:42:36,070
0,335 335,620 620,905 905,1130 1130,1420
Now, we repeat this process
现在，我们在训练过程中重复这个过程很多次，

1629
00:42:36,150 --> 00:42:38,930
0,400 1110,1430 1430,1700 1700,2050 2460,2780
essentially many, many times| over
|通过网络一遍又一遍地传播这些梯度，

1630
00:42:38,930 --> 00:42:40,010
0,180 180,345 345,555 555,810 810,1080
the course of training by

1631
00:42:40,010 --> 00:42:41,825
0,540 540,810 810,1335 1335,1635 1635,1815
propagating these gradients over and

1632
00:42:41,825 --> 00:42:43,000
0,240 240,570 570,780 780,915 915,1175
over again through the network,|
|

1633
00:42:43,290 --> 00:42:44,285
0,320 320,485 485,620 620,770 770,995
all the way from the
从输出到输入来确定每个单一权重，回答这个问题，

1634
00:42:44,285 --> 00:42:45,710
0,360 360,630 630,855 855,1200 1200,1425
output to the inputs to

1635
00:42:45,710 --> 00:42:47,170
0,270 270,480 480,740 760,1110 1110,1460
determine for every single weight,

1636
00:42:47,250 --> 00:42:49,420
0,545 545,740 740,1060 1560,1865 1865,2170
answering this question,| which is
|即这些权重的微小变化对我们的损失函数的影响有多大，

1637
00:42:49,620 --> 00:42:50,870
0,335 335,605 605,800 800,980 980,1250
how much does a small

1638
00:42:50,870 --> 00:42:52,720
0,315 315,540 540,720 720,1220 1450,1850
change in these weights affect

1639
00:42:52,770 --> 00:42:54,080
0,305 305,545 545,860 860,1100 1100,1310
our loss function,| if it
|它是增加或减少，

1640
00:42:54,080 --> 00:42:55,265
0,285 285,480 480,690 690,1035 1035,1185
increases it or decreases,| and
|以及我们如何利用它来最终改善损失，

1641
00:42:55,265 --> 00:42:56,270
0,255 255,465 465,585 585,750 750,1005
how we can use that

1642
00:42:56,270 --> 00:42:58,160
0,350 580,980 1000,1260 1260,1520 1540,1890
to improve the loss ultimately,|
|

1643
00:42:58,160 --> 00:42:59,440
0,225 225,450 450,600 600,900 900,1280
because that's our final goal
因为这是我们这门课的最终目标。

1644
00:42:59,800 --> 00:43:01,300
0,150 150,360 360,680
in this class.|
|

1645
00:43:02,710 --> 00:43:04,100
0,320 320,620 620,725 725,875 875,1390
So that's the back propagation
这就是反向传播算法，

1646
00:43:04,150 --> 00:43:06,060
0,500 500,1030 1080,1460 1460,1640 1640,1910
algorithm,| that's, that's the core
|那是训练神经网络的核心，

1647
00:43:06,060 --> 00:43:08,370
0,270 270,590 790,1155 1155,1430 1990,2310
of training neural networks,| {in,theory
|理论上，这很简单，

1648
00:43:08,370 --> 00:43:10,050
0,320 520,870 870,1050 1050,1305 1305,1680
-}, it's very simple,| it's,
|这只是链条规则的实例化，

1649
00:43:10,050 --> 00:43:12,050
0,470 550,915 915,1170 1170,1395 1395,2000
it's really just an instantiation

1650
00:43:12,160 --> 00:43:14,080
0,400 480,755 755,965 965,1300
of the chain rule,|
|

1651
00:43:14,380 --> 00:43:16,070
0,400 720,1100 1100,1235 1235,1400 1400,1690
but let's touch on some
但让我们来谈谈一些见解，

1652
00:43:16,090 --> 00:43:17,820
0,440 440,680 680,1025 1025,1385 1385,1730
insights,| that make training neural
|使训练神经网络在实践中变得极其复杂，

1653
00:43:17,820 --> 00:43:20,085
0,260 670,1005 1005,1305 1305,1670 1960,2265
networks actually extremely complicated in

1654
00:43:20,085 --> 00:43:21,780
0,305 385,750 750,1035 1035,1335 1335,1695
practice,| even though the algorithm
|尽管反向传播的算法很简单，

1655
00:43:21,780 --> 00:43:24,105
0,150 150,360 360,890 1060,1460 1930,2325
of back propagation is simple|
|

1656
00:43:24,105 --> 00:43:25,640
0,395 475,720 720,915 915,1200 1200,1535
and, you know, many decades
而且几十年时间，

1657
00:43:25,840 --> 00:43:28,905
0,400 1080,1400 1400,1720 1800,2200 2460,3065
old,| {in,practice -} though, optimization
|然而，在实践中，神经网络的优化看起来就像这样，

1658
00:43:28,905 --> 00:43:30,705
0,300 300,570 570,845 1225,1545 1545,1800
of neural networks looks something

1659
00:43:30,705 --> 00:43:31,680
0,225 225,375 375,495 495,675 675,975
like this,| it looks nothing
|它看起来一点也不像我之前给你看的那张图片，

1660
00:43:31,680 --> 00:43:32,730
0,285 285,480 480,705 705,900 900,1050
like that picture that I

1661
00:43:32,730 --> 00:43:34,140
0,195 195,360 360,620 1060,1305 1305,1410
showed {you,before -},| there are
|有很多方法可以让我们看到非常大、很深的神经网络，

1662
00:43:34,140 --> 00:43:35,265
0,180 180,360 360,465 465,600 600,1125
ways that we can visualize

1663
00:43:35,265 --> 00:43:36,890
0,285 285,635 715,1035 1035,1350 1350,1625
very large, deep neural networks,|
|

1664
00:43:37,300 --> 00:43:38,580
0,400 480,740 740,890 890,1070 1070,1280
and you can think of
你可以想象这些模型的图景是这样的，

1665
00:43:38,580 --> 00:43:39,860
0,320 370,645 645,795 795,975 975,1280
the landscape of these models

1666
00:43:40,240 --> 00:43:41,565
0,365 365,650 650,905 905,1130 1130,1325
looking like {something,like -} this,|
|

1667
00:43:41,565 --> 00:43:42,645
0,165 165,285 285,405 405,870 870,1080
this is an illustration from
这是几年前发表的一篇论文中的一个插图，

1668
00:43:42,645 --> 00:43:43,620
0,120 120,330 330,555 555,720 720,975
a paper that came out

1669
00:43:43,620 --> 00:43:45,180
0,330 330,630 630,980 1150,1410 1410,1560
several years ago,| where they
|他们试图可视化非常、非常深的神经网络的图景，

1670
00:43:45,180 --> 00:43:46,610
0,180 180,390 390,600 600,1110 1110,1430
tried to actually visualize the

1671
00:43:46,630 --> 00:43:47,895
0,260 260,440 440,725 725,1025 1025,1265
landscape of very, {very,deep -}

1672
00:43:47,895 --> 00:43:49,710
0,285 285,545 1195,1455 1455,1680 1680,1815
neural networks,| and that's what
|这就是这个图景的真实面貌，

1673
00:43:49,710 --> 00:43:50,925
0,290 310,690 690,930 930,1065 1065,1215
this landscape actually looks like,|
|

1674
00:43:50,925 --> 00:43:51,660
0,210 210,315 315,480 480,600 600,735
that's what you're trying to
这就是你试图处理的，在这个空间中找到最小的，

1675
00:43:51,660 --> 00:43:52,410
0,150 150,300 300,450 450,615 615,750
deal with and find {the,minimum

1676
00:43:52,410 --> 00:43:53,610
0,210 210,435 435,645 645,960 960,1200
-} in this space,| and
|你可以想象随之而来的挑战，

1677
00:43:53,610 --> 00:43:55,010
0,105 105,225 225,500 820,1110 1110,1400
you can imagine the challenges

1678
00:43:55,030 --> 00:43:57,780
0,290 290,470 470,650 650,940
that come with that,|
|

1679
00:43:57,790 --> 00:43:59,280
0,275 275,455 455,635 635,910 1110,1490
to cover the challenges,| let's
为了涵盖这些挑战，|让我们首先考虑并回想一下在梯度下降中定义的更新方程。

1680
00:43:59,280 --> 00:44:00,860
0,225 225,465 465,750 750,1130 1180,1580
first think of and recall

1681
00:44:01,030 --> 00:44:03,180
0,400 450,800 800,1150 1290,1690 1860,2150
that update equation defined in

1682
00:44:03,180 --> 00:44:05,655
0,360 360,770 940,1275 1275,1610 2200,2475
gradient descent, right.| {So\,,I,didn't -
|所以我没有太多地谈论这个参数 η ，

1683
00:44:05,655 --> 00:44:06,600
0,210 210,375 375,540 540,705 705,945
-} talk too much about

1684
00:44:06,600 --> 00:44:08,250
0,240 240,765 765,1160 1210,1485 1485,1650
this parameter η,| but now
|但是现在让我们花一点时间来思考这个，

1685
00:44:08,250 --> 00:44:09,150
0,255 255,390 390,570 570,750 750,900
let's spend a bit of

1686
00:44:09,150 --> 00:44:10,635
0,290 400,765 765,1020 1020,1260 1260,1485
time thinking about this,| this
|这就是所谓的学习率，

1687
00:44:10,635 --> 00:44:11,790
0,195 195,435 435,630 630,855 855,1155
is called the {learning,rate -},|
|

1688
00:44:11,790 --> 00:44:13,260
0,240 240,405 405,570 570,860 1150,1470
like {we,saw -} before,| it
就像我们之前看到的那样，|它决定了我们需要在梯度方向上迈出多大的一步，

1689
00:44:13,260 --> 00:44:15,075
0,405 405,740 850,1250 1420,1695 1695,1815
determines basically how big of

1690
00:44:15,075 --> 00:44:16,380
0,195 195,545 715,990 990,1140 1140,1305
a step we need to

1691
00:44:16,380 --> 00:44:17,790
0,290 520,795 795,960 960,1200 1200,1410
take in the direction of

1692
00:44:17,790 --> 00:44:19,005
0,150 150,525 525,675 675,930 930,1215
our gradient| and every single
|以及反向传播的每一次迭代，

1693
00:44:19,005 --> 00:44:20,920
0,390 390,600 600,795 795,1325
iteration of back propagation,|
|

1694
00:44:21,230 --> 00:44:23,515
0,305 305,610 870,1270 1470,1870 2010,2285
in practice, even setting the
在实践中，即使是设定学习速度也可能是非常具有挑战性的，

1695
00:44:23,515 --> 00:44:24,550
0,225 225,540 540,750 750,855 855,1035
learning rate can be very

1696
00:44:24,550 --> 00:44:25,870
0,320 340,615 615,890 910,1170 1170,1320
challenging,| {you,as -}, you as
|你作为神经网络的设计者，

1697
00:44:25,870 --> 00:44:26,920
0,240 240,630 630,735 735,855 855,1050
the designer of the neural

1698
00:44:26,920 --> 00:44:28,200
0,255 255,540 540,735 735,960 960,1280
network,| have to set this
|必须设置这个值，这个学习率，

1699
00:44:28,220 --> 00:44:30,070
0,395 395,695 695,950 950,1300 1560,1850
value, this learning rate| and
|如何选择这个值，

1700
00:44:30,070 --> 00:44:30,715
0,135 135,195 195,285 285,450 450,645
how do you pick this

1701
00:44:30,715 --> 00:44:32,050
0,300 300,600 600,885 885,1140 1140,1335
value, right,| so that can
|这可能是相当困难的，

1702
00:44:32,050 --> 00:44:32,935
0,165 165,285 285,540 540,780 780,885
actually be quite difficult,| it
|在构建神经网络时，它会产生非常大的影响，

1703
00:44:32,935 --> 00:44:36,055
0,195 195,545 1255,1655 1765,2165 2725,3120
has really large consequences when

1704
00:44:36,055 --> 00:44:37,255
0,330 330,525 525,720 720,960 960,1200
building a {neural,network -},| So,
|所以，比如，如果我们把学习率设定得太低，

1705
00:44:37,255 --> 00:44:38,460
0,165 165,455
for example,

1706
00:44:38,530 --> 00:44:40,305
0,305 305,610 960,1340 1340,1580 1580,1775
if we set the learning

1707
00:44:40,305 --> 00:44:41,980
0,335 385,690 690,995
rate too low,|
|

1708
00:44:41,980 --> 00:44:43,990
0,290 340,740 850,1230 1230,1610 1630,2010
then we learn very slowly,|
然后我们学得很慢，|

1709
00:44:43,990 --> 00:44:44,980
0,225 225,450 450,615 615,795 795,990
{so,let's -} assume we start
我们假设我们从右手边开始，

1710
00:44:44,980 --> 00:44:45,880
0,150 150,270 270,435 435,660 660,900
on the right hand side

1711
00:44:45,880 --> 00:44:47,335
0,320 400,675 675,885 885,1155 1155,1455
here,| at that initial guess,|
|在最初的猜测中，|

1712
00:44:47,335 --> 00:44:48,355
0,240 240,435 435,690 690,885 885,1020
if our learning rate is
如果我们的学习率不够大，

1713
00:44:48,355 --> 00:44:50,020
0,180 180,465 465,845 1135,1425 1425,1665
not large enough,| not only
|我们不仅收敛得很慢，

1714
00:44:50,020 --> 00:44:51,850
0,195 195,440 700,1110 1110,1430 1450,1830
do we converge slowly,| we
|我们甚至不会收敛到全局最小值，

1715
00:44:51,850 --> 00:44:53,185
0,300 300,585 585,825 825,1215 1215,1335
actually don't even converge to

1716
00:44:53,185 --> 00:44:54,850
0,120 120,395 565,965 1255,1530 1530,1665
the global minimum,| because we
|因为我们有点陷入局部最小值，

1717
00:44:54,850 --> 00:44:55,555
0,105 105,195 195,375 375,585 585,705
kind of get stuck in

1718
00:44:55,555 --> 00:44:57,180
0,105 105,345 345,725
a local minimum,|
|

1719
00:44:57,740 --> 00:44:58,780
0,335 335,515 515,635 635,845 845,1040
now, what if we set
现在，如果我们把我们的学习率设定得太高，

1720
00:44:58,780 --> 00:44:59,880
0,120 120,300 300,585 585,825 825,1100
our learning rate too high,

1721
00:44:59,960 --> 00:45:01,030
0,335 335,545 545,740 740,905 905,1070
right,| What can actually happen
|可能发生的情况是，

1722
00:45:01,030 --> 00:45:02,650
0,210 210,360 360,1095 1095,1425 1425,1620
is,| we overshoot and we
|我们超调了，我们可能开始偏离解决方案，

1723
00:45:02,650 --> 00:45:04,080
0,195 195,435 435,630 630,765 765,1430
can actually start to diverge

1724
00:45:04,160 --> 00:45:06,115
0,290 290,470 470,760 1200,1505 1505,1955
from the solution,| {the,gradients -}
|这些渐变可能会爆炸，

1725
00:45:06,115 --> 00:45:07,870
0,240 240,540 540,965 1225,1530 1530,1755
can actually explode,| very bad
|非常糟糕的事情发生，

1726
00:45:07,870 --> 00:45:09,115
0,240 240,560 610,885 885,1080 1080,1245
things happen,| and then the
|然后神经网络就不交易了，

1727
00:45:09,115 --> 00:45:10,700
0,225 225,450 450,825 825,1115
neural network doesn't trade,|
|

1728
00:45:10,700 --> 00:45:11,765
0,225 225,495 495,660 660,825 825,1065
so that's also not good,|
所以这也不是什么好事，|

1729
00:45:11,765 --> 00:45:13,330
0,210 210,485 655,1065 1065,1260 1260,1565
{in,reality -}, there's a very
实际上，在把它设置得太小，设置得太大，之间有一个非常好的中间选择，

1730
00:45:13,680 --> 00:45:15,545
0,350 350,700 1020,1420 1440,1715 1715,1865
happy medium between setting it

1731
00:45:15,545 --> 00:45:16,610
0,195 195,510 510,780 780,915 915,1065
too small, setting it too

1732
00:45:16,610 --> 00:45:18,335
0,290 790,1125 1125,1365 1365,1530 1530,1725
large,| where you set it,
|你设置的地方，大到足以超过某些局部极小值之间，

1733
00:45:18,335 --> 00:45:19,490
0,270 270,570 570,855 855,1035 1035,1155
just large enough to kind

1734
00:45:19,490 --> 00:45:20,810
0,165 165,840 840,1005 1005,1155 1155,1320
of overshoot some of these

1735
00:45:20,810 --> 00:45:22,400
0,290 400,980
local minima,|
|

1736
00:45:22,470 --> 00:45:23,470
0,275 275,395 395,560 560,740 740,1000
put you into a reasonable
把你放在搜索空间的合理部分，

1737
00:45:23,520 --> 00:45:24,670
0,275 275,425 425,575 575,800 800,1150
part of the search space,|
|

1738
00:45:24,930 --> 00:45:26,180
0,290 290,455 455,605 605,880 900,1250
where then you can actually
在那里你可以真正收敛到你最关心的解决方案上。

1739
00:45:26,180 --> 00:45:27,320
0,360 360,480 480,600 600,860 880,1140
converge on the solutions that

1740
00:45:27,320 --> 00:45:29,530
0,165 165,375 375,630 630,980 1810,2210
you care most about.| But,
|但是，实际上，你是如何在实践中设定这些学习率的，

1741
00:45:29,860 --> 00:45:30,630
0,260 260,365 365,440 440,560 560,770
actually, how do you set

1742
00:45:30,630 --> 00:45:32,090
0,320 430,765 765,990 990,1170 1170,1460
these learning rates in practice,|
|

1743
00:45:32,200 --> 00:45:33,315
0,260 260,335 335,455 455,760 810,1115
how do you pick what
你如何选择理想的学习率是多少。

1744
00:45:33,315 --> 00:45:34,790
0,255 255,540 540,810 810,1110 1110,1475
is the ideal learning rate.|
|

1745
00:45:35,080 --> 00:45:36,570
0,260 260,520 690,1025 1025,1235 1235,1490
One option, and this is
一种选择，实际上这在实践中是非常常见的选择，

1746
00:45:36,570 --> 00:45:37,670
0,225 225,330 330,510 510,765 765,1100
actually a very common option

1747
00:45:37,750 --> 00:45:39,140
0,290 290,580 600,920 920,1115 1115,1390
in practice,| is to simply
|就是简单地尝试一系列学习率，

1748
00:45:39,490 --> 00:45:40,320
0,275 275,410 410,515 515,650 650,830
try out a bunch of

1749
00:45:40,320 --> 00:45:41,430
0,225 225,480 480,705 705,885 885,1110
learning rates| and see what
|看看哪种效果最好，所以试一试，比方说，一个由不同学习率组成的整个网格，你知道，训练所有这些神经网络，看看哪一个工作得最好。

1750
00:45:41,430 --> 00:45:42,900
0,300 300,495 495,740 1060,1335 1335,1470
works the best,| {so,try -}
|所以试一试，比方说，一个由不同学习率组成的整个网格，

1751
00:45:42,900 --> 00:45:43,920
0,210 210,480 480,600 600,765 765,1020
out, let's say, a whole

1752
00:45:43,920 --> 00:45:45,290
0,300 300,480 480,720 720,1020 1020,1370
grid of different learning rates|
|

1753
00:45:45,670 --> 00:45:47,040
0,400 600,860 860,1025 1025,1220 1220,1370
and, you know, train all
训练所有这些神经网络，

1754
00:45:47,040 --> 00:45:48,015
0,120 120,270 270,510 510,735 735,975
of these neural networks,| see
|看看哪一个工作得最好。

1755
00:45:48,015 --> 00:45:48,950
0,150 150,315 315,525 525,690 690,935
which one works the best.|
|

1756
00:45:49,850 --> 00:45:50,920
0,350 350,575 575,740 740,890 890,1070
But I think we can
但我觉得我们可以做更聪明的事，

1757
00:45:50,920 --> 00:45:52,020
0,210 210,405 405,555 555,720 720,1100
do something a lot smarter,

1758
00:45:52,220 --> 00:45:53,230
0,350 350,560 560,680 680,845 845,1010
right,| so what are some
|那么，我们有哪些更聪明的方法可以做到这一点，

1759
00:45:53,230 --> 00:45:54,580
0,195 195,530 550,945 945,1215 1215,1350
more intelligent ways that we

1760
00:45:54,580 --> 00:45:55,390
0,120 120,240 240,420 420,600 600,810
could do this,| instead of
|而不是详尽地尝试一大堆不同的学习率，

1761
00:45:55,390 --> 00:45:56,665
0,675 675,870 870,1035 1035,1155 1155,1275
exhaustively trying out a whole

1762
00:45:56,665 --> 00:45:57,990
0,180 180,345 345,605 625,975 975,1325
bunch of different learning rates,|
|

1763
00:45:58,430 --> 00:45:59,905
0,245 245,440 440,790 900,1220 1220,1475
can we design a learning
我们能不能设计一种学习率算法

1764
00:45:59,905 --> 00:46:01,950
0,335 385,870 870,1170 1170,1380 1380,2045
rate algorithm| that actually adapts
|来适应我们的神经网络，并适应它的图景，

1765
00:46:02,120 --> 00:46:03,715
0,290 290,470 470,770 770,1060 1350,1595
to our neural network and

1766
00:46:03,715 --> 00:46:04,975
0,360 360,465 465,695 775,1095 1095,1260
adapts to its landscape,| so
|这样它就比以前的想法更智能。

1767
00:46:04,975 --> 00:46:05,605
0,90 90,225 225,300 300,420 420,630
that it's a bit more

1768
00:46:05,605 --> 00:46:08,010
0,335 565,965 1255,1575 1575,1895 2005,2405
intelligent than that previous idea.|
|

1769
00:46:09,450 --> 00:46:11,015
0,305 305,515 515,815 815,1190 1190,1565
So this really ultimately means
所以，这实际上最终意味着，

1770
00:46:11,015 --> 00:46:12,440
0,395
that,|
|

1771
00:46:12,440 --> 00:46:14,375
0,255 255,525 525,890 1360,1695 1695,1935
the learning rate, the speed
学习率，算法信任梯度的速度

1772
00:46:14,375 --> 00:46:16,325
0,210 210,515 745,1145 1405,1770 1770,1950
at which the algorithm is

1773
00:46:16,325 --> 00:46:17,795
0,525 525,750 750,1170 1170,1320 1320,1470
trusting the gradients that it

1774
00:46:17,795 --> 00:46:19,450
0,305 535,825 825,1005 1005,1275 1275,1655
sees| is going to depend
|将取决于该位置的梯度有多大，

1775
00:46:19,530 --> 00:46:21,365
0,400 540,920 920,1295 1295,1550 1550,1835
on how large the gradient

1776
00:46:21,365 --> 00:46:23,255
0,165 165,315 315,555 555,935 1525,1890
is in that location| and
|以及我们学习的速度有多快，

1777
00:46:23,255 --> 00:46:24,605
0,270 270,510 510,780 780,1025 1075,1350
how fast we're learning,| {how,many
|有多少其他选择，抱歉，许多其他选择，

1778
00:46:24,605 --> 00:46:27,485
0,225 225,540 540,905 2155,2555 2605,2880
-} other options, and sorry,

1779
00:46:27,485 --> 00:46:29,435
0,165 165,345 345,540 540,845 1585,1950
and many other options,| that
|作为神经网络训练的一部分，

1780
00:46:29,435 --> 00:46:30,755
0,240 240,420 420,725 775,1110 1110,1320
we might have as part

1781
00:46:30,755 --> 00:46:31,835
0,240 240,510 510,675 675,855 855,1080
of training in neural networks,

1782
00:46:31,835 --> 00:46:32,765
0,255 255,375 375,540 540,660 660,930
right,| so it's not only
|所以，不仅是我们学习的速度有多快，

1783
00:46:32,765 --> 00:46:34,235
0,300 300,570 570,855 855,1085 1195,1470
how quickly we're learning,| you
|你还可以根据学习图景中的许多不同因素进行判断。

1784
00:46:34,235 --> 00:46:35,495
0,275 475,765 765,915 915,1065 1065,1260
may judge in on many

1785
00:46:35,495 --> 00:46:36,790
0,255 255,605 655,930 930,1050 1050,1295
different factors in the learning

1786
00:46:37,020 --> 00:46:37,840
0,400
landscape.|
|

1787
00:46:39,210 --> 00:46:40,990
0,275 275,530 530,1030 1110,1445 1445,1780
In fact, we've all been
事实上，我们都是我所说的这些不同的算法，

1788
00:46:41,580 --> 00:46:43,775
0,400 540,940 1020,1510 1710,1985 1985,2195
these different algorithms, that I'm

1789
00:46:43,775 --> 00:46:45,260
0,195 195,435 435,675 675,1230 1230,1485
talking about,| {these,adaptive -} learning
|这些自适应学习率算法在实践中得到了非常广泛的研究，

1790
00:46:45,260 --> 00:46:46,775
0,330 330,795 795,1065 1065,1275 1275,1515
rate algorithms have been very

1791
00:46:46,775 --> 00:46:48,995
0,335 685,1085 1165,1470 1470,1775 1885,2220
widely studied in practice,| there
|在深度学习研究社区中，有一个非常蓬勃发展的社区，

1792
00:46:48,995 --> 00:46:50,980
0,195 195,455 475,870 870,1475 1585,1985
is a very thriving community,

1793
00:46:51,300 --> 00:46:52,390
0,275 275,410 410,560 560,770 770,1090
in the deep learning research

1794
00:46:52,500 --> 00:46:54,430
0,365 365,635 635,1115 1115,1390 1530,1930
community,| that focuses on developing
|专注于开发和设计学习率自适应新的算法，

1795
00:46:54,750 --> 00:46:57,040
0,290 290,695 695,1040 1040,1540 1890,2290
and designing new algorithms for

1796
00:46:57,390 --> 00:46:59,470
0,335 335,620 620,1150 1380,1730 1730,2080
learning rate adaptation| and faster
|更快地优化像这样的大型神经网络，

1797
00:46:59,580 --> 00:47:01,490
0,560 560,890 890,1270 1290,1655 1655,1910
optimization of large neural networks

1798
00:47:01,490 --> 00:47:03,365
0,285 285,590 1270,1545 1545,1710 1710,1875
{like,these -},| and during your
|在你的实验中，

1799
00:47:03,365 --> 00:47:04,900
0,405 405,780 780,1035 1035,1245 1245,1535
labs,| you'll actually get the
|你不仅有机会尝试许多不同的自适应算法，

1800
00:47:05,160 --> 00:47:06,755
0,400 510,845 845,1055 1055,1310 1310,1595
opportunity to not only try

1801
00:47:06,755 --> 00:47:08,060
0,305 595,870 870,1020 1020,1155 1155,1305
out a lot of these

1802
00:47:08,060 --> 00:47:09,605
0,285 285,885 885,1260 1260,1425 1425,1545
different adaptive algorithms,| which you
|你可以在这里看到的，

1803
00:47:09,605 --> 00:47:10,780
0,150 150,360 360,665
can see here,|
|

1804
00:47:10,780 --> 00:47:12,100
0,300 300,585 585,780 780,900 900,1320
but also try to uncover
但也试图揭示一种模式和另一种模式的好处，

1805
00:47:12,100 --> 00:47:12,835
0,120 120,300 300,450 450,570 570,735
what are kind of the

1806
00:47:12,835 --> 00:47:14,425
0,240 240,510 510,815 1045,1350 1350,1590
patterns and benefits of one

1807
00:47:14,425 --> 00:47:15,610
0,315 315,525 525,675 675,915 915,1185
versus the other,| {and,that's -}
|我认为这将是非常有洞察力的东西，

1808
00:47:15,610 --> 00:47:16,500
0,120 120,210 210,315 315,540 540,890
going to be something that

1809
00:47:17,240 --> 00:47:18,630
0,260 260,410 410,920 920,1085 1085,1390
I think you'll find very

1810
00:47:18,650 --> 00:47:19,900
0,575 575,815 815,965 965,1085 1085,1250
insightful| as part of your
|作为你们实验的一部分。

1811
00:47:19,900 --> 00:47:21,060
0,440
labs.|
|

1812
00:47:21,330 --> 00:47:23,060
0,400 630,980 980,1280 1280,1550 1550,1730
So another key component of
所以，你们将看到的实验的另一个关键部分是，

1813
00:47:23,060 --> 00:47:24,110
0,135 135,375 375,495 495,840 840,1050
your labs that you'll see

1814
00:47:24,110 --> 00:47:24,995
0,195 195,390 390,510 510,690 690,885
is,| how you can actually
|如何将我们今天介绍的所有这些信息实际放入一张大致类似于下面的图片中，

1815
00:47:24,995 --> 00:47:26,090
0,165 165,345 345,495 495,750 750,1095
put all of this information

1816
00:47:26,090 --> 00:47:27,760
0,210 210,375 375,570 570,920 1270,1670
that we've covered today into

1817
00:47:27,900 --> 00:47:29,420
0,400 450,785 785,1070 1070,1310 1310,1520
a single picture that looks

1818
00:47:29,420 --> 00:47:30,815
0,315 315,675 675,930 930,1155 1155,1395
roughly something like this,| which
|这张图片首先定义了你的模型，在这里的顶部，

1819
00:47:30,815 --> 00:47:32,720
0,425 775,1080 1080,1385 1465,1740 1740,1905
defines your model at the

1820
00:47:32,720 --> 00:47:33,845
0,255 255,495 495,645 645,840 840,1125
first, {at,the -} top here,|
|

1821
00:47:33,845 --> 00:47:34,670
0,300 300,390 390,525 525,690 690,825
that's where you define your
这是你定义模型的地方，

1822
00:47:34,670 --> 00:47:35,525
0,210 210,405 405,510 510,675 675,855
model,| where you talked about
|你在课程的开头部分谈到了这一点。

1823
00:47:35,525 --> 00:47:36,380
0,150 150,285 285,435 435,660 660,855
this in the beginning part

1824
00:47:36,380 --> 00:47:38,100
0,120 120,225 225,470
of the lecture.|
|

1825
00:47:38,100 --> 00:47:39,435
0,165 165,440 700,1005 1005,1185 1185,1335
For every piece in your
对于模型中的每一部分，

1826
00:47:39,435 --> 00:47:40,800
0,275 415,795 795,990 990,1200 1200,1365
model,| you're now going to
|你现在都需要定义这个优化器，

1827
00:47:40,800 --> 00:47:42,600
0,180 180,435 435,720 720,1070 1150,1800
need to define this optimizer,|
|

1828
00:47:42,600 --> 00:47:43,710
0,240 240,465 465,615 615,870 870,1110
which we've just talked about,|
我们刚刚谈到了这一点，|

1829
00:47:43,710 --> 00:47:45,585
0,255 255,735 735,900 900,1220 1540,1875
{this,optimizer -} is defined together
这个优化器是和学习率一起定义的，

1830
00:47:45,585 --> 00:47:47,025
0,195 195,315 315,525 525,875 1075,1440
with a learning rate, right,|
|

1831
00:47:47,025 --> 00:47:48,210
0,300 300,585 585,795 795,945 945,1185
how quickly you want to
你想要以多快的速度优化你的损失图景，

1832
00:47:48,210 --> 00:47:50,235
0,440 640,915 915,1190 1330,1730 1780,2025
optimize your loss landscape| and
|经过许多循环，你将通过你数据集中的所有例子，

1833
00:47:50,235 --> 00:47:51,720
0,180 180,465 465,990 990,1350 1350,1485
over many loops, you're going

1834
00:47:51,720 --> 00:47:52,680
0,165 165,360 360,615 615,825 825,960
to pass over all of

1835
00:47:52,680 --> 00:47:53,685
0,150 150,420 420,660 660,795 795,1005
the examples in your data

1836
00:47:53,685 --> 00:47:54,720
0,335
set,|
|

1837
00:47:54,720 --> 00:47:56,955
0,285 285,680 1090,1490 1600,1965 1965,2235
and observe essentially how to
从本质上观察如何改善你的神经网络，这就是梯度，

1838
00:47:56,955 --> 00:47:58,065
0,180 180,330 330,600 600,975 975,1110
improve your network, that's the

1839
00:47:58,065 --> 00:47:59,835
0,485 565,855 855,1145 1165,1500 1500,1770
gradient,| and then actually improves
|然后在这些方向上改善网络，

1840
00:47:59,835 --> 00:48:00,840
0,105 105,315 315,510 510,705 705,1005
the network in those directions,|
|

1841
00:48:00,840 --> 00:48:01,890
0,255 255,465 465,675 675,855 855,1050
{and,keep -} doing that over
一遍又一遍地这样做，

1842
00:48:01,890 --> 00:48:02,960
0,165 165,315 315,465 465,690 690,1070
and over and over again,|
|

1843
00:48:03,190 --> 00:48:04,610
0,305 305,610 630,920 920,1160 1160,1420
until eventually your neural network
直到你的神经网络最终收敛到某种解决方案。

1844
00:48:04,660 --> 00:48:06,720
0,580 930,1330 1470,1760 1760,1910 1910,2060
converges to some sort of

1845
00:48:06,720 --> 00:48:07,740
0,290
solution.|
|

1846
00:48:09,730 --> 00:48:10,710
0,260 260,380 380,515 515,710 710,980
So I want to very
所以，我想非常简短地，

1847
00:48:10,710 --> 00:48:12,240
0,350 550,900 900,1125 1125,1275 1275,1530
quickly, briefly,| in the remaining
|在剩下的时间里，

1848
00:48:12,240 --> 00:48:13,725
0,255 255,390 390,525 525,800 1150,1485
time that we have,| continue
|继续谈论训练这些神经网络的实践中的技巧，

1849
00:48:13,725 --> 00:48:15,320
0,210 210,420 420,735 735,1115 1195,1595
to talk about tips for

1850
00:48:15,340 --> 00:48:16,995
0,400 600,920 920,1175 1175,1370 1370,1655
training these neural networks in

1851
00:48:16,995 --> 00:48:19,005
0,335 805,1125 1125,1445 1465,1785 1785,2010
practice| and focus on this
|并专注于这个非常强大的想法，

1852
00:48:19,005 --> 00:48:21,795
0,300 300,695 955,1355 1735,2135 2245,2790
very powerful idea| of batching
|将你的数据批量处理成所谓的小批量较小的数据片段。

1853
00:48:21,795 --> 00:48:23,895
0,165 165,455 505,905 1735,1980 1980,2100
your data into what are

1854
00:48:23,895 --> 00:48:25,940
0,275 325,645 645,1175 1315,1680 1680,2045
called {mini-batches -} of of

1855
00:48:26,110 --> 00:48:28,665
0,380 380,760 840,1160 1160,1480 2310,2555
smaller pieces of data.| {To,do
|为此，让我们重温梯度下降算法，

1856
00:48:28,665 --> 00:48:29,955
0,120 120,300 300,630 630,1125 1125,1290
-} this, let's revisit that

1857
00:48:29,955 --> 00:48:31,940
0,345 345,725 895,1350 1350,1635 1635,1985
gradient descent algorithm, right,| so
|所以在这里，我们之前讨论的这个梯度，

1858
00:48:32,020 --> 00:48:33,585
0,380 380,680 680,1205 1205,1430 1430,1565
here, this gradient that we

1859
00:48:33,585 --> 00:48:35,570
0,195 195,405 405,695 1045,1445 1585,1985
talked about before is| actually
|计算起来非常昂贵，

1860
00:48:35,740 --> 00:48:39,080
0,640 660,1690 2040,2440 2670,2930 2930,3340
extraordinarily computationally expensive to compute,|
|

1861
00:48:39,160 --> 00:48:40,515
0,350 350,725 725,1070 1070,1205 1205,1355
because it's computed as a
因为它是通过数据集中所有部分的总和来计算的，

1862
00:48:40,515 --> 00:48:42,630
0,395 985,1385 1465,1770 1770,1965 1965,2115
summation across all of the

1863
00:48:42,630 --> 00:48:44,300
0,260 610,885 885,1065 1065,1320 1320,1670
pieces in your data set,|
|

1864
00:48:45,270 --> 00:48:46,610
0,245 245,365 365,640 660,1010 1010,1340
and in most real life
在大多数现实生活或现实世界的问题中，

1865
00:48:46,610 --> 00:48:48,395
0,285 285,495 495,800 850,1250 1540,1785
or real world problems,| you
|在你的整个数据集上计算梯度是不可行的，

1866
00:48:48,395 --> 00:48:50,015
0,135 135,420 420,690 690,1005 1005,1620
know, it's simply not feasible

1867
00:48:50,015 --> 00:48:51,530
0,240 240,690 690,945 945,1290 1290,1515
to compute a gradient over

1868
00:48:51,530 --> 00:48:52,910
0,315 315,615 615,885 885,1140 1140,1380
your entire data set,| {data,sets
|如今的数据集实在太大了。

1869
00:48:52,910 --> 00:48:53,950
0,195 195,345 345,540 540,750 750,1040
-} are just too large

1870
00:48:54,330 --> 00:48:56,870
0,320 320,640 1050,1450 2130,2390 2390,2540
these days.| So you know
|所以你知道还有其他选择，

1871
00:48:56,870 --> 00:48:58,100
0,150 150,315 315,570 570,990 990,1230
there are some alternatives, right,|
|

1872
00:48:58,100 --> 00:48:59,360
0,180 180,300 300,480 480,950 1000,1260
what are the alternatives,| instead
还有其他选择吗，|不是计算整个数据集的导数或梯度，

1873
00:48:59,360 --> 00:49:01,580
0,240 240,770 940,1335 1335,2010 2010,2220
of computing the derivative or

1874
00:49:01,580 --> 00:49:03,665
0,150 150,650 1210,1500 1500,1770 1770,2085
the gradients across your entire

1875
00:49:03,665 --> 00:49:05,750
0,270 270,605 1135,1440 1440,1725 1725,2085
data set,| what if you
|而是只计算数据集中的单个示例的梯度，

1876
00:49:05,750 --> 00:49:07,250
0,360 360,690 690,825 825,1230 1230,1500
instead computed the gradient over

1877
00:49:07,250 --> 00:49:09,740
0,350 880,1230 1230,1580 1780,2180 2230,2490
just a single example in

1878
00:49:09,740 --> 00:49:10,835
0,150 150,465 465,660 660,870 870,1095
your dataas set,| just one
|就一个例子，

1879
00:49:10,835 --> 00:49:12,490
0,335 355,630 630,810 810,1115 1255,1655
example,| well, of course this,
|当然，这个，对你的梯度的估计，就是这样，

1880
00:49:12,840 --> 00:49:14,705
0,400 840,1160 1160,1340 1340,1490 1490,1865
this estimate of your gradient

1881
00:49:14,705 --> 00:49:15,905
0,150 150,285 285,390 390,635 865,1200
is going to be exactly

1882
00:49:15,905 --> 00:49:16,925
0,180 180,330 330,545 565,825 825,1020
that,| it's an estimate, it's
|这是一个估计，它将是非常嘈杂的，

1883
00:49:16,925 --> 00:49:17,920
0,105 105,195 195,285 285,435 435,995
going to be {very,noisy -},|
|

1884
00:49:18,030 --> 00:49:19,700
0,260 260,500 500,880 1110,1430 1430,1670
it may roughly reflect the
它可能粗略地反映了你整个数据集的趋势，

1885
00:49:19,700 --> 00:49:21,050
0,285 285,525 525,735 735,1035 1035,1350
trends of your entire data

1886
00:49:21,050 --> 00:49:22,355
0,350 490,840 840,1065 1065,1230 1230,1305
set,| but because it's a
|但因为它是一个非常，它只是你整个数据集的一个例子，

1887
00:49:22,355 --> 00:49:23,720
0,245 415,705 705,840 840,1080 1080,1365
very, it's only one example

1888
00:49:23,720 --> 00:49:24,740
0,225 225,405 405,570 570,765 765,1020
in fact of your entire

1889
00:49:24,740 --> 00:49:26,015
0,255 255,590 730,990 990,1125 1125,1275
data set,| it may be
|它可能会非常嘈杂。

1890
00:49:26,015 --> 00:49:27,040
0,180 180,755
very noisy.|
|

1891
00:49:29,610 --> 00:49:31,055
0,350 350,635 635,965 965,1250 1250,1445
Well, the advantage of this
这样做的好处是，

1892
00:49:31,055 --> 00:49:32,840
0,225 225,420 420,645 645,1115 1435,1785
though is that,| it's much
|在一个例子中，计算梯度明显要快得多，

1893
00:49:32,840 --> 00:49:34,790
0,315 315,540 540,950 1030,1430 1630,1950
faster to compute obviously the

1894
00:49:34,790 --> 00:49:36,520
0,510 510,810 810,1020 1020,1310 1330,1730
gradient over a single example,|
|

1895
00:49:36,690 --> 00:49:38,015
0,275 275,485 485,680 680,1010 1010,1325
because it's one example,| {so,computationally
因为这是一个例子，|所以从计算上来说，这有很大的优势，

1896
00:49:38,015 --> 00:49:40,060
0,960 960,1200 1200,1395 1395,1680 1680,2045
-} this has huge advantages,|
|

1897
00:49:40,620 --> 00:49:41,990
0,320 320,515 515,995 995,1235 1235,1370
but the downside is that
但缺点是它是非常随机的，

1898
00:49:41,990 --> 00:49:43,985
0,255 255,480 480,980 1570,1905 1905,1995
it's extremely stochastic,| that's the
|这就是为什么这种算法不叫梯度下降的原因，

1899
00:49:43,985 --> 00:49:45,065
0,165 165,345 345,570 570,915 915,1080
reason why this algorithm is

1900
00:49:45,065 --> 00:49:46,415
0,195 195,465 465,855 855,1125 1125,1350
not called gradient descent,| it's
|这现在被称为随机梯度下降。

1901
00:49:46,415 --> 00:49:48,340
0,245 265,795 795,1200 1200,1575 1575,1925
called stochastic gradient descent now.|
|

1902
00:49:49,440 --> 00:49:50,645
0,365 365,695 695,785 785,935 935,1205
Now, what's the middle ground,
现在，中间立场是什么，

1903
00:49:50,645 --> 00:49:51,770
0,330 330,540 540,735 735,1035 1035,1125
right,| instead of computing it
|与其针对数据集中的一个示例计算它，

1904
00:49:51,770 --> 00:49:52,910
0,225 225,480 480,645 645,870 870,1140
with respect to one example

1905
00:49:52,910 --> 00:49:54,080
0,165 165,285 285,495 495,830 910,1170
in your data set,| what
|不如我们计算所谓的一小批示例，

1906
00:49:54,080 --> 00:49:55,655
0,165 165,470 820,1200 1200,1425 1425,1575
if we computed what's called

1907
00:49:55,655 --> 00:49:57,070
0,195 195,405 405,750 750,1080 1080,1415
a {mini-batch -} of examples,|
|

1908
00:49:57,180 --> 00:49:58,660
0,350 350,635 635,905 905,1145 1145,1480
a small batch of examples
一小批我们可以计算梯度的例子，

1909
00:49:59,070 --> 00:50:00,395
0,400 480,755 755,905 905,1190 1190,1325
that we can compute the

1910
00:50:00,395 --> 00:50:01,910
0,375 375,665 745,1080 1080,1290 1290,1515
gradients over,| {and,when -} we
|当我们计算这些梯度时，

1911
00:50:01,910 --> 00:50:04,700
0,285 285,620 790,1400 1990,2480 2500,2790
take these gradients,| they're still
|它们的计算效率仍然很高，

1912
00:50:04,700 --> 00:50:07,445
0,860 1180,1580 1600,1860 1860,2330 2440,2745
computationally efficient to compute,| because
|因为它是一个小批量，不是太大，

1913
00:50:07,445 --> 00:50:08,540
0,240 240,360 360,555 555,825 825,1095
it's a {mini-batch -}, it's

1914
00:50:08,540 --> 00:50:09,650
0,180 180,390 390,615 615,870 870,1110
not too large,| maybe we're
|也许我们谈论的是我们数据集中数十或数百个例子的数量级，

1915
00:50:09,650 --> 00:50:10,565
0,150 150,330 330,435 435,630 630,915
talking on the order of

1916
00:50:10,565 --> 00:50:12,590
0,300 300,540 540,815 1285,1665 1665,2025
tens or hundreds of examples

1917
00:50:12,590 --> 00:50:15,280
0,225 225,360 360,920 1450,1850
in our dataset,| but,
|但是，更重要的是，

1918
00:50:15,620 --> 00:50:17,875
0,350 350,700 1320,1625 1625,1955 1955,2255
more importantly,| because we've expanded
|因为我们已经从一个例子扩展到可能一百个例子，

1919
00:50:17,875 --> 00:50:19,060
0,210 210,330 330,585 585,960 960,1185
from a single example to

1920
00:50:19,060 --> 00:50:20,785
0,120 120,270 270,525 525,920 1480,1725
maybe a hundred examples,| the
|随机性显著降低，

1921
00:50:20,785 --> 00:50:23,155
0,810 810,990 990,1325 1765,2130 2130,2370
stochasticity is significantly reduced| and
|我们的梯度的精度大大提高。

1922
00:50:23,155 --> 00:50:24,385
0,255 255,645 645,735 735,885 885,1230
the accuracy of our gradient

1923
00:50:24,385 --> 00:50:25,960
0,270 270,645 645,1025
is much improved.|
|

1924
00:50:26,310 --> 00:50:28,370
0,400 600,1000 1200,1565 1565,1790 1790,2060
So normally we're thinking of
所以通常我们考虑的是批次大小，小型批次大小，

1925
00:50:28,370 --> 00:50:29,855
0,270 270,620 700,1005 1005,1215 1215,1485
batch sizes, {mini-batch -} sizes,|
|

1926
00:50:29,855 --> 00:50:30,800
0,300 300,495 495,585 585,750 750,945
roughly on the order of
大约 100 个数据点，数十个或数百个数据点，

1927
00:50:30,800 --> 00:50:32,720
0,105 105,350 670,1020 1020,1370 1570,1920
a hundred data points, tens

1928
00:50:32,720 --> 00:50:33,995
0,240 240,510 510,750 750,945 945,1275
or hundreds of data points,|
|

1929
00:50:33,995 --> 00:50:35,900
0,255 255,515 655,960 960,1265 1525,1905
{this,is -} much faster obviously
这显然比梯度下降的计算速度快得多，

1930
00:50:35,900 --> 00:50:37,220
0,240 240,525 525,690 690,1020 1020,1320
to compute than gradient descent|
|

1931
00:50:37,220 --> 00:50:38,780
0,195 195,375 375,680 700,1100 1300,1560
and much more accurate to
比随机梯度下降的计算精度高得多，

1932
00:50:38,780 --> 00:50:40,805
0,440 670,975 975,1155 1155,1620 1620,2025
compute compared to stochastic gradient

1933
00:50:40,805 --> 00:50:41,950
0,300 300,480 480,630 630,825 825,1145
descent,| which is that single
|后者是单点的例子。

1934
00:50:42,840 --> 00:50:44,420
0,335 335,635 635,1000
single point example.|
|

1935
00:50:44,830 --> 00:50:46,760
0,335 335,670 870,1190 1190,1430 1430,1930
So this increase in gradient
所以，梯度精度的提高使我们

1936
00:50:46,840 --> 00:50:49,760
0,640 1500,1835 1835,2135 2135,2500 2520,2920
accuracy allows us| to essentially
|能够更快地收敛到我们的解，

1937
00:50:49,900 --> 00:50:51,255
0,470 470,590 590,710 710,1000 1020,1355
converge to our solution much

1938
00:50:51,255 --> 00:50:52,965
0,465 465,845 1195,1485 1485,1620 1620,1710
quicker,| than it could have
|而不是由于梯度下降的限制而在实践中可能实现的，

1939
00:50:52,965 --> 00:50:54,690
0,135 135,390 390,645 645,935 1405,1725
been possible in practice due

1940
00:50:54,690 --> 00:50:56,385
0,225 225,555 555,870 870,1365 1365,1695
to gradient descent limitations,| {it,also
|这也意味着我们可以提高我们的学习率，

1941
00:50:56,385 --> 00:50:57,800
0,195 195,360 360,665 895,1155 1155,1415
-} means that we can

1942
00:50:57,940 --> 00:50:59,355
0,335 335,575 575,830 830,1160 1160,1415
increase our learning rate,| because
|因为我们可以更有效地信任每个梯度，

1943
00:50:59,355 --> 00:51:00,315
0,135 135,315 315,570 570,795 795,960
we can trust each of

1944
00:51:00,315 --> 00:51:02,600
0,210 210,780 780,1110 1110,1445 1585,2285
those gradients much more efficiently,

1945
00:51:02,650 --> 00:51:04,380
0,395 395,800 800,1025 1025,1535 1535,1730
right,| we're now averaging over
|我们现在是一批的平均数，

1946
00:51:04,380 --> 00:51:05,760
0,180 180,470 850,1185 1185,1290 1290,1380
a batch,| it's going to
|它将比随机版本精确得多，

1947
00:51:05,760 --> 00:51:06,870
0,90 90,255 255,555 555,885 885,1110
be much more accurate than

1948
00:51:06,870 --> 00:51:08,070
0,150 150,555 555,885 885,1095 1095,1200
the stochastic version,| so we
|所以我们可以提高学习率，

1949
00:51:08,070 --> 00:51:09,030
0,195 195,405 405,555 555,750 750,960
can increase that learning rate|
|

1950
00:51:09,030 --> 00:51:10,740
0,240 240,465 465,720 720,1100 1390,1710
and actually learn faster as
也可以学得更快。

1951
00:51:10,740 --> 00:51:11,480
0,320
well.|
|

1952
00:51:12,240 --> 00:51:14,015
0,320 320,575 575,860 860,1210 1440,1775
This allows us to also
这使得我们还可以大规模并行化整个算法和计算，

1953
00:51:14,015 --> 00:51:16,475
0,645 645,1230 1230,1515 1515,1865 2095,2460
massively parallelize this entire algorithm

1954
00:51:16,475 --> 00:51:17,930
0,150 150,755 775,1080 1080,1245 1245,1455
and computation, right,| we can
|我们可以把批次分成不同的工作者，

1955
00:51:17,930 --> 00:51:19,610
0,210 210,360 360,855 855,1220 1300,1680
split up batches onto separate

1956
00:51:19,610 --> 00:51:22,150
0,380 1000,1400 1510,1830 1830,2145 2145,2540
workers| and achieve even more
|并使用 GPU 实现整个问题更显著的加速。

1957
00:51:22,260 --> 00:51:24,460
0,400 750,1385 1385,1610 1610,1850 1850,2200
significant speedups of this entire

1958
00:51:24,600 --> 00:51:26,650
0,400 540,905 905,1480 1560,1805 1805,2050
problem using GPUs.| The last
|在今天的演讲中，我非常、非常简短地想要讨论的最后一个话题是，

1959
00:51:26,670 --> 00:51:28,700
0,400 870,1130 1130,1385 1385,1730 1730,2030
topic that I very, very

1960
00:51:28,700 --> 00:51:30,410
0,330 330,645 645,975 975,1370 1420,1710
briefly want to cover in

1961
00:51:30,410 --> 00:51:32,420
0,330 330,590 1060,1425 1425,1710 1710,2010
today's lecture is| this topic
|这个话题就是过度拟合，

1962
00:51:32,420 --> 00:51:34,565
0,360 360,1170 1170,1500 1500,1770 1770,2145
of overfitting, right,| when we're
|当我们使用随机梯度下降来优化神经网络的时候，

1963
00:51:34,565 --> 00:51:36,640
0,455 565,840 840,1065 1065,1325 1675,2075
optimizing a neural network with

1964
00:51:37,320 --> 00:51:39,810
0,435 435,825 825,1220 1720,2115 2115,2490
stochastic gradient descent,| we have
|我们遇到了这个挑战，所谓的过拟合，

1965
00:51:39,810 --> 00:51:41,325
0,300 300,620 670,975 975,1275 1275,1515
this challenge of what's called

1966
00:51:41,325 --> 00:51:44,085
0,315 315,690 690,1385 2275,2565 2565,2760
{overfitting -},| overfitting looks like
|过拟合看起来大致这样，

1967
00:51:44,085 --> 00:51:45,555
0,305 355,755 805,1110 1110,1305 1305,1470
this roughly, right,| so on
|所以在左手边，

1968
00:51:45,555 --> 00:51:47,400
0,120 120,285 285,525 525,845
the left hand side,|
|

1969
00:51:47,440 --> 00:51:48,675
0,275 275,440 440,620 620,910 930,1235
we want to build a
我们想要建立一个神经网络，

1970
00:51:48,675 --> 00:51:49,800
0,270 270,525 525,810 810,1035 1035,1125
neural network,| or let's say,
|或者说，总的来说，我们想要建立一个机器学习模型，

1971
00:51:49,800 --> 00:51:50,760
0,165 165,435 435,660 660,810 810,960
in general, we want to

1972
00:51:50,760 --> 00:51:52,020
0,150 150,360 360,600 600,900 900,1260
build a machine learning model,|
|

1973
00:51:52,020 --> 00:51:54,690
0,270 270,555 555,1190 1360,1760 2320,2670
that can accurately describe some
它可以准确地描述我们数据中的一些模式，

1974
00:51:54,690 --> 00:51:56,580
0,350 400,660 660,840 840,1160
patterns in our data,|
|

1975
00:51:56,580 --> 00:51:58,500
0,330 330,710 1150,1530 1530,1770 1770,1920
but remember, ultimately we don't
但请记住，最终我们不想描述我们的训练数据中的模式，

1976
00:51:58,500 --> 00:51:59,400
0,120 120,330 330,540 540,690 690,900
want to describe the patterns

1977
00:51:59,400 --> 00:52:01,320
0,210 210,435 435,765 765,1130 1300,1920
in our training data,| {ideally\,,we
|理想情况下，我们希望定义测试数据中的模式，

1978
00:52:01,320 --> 00:52:02,325
0,120 120,285 285,540 540,780 780,1005
-} want to define the

1979
00:52:02,325 --> 00:52:03,915
0,335 475,735 735,975 975,1275 1275,1590
patterns in our test data,|
|

1980
00:52:03,915 --> 00:52:05,220
0,270 270,435 435,585 585,930 930,1305
of course, we don't observe
当然，我们不观察测试数据，我们只观察训练数据，

1981
00:52:05,220 --> 00:52:06,555
0,300 300,585 585,795 795,1005 1005,1335
test data, we only observe

1982
00:52:06,555 --> 00:52:07,905
0,300 300,600 600,965 1015,1245 1245,1350
{training,data -},| so we have
|所以，我们面临着这个挑战，从训练数据中提取模式，

1983
00:52:07,905 --> 00:52:09,470
0,180 180,435 435,780 780,1260 1260,1565
this challenge of extracting patterns

1984
00:52:09,550 --> 00:52:11,235
0,320 320,560 560,880 1110,1415 1415,1685
from training data| and hoping
|并希望它们推广到我们的测试数据。

1985
00:52:11,235 --> 00:52:12,765
0,225 225,375 375,965 1135,1365 1365,1530
that they generalize to {our,test

1986
00:52:12,765 --> 00:52:14,565
0,255 255,575 1135,1440 1440,1605 1605,1800
-} data.| So said {in,one
|所以，换一种不同的说法，

1987
00:52:14,565 --> 00:52:16,170
0,225 225,495 495,875 1195,1455 1455,1605
-} different way,| we want
|我们希望构建能够从我们的训练数据中学习表示法的模型，

1988
00:52:16,170 --> 00:52:17,100
0,150 150,315 315,570 570,795 795,930
to build models that can

1989
00:52:17,100 --> 00:52:18,750
0,165 165,860 910,1170 1170,1365 1365,1650
learn representations from our training

1990
00:52:18,750 --> 00:52:21,105
0,350 730,1065 1065,1320 1320,1640 1660,2355
data,| that can still generalize,|
|这些模型仍然可以泛化，|

1991
00:52:21,105 --> 00:52:22,335
0,345 345,585 585,795 795,1020 1020,1230
even when we show them
即使我们向它们展示全新的、没见过的测试数据片段。

1992
00:52:22,335 --> 00:52:24,410
0,255 255,525 525,1145 1225,1625 1675,2075
brand new {unseen,pieces -} of

1993
00:52:24,460 --> 00:52:26,430
0,320 320,640 1260,1610 1610,1835 1835,1970
test data.| So assume that
|假设你想构建一条直线，

1994
00:52:26,430 --> 00:52:27,105
0,105 105,225 225,360 360,510 510,675
you want to build a

1995
00:52:27,105 --> 00:52:29,790
0,275 445,840 840,1235 1705,2105 2395,2685
line,| that can describe or
|它可以描述或找到幻灯片上这些点的模式，

1996
00:52:29,790 --> 00:52:30,900
0,180 180,345 345,615 615,885 885,1110
find the patterns in these

1997
00:52:30,900 --> 00:52:31,755
0,270 270,450 450,570 570,720 720,855
points, that you can see

1998
00:52:31,755 --> 00:52:33,600
0,105 105,255 255,545 1375,1665 1665,1845
on the slide,| if you
|如果你有一个非常简单的神经网络，

1999
00:52:33,600 --> 00:52:34,920
0,210 210,420 420,675 675,990 990,1320
have a very simple neural

2000
00:52:34,920 --> 00:52:36,165
0,260 400,675 675,885 885,1095 1095,1245
network,| which is just a
|这只是一条直线。

2001
00:52:36,165 --> 00:52:38,540
0,225 225,575 895,1185 1185,1475
single line, straight line.|
|

2002
00:52:38,610 --> 00:52:41,530
0,400 510,910 1290,1690 1770,2170 2520,2920
You can describe this data
你可以以次优的方式描述这个数据，

2003
00:52:41,820 --> 00:52:43,685
0,920 920,1250 1250,1490 1490,1655 1655,1865
suboptimally, right,| because the data
|因为这里的数据是非线性的，

2004
00:52:43,685 --> 00:52:45,230
0,195 195,360 360,1025 1135,1425 1425,1545
here is non-linear,| you're not
|你不会准确地捕捉到这个数据集中的所有细微差别和微妙之处，

2005
00:52:45,230 --> 00:52:46,490
0,180 180,405 405,795 795,1035 1035,1260
going to accurately capture all

2006
00:52:46,490 --> 00:52:48,250
0,120 120,240 240,830 940,1230 1230,1760
of the nuances and subtleties

2007
00:52:48,510 --> 00:52:50,090
0,290 290,500 500,755 755,1090 1260,1580
in this data set,| that's
|这在左手边，

2008
00:52:50,090 --> 00:52:50,870
0,90 90,195 195,345 345,555 555,780
on the left hand side,|
|

2009
00:52:50,870 --> 00:52:51,590
0,180 180,315 315,480 480,615 615,720
{if,you -} move to the
如果你移到右手边，

2010
00:52:51,590 --> 00:52:53,180
0,165 165,390 390,710 1060,1380 1380,1590
right hand side,| you can
|你可以看到一个更复杂的模型，

2011
00:52:53,180 --> 00:52:54,070
0,135 135,240 240,390 390,585 585,890
see a much more complicated

2012
00:52:54,330 --> 00:52:55,895
0,400 450,755 755,935 935,1250 1250,1565
model,| but here you're actually
|但这里你表达得太多了，你太有表现力了，

2013
00:52:55,895 --> 00:52:58,120
0,365 505,1080 1080,1380 1380,1635 1635,2225
over expressive, you're too expressive,|
|

2014
00:52:58,200 --> 00:52:59,525
0,275 275,515 515,1010 1010,1160 1160,1325
and you're capturing kind of
你捕捉到了训练数据中的某种细微差别，虚假的细微差别，

2015
00:52:59,525 --> 00:53:02,465
0,180 180,810 810,1175 1285,1925 2155,2940
the nuances, the spurious nuances

2016
00:53:02,465 --> 00:53:03,890
0,300 300,525 525,795 795,1110 1110,1425
in your training data,| that
|实际上并不能表示你的测试数据。

2017
00:53:03,890 --> 00:53:06,230
0,345 345,740 880,1260 1260,1640 2050,2340
are actually not representative of

2018
00:53:06,230 --> 00:53:07,460
0,195 195,435 435,770
your test data.|
|

2019
00:53:07,790 --> 00:53:08,860
0,620 620,755 755,875 875,980 980,1070
Ideally you want to end
理想情况下，你希望模型位于中间，

2020
00:53:08,860 --> 00:53:09,595
0,120 120,270 270,405 405,570 570,735
up with the model in

2021
00:53:09,595 --> 00:53:10,980
0,90 90,335 595,870 870,1065 1065,1385
the middle,| which is basically
|这就是中间地带，

2022
00:53:11,150 --> 00:53:12,190
0,245 245,425 425,665 665,830 830,1040
the middle ground, right,| it's
|这不是太复杂，也不是太简单，

2023
00:53:12,190 --> 00:53:13,885
0,165 165,435 435,800 1300,1530 1530,1695
not too complex and it's

2024
00:53:13,885 --> 00:53:15,370
0,180 180,515 535,935 955,1275 1275,1485
not too simple,| {it,still -}
|它仍然会给你你想要的表现良好的东西，

2025
00:53:15,370 --> 00:53:16,180
0,180 180,345 345,510 510,660 660,810
gives you what you want

2026
00:53:16,180 --> 00:53:18,330
0,225 225,465 465,770 1180,1580 1750,2150
to perform well| and even
|即使你给了它全新的数据。

2027
00:53:18,350 --> 00:53:19,225
0,260 260,380 380,500 500,665 665,875
when you give it brand

2028
00:53:19,225 --> 00:53:20,575
0,180 180,455 685,960 960,1140 1140,1350
new data.| So, to address
|所以，为了解决这个问题，

2029
00:53:20,575 --> 00:53:22,450
0,210 210,515 745,1170 1170,1475 1525,1875
this problem,| let's briefly talk
|让我们简要地谈谈所谓的正则化，

2030
00:53:22,450 --> 00:53:25,870
0,350 580,960 960,1200 1200,2060 2650,3420
about what's {called,regularization -},| regularization
|正则化是一种技术，

2031
00:53:25,870 --> 00:53:27,160
0,210 210,345 345,620 790,1110 1110,1290
is a technique,| that you
|你可以将其引入到训练管道中，

2032
00:53:27,160 --> 00:53:28,660
0,260 340,740 790,1035 1035,1215 1215,1500
can introduce to your training

2033
00:53:28,660 --> 00:53:31,290
0,350 880,1170 1170,1730 1960,2295 2295,2630
pipeline| to discourage complex models
|以阻止学习复杂的模型。

2034
00:53:31,640 --> 00:53:32,900
0,275 275,470 470,790
from being learned.|
|

2035
00:53:33,320 --> 00:53:34,830
0,350 350,605 605,860 860,1115 1115,1510
Now, as we've seen before,|
现在，正如我们之前看到的，|

2036
00:53:34,910 --> 00:53:36,745
0,290 290,485 485,755 755,1120 1470,1835
this is really critical,| because
这一点非常关键，|因为神经网络是非常大的模型，

2037
00:53:36,745 --> 00:53:38,605
0,360 360,635 655,1055 1105,1500 1500,1860
neural networks are extremely large

2038
00:53:38,605 --> 00:53:40,470
0,365 475,735 735,975 975,1355 1435,1865
models,| they are extremely prone
|它们非常容易过拟合，

2039
00:53:40,760 --> 00:53:43,165
0,365 365,680 680,1150 1470,1760 1760,2405
to {overfitting -},| so regularization
|所以正则化和拥有正则化技术对神经网络的成功有着极端的影响，

2040
00:53:43,165 --> 00:53:44,760
0,195 195,405 405,735 735,975 975,1595
and having techniques for regularization

2041
00:53:45,020 --> 00:53:47,455
0,400 720,1025 1025,1660 1920,2240 2240,2435
has extreme implications towards the

2042
00:53:47,455 --> 00:53:49,120
0,275 595,885 885,1125 1125,1380 1380,1665
success of neural networks| and
|让它们超越训练数据，

2043
00:53:49,120 --> 00:53:50,785
0,210 210,435 435,1010 1030,1380 1380,1665
having them generalize beyond training

2044
00:53:50,785 --> 00:53:52,420
0,335 625,915 915,1140 1140,1365 1365,1635
data| far into our testing
|推广到我们的测试领域。

2045
00:53:52,420 --> 00:53:53,320
0,380
domain.|
|

2046
00:53:53,810 --> 00:53:55,495
0,260 260,425 425,730 990,1390 1410,1685
The most popular technique for
深度学习中最流行的正则化技术称为 dropout ，

2047
00:53:55,495 --> 00:53:57,370
0,665 895,1185 1185,1380 1380,1635 1635,1875
regularization in deep learning is

2048
00:53:57,370 --> 00:53:58,960
0,270 270,830 970,1245 1245,1425 1425,1590
called dropout,| {and,the -} idea
|dropout 的想法其实很简单，

2049
00:53:58,960 --> 00:54:00,415
0,135 135,510 510,765 765,1130 1180,1455
of dropout is is actually

2050
00:54:00,415 --> 00:54:02,500
0,210 210,545 1225,1560 1560,1950 1950,2085
very simple,| let's revisit it
|让我们通过绘制深度神经网络的图片来重温它，

2051
00:54:02,500 --> 00:54:03,910
0,210 210,465 465,690 690,980 1060,1410
by drawing this picture of

2052
00:54:03,910 --> 00:54:04,915
0,240 240,480 480,675 675,870 870,1005
deep neural networks,| that we
|我们在今天的讲座前面看到的，

2053
00:54:04,915 --> 00:54:06,240
0,150 150,390 390,690 690,1080 1080,1325
saw earlier in today's lecture,|
|

2054
00:54:06,890 --> 00:54:08,590
0,320 320,755 755,995 995,1330 1380,1700
{in,dropout -}, during training,| we
在 dropout ，在训练期间，|我们随机选择这个神经网络中的一些神经元子集，

2055
00:54:08,590 --> 00:54:11,350
0,320 460,1305 1305,1700 1840,2205 2205,2760
essentially randomly select some subset

2056
00:54:11,350 --> 00:54:12,580
0,225 225,405 405,825 825,1065 1065,1230
of the neurons in this

2057
00:54:12,580 --> 00:54:13,760
0,240 240,500
neural network,|
|

2058
00:54:13,760 --> 00:54:16,010
0,260 490,890 1060,1395 1395,1730 1900,2250
and we try to tune
我们试着用一些随机的概率把他们排除在外，

2059
00:54:16,010 --> 00:54:17,390
0,195 195,470 580,885 885,1095 1095,1380
them out with some random

2060
00:54:17,390 --> 00:54:18,860
0,600 600,855 855,1020 1020,1260 1260,1470
probabilities,| {so\,,for -} example, we
|例如，我们可以选择这部分神经元，

2061
00:54:18,860 --> 00:54:20,560
0,180 180,500 610,930 930,1410 1410,1700
can select this subset of

2062
00:54:20,700 --> 00:54:22,295
0,320 320,725 725,965 965,1130 1130,1595
of neurons,| we can randomly
|我们可以以 50% 的概率随机选择它们，

2063
00:54:22,295 --> 00:54:23,300
0,195 195,360 360,480 480,600 600,1005
select them with a {probability,of

2064
00:54:23,300 --> 00:54:25,925
0,330 330,980 1630,2030 2080,2355 2355,2625
-} 50%| and with that
|使用这个概率，

2065
00:54:25,925 --> 00:54:27,770
0,605 685,975 975,1500 1500,1695 1695,1845
probability,| we randomly turn them
|我们在训练的不同迭代中随机关闭或打开它们。

2066
00:54:27,770 --> 00:54:29,470
0,260 370,645 645,920 1090,1395 1395,1700
off or on on different

2067
00:54:29,820 --> 00:54:31,820
0,580 630,890 890,1100 1100,1450
iterations of our training.|
|

2068
00:54:32,170 --> 00:54:35,190
0,400 870,1250 1250,1595 1595,1960 2340,3020
So this is essentially forcing
所以，这本质上是迫使神经网络学习，

2069
00:54:35,190 --> 00:54:36,420
0,240 240,465 465,735 735,1005 1005,1230
the neural network to learn,|
|

2070
00:54:36,420 --> 00:54:37,185
0,210 210,360 360,525 525,660 660,765
{you,can -} think of an
你可以想到一个由不同模型组成的整体，

2071
00:54:37,185 --> 00:54:39,360
0,815 865,1170 1170,1470 1470,1865 1885,2175
ensemble of different models,| on
|在每次迭代中，

2072
00:54:39,360 --> 00:54:41,010
0,225 225,740 1000,1350 1350,1500 1500,1650
every iteration,| it's going to
|它将在迭代不同的模型，

2073
00:54:41,010 --> 00:54:42,945
0,260 700,1100 1420,1680 1680,1815 1815,1935
be exposed to kind of

2074
00:54:42,945 --> 00:54:44,820
0,105 105,345 345,725 925,1575 1575,1875
a different model internally| than
|比起它在上一次迭代中，

2075
00:54:44,820 --> 00:54:45,510
0,165 165,270 270,405 405,555 555,690
the one it had on

2076
00:54:45,510 --> 00:54:46,365
0,90 90,225 225,615 615,780 780,855
the last iteration,| so it
|所以，它必须学习如何建立内部路径来处理相同的信息，

2077
00:54:46,365 --> 00:54:47,370
0,165 165,345 345,570 570,840 840,1005
has to learn how to

2078
00:54:47,370 --> 00:54:49,970
0,260 670,1065 1065,1730 2020,2310 2310,2600
build internal pathways to process

2079
00:54:50,050 --> 00:54:51,660
0,260 260,520 570,970 1260,1505 1505,1610
the {same,information -},| and it
|而且它不能依赖从以前的迭代中学到的信息，

2080
00:54:51,660 --> 00:54:53,025
0,315 315,540 540,795 795,1125 1125,1365
can't rely on information that

2081
00:54:53,025 --> 00:54:54,680
0,195 195,515 565,885 885,1140 1140,1655
it learned {on,previous -} iterations,|
|

2082
00:54:54,940 --> 00:54:56,355
0,400 480,785 785,1040 1040,1265 1265,1415
so it forces it to
它迫使它在某种程度上捕捉到神经网络路径中的一些更深层次的含义，

2083
00:54:56,355 --> 00:54:58,065
0,150 150,425 595,995 1165,1455 1455,1710
kind of capture some deeper

2084
00:54:58,065 --> 00:54:59,415
0,330 330,615 615,810 810,1200 1200,1350
meaning within the pathways {of,the

2085
00:54:59,415 --> 00:55:00,855
0,135 135,345 345,605 1015,1290 1290,1440
-} neural network,| and this
|这可能是非常强大的，

2086
00:55:00,855 --> 00:55:02,190
0,135 135,315 315,570 570,905 985,1335
can be extremely powerful,| because
|因为第一，它显著降低了神经网络的容量，

2087
00:55:02,190 --> 00:55:03,480
0,270 270,480 480,705 705,1080 1080,1290
number one, it lowers the

2088
00:55:03,480 --> 00:55:05,240
0,320 790,1095 1095,1260 1260,1485 1485,1760
capacity of the neural network

2089
00:55:05,260 --> 00:55:07,260
0,400 960,1370 1370,1655 1655,1775 1775,2000
significantly,| {you're,lowering -} it by
|在本例中，您将它降低了大约 50% ，

2090
00:55:07,260 --> 00:55:09,110
0,330 330,950 1060,1320 1320,1515 1515,1850
roughly 50% in this example,|
|

2091
00:55:10,310 --> 00:55:11,320
0,350 350,605 605,770 770,875 875,1010
but also because it makes
这也是因为它使它们更容易训练，

2092
00:55:11,320 --> 00:55:12,670
0,165 165,390 390,630 630,920 1060,1350
them easier to train,| because
|因为在这种情况下，具有梯度的权重的数量也减少了，

2093
00:55:12,670 --> 00:55:14,440
0,165 165,360 360,680 970,1530 1530,1770
the number of weights that

2094
00:55:14,440 --> 00:55:15,745
0,255 255,765 765,930 930,1095 1095,1305
have gradients in this case

2095
00:55:15,745 --> 00:55:16,915
0,255 255,540 540,780 780,930 930,1170
is also reduced,| so it's
|所以训练它们也要快得多。

2096
00:55:16,915 --> 00:55:17,950
0,165 165,315 315,600 600,870 870,1035
actually much faster to train

2097
00:55:17,950 --> 00:55:19,660
0,290 490,795 795,1100
them as well.|
|

2098
00:55:20,200 --> 00:55:21,990
0,350 350,560 560,695 695,970 1500,1790
Now like I mentioned,| on
现在就像我提到的，|在每一次迭代中，我们随机 dropout 一组不同的神经元，

2099
00:55:21,990 --> 00:55:24,465
0,225 225,705 705,1100 1360,2090 2170,2475
every iteration we randomly {dropout

2100
00:55:24,465 --> 00:55:25,710
0,305 385,660 660,885 885,1095 1095,1245
-} a different set of

2101
00:55:25,710 --> 00:55:27,135
0,360 360,710 820,1065 1065,1185 1185,1425
neurons, right,| and that helps
|这有助于更好地概括数据。

2102
00:55:27,135 --> 00:55:29,450
0,225 225,435 435,945 945,1265 1915,2315
the data generalize better.| {And,the
|第二种正则化技术，

2103
00:55:29,650 --> 00:55:31,545
0,290 290,560 560,1235 1235,1600 1620,1895
-} second regularization techniques,| which
|是一种远远超出神经网络的非常广泛的正则化技术，

2104
00:55:31,545 --> 00:55:32,600
0,210 210,390 390,495 495,705 705,1055
is actually a very broad

2105
00:55:32,920 --> 00:55:34,905
0,755 755,1120 1200,1535 1535,1745 1745,1985
regularization techniques far beyond neural

2106
00:55:34,905 --> 00:55:37,185
0,275 745,1145 1225,1605 1605,1950 1950,2280
networks| is simply called early
|简单地称为提前停止。

2107
00:55:37,185 --> 00:55:40,310
0,365 1105,1505 1765,2055 2055,2345 2725,3125
stopping.| Now we know the
|我们知道过拟合的定义是，

2108
00:55:40,480 --> 00:55:43,230
0,400 690,1090 1350,2210 2210,2495 2495,2750
definition of overfitting is simply|
|

2109
00:55:43,230 --> 00:55:44,685
0,225 225,390 390,680 700,1080 1080,1455
when our model starts to
当我们的模型开始更多地代表训练数据而不是测试数据时，

2110
00:55:44,685 --> 00:55:46,580
0,395 625,1025 1075,1350 1350,1560 1560,1895
represent basically the training data

2111
00:55:46,600 --> 00:55:48,585
0,400 690,1090 1110,1385 1385,1640 1640,1985
more than the testing data,|
|

2112
00:55:48,585 --> 00:55:49,965
0,315 315,495 495,690 690,1200 1200,1380
that's really what overfitting comes
这确实是过度适应的核心所在，

2113
00:55:49,965 --> 00:55:50,990
0,225 225,405 405,510 510,690 690,1025
down to at {its,core -},|
|

2114
00:55:51,700 --> 00:55:52,920
0,260 260,410 410,620 620,935 935,1220
if we set aside some
如果我们把一些训练数据放在一边单独使用，

2115
00:55:52,920 --> 00:55:54,150
0,180 180,360 360,600 600,950 970,1230
of the training data to

2116
00:55:54,150 --> 00:55:55,620
0,240 240,870 870,1050 1050,1185 1185,1470
use separately,| that we don't
|我们没有在上面训练，

2117
00:55:55,620 --> 00:55:56,340
0,180 180,345 345,465 465,585 585,720
train on it,| we can
|我们可以使用某种测试数据集，合成测试数据集，

2118
00:55:56,340 --> 00:55:57,710
0,260 460,705 705,825 825,1035 1035,1370
use kind of a testing

2119
00:55:58,930 --> 00:56:01,365
0,350 350,700 1230,1775 1775,2120 2120,2435
data set, synthetic {testing,data -}

2120
00:56:01,365 --> 00:56:03,090
0,195 195,345 345,495 495,755 1465,1725
set,| in some ways, we
|在某些方面，我们可以监控我们的网络如何学习这部分没看见的数据，

2121
00:56:03,090 --> 00:56:04,440
0,165 165,470 610,900 900,1095 1095,1350
can monitor how our network

2122
00:56:04,440 --> 00:56:06,230
0,270 270,585 585,900 900,1125 1125,1790
is learning on this {unseen,portion

2123
00:56:06,370 --> 00:56:07,920
0,400 540,845 845,1130 1130,1385 1385,1550
-} of data,| so for
|例如，我们可以在整个训练过程中，

2124
00:56:07,920 --> 00:56:09,255
0,255 255,495 495,770 790,1125 1125,1335
example, we can over the

2125
00:56:09,255 --> 00:56:10,155
0,165 165,345 345,570 570,765 765,900
course of training,| we can
|我们可以绘制出网络的性能，

2126
00:56:10,155 --> 00:56:11,985
0,275 355,755 985,1335 1335,1635 1635,1830
basically plot the performance of

2127
00:56:11,985 --> 00:56:13,060
0,150 150,455
our network,|
|

2128
00:56:13,060 --> 00:56:14,155
0,195 195,405 405,585 585,810 810,1095
on both the training set
在训练集和我们的坚持测试集上，

2129
00:56:14,155 --> 00:56:15,100
0,240 240,405 405,540 540,720 720,945
as well as our held

2130
00:56:15,100 --> 00:56:17,050
0,285 285,570 570,890 1360,1695 1695,1950
out test set| and as
|随着网络的训练，

2131
00:56:17,050 --> 00:56:18,280
0,180 180,360 360,600 600,920 940,1230
the network is trained,| we're
|我们将首先看到这两个参数都有所下降，

2132
00:56:18,280 --> 00:56:19,195
0,105 105,255 255,390 390,645 645,915
going to see that first

2133
00:56:19,195 --> 00:56:20,290
0,105 105,210 210,375 375,665 745,1095
of all these both decrease,|
|

2134
00:56:20,290 --> 00:56:21,205
0,210 210,420 420,585 585,735 735,915
{but,there's -} going to be
但总有一天，

2135
00:56:21,205 --> 00:56:23,305
0,240 240,545 1135,1535 1585,1860 1860,2100
a point,| where the loss
|损失会停滞不前并开始增加，

2136
00:56:23,305 --> 00:56:26,130
0,605 1195,1595 1705,2055 2055,2405 2425,2825
plateaus and starts to increase,|
|

2137
00:56:26,180 --> 00:56:27,505
0,305 305,575 575,875 875,1130 1130,1325
the training loss will actually
训练损失开始增加，

2138
00:56:27,505 --> 00:56:28,420
0,150 150,345 345,570 570,735 735,915
start {to,increase -},| this is
|这正是你开始过拟合的时候，

2139
00:56:28,420 --> 00:56:29,485
0,255 255,450 450,660 660,885 885,1065
exactly the point where you

2140
00:56:29,485 --> 00:56:31,000
0,225 225,390 390,935 1075,1350 1350,1515
start to overfit,| because now
|因为现在你开始有，

2141
00:56:31,000 --> 00:56:33,730
0,255 255,510 510,890 1120,1520 2410,2730
you're starting to have,| sorry,
|抱歉，这是测试损失，

2142
00:56:33,730 --> 00:56:34,540
0,165 165,270 270,405 405,585 585,810
that was {the,test -} loss,|
|

2143
00:56:34,540 --> 00:56:35,680
0,195 195,375 375,660 660,930 930,1140
the test loss actually starts
测试损失实际上开始增加，

2144
00:56:35,680 --> 00:56:36,910
0,255 255,585 585,855 855,1020 1020,1230
to increase,| because now you're
|因为现在你开始过拟合你的训练数据，

2145
00:56:36,910 --> 00:56:38,010
0,180 180,330 330,690 690,825 825,1100
starting to overfit {on,your -}

2146
00:56:38,030 --> 00:56:40,620
0,400 540,940 1530,1880 1880,2210 2210,2590
training data,| this pattern basically
|这种模式会在接下来的训练中持续下去，

2147
00:56:40,730 --> 00:56:41,770
0,335 335,515 515,620 620,800 800,1040
continues for the rest of

2148
00:56:41,770 --> 00:56:42,720
0,320
training,|
|

2149
00:56:42,720 --> 00:56:44,235
0,320 670,960 960,1140 1140,1305 1305,1515
and this is the point
这就是我希望你们关注的一点，

2150
00:56:44,235 --> 00:56:44,910
0,180 180,285 285,435 435,570 570,675
that I want you to

2151
00:56:44,910 --> 00:56:46,710
0,210 210,560 670,990 990,1545 1545,1800
focus on,| {this,middlepoint -} is
|这个中间点是我们需要停止训练的地方，

2152
00:56:46,710 --> 00:56:47,640
0,240 240,435 435,585 585,735 735,930
where we need to stop

2153
00:56:47,640 --> 00:56:49,010
0,315 315,585 585,795 795,1050 1050,1370
training,| because after this point,|
|因为在这一点之后，|

2154
00:56:49,540 --> 00:56:51,150
0,400 510,845 845,1115 1115,1370 1370,1610
assuming that this test set
假设该测试集是真实测试集的有效表示，

2155
00:56:51,150 --> 00:56:53,445
0,195 195,345 345,600 600,1400 1960,2295
is a valid representation of

2156
00:56:53,445 --> 00:56:54,960
0,240 240,450 450,675 675,995 1225,1515
the true test set,| this
|这是模型的准确性只会变差的地方，

2157
00:56:54,960 --> 00:56:56,040
0,180 180,375 375,645 645,870 870,1080
is the place where the

2158
00:56:56,040 --> 00:56:57,045
0,420 420,510 510,615 615,795 795,1005
accuracy of the model will

2159
00:56:57,045 --> 00:56:58,410
0,195 195,405 405,695 1015,1260 1260,1365
only {get,worse -},| so this
|所以这就是我们想要提前停止我们的模式，

2160
00:56:58,410 --> 00:56:59,145
0,150 150,330 330,480 480,600 600,735
is where we would want

2161
00:56:59,145 --> 00:57:00,045
0,120 120,315 315,540 540,690 690,900
to early stop our model|
|

2162
00:57:00,045 --> 00:57:02,580
0,225 225,875 1105,1440 1440,1775
and regularize the performance.|
并使性能正规化的地方。|

2163
00:57:02,970 --> 00:57:03,755
0,245 245,335 335,455 455,590 590,785
And we can see that,|
我们可以看到，|

2164
00:57:03,755 --> 00:57:05,560
0,315 315,840 840,1145 1165,1485 1485,1805
stopping anytime before this point
在这一点之前的任何时候停止也是不好的，

2165
00:57:06,060 --> 00:57:07,490
0,400 510,785 785,950 950,1175 1175,1430
is also not good,| we're
|我们将生产一个不适合的模型，

2166
00:57:07,490 --> 00:57:08,660
0,120 120,330 330,525 525,675 675,1170
going to produce an underfit

2167
00:57:08,660 --> 00:57:09,650
0,290 370,630 630,765 765,885 885,990
model,| where we could have
|在这个模型中，我们可以根据测试数据得到更好的模型，

2168
00:57:09,650 --> 00:57:10,700
0,120 120,240 240,435 435,765 765,1050
had a better model on

2169
00:57:10,700 --> 00:57:12,215
0,165 165,360 360,680 1090,1350 1350,1515
the test data,| {but,it's -}
|但这是一种权衡，

2170
00:57:12,215 --> 00:57:13,070
0,165 165,360 360,525 525,690 690,855
this trade off, right,| you
|你不能停得太晚，也不能停得太早。

2171
00:57:13,070 --> 00:57:14,000
0,285 285,465 465,645 645,810 810,930
can't stop too late, and

2172
00:57:14,000 --> 00:57:14,885
0,105 105,360 360,525 525,690 690,885
you can't stop too early

2173
00:57:14,885 --> 00:57:15,960
0,240 240,545
as well.|
|

2174
00:57:17,340 --> 00:57:18,820
0,350 350,725 725,1010 1010,1175 1175,1480
So I'll conclude this lecture
所以我将通过总结这三个要点来结束这节课，

2175
00:57:19,410 --> 00:57:20,870
0,290 290,455 455,920 920,1220 1220,1460
by just summarizing these three

2176
00:57:20,870 --> 00:57:22,090
0,225 225,510 510,765 765,975 975,1220
key points,| that we've covered
|我们在今天的课上讲到的。

2177
00:57:22,170 --> 00:57:24,305
0,290 290,680 680,940 1560,1865 1865,2135
in today's lecture so far.|
|

2178
00:57:24,305 --> 00:57:25,625
0,285 285,570 570,795 795,1065 1065,1320
{So,we -} first covered these
所以我们首先介绍了所有神经网络的这些基本组成部分，

2179
00:57:25,625 --> 00:57:27,290
0,305 445,825 825,1155 1155,1395 1395,1665
fundamental building blocks of all

2180
00:57:27,290 --> 00:57:28,595
0,375 375,650 670,945 945,1125 1125,1305
neural networks,| which is the
|这就是单个神经元，感知器，

2181
00:57:28,595 --> 00:57:30,605
0,225 225,615 615,795 795,1445 1645,2010
single neuron, the perceptron,| we've
|我们已经将它们构建成更大的神经层，

2182
00:57:30,605 --> 00:57:32,110
0,195 195,405 405,690 690,1085 1105,1505
built these up into larger

2183
00:57:32,730 --> 00:57:34,550
0,500 500,965 965,1175 1175,1450 1530,1820
neural layers,| and then from
|然后从它们的神经网络和深度神经网络中，

2184
00:57:34,550 --> 00:57:35,885
0,270 270,615 615,870 870,1140 1140,1335
their neural networks and deep

2185
00:57:35,885 --> 00:57:37,730
0,270 270,545 1165,1500 1500,1665 1665,1845
neural networks,| we've learned how
|我们学习了如何训练它们，

2186
00:57:37,730 --> 00:57:39,125
0,120 120,330 330,645 645,1010 1060,1395
we can train these,| apply
|将它们应用到数据集，

2187
00:57:39,125 --> 00:57:40,595
0,270 270,510 510,765 765,1115 1135,1470
them to data sets back,|
|

2188
00:57:40,595 --> 00:57:42,350
0,465 465,645 645,935 1195,1485 1485,1755
propagate through them,| and we've
通过它们进行传播，|我们已经看到了一些端到端优化这些系统的技巧。

2189
00:57:42,350 --> 00:57:44,140
0,165 165,405 405,770 850,1250 1390,1790
seen some trips, tips and

2190
00:57:44,220 --> 00:57:46,270
0,545 545,935 935,1420 1440,1745 1745,2050
tricks for optimizing these systems

2191
00:57:46,470 --> 00:57:47,700
0,290 290,425 425,670
end to end.|
|

2192
00:57:48,010 --> 00:57:49,095
0,260 260,365 365,530 530,815 815,1085
In the next lecture,| we'll
在下一节课中，|我们将听到 Ava 关于使用 RNN 深度序列建模，

2193
00:57:49,095 --> 00:57:50,730
0,150 150,455 565,1125 1125,1395 1395,1635
hear from Ava on deep

2194
00:57:50,730 --> 00:57:52,320
0,315 315,855 855,1160 1180,1455 1455,1590
sequence modeling using {RNNs -

2195
00:57:52,320 --> 00:57:55,400
0,260 700,1035 1035,1370 2350,2715 2715,3080
-},| and specifically this very
|特别是这种非常令人兴奋的新型模型，

2196
00:57:55,420 --> 00:57:57,300
0,400 660,1010 1010,1265 1265,1535 1535,1880
exciting new type of model|
|

2197
00:57:57,300 --> 00:57:59,520
0,270 270,560 580,1160 1270,1670 1900,2220
called the transformer architecture and
称为转换器体系结构和注意机制，

2198
00:57:59,520 --> 00:58:02,355
0,315 315,1100 1600,2000 2080,2445 2445,2835
attention mechanisms,| {so,maybe -} let's
|所以，大约五分钟后我们再继续上课，

2199
00:58:02,355 --> 00:58:03,405
0,270 270,465 465,645 645,855 855,1050
resume the class in about

2200
00:58:03,405 --> 00:58:04,380
0,225 225,465 465,720 720,900 900,975
five minutes| after we have
|也许在我们有机会交换演讲者之后，

2201
00:58:04,380 --> 00:58:05,445
0,105 105,300 300,495 495,735 735,1065
a chance to swap speakers,|
|

2202
00:58:05,445 --> 00:58:07,095
0,395 865,1140 1140,1305 1305,1470 1470,1650
and thank you so much
非常感谢大家的关注。

2203
00:58:07,095 --> 00:58:07,850
0,135 135,210 210,315 315,465 465,755
for all of your attention.
