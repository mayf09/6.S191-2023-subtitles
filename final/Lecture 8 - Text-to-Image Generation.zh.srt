1
00:00:09,150 --> 00:00:12,010
谢谢邀请，

2
00:00:12,180 --> 00:00:16,720
我是街对面 Google Research 的一名研究科学家，

3
00:00:18,000 --> 00:00:22,510
今天我想谈一谈我们刚刚放在存档的一篇论文，

4
00:00:23,010 --> 00:00:24,610
这是一种新的模型，

5
00:00:24,630 --> 00:00:29,045
通过掩码生成转换器文本到图像生成，

6
00:00:29,045 --> 00:00:32,110
正如你从这些图中可以猜到的那样，

7
00:00:32,520 --> 00:00:34,060
这个模型的名字是 Muse 。

8
00:00:36,090 --> 00:00:41,830
这是我和谷歌研究中心的一群了不起的同事一起做的工作，

9
00:00:43,110 --> 00:00:45,940
一篇论文和一个在线网页。

10
00:00:48,960 --> 00:00:52,990
所以，你们中的许多人一定很熟悉文本图像生成，

11
00:00:53,430 --> 00:00:58,000
它在过去一两年里确实取得了很大的进步，

12
00:00:58,890 --> 00:01:01,570
一个问题可能是为什么要文本图像生成，

13
00:01:02,430 --> 00:01:06,790
我认为这是因为文本是一种非常自然的生成控制机制，

14
00:01:07,560 --> 00:01:12,010
我们能够通过使用文本来表达我们的想法和创造性的想法，

15
00:01:12,390 --> 00:01:17,230
允许非专家、非艺术家生成引人注目的图像，

16
00:01:17,370 --> 00:01:20,050
然后能够通过编辑工具重复它们，

17
00:01:20,850 --> 00:01:24,340
从而创造出你自己的个人艺术和想法。

18
00:01:25,700 --> 00:01:27,870
另外，非常重要的是，

19
00:01:28,430 --> 00:01:30,060
深度学习需要大量的数据，

20
00:01:30,170 --> 00:01:34,740
收集大规模的配对图文数据要可行得多，

21
00:01:35,390 --> 00:01:39,025
一个例子是 LAION-5B 数据集，

22
00:01:39,025 --> 00:01:42,150
在这些数据集上已经训练了稳定扩散等模型。

23
00:01:43,060 --> 00:01:47,055
所以我们应该认识到这些数据集中存在着各种各样的偏差，

24
00:01:47,055 --> 00:01:50,300
而减轻偏差是一个重要的研究问题。

25
00:01:51,650 --> 00:01:58,260
最后，这些模型可以利用预先训练的大型语言模型，

26
00:01:58,790 --> 00:01:59,640
这些模型非常强大，

27
00:02:00,200 --> 00:02:03,630
它们允许对文本进行极其细粒度的理解，

28
00:02:04,370 --> 00:02:07,480
词性、名词、动词、形容词，

29
00:02:07,480 --> 00:02:13,170
然后能够将这些语义概念转换为输出图像，

30
00:02:14,730 --> 00:02:21,100
这些 LLM 可以在各种具有数量级更大的文本数据的文本任务上进行预训练，

31
00:02:21,150 --> 00:02:23,140
所以你可以用纯文本数据进行预训练，

32
00:02:23,190 --> 00:02:27,490
然后使用配对数据来进行文本图像转换训练。

33
00:02:29,600 --> 00:02:30,750
那么最先进的技术是什么，

34
00:02:31,100 --> 00:02:33,150
你一定对其中的许多都很熟悉，

35
00:02:34,190 --> 00:02:38,400
来自 OpenAI 的 Dall-E2 是最早的模型之一，

36
00:02:39,080 --> 00:02:40,650
这是一个扩散模型，

37
00:02:41,480 --> 00:02:45,790
这是建立在预先训练的 CLIP 表示法基础上的，

38
00:02:46,950 --> 00:02:49,265
我没有时间逐一介绍这些术语，

39
00:02:49,265 --> 00:02:51,100
所以很抱歉，对于你们中不熟悉这些的人。

40
00:02:52,680 --> 00:02:55,660
Google 的 Imagen 也是一个扩散模型，

41
00:02:55,890 --> 00:02:59,110
建立在预先训练好的大语言模型上。

42
00:03:00,300 --> 00:03:02,890
Parti 是 Google 的另一个大规模模型，

43
00:03:03,210 --> 00:03:06,760
是一个基于潜在符号空间的自回归模型。

44
00:03:07,540 --> 00:03:13,140
Stable/Latent 扩散是 Stability AI 发布的模型，

45
00:03:13,310 --> 00:03:15,780
它是关于潜在嵌入的扩散模型。

46
00:03:17,000 --> 00:03:18,870
这里有一个例子，

47
00:03:19,220 --> 00:03:26,250
显示了在这个特定的文本提示下 DALL·E 2, Imagen 和 Muse 模型的比较，

48
00:03:27,620 --> 00:03:30,295
稍后我将重温这个例子，

49
00:03:30,295 --> 00:03:33,810
并指出不同模型的一些优缺点。

50
00:03:36,290 --> 00:03:40,110
一些图像编辑应用程序基于这些模型构建的，

51
00:03:40,580 --> 00:03:46,320
一个例子是 Personalization 通过一个名为 Dreambooth 的工具，

52
00:03:46,520 --> 00:03:48,720
这是我的一些同事写的一篇论文，

53
00:03:49,310 --> 00:03:50,610
所以这里的想法是，

54
00:03:50,690 --> 00:03:55,170
不是生成例如一只猫的图像，

55
00:03:55,460 --> 00:03:57,480
而是生成一张你的猫的图片，

56
00:03:58,040 --> 00:04:01,320
通过在你自己的猫的一些图像上微调模型，

57
00:04:01,580 --> 00:04:05,430
然后在文本指导下生成你的猫的图像。

58
00:04:06,000 --> 00:04:08,560
这实际上是被证明非常受欢迎，

59
00:04:08,910 --> 00:04:13,750
几款基于 Dreambooth 工具构建的应用程序已经登上了 App Store 的榜首，

60
00:04:14,190 --> 00:04:20,560
许多人用它生成社交媒体上自己的头像。

61
00:04:22,700 --> 00:04:28,410
另一个例子是对指令跟随进行微调，

62
00:04:28,790 --> 00:04:32,160
以便可以进行各种灵活的[自由掩码]编辑，

63
00:04:32,780 --> 00:04:36,480
例如，在这张右上角的图像上，

64
00:04:36,500 --> 00:04:38,340
我们说向天空添加烟花，

65
00:04:38,690 --> 00:04:40,705
然后模型经过微调，

66
00:04:40,705 --> 00:04:44,460
对天空和烟花等概念有了理解，

67
00:04:44,630 --> 00:04:48,420
它能够很好地遵循指令。

68
00:04:52,070 --> 00:04:57,760
所以， Muse 和我之前列出的模型有什么不同。

69
00:04:58,350 --> 00:05:01,810
首先，它既不是扩散模型，也不是自回归模型，

70
00:05:02,010 --> 00:05:07,330
虽然它与扩散和自回归模型族都有一些联系。

71
00:05:08,460 --> 00:05:09,730
它的速度非常快，

72
00:05:10,620 --> 00:05:15,550
例如，一个 512x512 的图像在 1.3 秒内生成，

73
00:05:15,780 --> 00:05:18,940
而不是 Imagen 或 Parti 的 10 秒，

74
00:05:18,990 --> 00:05:21,260
或者 Stable Diffusion 的大约 4 秒，

75
00:05:21,260 --> 00:05:22,210
在相同的硬件上。

76
00:05:23,370 --> 00:05:27,430
还有在定量评估上，比如 SLIP score 和 FID ，

77
00:05:27,480 --> 00:05:32,450
它们是衡量文本提示和图像彼此对齐程度的指标，

78
00:05:32,450 --> 00:05:33,400
这就是 CLIP score ，

79
00:05:33,995 --> 00:05:37,990
而 FID 是衡量图像质量本身，多样性和保真度，

80
00:05:38,190 --> 00:05:40,690
这个模型表现得很好。

81
00:05:42,030 --> 00:05:46,900
所以它的语义性能和质量与这些更大的模型相似，

82
00:05:46,950 --> 00:05:48,730
但推理速度要快得多，

83
00:05:49,140 --> 00:05:52,360
而且它的性能明显好于 Stable Diffusion 。

84
00:05:52,960 --> 00:05:55,585
所有这些说法在这个时间点上是正确的，

85
00:05:55,585 --> 00:05:57,210
所有这些模式都在不断改进，

86
00:05:57,500 --> 00:06:02,400
所以，下周可能会有一款表现更好的新模型。

87
00:06:03,350 --> 00:06:10,110
我们训练模型的方式支持的一些应用程序是 Zero-shot 编辑，

88
00:06:11,720 --> 00:06:15,360
我将展示一些绘画中无掩码编辑的例子，

89
00:06:15,380 --> 00:06:19,560
以移除对象和裁剪扩展图像边界。

90
00:06:22,280 --> 00:06:25,060
我在这里先简要概述一下体系结构，

91
00:06:25,060 --> 00:06:27,750
然后再一个接一个地介绍各个组件。

92
00:06:28,850 --> 00:06:32,580
Muse 主要是一个基于转换器的架构，

93
00:06:34,230 --> 00:06:37,060
对于网络的文本和图像部分，

94
00:06:37,710 --> 00:06:39,550
但我们也使用 CNN ，

95
00:06:40,320 --> 00:06:43,575
我们也用矢量量化，

96
00:06:43,575 --> 00:06:44,700
我们也使用 GAN ，

97
00:06:44,700 --> 00:06:48,350
所以我们使用了现代 CNN 的很多工具包，

98
00:06:48,790 --> 00:06:53,540
现代深度网络，使用了很多工具包。

99
00:06:54,580 --> 00:06:59,270
我们使用了图像标记，在 CNN-VQGAN 的量化潜在空间中。

100
00:07:00,120 --> 00:07:01,900
我们用掩蔽损失来训练它，

101
00:07:02,490 --> 00:07:05,285
这类似于大型语言模型中使用的掩蔽损失，

102
00:07:05,285 --> 00:07:09,700
所以，模型基本上是在学习如何重建掩码 token ，

103
00:07:10,020 --> 00:07:11,290
你传入一组 token ，

104
00:07:11,520 --> 00:07:12,815
随机掩蔽其中的一组，

105
00:07:12,815 --> 00:07:14,080
然后学习预测，

106
00:07:14,480 --> 00:07:17,030
通过使用可变的掩蔽率来实现，

107
00:07:17,030 --> 00:07:19,210
它就可以获得非常高质量的生成。

108
00:07:21,230 --> 00:07:22,350
我们有两个模型，

109
00:07:22,370 --> 00:07:27,270
一个基本模型可以生成 256x256 大小的图像，

110
00:07:27,740 --> 00:07:32,700
然后一个超分辨率模型将其放大到 512x512 。

111
00:07:35,040 --> 00:07:39,425
好的，所以第一个主要的组成部分是，

112
00:07:39,425 --> 00:07:41,140
这个预先训练好的大语言模型。

113
00:07:41,460 --> 00:07:48,130
所以我们使用了谷歌的一个语言模型训练叫做 T5-XXL 模型，

114
00:07:48,630 --> 00:07:50,650
它有大约 50 亿个参数，

115
00:07:51,690 --> 00:07:53,860
在许多文本任务上进行了训练，

116
00:07:54,060 --> 00:07:54,980
文本到文本的任务，

117
00:07:54,980 --> 00:07:58,330
比如翻译，问题回答和分类。

118
00:07:59,460 --> 00:08:01,990
当提供文本提示时，

119
00:08:03,390 --> 00:08:05,105
它通过文本编码器传递进来，

120
00:08:05,105 --> 00:08:09,220
我们得到一个维度为 4096 的向量序列，

121
00:08:09,270 --> 00:08:13,270
然后向下投影到另一个较低维度的序列，

122
00:08:13,470 --> 00:08:18,040
然后将这些文本 token 传递到网络的其余部分，

123
00:08:20,060 --> 00:08:26,340
然后，我们使用从文本到图像 token 的 cross-attention 来指导生成过程。

124
00:08:30,220 --> 00:08:33,740
下一个组件是向量量化潜在空间，

125
00:08:34,240 --> 00:08:36,950
为此，我们首先建立了一个 VQ GAN ，

126
00:08:37,720 --> 00:08:42,075
这只是一种形式的自编码器，

127
00:08:42,075 --> 00:08:45,590
在潜在空间中内置了一些向量量化。

128
00:08:46,150 --> 00:08:51,200
我们之所以使用一组量化的 token ，

129
00:08:52,030 --> 00:08:55,250
在大多数模型中，我们使用 8000 个 token ， 8192 ，

130
00:08:55,570 --> 00:08:59,150
这是服从于一个交叉熵分类型损失，

131
00:08:59,320 --> 00:09:01,970
所以，当你想要预测丢失哪个 token ，

132
00:09:02,110 --> 00:09:06,150
你只需说需要是 8192 个 token 的哪一个，

133
00:09:06,150 --> 00:09:07,640
这就是分类损失，

134
00:09:08,080 --> 00:09:12,450
我们发现，这比回归损失要有效得多，

135
00:09:12,450 --> 00:09:16,400
在那里，你可能尝试预测丢失 token 的准确像素值。

136
00:09:17,740 --> 00:09:23,150
VQGAN 具有完全卷积结构和编解码器结构，

137
00:09:23,710 --> 00:09:28,550
我们发现比基于 transformer 的 VQGAN 性能更好，

138
00:09:28,630 --> 00:09:32,360
或者 transformer 和 CNN 的混合方法。

139
00:09:34,150 --> 00:09:36,140
我们使用 16 的下采样率，

140
00:09:36,820 --> 00:09:38,030
这会产生潜在，

141
00:09:38,260 --> 00:09:41,900
所以，当你到 CNN 的编码层时，

142
00:09:42,400 --> 00:09:45,350
你会得到一个大小为 16x16 的潜在（空间），

143
00:09:45,970 --> 00:09:48,140
从 256x256 的输入。

144
00:09:48,760 --> 00:09:51,420
而超分辨率模型使用了另一个 VQGAN ，

145
00:09:52,070 --> 00:09:54,630
潜在空间是 64x64 ，

146
00:09:54,890 --> 00:09:56,400
所以这些是更大的潜在空间，

147
00:09:56,660 --> 00:10:01,530
因为我们想要在每个超分辨率潜在空间中获得更多的信息。

148
00:10:02,240 --> 00:10:12,580
VQGAN 是建立在这篇论文 MaskGIT 之上的，

149
00:10:12,580 --> 00:10:14,790
也是来自我们的团队。

150
00:10:18,950 --> 00:10:26,130
所以，我们训练一个关键组成部分是可变比率掩码的想法。

151
00:10:26,570 --> 00:10:28,170
所以，给定一组 token ，

152
00:10:28,670 --> 00:10:29,400
你就有了一个图像，

153
00:10:29,900 --> 00:10:31,620
它经过 VQ tokenizer ，

154
00:10:31,910 --> 00:10:36,000
然后你得到一个序列，比方说 196 个 token ，

155
00:10:36,470 --> 00:10:39,655
我们所做的是丢弃这些 token 中的一部分，

156
00:10:39,655 --> 00:10:41,850
然后传递到网络进行学习预测。

157
00:10:42,570 --> 00:10:44,780
所以与 LLM 不同，

158
00:10:44,780 --> 00:10:47,410
它通常有固定的 token 投放比例，

159
00:10:47,490 --> 00:10:50,830
我们使用一个变量分布，

160
00:10:51,090 --> 00:10:55,175
它偏向于非常高的值，

161
00:10:55,175 --> 00:10:57,670
大约 64% 的 token 投放，

162
00:10:58,050 --> 00:11:03,790
我们发现，这使得该网络更容易接受编辑应用程序，

163
00:11:04,290 --> 00:11:05,980
比如绘画和裁剪，

164
00:11:06,150 --> 00:11:10,150
因为它在推理时间上是可变的，

165
00:11:10,150 --> 00:11:12,180
你可以传递不同大小的掩码，

166
00:11:12,320 --> 00:11:14,400
因为它在训练期间这样做，

167
00:11:14,660 --> 00:11:16,800
所以它能够在推理时间这样做。

168
00:11:20,260 --> 00:11:21,950
好的，这是基本模型，

169
00:11:22,900 --> 00:11:28,040
它产生 256x256 大小的图像。

170
00:11:29,150 --> 00:11:33,360
这是一个基于 transformer 的模型，

171
00:11:33,530 --> 00:11:37,660
获取掩码 token ，以及文本 token ，

172
00:11:37,660 --> 00:11:40,140
这些掩码令牌是图像 token ，

173
00:11:40,910 --> 00:11:42,600
然后是文本 token ，

174
00:11:43,310 --> 00:11:45,900
所以，我们有从文本 token 到图像符号的交叉注意，

175
00:11:46,190 --> 00:11:48,720
也有图像 token 之间的自我注意，

176
00:11:49,730 --> 00:11:55,650
在训练期间，与交叉熵损失并行地预测所有 token ，

177
00:11:56,480 --> 00:12:01,050
但在推断期间，我们发现更好的做法是进行迭代调度，

178
00:12:01,670 --> 00:12:05,190
其中我们首先预测 token 的子集，

179
00:12:05,840 --> 00:12:08,340
我们选择具有最高置信度的 token ，

180
00:12:08,870 --> 00:12:11,860
将那些作为未屏蔽的 token 传回，

181
00:12:11,860 --> 00:12:16,050
并且重复该过程约 12 24 次，

182
00:12:16,250 --> 00:12:18,180
直到所有 token 都被解除屏蔽。

183
00:12:18,730 --> 00:12:22,700
我们发现，这显著提高了结果的质量。

184
00:12:26,360 --> 00:12:28,240
然后超分辨率模型，

185
00:12:28,980 --> 00:12:34,090
向上采样 256x256 到 512x512 的图像，

186
00:12:35,040 --> 00:12:37,360
重要的是，这是在 token 空间中完成的，

187
00:12:37,500 --> 00:12:41,710
通过将隐空间从 16x16 转换到 64x64 。

188
00:12:42,710 --> 00:12:47,490
我们使用来自文本嵌入的交叉注意以及低分辨率 token ，

189
00:12:48,050 --> 00:12:52,440
并将其传递到高分辨率 transformer 。

190
00:12:52,700 --> 00:12:55,260
这是我们掩蔽高分辨率 token ，

191
00:12:55,880 --> 00:12:57,750
但低分辨率 token 不被掩蔽，

192
00:12:57,770 --> 00:12:59,740
文本嵌入不被掩蔽，

193
00:12:59,740 --> 00:13:03,660
超分辨率模型被训练来预测被掩蔽的高 token 。

194
00:13:03,920 --> 00:13:06,480
所以你可以在这里看到效果，特别是在文本上，

195
00:13:07,120 --> 00:13:10,975
低分辨率输出是 256x256 ，

196
00:13:10,975 --> 00:13:12,270
很难看到文本，

197
00:13:12,740 --> 00:13:15,510
然后一旦在 token 空间中[更正]，

198
00:13:16,940 --> 00:13:18,640
你就可以阅读文本，

199
00:13:18,640 --> 00:13:22,410
仓鼠的面部特征要清晰得多，

200
00:13:23,770 --> 00:13:28,580
许多细节都在 token 空间中重构。

201
00:13:29,940 --> 00:13:31,565
我们发现的是，

202
00:13:31,565 --> 00:13:33,760
一连串的模式其实是非常重要的，

203
00:13:33,810 --> 00:13:37,120
如果你直接尝试训练一个 512x512 的模型，

204
00:13:37,840 --> 00:13:41,340
模型倾向于过于关注细节，

205
00:13:42,470 --> 00:13:44,730
而如果你首先训练一个低分辨率的模型，

206
00:13:45,110 --> 00:13:49,170
你会得到场景的整体语义或布局，

207
00:13:49,190 --> 00:13:50,940
有点像一个艺术家可能会做的事情，

208
00:13:51,170 --> 00:13:53,260
你首先把场景弄对，

209
00:13:53,260 --> 00:13:54,780
然后开始填充细节。

210
00:13:55,490 --> 00:13:59,130
那么我们研究的事情之一就是如何，

211
00:13:59,420 --> 00:14:01,315
可能增加这个[瀑布]的深度，

212
00:14:01,315 --> 00:14:03,750
从 128 到 256 到 512 ，

213
00:14:04,260 --> 00:14:06,170
看看这是否会进一步提高质量。

214
00:14:10,100 --> 00:14:12,090
所以你可以在这里看到，

215
00:14:13,610 --> 00:14:16,740
所以，这是我们基于 token 的超分辨率

216
00:14:17,660 --> 00:14:20,970
与基于扩散的基于像素的超分辨率的比较。

217
00:14:21,800 --> 00:14:24,120
所以，在左边，

218
00:14:24,710 --> 00:14:27,690
文本提示符是一只拉小提琴的兔子，

219
00:14:28,190 --> 00:14:35,190
这是两个样本，来自 256x256 分辨率的模型，

220
00:14:35,870 --> 00:14:41,100
右上角是基于 token 的超分辨率输出的结果，

221
00:14:41,390 --> 00:14:44,880
所以你可以看到音符，小提琴上的细节，

222
00:14:45,170 --> 00:14:46,590
所有这些都要清晰得多，

223
00:14:47,220 --> 00:14:51,050
但如果我们只是用这个，来做一个基于扩散的超分辨率，

224
00:14:51,220 --> 00:14:55,580
这只是像素[幻觉]，看起来就不那么好了，

225
00:14:56,130 --> 00:15:01,720
所以，我们的 token 超分辨率与基于 token 的方法配合得很好。

226
00:15:08,870 --> 00:15:13,885
另一种重要的启发式或技巧是

227
00:15:13,885 --> 00:15:15,570
所谓的无分类器指导，

228
00:15:16,400 --> 00:15:18,505
我们发现这在推理时至关重要，

229
00:15:18,505 --> 00:15:21,120
以权衡多样性和质量，

230
00:15:23,980 --> 00:15:29,750
这个技巧所做的是简单地权衡，

231
00:15:30,040 --> 00:15:35,540
抱歉，将日志从无条件生成推向文本条件生成，

232
00:15:36,280 --> 00:15:41,450
这样你可以将比例视为[推离]无条件生成的量。

233
00:15:42,130 --> 00:15:43,610
这是一个对比，

234
00:15:44,140 --> 00:15:50,630
左侧的两个图像是由指导比例为 0.5 生成的，

235
00:15:51,010 --> 00:15:54,890
所以猫的姿势和背景有更多的多样性，

236
00:15:55,330 --> 00:15:58,250
但它有更多的错误，

237
00:15:58,420 --> 00:16:00,620
例如在猫的嘴巴上，

238
00:16:01,160 --> 00:16:03,810
与右侧的两幅图像相比，

239
00:16:03,830 --> 00:16:08,010
这两幅图像的指导比例要大得多。

240
00:16:09,110 --> 00:16:09,960
所以在训练期间，

241
00:16:10,970 --> 00:16:15,565
我们以 10% 的概率丢掉一些 token ，

242
00:16:15,565 --> 00:16:18,750
抱歉，我们以 10% 的概率丢掉[训练]，

243
00:16:19,520 --> 00:16:22,200
但在推断时，我们选择了一个特定的指导标度。

244
00:16:25,970 --> 00:16:27,570
另一个技巧，

245
00:16:27,980 --> 00:16:32,280
这里有很多让图像看起来很好的技巧是一种叫做负面提示的东西，

246
00:16:33,170 --> 00:16:34,680
所以这里的想法是，

247
00:16:35,030 --> 00:16:39,505
有一些概念是我们不能在文本提示本身中说出来的，

248
00:16:39,505 --> 00:16:42,030
例如，如果你想要生成一个图像，

249
00:16:42,530 --> 00:16:44,010
但其中没有树，

250
00:16:44,390 --> 00:16:47,040
那么在文本提示中这样说就有点麻烦了，

251
00:16:47,660 --> 00:16:51,870
所以无分类器指导允许我们说，

252
00:16:52,160 --> 00:16:54,910
生成这个场景，但其中没有树，

253
00:16:54,910 --> 00:16:57,960
我们通过负面提示来做到这一点。

254
00:16:58,370 --> 00:16:59,310
这里有一个例子，

255
00:17:00,230 --> 00:17:04,230
文本提示说 Salvador Dali 的赛博朋克森林，

256
00:17:04,460 --> 00:17:05,130
所以这就产生了，

257
00:17:06,220 --> 00:17:10,010
这是使用该提示生成的两个示例，

258
00:17:10,540 --> 00:17:12,435
然后我添加了一个负面提示，

259
00:17:12,435 --> 00:17:13,880
我说我不想要树，

260
00:17:13,900 --> 00:17:15,650
我不想要绿色和模糊，

261
00:17:15,880 --> 00:17:17,750
这是我提供的文本提示，

262
00:17:18,250 --> 00:17:20,480
然后它会生成其他类型的样式，

263
00:17:21,400 --> 00:17:23,240
似乎遵守负面提示。

264
00:17:24,830 --> 00:17:26,850
所以这是一个非常有用的技巧，

265
00:17:28,960 --> 00:17:32,630
用来生成更接近你脑海中所想的图像。

266
00:17:38,430 --> 00:17:38,980
这是，

267
00:17:39,240 --> 00:17:43,270
这是我之前在推理时提到的迭代解码，

268
00:17:43,830 --> 00:17:45,070
而且你可以看到，

269
00:17:45,750 --> 00:17:52,120
多步解码 token 对于高质量的生成非常有帮助，

270
00:17:52,830 --> 00:17:56,290
所以，这有一系列的[去屏蔽]步骤，

271
00:17:56,340 --> 00:18:00,760
它们生成高达 18 个步骤的 token ，

272
00:18:00,900 --> 00:18:05,050
并且输出质量有了稳步的提高，

273
00:18:05,420 --> 00:18:07,570
我们的基本模式使用 24 个步骤，

274
00:18:07,950 --> 00:18:10,570
超分辨率模式使用 8 个步骤，

275
00:18:12,060 --> 00:18:14,410
但这些步骤的数量明显低于

276
00:18:14,700 --> 00:18:18,070
扩散模型所需的 50 或 1000 个步骤，

277
00:18:18,540 --> 00:18:22,570
这就是我们方法速度最重要的原因之一，

278
00:18:23,220 --> 00:18:26,540
所以我们需要的向前模型数量减少了十分之一，

279
00:18:26,540 --> 00:18:28,330
我们需要通过模型。

280
00:18:29,530 --> 00:18:33,980
然而，现在有一些渐进[净化]的想法，

281
00:18:34,900 --> 00:18:38,810
它可能会减少扩散模型的步骤，

282
00:18:39,010 --> 00:18:45,620
我们希望利用它来进一步减少我们的采样步骤，

283
00:18:48,830 --> 00:18:50,040
好的，我们做了一些，

284
00:18:50,840 --> 00:18:52,060
所以现在来评估一下，

285
00:18:52,060 --> 00:18:56,100
这是对整个模型的快速浏览。

286
00:18:57,470 --> 00:18:59,230
所以，我们做了一些定性的评估，

287
00:18:59,230 --> 00:19:05,340
我们从[]论文中提取了 1650 条提示，

288
00:19:05,450 --> 00:19:10,060
从我们的模型和稳定扩散模型生成了图像，

289
00:19:10,060 --> 00:19:14,280
并将其发送给评分员来回答问题，

290
00:19:14,840 --> 00:19:15,810
这两张图像中，

291
00:19:16,670 --> 00:19:17,635
一张来自我们的模型，

292
00:19:17,635 --> 00:19:18,760
一张来自稳定扩散，

293
00:19:18,760 --> 00:19:20,580
哪一张更符合提示。

294
00:19:21,520 --> 00:19:24,920
评分者在 70% 的时间里喜欢我们的模型，

295
00:19:25,960 --> 00:19:29,060
相比之下， 25% 的时间是稳定扩散的，

296
00:19:29,320 --> 00:19:34,170
剩下的几个百分点它们不感兴趣，

297
00:19:34,170 --> 00:19:36,290
所以我们把它们从这块移走了。

298
00:19:39,020 --> 00:19:42,880
我们还对模型的各种性质做了一些定性的评价，

299
00:19:42,880 --> 00:19:45,420
它在多大程度上尊重基数之类的东西。

300
00:19:45,960 --> 00:19:48,860
所以，所以这里是关于计数，

301
00:19:49,120 --> 00:19:51,770
所以我们有三头大象站在一起，

302
00:19:52,210 --> 00:19:53,210
我们有三只大象，

303
00:19:54,670 --> 00:19:57,200
四个酒瓶，一，二，三，四，

304
00:19:57,940 --> 00:20:01,250
一个小足球在三个黄色网球前，

305
00:20:01,510 --> 00:20:04,060
所以，似乎遵守所有这些，

306
00:20:05,010 --> 00:20:09,040
然而，当计数超过 6 或 7 个时，

307
00:20:09,480 --> 00:20:11,620
模型往往会开始出错。

308
00:20:13,860 --> 00:20:16,150
这里我们看到不同的风格，

309
00:20:16,500 --> 00:20:19,265
这是一幅衣着考究的浣熊的肖像，

310
00:20:19,265 --> 00:20:21,070
伦勃朗风格的油画，

311
00:20:22,390 --> 00:20:26,840
波普艺术风格和中国水墨画风格。

312
00:20:30,320 --> 00:20:33,990
其他的评估是关于几何构图，

313
00:20:34,430 --> 00:20:37,920
三个小黄盒在一个大蓝盒上，

314
00:20:39,200 --> 00:20:43,660
一个有红丝带的大礼物在圣诞树左边，

315
00:20:44,040 --> 00:20:44,620
等等。

316
00:20:46,130 --> 00:20:47,130
我们发现，

317
00:20:47,210 --> 00:20:50,160
如果你想要渲染文本，

318
00:20:50,690 --> 00:20:51,910
那么它就做得很好，

319
00:20:51,910 --> 00:20:55,830
如果一个词或两个词的文本中没有太多的词，

320
00:20:56,780 --> 00:20:58,500
模型能够很好地呈现它们。

321
00:20:59,840 --> 00:21:06,900
我们还可以提供非常长的、详细的提示，

322
00:21:07,700 --> 00:21:09,220
这里有一个例子，

323
00:21:09,220 --> 00:21:11,640
一个展示莫奈画作的美术馆，

324
00:21:11,660 --> 00:21:13,110
美术馆被淹了，

325
00:21:13,310 --> 00:21:16,260
机器人正在使用划桨板在美术馆里转悠。

326
00:21:16,310 --> 00:21:20,700
这种力量很大程度上来自语言模型本身，

327
00:21:20,930 --> 00:21:22,530
它非常非常强大，

328
00:21:23,090 --> 00:21:28,530
能够在[嵌入]空间中表示这些概念中的每一个，

329
00:21:29,000 --> 00:21:32,220
我们所能做的就是将其映射到像素。

330
00:21:34,150 --> 00:21:35,115
这仍然令人惊叹，

331
00:21:35,115 --> 00:21:36,015
这仍然是令人惊讶的，

332
00:21:36,015 --> 00:21:39,590
我们可以从文本提示中获得这样的输出，

333
00:21:40,860 --> 00:21:43,060
这里有一些失败的案例，就像我提到的那样，

334
00:21:45,670 --> 00:21:49,710
这里我们要求模型呈现很长的单词，

335
00:21:49,710 --> 00:21:51,980
但它做得不是很好，

336
00:21:53,740 --> 00:21:58,410
十个酒瓶，它停在一，二，三，四，五，六，七，

337
00:21:59,370 --> 00:22:00,100
诸若此类。

338
00:22:04,250 --> 00:22:08,010
这是与其他最先进模型的主观比较，

339
00:22:08,990 --> 00:22:11,340
所以可以说一件事是，

340
00:22:12,340 --> 00:22:16,190
在这种情况下，在我看来，评估并不是很可靠，

341
00:22:17,260 --> 00:22:18,740
因为根据定义，

342
00:22:18,880 --> 00:22:23,355
我们要求的文本提示和样式通常不是自然的，

343
00:22:23,355 --> 00:22:25,700
我们想要各种款式的混合搭配，

344
00:22:26,350 --> 00:22:29,925
因此，在我看来，一个重要的开放研究问题是，

345
00:22:29,925 --> 00:22:32,910
你如何评价模型 a 比模型 b 更好，

346
00:22:32,910 --> 00:22:35,270
除了只看一些结果，

347
00:22:36,190 --> 00:22:41,340
我想这是一个非常有趣和开放的问题。

348
00:22:42,200 --> 00:22:42,720
所以在这里，

349
00:22:43,070 --> 00:22:48,000
所以在这里，我通过底部的例子指出一些事情，

350
00:22:48,410 --> 00:22:50,220
这是一只彩虹色的企鹅，

351
00:22:50,360 --> 00:22:52,885
DALL-E 2 在生成企鹅，

352
00:22:52,885 --> 00:22:55,080
但在颜色方面做得不是很好，

353
00:22:56,270 --> 00:23:01,380
而 Imagen 和 MUSE 模型似乎能够遵守[]，

354
00:23:01,940 --> 00:23:04,140
我们认为这可能是因为，

355
00:23:05,210 --> 00:23:08,460
DALL-E 2 模型依靠的是 CLIP embedding ，

356
00:23:08,570 --> 00:23:10,440
这可能会丢失一些细节。

357
00:23:17,120 --> 00:23:20,020
我们对我们已有的指标做了一些量化评估，

358
00:23:20,020 --> 00:23:21,390
也就是 FID 和 CLIP ，

359
00:23:21,800 --> 00:23:24,330
在一个名为 CC3M 的数据集上，

360
00:23:25,100 --> 00:23:27,870
并与其他一些模型进行了比较，

361
00:23:28,340 --> 00:23:31,140
包括扩散模型和自回归模型。

362
00:23:32,100 --> 00:23:35,290
对于 FID ，分数越低越好，

363
00:23:35,670 --> 00:23:37,300
对于 CLIP ，分数越高越好。

364
00:23:38,260 --> 00:23:38,780
所以，总体而言，

365
00:23:40,050 --> 00:23:44,650
我们似乎在这两个指标上都取得了最好的成绩。

366
00:23:47,420 --> 00:23:49,705
这是在 COCO 上的评估，

367
00:23:49,705 --> 00:23:52,480
与许多最先进的模式比较，

368
00:23:52,480 --> 00:23:55,530
比如 DALL-E, DALL-E 2, Imagen, Parti ，

369
00:23:58,160 --> 00:24:03,160
我们几乎与 Parti-20B 模型是一样好的，

370
00:24:05,550 --> 00:24:08,440
只是在 FID 得分上略差一些，

371
00:24:09,060 --> 00:24:11,680
但在 CLIP 得分上要好得多，

372
00:24:13,500 --> 00:24:15,410
所以，这很好，

373
00:24:16,090 --> 00:24:20,120
这意味着我们能够尊重文本提示符的语义，

374
00:24:20,170 --> 00:24:21,600
比那些模型要好，

375
00:24:21,600 --> 00:24:23,660
如果你相信 CLIP 的评分的话。

376
00:24:25,930 --> 00:24:30,770
最后，在 TPU-v4 硬件运行时上，

377
00:24:31,450 --> 00:24:33,830
这里是模型，

378
00:24:34,450 --> 00:24:38,720
生成的分辨率和所用的时钟时间，

379
00:24:39,070 --> 00:24:42,980
大部分计算都是在超分辨率下进行的，

380
00:24:43,720 --> 00:24:45,770
基础模型需要 0.5 秒，

381
00:24:46,390 --> 00:24:49,520
然后它需要 0.8 秒来做超分辨率，

382
00:24:49,750 --> 00:24:51,830
总共 1.3 秒。

383
00:24:54,750 --> 00:25:00,370
这里有一些由模型启用的编辑示例，

384
00:25:01,350 --> 00:25:03,130
左边是一张真实的输入图像，

385
00:25:03,570 --> 00:25:07,030
我们在图像的一部分绘制了一个蒙版，

386
00:25:07,620 --> 00:25:10,780
然后要求模型使用文本指导提示来填充它，

387
00:25:11,310 --> 00:25:13,510
一只有趣的充气黄色大鸭子，

388
00:25:15,030 --> 00:25:15,970
热气球

389
00:25:17,550 --> 00:25:21,100
和未来主义、流线型的现代建筑。

390
00:25:22,760 --> 00:25:24,660
所有这些都是零次学习的，

391
00:25:24,800 --> 00:25:26,610
我们没有对模型进行任何微调，

392
00:25:27,110 --> 00:25:30,220
但我们使用变量掩码进行训练

393
00:25:30,220 --> 00:25:33,210
允许它开箱即用地进行调整。

394
00:25:35,900 --> 00:25:39,720
这是另一个我们扩图的例子，

395
00:25:39,950 --> 00:25:42,660
所以这里的遮罩是剩下的，

396
00:25:43,250 --> 00:25:46,860
我们只想保留这个人，

397
00:25:46,970 --> 00:25:50,250
把图像的其余部分替换掉，

398
00:25:50,980 --> 00:25:56,190
提供了文本引导提示，以填充该区域的其余部分，

399
00:25:56,420 --> 00:25:57,750
伦敦的天际线，

400
00:25:58,540 --> 00:26:01,370
一朵野花盛开在雷尼尔山，

401
00:26:01,840 --> 00:26:03,140
在土星环上。

402
00:26:06,180 --> 00:26:09,790
我们还可以做其他的编辑，

403
00:26:09,810 --> 00:26:13,600
我们在那里拍了一张真实的照片，

404
00:26:14,280 --> 00:26:15,530
有一个消极的提示，

405
00:26:15,530 --> 00:26:16,930
一个穿着 T 恤的人，

406
00:26:17,220 --> 00:26:19,060
然后这些是积极的提示，

407
00:26:19,140 --> 00:26:22,420
做各种款式的转换，

408
00:26:23,550 --> 00:26:25,810
在 T 恤上。

409
00:26:31,050 --> 00:26:33,860
这里有一些无遮罩编辑的例子，

410
00:26:33,860 --> 00:26:36,880
上面一行是输入图像，

411
00:26:37,620 --> 00:26:40,450
底部是转换后的输出，

412
00:26:40,860 --> 00:26:45,160
它只依赖于文本和文本与图像之间的注意力，

413
00:26:45,330 --> 00:26:46,450
来进行各种更改。

414
00:26:47,420 --> 00:26:50,710
所以，比如说，

415
00:26:50,790 --> 00:26:53,060
我们说一只柴犬，

416
00:26:53,060 --> 00:26:57,940
这个模型把猫变成柴犬，

417
00:26:59,300 --> 00:27:01,290
一只狗嘴里叼着一只橄榄球球，

418
00:27:01,730 --> 00:27:03,420
所以这里，狗本身发生了变化，

419
00:27:03,830 --> 00:27:07,080
然后，这个球变成了橄榄球，

420
00:27:07,820 --> 00:27:09,150
一篮子橙子，

421
00:27:09,590 --> 00:27:13,405
模型能够保持场景的大体构图，

422
00:27:13,405 --> 00:27:16,110
只需将苹果更改为橙子，

423
00:27:16,760 --> 00:27:19,260
篮子的材质也有了一些变化，

424
00:27:19,370 --> 00:27:21,990
但大部分成分都保持不变。

425
00:27:23,840 --> 00:27:26,010
在这里，它能够改变，

426
00:27:26,060 --> 00:27:31,500
只需让猫打哈欠，而不改变场景的组成。

427
00:27:32,330 --> 00:27:33,910
我们正在探索的一件事是，

428
00:27:33,910 --> 00:27:36,570
如何进一步控制模型，

429
00:27:37,070 --> 00:27:42,600
能够在不影响其余部分的情况下调整图像的特定部分。

430
00:27:45,530 --> 00:27:49,920
是的，这是一个迭代编辑的例子，

431
00:27:50,450 --> 00:27:56,610
上面和右上角提供了图像，

432
00:27:56,660 --> 00:27:58,770
然后在右下角是输出，

433
00:27:59,930 --> 00:28:03,510
一个牛角包，旁边是一杯带有花朵艺术的拿铁。

434
00:28:04,160 --> 00:28:05,380
这些是，

435
00:28:06,540 --> 00:28:08,830
我们进行了一百个步骤的编辑，

436
00:28:09,720 --> 00:28:12,130
逐步调整 token ，

437
00:28:12,390 --> 00:28:17,320
这些是推理序列的不同迭代的结果，

438
00:28:18,300 --> 00:28:19,205
所以你可以看到，

439
00:28:19,205 --> 00:28:20,675
它从蛋糕和拿铁开始，

440
00:28:20,675 --> 00:28:25,960
然后逐渐将蛋糕转变为介于蛋糕和牛角面包之间的东西，

441
00:28:26,040 --> 00:28:28,420
最后，它看起来像牛角包，

442
00:28:28,920 --> 00:28:33,250
同样，拿铁的艺术也从一颗心变成了一朵花，

443
00:28:34,380 --> 00:28:40,280
有点某种潜在空间中的一种[插补]。

444
00:28:43,150 --> 00:28:45,170
所以，由于模型的速度，

445
00:28:45,580 --> 00:28:48,290
这里存在一些交互编辑的可能性，

446
00:28:48,430 --> 00:28:50,220
所以我将播放这个短片，

447
00:28:50,220 --> 00:28:55,640
展示了我们如何能够与模型进行交互工作。

448
00:29:25,040 --> 00:29:26,310
所以这是真实的时间，

449
00:29:26,570 --> 00:29:29,250
也就是说，它没有加速或减速，

450
00:29:39,275 --> 00:29:41,950
这不是完美的，但你可以看到这个想法。

451
00:29:51,750 --> 00:29:56,180
对于我们来说，下一步是提高分辨率质量，

452
00:29:56,500 --> 00:29:58,670
处理细节，如渲染文本，

453
00:30:00,280 --> 00:30:04,160
探测文本和图像之间的交叉关注以实现更多控制，

454
00:30:04,750 --> 00:30:05,720
探索应用程序。

455
00:30:06,850 --> 00:30:11,980
所以，是的，论文和网页在这里列出，

456
00:30:12,060 --> 00:30:13,900
我很乐意回答问题。

457
00:30:21,570 --> 00:30:22,690
太好了，非常感谢，

458
00:30:22,920 --> 00:30:25,150
也许我有一个问题，只是为了让事情开始，

459
00:30:25,680 --> 00:30:30,250
我很好奇，在你看来，什么是 MUSE 做出的最重要的贡献，

460
00:30:30,900 --> 00:30:33,430
特别是实现令人印象深刻的加速结果，

461
00:30:33,930 --> 00:30:35,795
因为与过去的方法相比，

462
00:30:35,795 --> 00:30:37,175
这真的是一个巨大的差距，

463
00:30:37,175 --> 00:30:37,715
所以我很好奇，

464
00:30:37,715 --> 00:30:41,680
在你看来，在众多的贡献中，这是什么样子的。

465
00:30:41,850 --> 00:30:44,320
是的，主要是并行解码，

466
00:30:44,430 --> 00:30:45,970
所以在自回归模型中，

467
00:30:46,780 --> 00:30:49,850
根据定义，你一次只能解码一个 token ，

468
00:30:50,200 --> 00:30:50,895
所以你必须，

469
00:30:50,895 --> 00:30:54,390
让我们假设图像由 196 个 token 组成，

470
00:30:54,390 --> 00:30:56,270
然后，你将执行 196 个步骤，

471
00:30:56,830 --> 00:30:58,440
因为你要取消屏蔽一个 token ，

472
00:30:58,440 --> 00:31:01,280
将其传递回第二个，依此类推，

473
00:31:01,630 --> 00:31:05,985
在扩散模型中，你要做的是一步一步地消除噪声，

474
00:31:05,985 --> 00:31:07,910
是的，你从纯噪音开始，

475
00:31:08,620 --> 00:31:10,820
通过网络传递，得到一些东西，

476
00:31:10,930 --> 00:31:12,920
然后把它传递回来，重复这个过程，

477
00:31:14,020 --> 00:31:16,790
而这一过程需要相当缓慢，

478
00:31:17,610 --> 00:31:19,480
否则，它就会崩溃，

479
00:31:20,280 --> 00:31:22,670
很多研究都是关于如何加快这一进程的，

480
00:31:23,340 --> 00:31:24,665
所以，这需要成千上万的步骤，

481
00:31:24,665 --> 00:31:25,930
所以，如果你有一个类似的模型，

482
00:31:26,580 --> 00:31:29,650
只需要做一些基本的向前[]，

483
00:31:29,910 --> 00:31:32,830
所以，我们所做的是进行并行解码，

484
00:31:33,300 --> 00:31:35,700
一次一个 token ，

485
00:31:35,700 --> 00:31:38,090
你一次做 n 个 token ，

486
00:31:38,260 --> 00:31:39,970
然后如果它是固定的，

487
00:31:39,970 --> 00:31:42,480
你需要 196 x n 步，

488
00:31:44,300 --> 00:31:45,420
我们的想法是，

489
00:31:46,080 --> 00:31:47,520
如果你使用高置信度 token ，

490
00:31:47,520 --> 00:31:51,380
那么它们在每一步都可能是有条件的相互独立的，

491
00:31:52,000 --> 00:31:56,090
我们可以，预测它们中的每一个，而不会影响另一个，

492
00:31:56,720 --> 00:31:57,840
所以，这似乎是站得住脚的。

493
00:32:00,430 --> 00:32:00,830
有意思。

494
00:32:09,900 --> 00:32:15,760
提示允许你导航图像本身的潜在空间，

495
00:32:16,140 --> 00:32:19,240
你有没有做过任何分析或看过，

496
00:32:19,530 --> 00:32:22,990
提示与方向的关系，

497
00:32:23,910 --> 00:32:26,350
或者速度或任何其他指标，

498
00:32:26,370 --> 00:32:31,360
你可以在潜在表征探索中看到的。

499
00:32:32,400 --> 00:32:33,365
是的，这是个好问题，

500
00:32:33,365 --> 00:32:35,920
我们还没有进行非常彻底的调查，

501
00:32:37,050 --> 00:32:38,080
我们看到的是，

502
00:32:38,700 --> 00:32:39,770
我这里没有，

503
00:32:39,770 --> 00:32:42,070
但我们看到了交叉注意映射，

504
00:32:42,900 --> 00:32:47,990
在文本嵌入和生成图像之间，

505
00:32:48,010 --> 00:32:49,700
你可以看到的是，

506
00:32:50,140 --> 00:32:54,600
它做的是你可能会想到的，

507
00:32:54,600 --> 00:32:59,270
那就是不同的名词和图像中的物体之间存在交叉注意，

508
00:32:59,740 --> 00:33:02,535
当你有一个动词连接两个宾语时，

509
00:33:02,535 --> 00:33:05,120
它似乎会突出显示这两个宾语，

510
00:33:05,620 --> 00:33:07,190
有点像是在注意这一点，

511
00:33:08,120 --> 00:33:10,920
所以这是一个层面的探索，

512
00:33:11,300 --> 00:33:14,190
但我认为我们需要做更多的（工作），

513
00:33:14,800 --> 00:33:18,600
如果我们在潜在空间里走来走去，结果是什么图像，

514
00:33:18,740 --> 00:33:21,660
如果我们在两个文本提示之间移动，会发生什么，

515
00:33:21,950 --> 00:33:23,335
我们还不知道，

516
00:33:23,335 --> 00:33:26,490
潜在空间平坦或有突变，

517
00:33:27,880 --> 00:33:31,560
编辑表明这里有一些[流畅]，

518
00:33:32,240 --> 00:33:32,980
随着你的迭代，

519
00:33:33,510 --> 00:33:34,760
但这是一个固定的提示，

520
00:33:34,760 --> 00:33:35,980
而不是不断变化的提示，

521
00:33:37,350 --> 00:33:38,590
所以我认为我们需要做更多的工作。

522
00:33:42,230 --> 00:33:43,020
还有其他问题吗？

523
00:33:56,490 --> 00:34:00,910
是的，我对结果的基数部分有个问题，

524
00:34:01,050 --> 00:34:02,885
其中一个失败案例表明，

525
00:34:02,885 --> 00:34:04,205
模型不能处理，

526
00:34:04,205 --> 00:34:06,620
如果给它超过六到七件相同的物品，

527
00:34:06,620 --> 00:34:09,140
但有时，当你没有在提示符中指定任何内容时，

528
00:34:09,140 --> 00:34:12,970
它会自动生成比项目数更多的内容，

529
00:34:12,990 --> 00:34:14,420
你知道为什么会破坏，

530
00:34:14,420 --> 00:34:17,000
当你给六、七或八个。

531
00:34:17,500 --> 00:34:19,130
我想，这是我的感觉，

532
00:34:19,270 --> 00:34:21,120
再说一次，我们没有分析，

533
00:34:21,120 --> 00:34:25,310
这只是因为数据中更小的数字更常见。

534
00:34:25,450 --> 00:34:26,330
啊，明白了。

535
00:34:27,010 --> 00:34:30,230
大多数图像只会有几个，两个，三个，四个，

536
00:34:30,940 --> 00:34:35,510
然后模型就从来没有见过十或二十，

537
00:34:36,040 --> 00:34:39,000
还有，我们训练它的方式也不一样，

538
00:34:39,000 --> 00:34:42,140
不存在优雅地从十个退化到许多的概念，

539
00:34:43,000 --> 00:34:44,870
可能只会说，好的，一群人，

540
00:34:45,040 --> 00:34:48,700
然后你产生比如三十个人，

541
00:34:50,400 --> 00:34:51,370
我想我们需要更多，

542
00:34:51,570 --> 00:34:54,130
已经有一些论文试图解决这个问题，

543
00:34:55,350 --> 00:34:58,210
我认为，仅仅通过丰富数据就可以了，

544
00:34:59,040 --> 00:35:01,130
我认为还需要更聪明的解决方案。

545
00:35:06,380 --> 00:35:11,880
当你未指定请求的背景时，

546
00:35:13,110 --> 00:35:15,010
为什么它，

547
00:35:15,210 --> 00:35:20,770
是否必须随机选择有限数量的背景，

548
00:35:21,380 --> 00:35:24,330
或者有没有一种生成器，

549
00:35:24,710 --> 00:35:27,010
它是如何工作的，

550
00:35:27,010 --> 00:35:29,820
当你完全不指定背景时。

551
00:35:30,020 --> 00:35:31,020
是的，这是个很好的问题，

552
00:35:31,310 --> 00:35:32,320
所以我们做的一件事是，

553
00:35:32,320 --> 00:35:34,360
在文本提示符中输入无意义的话，

554
00:35:34,360 --> 00:35:35,190
然后看看会发生什么，

555
00:35:35,630 --> 00:35:37,890
它似乎只会生成随机的场景，

556
00:35:38,700 --> 00:35:42,705
比如山和海滩等等，

557
00:35:42,705 --> 00:35:44,060
它不会产生无意义的图，

558
00:35:44,440 --> 00:35:48,980
所以，我们认为潜在空间中的[编码本]是由这类背景主导的，

559
00:35:49,520 --> 00:35:54,670
不知何故，当你通过解码器时，这些东西就会被输入，

560
00:35:55,360 --> 00:35:59,360
所以，是的，我没有比这更好的答案了。

561
00:36:02,910 --> 00:36:04,150
嗨，谢谢你的演讲，

562
00:36:05,370 --> 00:36:08,410
我认为无遮罩的画画非常有趣，

563
00:36:09,270 --> 00:36:11,050
你展示的很多提示都有，

564
00:36:12,070 --> 00:36:15,255
正确性有一点改变，

565
00:36:15,255 --> 00:36:15,960
可能是下一张，

566
00:36:15,960 --> 00:36:18,780
我想更多一个，抱歉，

567
00:36:18,780 --> 00:36:20,780
我在找那个嘴里叼着橄榄球的狗。

568
00:36:21,770 --> 00:36:25,135
是的，其中许多都是与里面的东西不同的小变化，

569
00:36:25,135 --> 00:36:28,030
给它一个装着苹果的篮子来输入，

570
00:36:28,030 --> 00:36:28,840
而你说的是橘子，

571
00:36:28,840 --> 00:36:29,520
你有没有试过，

572
00:36:30,020 --> 00:36:33,900
如果比如编辑文本是完全不同的，

573
00:36:33,920 --> 00:36:37,480
比如从一只带着篮球的狗变成了一只猫带着保龄球，

574
00:36:37,480 --> 00:36:38,200
或类似的东西？

575
00:36:38,200 --> 00:36:39,640
是的，甚至有一些疯狂的东西，

576
00:36:39,640 --> 00:36:40,470
比如一座城市。

577
00:36:41,330 --> 00:36:42,120
这不管用，

578
00:36:42,560 --> 00:36:43,440
我们已经试过了，

579
00:36:43,730 --> 00:36:44,850
它不是，

580
00:36:45,560 --> 00:36:48,090
我觉得像指示[]，

581
00:36:48,560 --> 00:36:50,370
它是管用的，

582
00:36:50,480 --> 00:36:53,820
因为他们明确地用大编辑来训练它，

583
00:36:54,950 --> 00:36:57,205
部分问题可能是我们进行编辑的方式，

584
00:36:57,205 --> 00:36:59,160
这是基于这些小的背景步骤，

585
00:36:59,450 --> 00:37:01,170
只允许你进行本地更改，

586
00:37:01,970 --> 00:37:05,670
所以我们不知道这是模型的限制，

587
00:37:05,900 --> 00:37:08,070
还是梯度的限制，

588
00:37:08,390 --> 00:37:12,175
我们在编辑时所做的 SGD 步骤，

589
00:37:12,175 --> 00:37:14,970
所以，编辑过程是从原始图像开始，

590
00:37:15,470 --> 00:37:16,495
然后接受文本提示，

591
00:37:16,495 --> 00:37:17,760
然后一直回溯，

592
00:37:17,960 --> 00:37:20,720
直到它会稳定下来[聚合]，

593
00:37:23,440 --> 00:37:24,440
比方说一百步，

594
00:37:25,060 --> 00:37:29,390
所以像我在这里展示的每一个步骤都是很小的变化，

595
00:37:30,160 --> 00:37:31,920
如果你想要像你描述的那样，

596
00:37:31,920 --> 00:37:36,390
你需要有在一百步中跳得更快的能力，

597
00:37:36,390 --> 00:37:38,010
也许如果你走了一百万步，

598
00:37:38,010 --> 00:37:39,050
你就会有这样的改变，

599
00:37:39,100 --> 00:37:40,610
我们还没有看到这一点。

600
00:37:40,990 --> 00:37:44,610
是的，几乎很难想象这种中间场景会是什么样子。

601
00:37:44,610 --> 00:37:49,040
是的，比如，你必须穿过一些不切实际的图像，

602
00:37:49,120 --> 00:37:50,630
才能到达这里的巨大变化，

603
00:37:51,010 --> 00:37:54,020
这些步骤中的每一个看起来都是可信的，

604
00:37:55,510 --> 00:37:57,830
所以这可能是一个最优化问题。

605
00:37:58,150 --> 00:37:58,700
谢谢。

606
00:38:00,660 --> 00:38:02,290
也许延伸同样的问题，

607
00:38:02,340 --> 00:38:04,295
我很好奇这种情况，

608
00:38:04,295 --> 00:38:06,410
不是整个图像都在变化，

609
00:38:06,410 --> 00:38:10,480
但可能是在图像层面上，比如风格，

610
00:38:11,100 --> 00:38:13,000
比如主要内容保持不变，

611
00:38:13,140 --> 00:38:15,790
但风格在改变和修改，

612
00:38:15,810 --> 00:38:18,190
你以前尝试过这种类型的东西吗？

613
00:38:21,830 --> 00:38:23,605
啊，是的，我想是的，

614
00:38:23,605 --> 00:38:25,110
比如，我没有这样的例子，

615
00:38:25,160 --> 00:38:26,400
也许是这样的，

616
00:38:27,910 --> 00:38:29,060
也许是这样的，

617
00:38:31,120 --> 00:38:37,210
我想，像这样做现实的停顿改变似乎要困难得多，

618
00:38:39,020 --> 00:38:42,240
他问的是与这些全局风格的变化相比，

619
00:38:42,380 --> 00:38:43,510
因为我认为全局风格可能

620
00:38:43,510 --> 00:38:48,480
由编码本的一两个元素或类似的东西控制，

621
00:38:49,300 --> 00:38:53,780
要改变姿势或进行剧烈的，

622
00:38:54,280 --> 00:38:58,010
几何变化可能需要不同 token 之间更多的交互。

623
00:39:08,710 --> 00:39:10,190
嗨，谢谢你的精彩演讲，

624
00:39:10,840 --> 00:39:16,320
我想知道，模型如何确定哪张图片更符合描述，

625
00:39:16,430 --> 00:39:19,740
或者哪张图片的分辨率更高，

626
00:39:20,990 --> 00:39:22,500
模型的一部分决定了这一点，

627
00:39:22,940 --> 00:39:26,790
而不需要人类的监督。

628
00:39:28,370 --> 00:39:30,450
所以在实践中并不是这样，

629
00:39:31,310 --> 00:39:33,870
你是在说训练还是推理？

630
00:39:35,330 --> 00:39:36,360
啊，推理。

631
00:39:36,800 --> 00:39:41,160
我们所做的就是使用随机的种子，生成一大堆图像，

632
00:39:41,720 --> 00:39:43,500
然后我们挑选出我们喜欢的一个，

633
00:39:43,670 --> 00:39:45,160
经常发生的情况是，

634
00:39:45,160 --> 00:39:46,680
如果你有 8 张或 16 张图片，

635
00:39:47,360 --> 00:39:48,870
其中 3 或 4 张会很好，

636
00:39:49,070 --> 00:39:50,760
但有几张看起来会很糟糕，

637
00:39:51,330 --> 00:39:55,850
我们仍然没有一种自我纠正的方式或自动的方式，

638
00:39:55,850 --> 00:39:58,090
来说这张图片更符合提示。

639
00:40:00,940 --> 00:40:02,870
所以，是的，所以，

640
00:40:04,190 --> 00:40:05,700
所以，总的来说，希望的是，

641
00:40:05,780 --> 00:40:08,545
潜在的空间已经以一种合理的方式进行了训练，

642
00:40:08,545 --> 00:40:10,500
这样它就会产生可信的图像，

643
00:40:12,230 --> 00:40:17,280
但是，例如，你可能会产生一只猫，腿都在错误的位置，

644
00:40:17,750 --> 00:40:20,935
然后，它仍在使用编码本中正确的元素，

645
00:40:20,935 --> 00:40:22,290
但排列错误，

646
00:40:22,370 --> 00:40:24,480
所以，我们确实看到了这类错误。

647
00:40:24,950 --> 00:40:28,500
好的，那么，对于你想的提高分辨率的下一步，

648
00:40:29,090 --> 00:40:30,300
你会以同样的方式去做吗？

649
00:40:30,530 --> 00:40:34,690
是的，我们必须设法解决这些问题。

650
00:40:36,630 --> 00:40:37,600
好了，还有一个问题。

651
00:40:46,480 --> 00:40:48,650
我有两个问题，

652
00:40:48,760 --> 00:40:50,210
我的第一个问题是，

653
00:40:50,890 --> 00:40:58,340
我不确定文本和图像语料库的大小限制是什么，

654
00:41:00,010 --> 00:41:05,445
但比如说，有一个新的但是还没有出来，

655
00:41:05,445 --> 00:41:07,220
或者在接下来的两个月里，

656
00:41:08,380 --> 00:41:11,570
如果我在提示符中问，

657
00:41:11,740 --> 00:41:16,400
我想让你用这个还没有发明的东西的风格来画这个，

658
00:41:16,810 --> 00:41:19,130
这个模型会对这种变化有什么反应，

659
00:41:19,330 --> 00:41:22,190
另外，一个单独的问题是，

660
00:41:22,450 --> 00:41:27,140
关于这些，特别是带有遮罩图像的示例，

661
00:41:27,850 --> 00:41:31,390
上面一行到下面一行，

662
00:41:31,620 --> 00:41:38,470
我们从这些模型给出的图像中获得了多少新信息。

663
00:41:40,290 --> 00:41:43,720
所以，这是一个非常好的问题，

664
00:41:43,980 --> 00:41:45,400
所以对于数据来说，

665
00:41:46,260 --> 00:41:49,850
很明显，数据偏向于著名艺术家，

666
00:41:50,170 --> 00:41:54,420
这就是为什么我们可以说伦勃朗或莫奈的风格，

667
00:41:54,420 --> 00:41:56,270
它已经看到了许多这样的例子，

668
00:41:56,560 --> 00:41:58,700
因为这是从网络上收集的数据，

669
00:41:59,880 --> 00:42:03,930
如果你有一个新艺术家的风格，

670
00:42:04,130 --> 00:42:06,805
目前唯一的方法是通过微调，

671
00:42:06,805 --> 00:42:11,360
我们将这些人的图像将其与一些文本配对，

672
00:42:11,710 --> 00:42:13,940
然后对模型进行训练来生成，

673
00:42:14,350 --> 00:42:17,720
这就是 Dreambooth 方法试图做到的，

674
00:42:17,830 --> 00:42:22,460
尽管这是特定的对象，而不是风格，

675
00:42:22,630 --> 00:42:24,900
但你可以想象用这样的东西，

676
00:42:24,900 --> 00:42:26,900
对于一位新艺术家的风格。

677
00:42:29,320 --> 00:42:30,615
这不是零次学习的，

678
00:42:30,615 --> 00:42:32,925
你只是展示这些例子，然后说，

679
00:42:32,925 --> 00:42:36,320
你能根据这个文本提示生成一些东西，但以那种风格，

680
00:42:37,610 --> 00:42:39,270
所以，这是我们愿意努力的方向。

681
00:42:40,800 --> 00:42:43,420
关于遮罩，我没有完全理解问题，

682
00:42:43,800 --> 00:42:46,300
所以你说了一些关于上面和下面的事情。

683
00:42:48,370 --> 00:42:52,430
是的，我认为这是其中一张幻灯片，

684
00:42:53,020 --> 00:42:54,860
但这是一个非常普遍的问题，

685
00:42:55,690 --> 00:43:02,030
它是我们从这些图片中获得了多少新信息，

686
00:43:02,590 --> 00:43:07,060
我想还没有人见过的图片，

687
00:43:07,320 --> 00:43:08,740
比如骑自行车的熊一样。

688
00:43:09,660 --> 00:43:16,115
哦，你是说你是在说记忆和它实际的样子，

689
00:43:16,115 --> 00:43:19,390
你的意思是，如果这已经在训练数据集中？

690
00:43:21,110 --> 00:43:22,170
嗯，是的。

691
00:43:23,250 --> 00:43:25,640
嗯，这是一个很好的问题，

692
00:43:25,640 --> 00:43:29,020
我认为我们对这个问题仍然没有很好的答案，

693
00:43:31,620 --> 00:43:34,145
这是一个大型语言模型，

694
00:43:34,145 --> 00:43:36,290
只是出现了以前见过的幻觉，

695
00:43:36,290 --> 00:43:37,210
只要混合搭配，

696
00:43:38,520 --> 00:43:42,245
它可能也在做类似的事情，

697
00:43:42,245 --> 00:43:45,185
它以前见过熊和自行车，

698
00:43:45,185 --> 00:43:48,560
已经能够以一种可信的方式将它们结合在一起，

699
00:43:49,120 --> 00:43:52,160
数据集中不太可能有完全相同的图像，

700
00:43:52,270 --> 00:43:57,710
但同样，我们没有好的工具，

701
00:43:57,850 --> 00:44:01,910
我们可以尝试搜索嵌入 CLIP 或其他，

702
00:44:02,350 --> 00:44:04,280
以寻找相似的图像，

703
00:44:05,050 --> 00:44:08,840
但是在我们训练数以亿计的图像的规模上，

704
00:44:09,160 --> 00:44:11,300
我们还没有去看，

705
00:44:11,770 --> 00:44:13,520
看看它的记忆力有多强，

706
00:44:15,290 --> 00:44:18,445
新概念的结合，

707
00:44:18,445 --> 00:44:19,530
我认为是后者，

708
00:44:19,610 --> 00:44:21,270
因为这似乎不太可能，

709
00:44:21,820 --> 00:44:25,430
这些类型的图像会在训练数据集中。

710
00:44:27,530 --> 00:44:28,260
好的，谢谢。

711
00:44:29,570 --> 00:44:30,310
非常感谢。

712
00:44:30,310 --> 00:44:32,070
让我们再给 Dilip 更多掌声。

