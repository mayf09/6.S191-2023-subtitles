1
00:00:09,160 --> 00:00:12,740
机器人学是未来一个非常酷和重要的方向，

2
00:00:13,090 --> 00:00:18,230
我相信我们正在走向一个这样一个世界，

3
00:00:18,340 --> 00:00:21,770
很多日常工作都从你的盘子里消失，

4
00:00:22,420 --> 00:00:26,600
新鲜的农产品出现在你的门口，由无人机运送，

5
00:00:27,160 --> 00:00:30,020
垃圾桶会自己倒出来，

6
00:00:30,160 --> 00:00:34,280
智能基础设施确保垃圾被清除，

7
00:00:34,810 --> 00:00:39,350
机器人帮助回收、搁置和清洁窗户，

8
00:00:41,770 --> 00:00:44,415
机器人可以为我们做很多物理上的事情，

9
00:00:44,415 --> 00:00:45,170
顺便问一下，

10
00:00:45,310 --> 00:00:49,190
你能数一下在这张图片中有多少个机器人吗，

11
00:00:49,750 --> 00:00:51,530
有人想猜一下吗，

12
00:00:52,510 --> 00:00:54,470
在这张图中，我有多少个机器人？

13
00:00:59,690 --> 00:01:01,500
好的，这很接近了，

14
00:01:01,700 --> 00:01:03,330
结果是 19 个，

15
00:01:03,620 --> 00:01:04,810
所有的机器人都在这里，

16
00:01:04,810 --> 00:01:06,660
我们有会飞的机器人，

17
00:01:06,770 --> 00:01:07,770
我们有汽车，

18
00:01:07,880 --> 00:01:09,730
我们有购物车机器人，

19
00:01:09,730 --> 00:01:11,575
我们有搬运的机器人，

20
00:01:11,575 --> 00:01:13,110
我们在那个架子上有机器人，

21
00:01:13,130 --> 00:01:15,390
我们有清洁机器人。

22
00:01:15,740 --> 00:01:17,065
所以，我真的相信，

23
00:01:17,065 --> 00:01:20,940
在未来，我们将有人工智能的帮助，

24
00:01:21,200 --> 00:01:23,910
无论它们是否体现，

25
00:01:24,620 --> 00:01:26,880
作为我们的守护天使，

26
00:01:27,530 --> 00:01:30,145
提供建议，以确保，

27
00:01:30,145 --> 00:01:32,940
我们最大化和优化我们的生活，

28
00:01:33,050 --> 00:01:34,950
生活得好和工作有效。

29
00:01:35,360 --> 00:01:39,990
这些代理将帮助我们进行认知和体力工作。

30
00:01:41,090 --> 00:01:44,370
所以，今天我们可以说，

31
00:01:45,110 --> 00:01:50,910
有了人工智能，我们将看到如此广泛的应用，

32
00:01:51,020 --> 00:01:57,420
例如，这些技术有可能减少和消除车祸，

33
00:01:57,440 --> 00:02:01,540
它们有可能更好地诊断监测和治疗疾病，

34
00:02:01,540 --> 00:02:04,890
正如你在之前的一些讲座中看到的那样，

35
00:02:04,940 --> 00:02:07,500
这些技术有可能，

36
00:02:07,910 --> 00:02:10,350
保持你的信息隐私和安全，

37
00:02:10,880 --> 00:02:16,650
更有效、更快、更便宜地运输人员和货物，

38
00:02:16,970 --> 00:02:22,620
通过提供即时翻译来真正使全球沟通变得更容易，

39
00:02:23,240 --> 00:02:25,675
为每个人发展教育，

40
00:02:25,675 --> 00:02:30,060
让人类工作人员专注于宏大的任务，

41
00:02:30,500 --> 00:02:33,120
让机器承担日常任务。

42
00:02:33,620 --> 00:02:37,710
所以，这一未来是由三个相互关联的领域实现的，

43
00:02:38,330 --> 00:02:40,080
一方面我们有机器人，

44
00:02:40,100 --> 00:02:44,260
我喜欢把机器人看作是一台机器，

45
00:02:44,260 --> 00:02:46,260
它让计算运转起来，

46
00:02:46,670 --> 00:02:54,960
让我们在世界上的机器具有导航和操纵世界的能力，

47
00:02:55,460 --> 00:02:56,940
我们有人工智能，

48
00:02:57,980 --> 00:03:04,060
它能让机器像人一样看到、听到、交流和做出决定，

49
00:03:04,500 --> 00:03:05,800
然后我们有机器学习，

50
00:03:06,720 --> 00:03:12,740
对我来说，机器学习是关于从数据中学习并做出预测，

51
00:03:13,330 --> 00:03:17,930
机器学习的这种应用是广泛的，

52
00:03:18,040 --> 00:03:20,600
它适用于认知任务，

53
00:03:20,650 --> 00:03:22,760
它适用于物理任务，

54
00:03:23,230 --> 00:03:24,860
但不管任务是什么，

55
00:03:25,240 --> 00:03:30,770
我们可以描述机器学习如何使用数据工作，

56
00:03:31,540 --> 00:03:36,800
来回答描述性、预测性或说明性的问题，

57
00:03:37,180 --> 00:03:38,510
所以你可以查看数据，

58
00:03:38,680 --> 00:03:41,510
看看发生了什么，或者这是什么，

59
00:03:42,100 --> 00:03:43,130
你可以查看数据，

60
00:03:43,390 --> 00:03:45,380
看看未来会发生什么，

61
00:03:45,760 --> 00:03:47,960
未来世界可能是什么样子，

62
00:03:48,700 --> 00:03:53,690
或者你可以用数据来问我应该去哪里，我应该做什么。

63
00:03:56,550 --> 00:04:01,840
所以，当我们在机器人的背景下思考这些问题时，

64
00:04:02,220 --> 00:04:06,680
我们必须在机器人是什么的问题上达成一致。

65
00:04:06,680 --> 00:04:11,530
所以，可以把机器人想象成一个可编程的机械设备，

66
00:04:12,300 --> 00:04:15,460
它通过传感器接受输入，

67
00:04:15,840 --> 00:04:17,480
对输入进行推理，

68
00:04:17,480 --> 00:04:20,740
然后在物理世界中产生一个动作。

69
00:04:22,110 --> 00:04:24,940
机器人是由身体和大脑组成的，

70
00:04:25,830 --> 00:04:29,710
身体由执行器和传感器组成，

71
00:04:30,540 --> 00:04:34,430
决定了机器人可以完成的任务范围，

72
00:04:34,430 --> 00:04:37,690
机器人只能做它的身体能够做的事情，

73
00:04:38,100 --> 00:04:42,790
轮子上的机器人不能完成爬楼梯的任务，

74
00:04:43,200 --> 00:04:44,710
所以我们必须考虑那个身体，

75
00:04:45,300 --> 00:04:46,865
我们有[]，

76
00:04:46,865 --> 00:04:50,320
我们有很多基于机器学习的研究，

77
00:04:50,940 --> 00:04:54,550
使我们能够研究，

78
00:04:54,930 --> 00:04:59,290
如何为特定任务优化设计机器人身体。

79
00:05:00,360 --> 00:05:03,490
现在，为了让身体做它应该做的事情，

80
00:05:03,690 --> 00:05:04,490
我们需要大脑，

81
00:05:04,490 --> 00:05:07,870
我们需要机器学习，需要推理和决策引擎，

82
00:05:08,250 --> 00:05:11,620
这就是我们今天要谈论的。

83
00:05:13,320 --> 00:05:15,670
现在，在机器人的背景下，

84
00:05:15,810 --> 00:05:18,065
我们有三种类型的学习，

85
00:05:18,065 --> 00:05:23,015
你们在整个课程中看到了这些方法的不同方面。

86
00:05:23,015 --> 00:05:24,280
我们有监督学习，

87
00:05:24,690 --> 00:05:27,760
在这个学习方法中，

88
00:05:28,020 --> 00:05:32,950
我们要用数据来找出投入产出之间的关系。

89
00:05:33,720 --> 00:05:35,590
我们有无监督学习，

90
00:05:35,670 --> 00:05:38,060
在无监督学习的背景下，

91
00:05:38,060 --> 00:05:40,090
我们可以使用数据来获得模式，

92
00:05:40,320 --> 00:05:42,940
找到数据的模式和分类。

93
00:05:43,440 --> 00:05:45,550
然后我们有强化学习，

94
00:05:46,170 --> 00:05:50,860
这是关于通过最大化奖励来学习执行任务。

95
00:05:52,380 --> 00:05:53,290
所以对于机器人来说，

96
00:05:53,700 --> 00:05:59,290
我们最终得到了一个最经常由三个步骤组成的循环，

97
00:05:59,730 --> 00:06:01,745
我们有感知的步骤，

98
00:06:01,745 --> 00:06:03,545
我们有计划和推理的步骤，

99
00:06:03,545 --> 00:06:04,960
我们有行动的步骤。

100
00:06:05,640 --> 00:06:09,640
这就是我们今天要讨论的问题。

101
00:06:09,720 --> 00:06:11,800
所以让我从一些例子开始，

102
00:06:11,970 --> 00:06:18,490
我们如何使用机器学习来增强机器人的感知能力。

103
00:06:19,890 --> 00:06:22,625
所以这就解决了这是什么的问题，

104
00:06:22,625 --> 00:06:24,940
这个问题真的很重要，

105
00:06:25,110 --> 00:06:30,250
因为比如说，训练机器人汽车识别道路上的所有物体，

106
00:06:30,390 --> 00:06:32,770
包括鸭子，汽车，人，

107
00:06:33,150 --> 00:06:36,280
对于自动驾驶来说是非常关键的，

108
00:06:36,900 --> 00:06:38,855
那么这是如何工作的，

109
00:06:38,855 --> 00:06:41,585
好的，让我给你一个高层次的视角，

110
00:06:41,585 --> 00:06:45,100
看看机器人汽车是如何真正识别场景的。

111
00:06:47,290 --> 00:06:53,450
所以，为了把深度学习用于机器人的感知任务，

112
00:06:53,590 --> 00:06:57,020
我们使用数据，这是手动标记的数据，

113
00:06:57,580 --> 00:07:01,010
输入卷积神经网络，

114
00:07:01,570 --> 00:07:05,130
然后使用这些标记来对数据进行分类，

115
00:07:05,130 --> 00:07:06,555
例如，对于这张图像，

116
00:07:06,555 --> 00:07:10,400
我们可能会有像汽车、鸭子、路这样的分类，

117
00:07:10,930 --> 00:07:11,930
我们这样做，

118
00:07:11,980 --> 00:07:15,860
当系统看到新的图像时，

119
00:07:15,970 --> 00:07:17,270
例如这张，

120
00:07:17,440 --> 00:07:21,320
汽车可能会说，哦，这是鸭子路，

121
00:07:21,580 --> 00:07:30,135
现在，为了真正为这个对象分类问题提供解决方案，

122
00:07:30,135 --> 00:07:34,500
我们必须使用多种算法，

123
00:07:34,500 --> 00:07:39,320
我们必须使用的第一个算法称为图像分割，

124
00:07:39,700 --> 00:07:44,690
在图像分割中，我们把输入当作输入图像，

125
00:07:45,310 --> 00:07:51,350
然后属于图像中同一对象的像素组合在一起，

126
00:07:51,880 --> 00:07:53,930
这是一种词汇步骤，

127
00:07:54,280 --> 00:07:58,010
然后我们需要标记和识别这些图像，

128
00:07:58,150 --> 00:07:59,780
这是一个语义步骤，

129
00:08:00,280 --> 00:08:01,815
令人兴奋的是，

130
00:08:01,815 --> 00:08:06,110
我们已经有了非常好的算法，

131
00:08:06,190 --> 00:08:09,290
可以非常快地分割图像，

132
00:08:09,340 --> 00:08:12,840
所以，我们可以这样做，

133
00:08:12,840 --> 00:08:14,360
我们可以[拍摄]一张图像，

134
00:08:14,620 --> 00:08:17,205
我们可以非常快地在图像中找到物体，

135
00:08:17,205 --> 00:08:18,650
我们不知道这些物体是什么，

136
00:08:18,910 --> 00:08:20,930
所以为了知道这些物体是什么，

137
00:08:21,820 --> 00:08:23,180
你知道我们做了什么，

138
00:08:23,800 --> 00:08:28,550
我们雇佣了数千人来为我们拥有的物品做标注，

139
00:08:28,600 --> 00:08:30,710
所以这是令人兴奋的，

140
00:08:30,730 --> 00:08:34,100
但是标注是一项非常劳动密集型的活动，

141
00:08:34,180 --> 00:08:37,260
对机器学习来说是一个巨大的挑战，

142
00:08:37,260 --> 00:08:38,840
让我们把这个想法留到以后再考虑。

143
00:08:40,310 --> 00:08:47,490
现在最流行的衡量图像分类准确性的基准是 ImageNet ，

144
00:08:47,990 --> 00:08:52,015
这里我们看到 ImageNet 的排行榜，

145
00:08:52,015 --> 00:09:00,420
我们看到各种图像分类算法变体的性能，

146
00:09:00,530 --> 00:09:05,280
性能很好地达到了 90% 的准确率，

147
00:09:05,450 --> 00:09:07,260
这真的非常令人兴奋，

148
00:09:08,200 --> 00:09:08,960
这很令人兴奋，

149
00:09:09,040 --> 00:09:13,850
但如果这些算法在汽车上运行，

150
00:09:15,070 --> 00:09:15,990
这还不够好，

151
00:09:17,010 --> 00:09:19,750
因为因为汽车是一个安全关键系统，

152
00:09:20,220 --> 00:09:24,040
事实上我们不能有任何错误，

153
00:09:24,300 --> 00:09:28,390
在汽车图像识别方面，

154
00:09:28,410 --> 00:09:36,095
这是一个运行在汽车上的物体探测器的例子，

155
00:09:36,095 --> 00:09:39,310
我们可以看到三个摄像头拍摄的图像，

156
00:09:39,420 --> 00:09:40,990
有一个摄像头指向前方，

157
00:09:41,010 --> 00:09:42,880
有一个摄像头指向左边，

158
00:09:43,260 --> 00:09:45,400
有一个摄像头指向右边，

159
00:09:45,840 --> 00:09:50,255
所以你可以看到这辆车接受了这个输入，

160
00:09:50,255 --> 00:09:51,410
它做得很好，

161
00:09:51,410 --> 00:09:52,450
那里有一辆蓝色的汽车，

162
00:09:53,100 --> 00:09:54,640
那里有一些骑自行车的人，

163
00:09:54,810 --> 00:09:57,070
有个骑自行车的，

164
00:09:57,330 --> 00:09:58,630
还有一辆车，

165
00:09:58,740 --> 00:10:00,400
所以这个系统做得很好，

166
00:10:00,570 --> 00:10:04,925
设法找到了道路的缺口来转弯，

167
00:10:04,925 --> 00:10:07,780
但它并不是百分之百正确的，

168
00:10:08,130 --> 00:10:11,890
特别是这辆移动的卡车，

169
00:10:12,750 --> 00:10:17,170
当当移动的卡车的图像通过时，

170
00:10:17,480 --> 00:10:21,720
这个系统的物体识别部分，

171
00:10:22,850 --> 00:10:26,640
我们最终得到了很多有趣的东西，

172
00:10:26,840 --> 00:10:28,860
[]被认为是一道栅栏，

173
00:10:30,120 --> 00:10:35,020
这些是极端错误的类型，

174
00:10:35,850 --> 00:10:40,150
或者我们表示它们的方式，即边界条件，

175
00:10:40,740 --> 00:10:45,250
在我们训练机器学习时，我们需要注意的，

176
00:10:45,900 --> 00:10:48,350
对于安全关键型应用，如驾驶。

177
00:10:49,240 --> 00:10:51,080
事实证明，

178
00:10:51,490 --> 00:10:56,150
如果你使用深度神经网络解决方案来进行图像分类，

179
00:10:57,130 --> 00:10:58,970
这些解决方案效果非常好，

180
00:10:59,170 --> 00:11:02,750
因为它们是在一个巨大的数据集 ImageNet 上进行训练的，

181
00:11:03,830 --> 00:11:07,915
但解决方案捕捉到的不仅仅是对象的本质，

182
00:11:07,915 --> 00:11:12,720
这些解决方案还捕获对象出现的上下文，

183
00:11:13,130 --> 00:11:14,580
所以 MIT 的研究人员，

184
00:11:14,900 --> 00:11:17,940
由 Josh Tenenbaum 和 Boris Katz 带领，

185
00:11:18,140 --> 00:11:20,850
在几年前做了一个实验，

186
00:11:21,380 --> 00:11:23,160
他们用普通物体，

187
00:11:24,150 --> 00:11:25,810
他们把它们放在不同的背景下，

188
00:11:25,860 --> 00:11:28,480
例如，他们把鞋子放在床上，

189
00:11:28,890 --> 00:11:31,990
他们拿起花盆和钢笔，放在浴室里，

190
00:11:32,490 --> 00:11:35,290
随着环境的显著变化，

191
00:11:35,940 --> 00:11:42,850
性能最好的 ImageNet 算法的性能下降了 40%-50% ，

192
00:11:43,350 --> 00:11:44,500
这真的是非同寻常。

193
00:11:45,280 --> 00:11:46,795
我与你们分享这一点，

194
00:11:46,795 --> 00:11:49,860
不是为了阻止你们使用这些算法，

195
00:11:49,940 --> 00:11:51,570
而是要指出，

196
00:11:52,160 --> 00:11:55,240
当你部署一个算法时，

197
00:11:55,240 --> 00:11:58,380
特别是当你把它部署在一个安全关键的应用程序中时，

198
00:11:58,580 --> 00:12:01,780
理解这个算法的范围是很重要的，

199
00:12:01,780 --> 00:12:03,270
重要的是要了解，

200
00:12:03,410 --> 00:12:05,080
什么是有效的，什么无效的，

201
00:12:05,080 --> 00:12:06,900
什么时候可以应用算法，

202
00:12:06,950 --> 00:12:10,710
什么时候不应该应用算法。

203
00:12:11,620 --> 00:12:14,715
所以，请记住这一点，

204
00:12:14,715 --> 00:12:20,060
当你考虑构建或部署深度神经网络解决方案时。

205
00:12:20,920 --> 00:12:27,590
还有一件事对自动驾驶和机器人非常关键，

206
00:12:28,150 --> 00:12:32,540
你已经听过一节关于对抗性攻击的精彩讲座，

207
00:12:32,920 --> 00:12:35,510
事实证明，你可以非常容易地攻击，

208
00:12:35,950 --> 00:12:43,220
从汽车摄像头流到汽车决策引擎的图像，

209
00:12:43,420 --> 00:12:47,300
事实上，很容易拿起停车标志，

210
00:12:47,620 --> 00:12:50,690
稍微扰乱它，

211
00:12:50,980 --> 00:12:55,425
以一种你甚至用肉眼都看不出的方式，

212
00:12:55,425 --> 00:12:58,010
这两个图像之间有区别，

213
00:12:58,880 --> 00:13:00,550
有了这些小的扰动，

214
00:13:00,600 --> 00:13:03,910
你可以把停车标志变成一个让行标志，

215
00:13:04,140 --> 00:13:10,330
你可以想象这会在物理道路上造成什么样的混乱。

216
00:13:11,410 --> 00:13:17,540
所以，机器学习在为机器人构建感知系统方面非常强大，

217
00:13:18,040 --> 00:13:21,705
但是，当我们在机器人的环境中使用机器学习时，

218
00:13:21,705 --> 00:13:24,530
重要的是记住它们范围，

219
00:13:24,730 --> 00:13:26,540
什么时候工作，什么时候不工作，

220
00:13:26,890 --> 00:13:29,960
然后，重要的是要考虑，

221
00:13:30,880 --> 00:13:36,650
在决策时我们可能会设置什么样的护栏，

222
00:13:37,240 --> 00:13:40,040
这样我们就有了健壮的行为。

223
00:13:41,410 --> 00:13:42,470
那么，我们能做些什么呢，

224
00:13:42,550 --> 00:13:47,630
对于停车标志上可能出现的对抗性干扰。

225
00:13:48,550 --> 00:13:51,560
好的，让我们来讨论一下决策，

226
00:13:51,940 --> 00:13:56,960
然后我们来谈谈汽车是如何计算出在给定输入的情况下要做什么的。

227
00:13:57,250 --> 00:14:02,360
强化学习正在引发机器人学的一场巨大革命，

228
00:14:03,160 --> 00:14:05,625
那么为什么会这样，

229
00:14:05,625 --> 00:14:10,710
强化学习之所以在机器人学中引发一场巨大的革命，

230
00:14:10,710 --> 00:14:17,270
是因为我们已经建立了快速的模拟系统和模拟方法，

231
00:14:17,530 --> 00:14:20,930
允许我们并行运行数千次模拟，

232
00:14:21,790 --> 00:14:24,380
以便训练强化学习策略，

233
00:14:24,910 --> 00:14:32,990
我们也缩小了硬件平台和仿真引擎之间的差距。

234
00:14:34,410 --> 00:14:39,940
所以你在课程早期已经看到了强化学习，

235
00:14:40,740 --> 00:14:44,990
所以强化学习关注的是，

236
00:14:44,990 --> 00:14:49,150
智能代理应该如何在环境中采取行动，

237
00:14:49,470 --> 00:14:53,500
以便最大化累积奖励的概念。

238
00:14:54,310 --> 00:14:58,550
所以，强化学习实际上就是学习行动，

239
00:14:58,630 --> 00:15:01,130
这与监督学习的不同之处在于

240
00:15:01,930 --> 00:15:08,600
不需要标记的输入或不需要标记的输入输出对，

241
00:15:09,790 --> 00:15:10,850
所以，在这个例子中，

242
00:15:10,930 --> 00:15:13,580
代理必须学会避免火灾，

243
00:15:13,630 --> 00:15:14,895
这很简单，

244
00:15:14,895 --> 00:15:17,780
如果它着火了，它会得到负的奖励，

245
00:15:17,800 --> 00:15:20,790
当它到达水中时，它会得到正的奖励，

246
00:15:20,790 --> 00:15:24,600
这就是这种方法的本质，

247
00:15:24,600 --> 00:15:26,690
你不断地尝试和犯错，

248
00:15:27,070 --> 00:15:32,370
最终，积极的回报主导了消极的回报，

249
00:15:32,370 --> 00:15:38,480
这将引导代理人朝着最好的行动前进。

250
00:15:39,500 --> 00:15:41,610
这是一个例子，

251
00:15:41,870 --> 00:15:45,000
我们有一个强化学习代理，

252
00:15:45,560 --> 00:15:48,780
它试图在赛道上驾驶，

253
00:15:49,040 --> 00:15:54,325
你可以看到它一开始就会犯错误，

254
00:15:54,325 --> 00:16:00,120
但最终它会学会如何在高速下转弯，

255
00:16:00,590 --> 00:16:04,885
有趣的是，你可以采用这个想法，

256
00:16:04,885 --> 00:16:06,330
你可以并行运行它，

257
00:16:06,590 --> 00:16:08,670
所以你可以使用数千辆汽车，

258
00:16:09,080 --> 00:16:11,970
你可以把它们放在同一条轨道上，

259
00:16:12,470 --> 00:16:14,910
一开始他们都会犯错误，

260
00:16:15,710 --> 00:16:19,080
但最终他们得到了正确的解决方案，

261
00:16:19,160 --> 00:16:27,690
最终[联合]策略最终成为了可靠地控制车辆的策略，

262
00:16:28,460 --> 00:16:33,480
所以，它确实是一个非常令人兴奋的领域，

263
00:16:33,800 --> 00:16:38,610
强化学习，像深度学习一样，在几十年前就发明出来了，

264
00:16:39,170 --> 00:16:41,310
但是它现在运行得很好，

265
00:16:41,600 --> 00:16:44,725
因为有了计算的出现，

266
00:16:44,725 --> 00:16:48,810
我们今天拥有的计算比 40 年前要多得多，

267
00:16:49,280 --> 00:16:54,210
我们今天拥有的深度神经网络数据比 40 年前多得多，

268
00:16:54,380 --> 00:16:58,140
所以，这些当时做得不是很好的技术，

269
00:16:58,340 --> 00:17:03,810
突然之间为我们的代理创造了非凡的可能性和能力。

270
00:17:04,910 --> 00:17:06,810
这是一个简单的模拟，

271
00:17:07,130 --> 00:17:10,350
为了让模拟驱动一个真正的机器人，

272
00:17:10,640 --> 00:17:12,270
我们实际上需要考虑，

273
00:17:12,320 --> 00:17:14,640
机器人的动力学，

274
00:17:14,900 --> 00:17:16,330
换句话说，

275
00:17:16,330 --> 00:17:20,035
我们必须考虑到车辆的样子，

276
00:17:20,035 --> 00:17:22,980
它的运动学是什么，它的动力学是什么，

277
00:17:23,000 --> 00:17:27,810
所以，这是一辆运行在模拟中学到的策略的车辆，

278
00:17:27,980 --> 00:17:29,160
所以这真的很酷，

279
00:17:29,990 --> 00:17:33,205
因为我们现在能够进行模拟训练，

280
00:17:33,205 --> 00:17:37,590
如果我们拥有的车辆动力学模型足够好，

281
00:17:37,760 --> 00:17:41,130
我们可以采用在模拟中非常快地学习到的策略，

282
00:17:41,420 --> 00:17:43,530
我们可以将其部署在车辆上，

283
00:17:43,880 --> 00:17:47,430
这是两辆车，互相比赛，

284
00:17:47,510 --> 00:17:49,270
白色的和绿色的，

285
00:17:49,270 --> 00:17:50,890
所以看看那辆白色的车在做什么，

286
00:17:50,890 --> 00:17:51,750
看看它是怎么偷偷溜过去的，

287
00:17:51,950 --> 00:17:53,080
哇，真的很棒，

288
00:17:53,080 --> 00:17:54,390
我希望我能像这样比赛，

289
00:17:55,970 --> 00:17:58,440
所以这真的非常令人兴奋，

290
00:17:59,300 --> 00:18:03,300
在这种情况下，车辆的视野有限，

291
00:18:04,190 --> 00:18:08,340
而且它们获得赛道上其他车辆的位置，

292
00:18:08,960 --> 00:18:13,290
它们从外部定位系统获得这个位置，

293
00:18:13,880 --> 00:18:18,340
但它们只知道它们视野内的车辆在哪里，

294
00:18:18,340 --> 00:18:20,430
所以，看看这个，太棒了。

295
00:18:21,440 --> 00:18:25,380
那么，好的，我们可以用这些方法做什么，

296
00:18:25,400 --> 00:18:30,570
所以，我们已经看到了如何使用深度学习来理解

297
00:18:30,680 --> 00:18:33,450
从摄像机中看到的车辆，

298
00:18:33,950 --> 00:18:37,410
我们已经看到了一个学习驾驶的例子，

299
00:18:37,820 --> 00:18:38,790
我们能做些什么。

300
00:18:39,140 --> 00:18:44,310
我认为这些机器人技术的进步，

301
00:18:44,510 --> 00:18:49,285
确实使你在第一张幻灯片中看到成为可能，

302
00:18:49,285 --> 00:18:52,440
创造出可以完成许多任务的许多机器人的可能性，

303
00:18:52,850 --> 00:18:55,830
而且比我们这里看到的任务复杂得多。

304
00:18:56,090 --> 00:18:59,980
所以我接下来想要谈的是自动驾驶，

305
00:18:59,980 --> 00:19:03,940
我们如何把这些部分放在一起来启用自动驾驶，

306
00:19:03,940 --> 00:19:06,635
也就是启用自动驾驶汽车，

307
00:19:06,635 --> 00:19:07,880
我不是这个意思，

308
00:19:07,880 --> 00:19:09,910
这个自动驾驶不是特斯拉，

309
00:19:10,140 --> 00:19:15,160
自动驾驶，这只是一个想法，你有一辆完整的自动驾驶汽车，

310
00:19:16,170 --> 00:19:17,330
现在，为了做到这一点，

311
00:19:17,330 --> 00:19:19,060
我们需要更多地推动大脑的发展，

312
00:19:19,410 --> 00:19:24,580
我们需要在汽车的学习、推理和规划方面做更多的工作。

313
00:19:24,840 --> 00:19:26,705
所以让我问你，

314
00:19:26,705 --> 00:19:30,790
你知道美国第一辆自动驾驶的海岸到海岸是什么时候，

315
00:19:31,350 --> 00:19:32,140
有没有人猜到？

316
00:19:36,870 --> 00:19:37,990
不是你，我知道你知道的。

317
00:19:42,440 --> 00:19:43,020
2000 年，有意思。

318
00:19:46,340 --> 00:19:48,030
实际上，那是在 1995 年，

319
00:19:49,100 --> 00:19:53,550
在 1995 年，卡内基梅隆大学的一个项目叫 Navlab ，

320
00:19:55,700 --> 00:20:02,370
制造了一辆实际上是由机器学习引擎 Alvinn 驱动的汽车，

321
00:20:02,980 --> 00:20:08,430
Alvinn 开着这辆车从华盛顿特区一路开到洛杉矶，

322
00:20:09,470 --> 00:20:14,520
在高速公路上的大部分时间里，这辆车都处于自动驾驶模式，

323
00:20:14,780 --> 00:20:18,360
但总有一个学生准备好控制汽车，

324
00:20:18,470 --> 00:20:22,110
汽车有时不在自动驾驶模式下驾驶，

325
00:20:22,460 --> 00:20:30,900
当有雨、拥堵或汽车不得不离开出口时，

326
00:20:31,430 --> 00:20:33,360
这就是这辆车做的事情，

327
00:20:33,710 --> 00:20:37,620
它从华盛顿特区一直到洛杉矶，

328
00:20:39,240 --> 00:20:41,950
现在 1995 年是很久以前的事了，

329
00:20:42,030 --> 00:20:44,380
我是说，那是在你们很多人出生之前，

330
00:20:44,820 --> 00:20:46,960
所以，特别需要思考，

331
00:20:47,400 --> 00:20:52,000
需要哪些方面的进步，

332
00:20:52,110 --> 00:20:55,805
才能从我们回到当时的位置，

333
00:20:55,805 --> 00:20:58,120
我们可以真正看到部署的自动驾驶汽车，

334
00:20:58,500 --> 00:20:59,150
顺便说一句，

335
00:20:59,150 --> 00:21:02,290
你应该来看看 MIT 的自动驾驶汽车，

336
00:21:02,910 --> 00:21:06,155
是由 Alexander 在过去五年里制造的，

337
00:21:06,155 --> 00:21:09,070
它们非常强大，可以在我们附近驾驶，

338
00:21:10,730 --> 00:21:11,970
我们将讨论他们是如何驾驶的。

339
00:21:13,070 --> 00:21:20,160
但有趣的是，这并不是我们第一次在高速公路上以自动驾驶模式驾驶汽车，

340
00:21:21,380 --> 00:21:27,900
事实上，你知道世界上任何地方第一次自动驾驶高速公路是什么时候吗，

341
00:21:32,670 --> 00:21:35,020
它是在 1986 年，

342
00:21:35,870 --> 00:21:41,280
在 1986 年，德国工程师 Ernst Dickmann 开始考虑，

343
00:21:41,480 --> 00:21:45,270
如何将他的面包车变成一辆自动驾驶汽车，

344
00:21:46,220 --> 00:21:49,075
所以，他在货车上安装了电脑和摄像头，

345
00:21:49,075 --> 00:21:53,845
并开始在德国高速公路的一个空的部分进行测试，

346
00:21:53,845 --> 00:21:58,320
那里还没有对公共驾驶开放，

347
00:21:59,030 --> 00:22:04,170
他能够让他的货车在那条空的道路上行驶，

348
00:22:05,000 --> 00:22:10,040
但有趣的是，当他开始开发这项工作时，

349
00:22:11,350 --> 00:22:14,570
计算机大约需要 10 分钟来分析一幅图像，

350
00:22:16,080 --> 00:22:17,200
好的，你能想象吗，

351
00:22:17,790 --> 00:22:21,700
好的，那么你如何从这个，

352
00:22:21,870 --> 00:22:26,350
使自动驾驶汽车能够以每小时 90 公里的速度行驶，

353
00:22:27,740 --> 00:22:29,860
他们所做的就是，

354
00:22:29,860 --> 00:22:34,470
他们开发了一些非常快速的解决方案，

355
00:22:34,820 --> 00:22:41,335
把图像缩小到只需要看他们需要看的方面，

356
00:22:41,335 --> 00:22:43,480
他们假设世界上没有障碍，

357
00:22:43,480 --> 00:22:45,630
这使得问题变得容易得多，

358
00:22:45,800 --> 00:22:49,320
因为汽车所要做的就是保持在路上，

359
00:22:49,790 --> 00:22:55,140
所以，思考视觉处理如何提高是非常有趣的，

360
00:22:55,880 --> 00:23:00,090
从每十分钟一帧提高到每秒一百帧，

361
00:23:01,210 --> 00:23:04,575
这已经改变了自动驾驶汽车的游戏规则，

362
00:23:04,575 --> 00:23:08,270
我们又回到了硬件和软件之间的联系，

363
00:23:08,800 --> 00:23:13,160
我们两者都需要，才能为实际问题找到好的解决方案。

364
00:23:14,980 --> 00:23:19,310
自动驾驶中发生的另一件事是激光雷达传感器，

365
00:23:19,780 --> 00:23:22,820
减少了不确定性，增加了安全性。

366
00:23:23,320 --> 00:23:28,815
今天，我们有许多公司和组织正在部署自动驾驶汽车，

367
00:23:28,815 --> 00:23:31,365
这是一个来自新加坡的例子，

368
00:23:31,365 --> 00:23:33,710
这是我们部署的车辆，

369
00:23:33,790 --> 00:23:39,045
事实上，我们在 2014 年让公众乘坐我们的车辆，

370
00:23:39,045 --> 00:23:40,490
我们在 MIT 有车，

371
00:23:40,660 --> 00:23:44,780
我们还有很多其他的团队开发这些车辆，

372
00:23:45,550 --> 00:23:49,370
在我们有激光雷达之前，我们有声纳，

373
00:23:49,750 --> 00:23:51,830
当我们有声纳的时候，什么都不能用，

374
00:23:53,230 --> 00:23:57,830
因为，当你使用声纳时，

375
00:23:58,180 --> 00:24:02,270
声纳波束只是向前移动，然后它们就会反弹，

376
00:24:02,860 --> 00:24:06,950
如果角度大约是正负 7 度，

377
00:24:07,720 --> 00:24:09,500
你会听到 ping 回来，

378
00:24:09,820 --> 00:24:13,580
但是如果角度，所以如果声纳在一个平面，

379
00:24:13,660 --> 00:24:19,490
与声纳方向成七度角以上，

380
00:24:19,690 --> 00:24:22,310
那个声纳 ping 就会反弹，

381
00:24:22,510 --> 00:24:25,735
它会反弹到另一个物体和墙壁，

382
00:24:25,735 --> 00:24:30,180
你会得到方向测量错误，距离测量错误。

383
00:24:31,550 --> 00:24:33,780
有了激光雷达，这个问题就消失了，

384
00:24:33,830 --> 00:24:39,210
突然之间，一个强大、准确的传感器产生了巨大的变化，

385
00:24:39,230 --> 00:24:43,680
所有在声纳上开发的、不起作用的算法都开始工作，

386
00:24:43,700 --> 00:24:45,210
当激光雷达被引入时，

387
00:24:45,320 --> 00:24:46,200
这真的很令人兴奋。

388
00:24:48,010 --> 00:24:53,330
好的，现在，当我们思考自动驾驶，

389
00:24:53,470 --> 00:24:56,580
会出现几个关键参数，

390
00:24:56,580 --> 00:25:00,890
当我们思考这些系统的能力时，

391
00:25:01,540 --> 00:25:03,090
有一个问题是，

392
00:25:03,090 --> 00:25:05,895
汽车行驶的环境有多复杂，

393
00:25:05,895 --> 00:25:08,775
如果像德国的情况一样，这是一条空路，

394
00:25:08,775 --> 00:25:10,310
那么问题就容易多了，

395
00:25:11,650 --> 00:25:13,050
然后我们必须问自己，

396
00:25:13,050 --> 00:25:16,850
汽车和环境之间的相互作用有多复杂，

397
00:25:17,050 --> 00:25:18,710
我们还必须考虑，

398
00:25:19,000 --> 00:25:20,955
汽车的推理有多复杂，

399
00:25:20,955 --> 00:25:22,460
汽车开得有多快，

400
00:25:23,290 --> 00:25:27,020
所有这些问题背后都有一个根本性的问题，

401
00:25:27,400 --> 00:25:29,150
而这个根本性的问题是，

402
00:25:29,320 --> 00:25:31,220
汽车如何应对不确定性，

403
00:25:32,800 --> 00:25:34,580
现在你已经看到，

404
00:25:34,660 --> 00:25:38,210
机器学习具有与之相关的不确定性，

405
00:25:38,710 --> 00:25:43,520
所以，当你考虑在安全关键型应用程序上部署机器学习时，

406
00:25:43,660 --> 00:25:44,840
这是极其重要的，

407
00:25:45,250 --> 00:25:55,670
考虑你的环境、你要部署的模型的不确定性和实际应用程序需求之间的联系。

408
00:25:56,520 --> 00:25:57,285
我要告诉你，

409
00:25:57,285 --> 00:26:02,540
今天我们有非常有效和可部署的机器人汽车解决方案，

410
00:26:02,590 --> 00:26:05,655
可以在简单的环境中安全移动，

411
00:26:05,655 --> 00:26:08,900
那里没有太多静态或移动的障碍物，

412
00:26:09,760 --> 00:26:12,740
你可以从这个例子中看到，

413
00:26:12,910 --> 00:26:15,440
这是一个 MIT 汽车的例子，

414
00:26:15,730 --> 00:26:17,960
你可以看到，

415
00:26:18,040 --> 00:26:22,440
这辆车在 Fort Devens 自动驾驶，没有任何问题，

416
00:26:22,440 --> 00:26:24,920
那里没有太多障碍，

417
00:26:25,340 --> 00:26:28,385
这辆车完全有能力避开障碍物，

418
00:26:28,385 --> 00:26:29,410
顺便说一下，那是我的车，

419
00:26:29,640 --> 00:26:34,790
所以我很高兴这辆车能够很好地避开障碍物，

420
00:26:34,790 --> 00:26:36,550
事实上，我非常确信，

421
00:26:36,570 --> 00:26:39,250
我说，好的，我们可以用我的车作为障碍。

422
00:26:40,230 --> 00:26:42,460
但是传感器在天气中不能很好地工作，

423
00:26:42,750 --> 00:26:49,150
如果下大雨或下雪，感知系统的不确定性会显著增加，

424
00:26:49,530 --> 00:26:53,320
而车辆先验的不确定性也增加了，

425
00:26:53,700 --> 00:26:55,900
在极端拥堵的情况下，

426
00:26:56,010 --> 00:26:58,570
车辆行驶不稳定，

427
00:26:58,800 --> 00:27:00,160
人们骑着踏板车，

428
00:27:00,420 --> 00:27:02,890
甚至路上还有牛，

429
00:27:03,180 --> 00:27:07,660
这是我在班加罗尔打车时拍摄的一段视频，

430
00:27:08,070 --> 00:27:08,920
牛来了。

431
00:27:11,520 --> 00:27:15,070
所以，有这么多重要的前提条件，

432
00:27:15,570 --> 00:27:21,670
其中许多前提条件围绕着感知、规划、学习、推理和执行方面的不确定性，

433
00:27:22,080 --> 00:27:24,070
然后我们进入 RoboTaxi 之前，

434
00:27:25,760 --> 00:27:28,350
我们可以有许多其他的机器人解决方案，

435
00:27:29,030 --> 00:27:32,700
这些解决方案在今天可以发生，

436
00:27:33,500 --> 00:27:34,975
所以我想告诉你，

437
00:27:34,975 --> 00:27:40,410
许多公司和研究团队正在部署和开发自动驾驶汽车，

438
00:27:41,000 --> 00:27:44,310
其中许多公司遵循一个非常简单的解决方案，

439
00:27:44,570 --> 00:27:47,920
你可以采用这个解决方案，将你的汽车变成自动驾驶汽车，

440
00:27:47,920 --> 00:27:49,170
所以这就是你要做的，

441
00:27:49,800 --> 00:27:53,910
你拿着你的车，你把它扩展到有线驾驶，

442
00:27:53,910 --> 00:27:59,690
这样你的计算机就可以与方向盘、加速和油门控制进行交互，

443
00:28:00,650 --> 00:28:04,525
然后，你进一步扩展这辆车的传感器，

444
00:28:04,525 --> 00:28:08,400
我们使用的大多数传感器是摄像头和激光雷达，

445
00:28:09,350 --> 00:28:12,300
然后是一套软件模块，

446
00:28:12,560 --> 00:28:14,425
其中包括一个感知模块，

447
00:28:14,425 --> 00:28:20,310
它为绘制地图和检测静态和动态障碍物提供支持，

448
00:28:20,900 --> 00:28:22,980
然后我们有一个评估模块，

449
00:28:23,570 --> 00:28:27,415
来识别机器人的位置，

450
00:28:27,415 --> 00:28:38,610
它通过将感知系统现在看到的东西与通过查看基础设施创建的地图进行比较来做到这一点，

451
00:28:39,530 --> 00:28:43,080
最后，还有一个学习计划和控制系统，

452
00:28:43,220 --> 00:28:47,010
它可以根据汽车所在的位置计算出它应该做什么，

453
00:28:47,450 --> 00:28:49,530
所以就是这样，这是清单，

454
00:28:50,270 --> 00:28:53,790
你可以把你的车变成一辆自动驾驶汽车，

455
00:28:54,140 --> 00:28:55,290
当你这样做的时候，

456
00:28:55,310 --> 00:28:59,130
你真的必须从根本上考虑，

457
00:28:59,920 --> 00:29:04,550
你必须制作的计算单元是什么，

458
00:29:05,560 --> 00:29:07,580
所以你必须处理传感器数据，

459
00:29:07,990 --> 00:29:09,620
你必须检测障碍物，

460
00:29:09,730 --> 00:29:11,240
你必须定位车辆，

461
00:29:11,410 --> 00:29:12,800
你必须规划，

462
00:29:13,030 --> 00:29:14,090
然后你必须移动。

463
00:29:15,370 --> 00:29:21,890
有如此多的工作来解决涉及到自主导航的每一个子任务，

464
00:29:22,330 --> 00:29:25,215
其中一些工作是基于模型的，

465
00:29:25,215 --> 00:29:28,160
一些工作是基于机器学习的，

466
00:29:29,290 --> 00:29:31,725
但真正有趣的是，

467
00:29:31,725 --> 00:29:37,700
在这条自动驾驶管道中，经典的自动驾驶管道，

468
00:29:38,880 --> 00:29:40,300
有很多参数，

469
00:29:41,070 --> 00:29:44,170
所以对于这些单个问题的每一个解决方案，

470
00:29:44,490 --> 00:29:50,710
你必须为汽车将遇到的任何类型的道路情况向工程师提供参数，

471
00:29:51,330 --> 00:29:54,980
然后，你将不得不考虑如何将模块缝合在一起，

472
00:29:54,980 --> 00:29:59,600
你需要定义将模块连接在一起的参数，

473
00:29:59,600 --> 00:30:03,140
这很难用一种很健壮的方式来实现，

474
00:30:03,140 --> 00:30:05,530
它给这些解决方案带来了脆弱性，

475
00:30:06,090 --> 00:30:09,590
事实上，你必须考虑这些参数是什么，

476
00:30:09,590 --> 00:30:10,840
如果你在夜间开车，

477
00:30:11,220 --> 00:30:12,940
或者下雨的天气，

478
00:30:12,960 --> 00:30:16,235
或者你在乡下的乡间道路上，

479
00:30:16,235 --> 00:30:18,230
或者在城市的道路上，

480
00:30:18,230 --> 00:30:20,620
或者你在没有车道标志的道路上，

481
00:30:22,380 --> 00:30:25,390
所以，这些都是具有挑战性的事情，

482
00:30:25,740 --> 00:30:31,330
自动驾驶的第一个解决方案必须处理的。

483
00:30:32,180 --> 00:30:35,520
现在，在 Alexander 的博士论文中，

484
00:30:35,520 --> 00:30:39,650
他的想法是利用大型数据集，

485
00:30:40,120 --> 00:30:44,480
来了解人类在类似情况下的表现，

486
00:30:45,070 --> 00:30:47,300
并开发自动驾驶解决方案，

487
00:30:47,470 --> 00:30:52,970
驾驶起来比传统管道更像人类，

488
00:30:53,020 --> 00:30:56,630
如果你愿意的话，传统管道更像机器人。

489
00:30:58,500 --> 00:30:59,680
那么接下来的问题是，

490
00:30:59,730 --> 00:31:05,860
我们如何使用机器学习来直接从传感器到执行机构，

491
00:31:06,450 --> 00:31:09,790
换句话说，我们能压缩中间的所有东西，

492
00:31:10,050 --> 00:31:13,780
并用学习将感知和行动直接联系起来。

493
00:31:16,220 --> 00:31:19,230
所以，我们采用的解决方案，

494
00:31:20,030 --> 00:31:22,650
建立在我们已经讨论过的东西之上，

495
00:31:23,660 --> 00:31:26,520
我们可以使用深度学习和强化学习，

496
00:31:26,870 --> 00:31:35,310
将我们从道路的图像带到转向和油门上，决定要做什么，

497
00:31:37,440 --> 00:31:39,760
所以这真的很棒，

498
00:31:40,020 --> 00:31:43,240
因为你可以在某些类型的道路上训练，

499
00:31:44,180 --> 00:31:52,060
然后，你可以把你的车放在完全不同的驾驶环境和驾驶情况下。

500
00:31:52,410 --> 00:31:53,920
你不需要新的参数，

501
00:31:54,360 --> 00:31:55,450
你不需要再训练，

502
00:31:55,800 --> 00:31:59,620
你可以直接直接去做汽车必须做的事情。

503
00:32:01,570 --> 00:32:03,920
换句话说，我们可以学习一种模型，

504
00:32:04,570 --> 00:32:06,080
从原始的感知出发，

505
00:32:06,250 --> 00:32:09,980
在这里，你可以把它想象成来自相机的像素，

506
00:32:11,130 --> 00:32:15,460
我们给车提供的另一样东西是嘈杂的街景地图，

507
00:32:15,750 --> 00:32:18,040
这些地图并不是高清地图，

508
00:32:18,060 --> 00:32:24,010
通常由自动驾驶实验室和公司制作。

509
00:32:24,830 --> 00:32:25,950
所以你可以这样做，

510
00:32:26,480 --> 00:32:33,720
直接推断出在所有控制空间上的完全连续的概率分布，

511
00:32:34,130 --> 00:32:41,850
这里的红线表示投射到图像框上的车辆的推断轨迹，

512
00:32:42,530 --> 00:32:47,040
不透明度用我们的模型表示学习到的密度函数，

513
00:32:47,720 --> 00:32:50,820
这是通过训练深度学习模型来完成的，

514
00:32:51,290 --> 00:32:59,670
并且深度学习模型可以直接从人类驾驶数据输出该条件分布的参数。

515
00:33:02,620 --> 00:33:05,960
好的，更准确地说，

516
00:33:06,400 --> 00:33:12,380
我们学习系统的输入包括三个摄像头，

517
00:33:12,850 --> 00:33:15,830
前视摄像头和两个侧视摄像头，

518
00:33:16,600 --> 00:33:19,820
还有街景地图，

519
00:33:21,310 --> 00:33:27,170
而且从这个数据进行处理，

520
00:33:27,460 --> 00:33:30,230
从这些数据中，

521
00:33:30,460 --> 00:33:38,900
我们可以学习最大限度地提高特定情况下特定控制信号的可能性，

522
00:33:40,370 --> 00:33:47,340
令人惊讶的是，这个解决方案还允许我们对车辆进行本地化，

523
00:33:48,580 --> 00:33:49,830
所以这真的是非常令人兴奋。

524
00:33:52,850 --> 00:33:57,850
好的，所以我们可以像人类一样控制，

525
00:33:57,850 --> 00:34:00,445
但这种像人类一样的控制需要大量的数据，

526
00:34:00,445 --> 00:34:01,860
我在一开始就告诉过你们，

527
00:34:02,090 --> 00:34:03,840
我们必须小心处理数据，

528
00:34:04,040 --> 00:34:05,820
因为有很多边界条件，

529
00:34:06,740 --> 00:34:11,190
有很多典型的近事故情况，

530
00:34:11,540 --> 00:34:14,580
很难产生真实的数据。

531
00:34:16,080 --> 00:34:17,260
所以，举个例子，

532
00:34:18,890 --> 00:34:19,300
让我们看看，

533
00:34:19,920 --> 00:34:22,100
例如，如果你有，

534
00:34:23,620 --> 00:34:28,550
如果你想确保汽车不会对事故负责，

535
00:34:28,570 --> 00:34:32,270
并且在遇到这样的道路情况时，知道该怎么做，

536
00:34:32,470 --> 00:34:33,650
这将是相当昂贵的，

537
00:34:34,180 --> 00:34:37,550
开一辆车撞毁那辆车，来生成数据。

538
00:34:39,130 --> 00:34:41,300
所以，我们所做的是，

539
00:34:41,830 --> 00:34:44,330
我们在模拟中进行训练，

540
00:34:45,460 --> 00:34:50,420
所以 Alexander 开发了 Vista 模拟器，

541
00:34:51,040 --> 00:34:57,430
Vista 模拟器可以对多个代理、多种类型的传感器，

542
00:34:57,430 --> 00:35:00,990
以及多种类型的代理之间的交互进行建模，

543
00:35:02,030 --> 00:35:08,070
Vista 模拟器最近是开源的，

544
00:35:08,420 --> 00:35:12,370
你可以从 vista.csail.mit.edu 获得代码，

545
00:35:12,370 --> 00:35:15,900
而且很多人已经在使用这个系统了。

546
00:35:16,580 --> 00:35:18,210
所以我们从 Vista 得到什么，

547
00:35:19,030 --> 00:35:24,540
是模拟不同物理感应模式的能力，

548
00:35:24,540 --> 00:35:31,280
这意味着包括 2D 摄像机， 3D 激光雷达，活动摄像机等等，

549
00:35:32,170 --> 00:35:41,600
然后你有可能模拟不同类型的环境，情况和扰动，

550
00:35:41,740 --> 00:35:45,020
你可以模拟天气，你可以模拟不同的照明，

551
00:35:45,610 --> 00:35:48,030
你可以模拟不同类型的道路，

552
00:35:48,030 --> 00:35:52,670
你还可以模拟不同类型的互动。

553
00:35:53,110 --> 00:35:54,860
下面是我们使用 Vista 的方法，

554
00:35:57,840 --> 00:36:04,480
我们可以获取一个高质量的数据集，

555
00:36:05,550 --> 00:36:07,990
从人类驾驶的车辆上，

556
00:36:08,850 --> 00:36:12,310
我们可以利用这个非常高清晰度的数据集，

557
00:36:12,600 --> 00:36:16,660
然后在模拟中，我们可以把它变成我们想要的任何东西，

558
00:36:16,830 --> 00:36:18,880
我们可以把它变成不稳定的驾驶，

559
00:36:18,930 --> 00:36:22,810
我们可以把它变成接近事故的情况，

560
00:36:23,040 --> 00:36:25,690
我们可以把它变成任何你想要的东西，

561
00:36:26,040 --> 00:36:30,310
例如，你可以在这里看到我们的原始数据，

562
00:36:30,660 --> 00:36:35,140
你可以看到如何在模拟中映射原始数据，

563
00:36:36,300 --> 00:36:42,280
并以一种非常逼真的方式映射到新的模拟轨迹中，

564
00:36:42,450 --> 00:36:43,570
这个轨迹是不稳定的，

565
00:36:43,830 --> 00:36:48,040
现在作为我们 Vista 中训练集的一部分存在。

566
00:36:49,820 --> 00:36:52,770
所以我们可以这样做，

567
00:36:53,450 --> 00:36:54,540
我们可以使用这些数据，

568
00:36:54,860 --> 00:36:56,730
然后我们可以学习一项策略，

569
00:36:56,900 --> 00:36:59,040
我们可以离线评估这一策略，

570
00:36:59,270 --> 00:37:01,800
并最终将其部署在车辆上。

571
00:37:02,300 --> 00:37:07,230
这是通过首先更新所有代理的状态，

572
00:37:07,760 --> 00:37:10,290
根据车辆动力学和相互作用，

573
00:37:10,790 --> 00:37:17,220
然后从代理的新视角重建世界，

574
00:37:17,420 --> 00:37:18,330
一旦你移动它们，

575
00:37:18,800 --> 00:37:25,170
在代理看来，世界将与原始驾驶数据集中不同。

576
00:37:25,550 --> 00:37:28,560
最后，我们可以，

577
00:37:28,880 --> 00:37:33,810
然后我们可以从不同的代理视角渲染图像，

578
00:37:34,070 --> 00:37:37,560
我们可以执行控制。

579
00:37:39,220 --> 00:37:41,115
所以，这真的很酷，

580
00:37:41,115 --> 00:37:43,730
还有其他几个模拟引擎，

581
00:37:44,080 --> 00:37:46,580
有一些模拟引擎，

582
00:37:46,780 --> 00:37:54,230
依赖于 Imitation Learning 或 Domain Randomization 或 CARLA ，

583
00:37:54,430 --> 00:37:56,630
它非常有效，看起来是真实的，

584
00:37:57,580 --> 00:38:00,740
然而，我们的解决方案比所有其他解决方案工作得更好，

585
00:38:00,820 --> 00:38:03,770
在这里你可以看到比较的结果，

586
00:38:04,210 --> 00:38:12,110
Vista 中发生的情况与现有模拟器中最先进的情况。

587
00:38:12,940 --> 00:38:16,130
所以，上面的线以红色显示撞车地点，

588
00:38:16,480 --> 00:38:21,080
下面的线以颜色显示平均轨迹变化，

589
00:38:21,590 --> 00:38:25,405
你可以看到我们的解决方案确实做得最好，

590
00:38:25,405 --> 00:38:35,310
事实上，这个解决方案能够做其他基于控制学习的模拟不能做的事情，

591
00:38:36,290 --> 00:38:38,010
例如，在我们的解决方案中，

592
00:38:38,270 --> 00:38:43,860
我们能够从指向车辆离开道路的方向恢复，

593
00:38:44,720 --> 00:38:49,740
或者我们能够从错误的车道中恢复。

594
00:38:50,740 --> 00:38:54,945
这是一辆执行基于控制学习的车辆，

595
00:38:54,945 --> 00:38:57,770
这是 Alexander 和他的车辆，

596
00:38:57,880 --> 00:39:02,270
这辆车是用城市驾驶的数据训练的，

597
00:39:02,800 --> 00:39:05,120
现在他正开车去足球场，

598
00:39:05,720 --> 00:39:07,570
你可以看到，

599
00:39:07,800 --> 00:39:11,585
他能够开这辆车，让车送他去足球场，

600
00:39:11,585 --> 00:39:13,370
不需要做任何训练，

601
00:39:13,370 --> 00:39:16,180
也不需要看过这条路，

602
00:39:16,440 --> 00:39:20,405
也不需要明确提供这条路的数据，

603
00:39:20,405 --> 00:39:21,760
所以这很酷，对吧。

604
00:39:22,920 --> 00:39:26,350
好的，所以，好的，我要为你打开[引擎盖]，

605
00:39:27,200 --> 00:39:32,730
我将向你展示此解决方案的决策引擎内部发生了什么，

606
00:39:33,800 --> 00:39:36,600
所以，让我在这张图片中为你定位，

607
00:39:37,640 --> 00:39:40,680
右下角，你可以看到环境地图，

608
00:39:41,270 --> 00:39:43,800
左上角，你看到摄像头输入流，

609
00:39:44,210 --> 00:39:48,480
左下角，你看到车辆的注意力图，

610
00:39:48,590 --> 00:39:51,420
然后在中间，你可以看到决策引擎，

611
00:39:51,950 --> 00:39:56,850
决策引擎有大约 10 万个神经元和大约 50 万个参数。

612
00:39:57,920 --> 00:40:03,790
我会向你挑战弄清楚，

613
00:40:03,790 --> 00:40:10,990
是否有任何模式将神经元的状态与车辆的行为联系起来，

614
00:40:10,990 --> 00:40:12,220
真的很难看到，

615
00:40:12,220 --> 00:40:13,500
因为有太多这样的模式，

616
00:40:13,670 --> 00:40:18,480
在同一时间，有太多的事情同时发生。

617
00:40:19,390 --> 00:40:21,800
然后看一下注意力图，

618
00:40:22,090 --> 00:40:25,520
这辆车是在看着道路上的灌木丛，

619
00:40:25,540 --> 00:40:27,890
以便在道路上做出决定，

620
00:40:29,860 --> 00:40:32,210
尽管如此，它似乎做得相当好，

621
00:40:32,650 --> 00:40:34,190
但我们问自己，

622
00:40:34,450 --> 00:40:35,330
我们能做得更好吗，

623
00:40:35,980 --> 00:40:39,920
我们能有更可靠的基于学习的解决方案。

624
00:40:40,900 --> 00:40:49,040
昨天 Ramin 介绍了液态网络，并介绍了神经回路策略，

625
00:40:50,020 --> 00:40:54,210
所以我只想深入到这个领域，

626
00:40:54,710 --> 00:40:56,370
因为你现在可以比较，

627
00:40:56,600 --> 00:41:02,430
你现在可以理解最初的引擎是如何工作的，

628
00:41:02,480 --> 00:41:06,900
你可以把它与我们从液态网络中获得的进行比较。

629
00:41:07,910 --> 00:41:11,430
看看这个，我们有 19 个神经元，

630
00:41:11,510 --> 00:41:18,480
现在很容易看到这些神经元的激活模式，

631
00:41:18,800 --> 00:41:22,050
并将它们与车辆的行为联系起来，

632
00:41:22,860 --> 00:41:26,520
注意力地图也干净多了，

633
00:41:26,520 --> 00:41:30,915
车辆看着道路地平线和道路两边，

634
00:41:30,915 --> 00:41:33,080
这是我们开车时都会做的事情。

635
00:41:35,060 --> 00:41:41,070
记得， Ramin 告诉我们，

636
00:41:41,120 --> 00:41:44,910
这个模型叫做液态时间常数网络，

637
00:41:45,680 --> 00:41:50,340
是一个连续的时间网络，

638
00:41:53,150 --> 00:41:56,820
这个模型改变了神经元计算的内容，

639
00:41:57,230 --> 00:42:01,410
特别是我们从行为良好的状态空间模型开始，

640
00:42:01,520 --> 00:42:03,870
以增加神经元在学习过程中的稳定性，

641
00:42:04,580 --> 00:42:08,125
然后，我们对突触输入进行非线性处理，

642
00:42:08,125 --> 00:42:10,170
以增加模型的表现力，

643
00:42:11,060 --> 00:42:15,630
并在训练和推理过程中增加模型状态，

644
00:42:16,040 --> 00:42:19,765
通过将这些方程相互插入，

645
00:42:19,765 --> 00:42:23,400
我们可以看到 LTC 神经元的方程，

646
00:42:24,810 --> 00:42:26,830
这里的函数，

647
00:42:28,470 --> 00:42:33,070
这里的函数，不仅决定神经元的状态，

648
00:42:34,450 --> 00:42:44,000
而且这个函数还可以在执行推理时由新的输入来控制，

649
00:42:44,650 --> 00:42:47,900
所以，这个模型真正酷的地方在于，

650
00:42:48,520 --> 00:42:55,130
它能够在训练后根据它看到的输入动态适应，

651
00:42:55,890 --> 00:42:59,020
这是液态网络的一个非常强大的东西。

652
00:42:59,850 --> 00:43:03,580
现在，除了改变神经元方程，

653
00:43:04,800 --> 00:43:06,430
我们还改变了连线，

654
00:43:06,690 --> 00:43:08,960
这种新型的连线，

655
00:43:08,960 --> 00:43:14,740
本质上将函数给了深层神经网络中的神经元，

656
00:43:15,090 --> 00:43:16,540
每个神经元都是一样的，

657
00:43:16,890 --> 00:43:18,640
在我们的体系结构中，

658
00:43:19,200 --> 00:43:20,765
我们有输入神经元，

659
00:43:20,765 --> 00:43:22,960
我们有控制神经元，

660
00:43:22,980 --> 00:43:24,370
我们有中间神经元，

661
00:43:24,600 --> 00:43:26,140
它们各自做不同的事情。

662
00:43:26,460 --> 00:43:27,560
考虑到这一点，

663
00:43:27,560 --> 00:43:33,250
我们可以再次看看由液态网络实现的漂亮的解决方案，

664
00:43:33,510 --> 00:43:37,600
这个解决方案让汽车保持在道路上行驶，

665
00:43:37,680 --> 00:43:42,880
只需要 19 个神经元就能实现这种功能。

666
00:43:43,680 --> 00:43:44,770
你可以在这里看到，

667
00:43:44,850 --> 00:43:48,880
解决方案的注意力非常集中，

668
00:43:49,350 --> 00:43:51,070
与其他模型相比，

669
00:43:51,240 --> 00:43:54,730
比如 CNN 或 CT-RNN 或 LSTM ，

670
00:43:55,380 --> 00:43:56,860
它们的噪音要大得多。

671
00:43:58,080 --> 00:44:03,160
所以，我希望你们现在能更好地理解流动网络是如何工作的，

672
00:44:03,210 --> 00:44:05,110
以及它们的特性是什么，

673
00:44:06,240 --> 00:44:10,370
现在我们可以把这个模型应用到许多其他问题上。

674
00:44:10,810 --> 00:44:12,800
所以这是一个我们称为 Kenyon Run 的问题，

675
00:44:13,360 --> 00:44:16,640
我们采用了液态网络，

676
00:44:16,900 --> 00:44:21,620
并在一个飞行的飞机上实施了它，

677
00:44:22,660 --> 00:44:23,930
只有一个自由度，

678
00:44:24,010 --> 00:44:26,070
飞机只能上下移动，

679
00:44:26,070 --> 00:44:28,350
但它必须撞到这些障碍物，

680
00:44:28,350 --> 00:44:32,565
这些障碍物位于飞机不知道的位置，

681
00:44:32,565 --> 00:44:35,000
飞机也不知道环境是什么样子的，

682
00:44:36,190 --> 00:44:38,330
所以特别是当你有，

683
00:44:38,590 --> 00:44:44,270
当你用一个自由度控制飞机执行任务时，

684
00:44:44,470 --> 00:44:48,320
你所需要的只是 11 个液态神经元，

685
00:44:48,880 --> 00:44:51,950
如果你想控制飞机的所有自由度，

686
00:44:52,480 --> 00:44:54,195
那么你需要 24 个神经元，

687
00:44:54,195 --> 00:44:58,220
它仍然比我们今天谈论的巨大模型小得多。

688
00:44:59,140 --> 00:45:02,580
这是另一项任务，我们称之为无人机躲避球，

689
00:45:02,580 --> 00:45:08,190
目标是让无人机保持在指定的位置，

690
00:45:08,720 --> 00:45:14,100
无人机必须保护自己，当球飞过来的时候，

691
00:45:14,630 --> 00:45:18,580
你可以看到无人机躲避球的两个自由度的解决方案，

692
00:45:18,580 --> 00:45:19,470
这就是网络，

693
00:45:19,610 --> 00:45:23,610
你可以看到它是如何激发所有神经元的，

694
00:45:24,140 --> 00:45:28,890
你可以真正将函数联系起来，

695
00:45:29,240 --> 00:45:35,250
这种基于学习的控制与神经元的激活模式联系起来，

696
00:45:36,070 --> 00:45:37,100
这非常令人兴奋，

697
00:45:37,300 --> 00:45:42,470
因为，事实上，我们能够从这些类型的解决方案中提取决策树，

698
00:45:42,730 --> 00:45:49,790
这些决策树提供了人类可以理解的解释，

699
00:45:50,730 --> 00:45:54,460
这对安全关键系统来说非常重要。

700
00:45:56,310 --> 00:45:59,140
好的，我们来看看，

701
00:45:59,370 --> 00:46:01,510
Ramin 告诉你们，

702
00:46:01,890 --> 00:46:04,660
这些液态网络是动态因果模型，

703
00:46:04,740 --> 00:46:07,480
我想给你们展示更多的例子，

704
00:46:07,830 --> 00:46:12,610
解释这些模型如何是动态因果模型。

705
00:46:13,110 --> 00:46:14,710
所以在这里你可以看到，

706
00:46:14,880 --> 00:46:16,715
所以我们在这里研究的任务是，

707
00:46:16,715 --> 00:46:21,070
在树木繁茂的环境中寻找物体，

708
00:46:21,810 --> 00:46:26,170
这里有一些数据的例子，

709
00:46:26,580 --> 00:46:28,685
我们用来这项任务，

710
00:46:28,685 --> 00:46:35,740
从本质上说，我们让一名飞行员驾驶无人机来完成这项任务。

711
00:46:36,350 --> 00:46:37,920
现在，这就是数据，

712
00:46:39,320 --> 00:46:40,200
来看看这个，

713
00:46:40,820 --> 00:46:43,980
然后我们使用了一个标准的深度神经网络，

714
00:46:45,110 --> 00:46:48,930
我们要求这个网络来解决这个问题，

715
00:46:49,460 --> 00:46:53,310
而且网络的关注点地图无处不在，

716
00:46:53,540 --> 00:46:58,950
你可以看到深度神经网络的解决方案非常混乱，

717
00:46:59,450 --> 00:47:00,990
但看看其他的东西，

718
00:47:01,550 --> 00:47:05,250
我们收集的数据是夏季数据，

719
00:47:05,510 --> 00:47:06,510
而现在是秋天，

720
00:47:07,070 --> 00:47:09,060
所以，背景不再是绿色的，

721
00:47:09,530 --> 00:47:13,050
我们没有那么多树上的叶子，

722
00:47:13,550 --> 00:47:17,970
所以这项任务的背景完全改变了。

723
00:47:21,090 --> 00:47:27,010
相比之下，液态网络能够专注于任务，

724
00:47:27,540 --> 00:47:28,600
不会感到困惑，

725
00:47:29,010 --> 00:47:33,815
能够直接到达它需要找到的对象，

726
00:47:33,815 --> 00:47:38,620
看看这个例子的注意力图是多么清晰，

727
00:47:39,000 --> 00:47:40,390
所以这是非常令人兴奋的，

728
00:47:40,920 --> 00:47:43,360
因为它真的表明，

729
00:47:43,410 --> 00:47:44,470
对于液态网络，

730
00:47:44,970 --> 00:47:45,910
我们有一个解决方案，

731
00:47:46,290 --> 00:47:53,080
在某种意义上能够抽象出训练的背景，

732
00:47:53,430 --> 00:47:58,570
这意味着我们可以从一个环境到另一个环境进行零训练转移。

733
00:47:59,430 --> 00:48:04,510
此外，我们在冬季也做了同样的工作，

734
00:48:04,920 --> 00:48:06,290
那时我们不再有树叶了，

735
00:48:06,290 --> 00:48:08,140
我们有黑色的树干，

736
00:48:08,610 --> 00:48:15,730
环境看起来与我们训练的环境有很大不同，

737
00:48:16,290 --> 00:48:24,310
这种从一组训练数据转移到完全不同环境的能力，

738
00:48:24,660 --> 00:48:28,810
对于机器学习的能力来说是真正的变革性。

739
00:48:29,700 --> 00:48:31,450
嗯，我们做的还不止这些，

740
00:48:31,470 --> 00:48:34,330
所以我们采用了我们的系列解决方案，

741
00:48:34,560 --> 00:48:36,280
并在实验室中进行了部署，

742
00:48:37,860 --> 00:48:42,030
这是 Markram ，

743
00:48:42,530 --> 00:48:43,710
他致力于解决这个问题，

744
00:48:43,760 --> 00:48:44,940
他是，

745
00:48:45,620 --> 00:48:47,275
然后看一下注意力图，

746
00:48:47,275 --> 00:48:50,580
我的意思是，他的环境甚至不是树林，

747
00:48:50,870 --> 00:48:53,020
这是一间办公室，

748
00:48:53,020 --> 00:48:54,090
这是一个室内环境。

749
00:48:54,650 --> 00:48:57,030
我们看到其他例子，

750
00:48:57,230 --> 00:48:59,670
我们使用我们的解决方案，

751
00:48:59,900 --> 00:49:03,840
我们部署它来寻找相同的对象，那把椅子，

752
00:49:04,130 --> 00:49:06,000
就在这个数据大楼外面，

753
00:49:06,200 --> 00:49:09,390
这是深度神经网络解决方案，

754
00:49:09,560 --> 00:49:11,670
它完全被搞糊涂了，

755
00:49:11,690 --> 00:49:13,890
这是液态网络解决方案，

756
00:49:14,750 --> 00:49:16,465
它有完全相同的输入，

757
00:49:16,465 --> 00:49:21,990
没有问题去找机器人。

758
00:49:22,530 --> 00:49:24,545
让我们再看几个例子，

759
00:49:24,545 --> 00:49:27,790
我们正在一个一个地做，

760
00:49:28,050 --> 00:49:30,500
我们是在搜索物体，

761
00:49:30,500 --> 00:49:32,860
并做多步解决方案，

762
00:49:33,060 --> 00:49:35,240
事实上，我们可以，

763
00:49:35,240 --> 00:49:37,780
如果我能看到我的下一个视频，

764
00:49:39,010 --> 00:49:42,170
抱歉，所以，我，

765
00:49:42,430 --> 00:49:43,550
下一个是，

766
00:49:43,810 --> 00:49:46,280
它展示了我们实际上可以永远这样做，

767
00:49:46,300 --> 00:49:48,890
这是一个无限跳跃的演示，

768
00:49:49,420 --> 00:49:52,700
就在外面的棒球场上完成的，

769
00:49:52,990 --> 00:49:58,160
我们放置了三个我们训练过的相同对象，

770
00:49:58,480 --> 00:50:01,850
并将它们放置在未知位置，

771
00:50:02,050 --> 00:50:07,160
然后我们将搜索功能添加到我们的机器学习解决方案中，

772
00:50:07,750 --> 00:50:11,480
所以，系统可以继续运行，

773
00:50:11,740 --> 00:50:13,370
并从一个跳到另一个。

774
00:50:14,380 --> 00:50:16,430
我要给你们展示的最后一个例子是，

775
00:50:16,840 --> 00:50:19,940
在圣州大楼的露台上，

776
00:50:20,470 --> 00:50:23,720
我们放了一些椅子，

777
00:50:24,700 --> 00:50:26,310
我们放了我们最喜欢的椅子，

778
00:50:26,310 --> 00:50:30,110
但我们也放了很多其他类似的椅子，

779
00:50:30,220 --> 00:50:32,565
我们可以看到，

780
00:50:32,565 --> 00:50:35,330
液态网络的普适性很好，

781
00:50:35,380 --> 00:50:38,630
然而，如果我们采用 LSTM 解决方案，

782
00:50:39,100 --> 00:50:42,440
它会困惑，并到达错误的对象。

783
00:50:43,830 --> 00:50:47,350
那么，所有这些想法结合在一起，

784
00:50:47,790 --> 00:50:51,550
指向一种新型的机器学习，

785
00:50:52,110 --> 00:50:57,280
这就产生了可以概括到未知场景的模型，

786
00:50:57,360 --> 00:51:01,420
本质上解决了当今神经网络，

787
00:51:01,500 --> 00:51:04,900
不能很好地概括到未知测试场景的挑战，

788
00:51:06,150 --> 00:51:09,040
因为这些模型是如此快速和紧凑，

789
00:51:09,300 --> 00:51:10,570
你可以在线上训练它们，

790
00:51:10,890 --> 00:51:12,760
你可以在边缘设备上训练它们，

791
00:51:13,320 --> 00:51:18,680
你可以看到它们开始理解赋予它们的任务，

792
00:51:18,680 --> 00:51:19,835
所以你可以看到，

793
00:51:19,835 --> 00:51:24,460
我们真的开始了解这些系统的语义，

794
00:51:25,860 --> 00:51:27,275
这些系统必须做什么。

795
00:51:27,275 --> 00:51:30,540
所以，这与未来有什么关系？

796
00:51:32,790 --> 00:51:36,970
我认为使用机器学习来研究自然是非常令人兴奋的，

797
00:51:37,500 --> 00:51:41,140
并开始了解智能的本质，

798
00:51:42,150 --> 00:51:45,305
在我们的实验室 csail 中，

799
00:51:45,305 --> 00:51:47,080
我们有一个项目，

800
00:51:47,310 --> 00:51:51,370
看我们是否能够理解鲸鱼的生活，

801
00:51:52,230 --> 00:51:53,825
所以我这么说是什么意思。

802
00:51:53,825 --> 00:51:55,240
所以这是一个例子，

803
00:51:55,350 --> 00:52:01,600
我们使用了机器人无人机去做，

804
00:52:01,620 --> 00:52:03,460
抱歉，这是非常吵的。

805
00:52:07,140 --> 00:52:10,600
我们使用机器人无人机来寻找鲸鱼，

806
00:52:10,710 --> 00:52:12,370
看看它们做了什么，

807
00:52:12,750 --> 00:52:13,630
并跟踪它们，

808
00:52:14,280 --> 00:52:16,150
这是一些图像，

809
00:52:17,220 --> 00:52:22,385
这是这个系统能够做的一些剪辑，

810
00:52:22,385 --> 00:52:26,390
我们已经使用机器学习来识别鲸鱼，

811
00:52:26,390 --> 00:52:28,565
然后一旦你识别了鲸鱼，

812
00:52:28,565 --> 00:52:32,470
我们可以伺服到鲸鱼中心，

813
00:52:32,610 --> 00:52:35,675
沿途跟踪鲸鱼，

814
00:52:35,675 --> 00:52:38,615
这就是这个系统是如何运作的，

815
00:52:38,615 --> 00:52:43,780
你可以看到一群鲸鱼，

816
00:52:44,730 --> 00:52:51,400
你可以看到我们的机器人在鲸鱼移动时服务和跟踪它们。

817
00:52:52,170 --> 00:52:56,920
现在，这是一个非常令人兴奋的项目，

818
00:52:57,270 --> 00:53:01,570
因为鲸鱼是如此威严、聪明和神秘的生物，

819
00:53:02,360 --> 00:53:07,390
所以，如果我们能够使用我们的技术来更好地了解它们的生活，

820
00:53:08,040 --> 00:53:16,630
我们就能够更多地了解其他动物和其他生物，

821
00:53:16,710 --> 00:53:18,640
我们共享这个美丽星球。

822
00:53:19,290 --> 00:53:22,475
所以我们可以从空中研究这些鲸鱼，

823
00:53:22,475 --> 00:53:28,970
我们也可以大海里面来研究鲸鱼，

824
00:53:28,970 --> 00:53:32,290
这是 Sophie ，我们的软机器鱼，

825
00:53:32,730 --> 00:53:39,500
和 Joseph ，今天和我们一起参与建造的，

826
00:53:39,500 --> 00:53:46,180
这是一个漂亮的，非常自然的，移动的机器人，

827
00:53:46,380 --> 00:53:49,270
它可以接近水生生物，

828
00:53:49,380 --> 00:53:52,600
它可以像水生生物一样移动，

829
00:53:52,860 --> 00:53:55,910
而不会打扰它们，

830
00:53:55,910 --> 00:53:59,660
当你把机器人放在海洋环境中时，

831
00:53:59,660 --> 00:54:02,870
它们的行为与鱼不同，

832
00:54:02,870 --> 00:54:04,840
它们往往会吓到鱼。

833
00:54:05,220 --> 00:54:05,980
所以，如果你好奇，

834
00:54:07,260 --> 00:54:10,355
尾巴是由硅胶制成的，

835
00:54:10,355 --> 00:54:17,090
还有一个泵，它可以在它的两个腔室中抽水，

836
00:54:17,090 --> 00:54:19,280
所以你可以看到它有两个裂开的腔室，

837
00:54:19,280 --> 00:54:22,295
你可以把水从一个腔室转移到另一个腔室，

838
00:54:22,295 --> 00:54:25,810
根据我们移动的水量和比例，

839
00:54:25,980 --> 00:54:29,320
你可以让鱼向前移动，向左转或向右转。

840
00:54:30,180 --> 00:54:34,690
所以我们可以使用机器人技术来观察动物的运动，

841
00:54:34,920 --> 00:54:35,920
但我们可以做得更多，

842
00:54:36,180 --> 00:54:38,765
我们还可以收听，

843
00:54:38,765 --> 00:54:40,850
糟糕，其实我这里需要声音，

844
00:54:40,850 --> 00:54:41,800
我忘了这件事，

845
00:54:42,210 --> 00:54:46,685
我们可以观察它们互相说什么的方式，

846
00:54:46,685 --> 00:54:47,260
你能听到吗？

847
00:55:21,550 --> 00:55:23,780
这是抹香鲸，

848
00:55:24,820 --> 00:55:27,800
你们听到了抹香鲸的叫声，

849
00:55:28,240 --> 00:55:32,990
我们相信，抹香鲸在和它的家人和朋友说话，

850
00:55:33,760 --> 00:55:35,190
我们想知道它在说什么，

851
00:55:37,630 --> 00:55:38,660
我们不知道，

852
00:55:38,740 --> 00:55:41,180
但我们可以使用机器学习来取得进展。

853
00:55:42,140 --> 00:55:45,930
我们可以做到这一点的方法是通过，

854
00:55:48,560 --> 00:55:54,330
使用你刚刚听到的那种数据，

855
00:55:55,220 --> 00:55:59,100
去寻找语言的存在，

856
00:55:59,300 --> 00:56:01,350
这是智力的一个主要标志，

857
00:56:03,280 --> 00:56:05,805
我们可以看看我们是否有离散的复合词，

858
00:56:05,805 --> 00:56:07,970
我们可以看看是否有语法，

859
00:56:08,140 --> 00:56:11,120
我们可以看看我们是否有长期依存关系，

860
00:56:11,380 --> 00:56:15,530
我们可以看看我们是否有人类语言所具有的其他性质。

861
00:56:16,940 --> 00:56:22,345
所以，我们的项目正在进行中，

862
00:56:22,345 --> 00:56:25,260
今天我不能告诉你鲸鱼们在说什么，

863
00:56:25,490 --> 00:56:26,785
但我可以告诉你，

864
00:56:26,785 --> 00:56:27,900
我们已经取得了进展，

865
00:56:28,730 --> 00:56:29,970
我可以告诉你，

866
00:56:30,140 --> 00:56:35,490
我们开始发现他们通话的哪些部分携带了信息，

867
00:56:36,360 --> 00:56:39,620
我们可以使用机器学习来区分响声，

868
00:56:39,620 --> 00:56:42,340
允许鲸鱼回声定位，

869
00:56:42,750 --> 00:56:47,320
与似乎是发声和携带信息的响声，

870
00:56:48,550 --> 00:56:52,550
我们可以开始看看信息交换的协议是什么，

871
00:56:52,630 --> 00:56:54,170
他们如何进行对话，

872
00:56:54,960 --> 00:56:56,810
我们可以开始问，

873
00:56:57,040 --> 00:56:59,990
他们对彼此所说的信息是什么，

874
00:57:00,880 --> 00:57:04,845
所以，在我们的项目中，我们试图理解，

875
00:57:04,845 --> 00:57:10,020
鲸鱼的语音、语义、句法和语篇。

876
00:57:11,570 --> 00:57:16,410
我们有一个由大约 22000 次响声组成的大数据集，

877
00:57:17,240 --> 00:57:20,400
响声被分组为 coda ，

878
00:57:20,420 --> 00:57:22,350
coda 就像[音素]一样，

879
00:57:23,640 --> 00:57:25,750
使用机器学习，

880
00:57:25,830 --> 00:57:27,670
我们可以识别 coda 类型，

881
00:57:28,260 --> 00:57:31,690
我们可以识别 coda 交换的模式，

882
00:57:32,160 --> 00:57:37,090
我们可以开始真正问自己，

883
00:57:38,580 --> 00:57:43,450
鲸鱼是如何交换信息的。

884
00:57:44,080 --> 00:57:45,570
如果你对这个问题感兴趣，

885
00:57:45,710 --> 00:57:47,220
请来找我们，

886
00:57:47,420 --> 00:57:52,380
因为我们有很多非常令人兴奋的项目，

887
00:57:52,790 --> 00:57:55,200
对于反向工程很重要，

888
00:57:55,730 --> 00:58:02,730
这种真正非凡和威严的动物能够做什么。

889
00:58:04,120 --> 00:58:06,980
让我以此来结束我的演讲。

890
00:58:08,130 --> 00:58:14,080
在这节课中，你已经看到了许多非常令人兴奋的机器学习算法，

891
00:58:14,660 --> 00:58:20,290
但你也了解了机器学习算法的一些技术挑战，

892
00:58:21,460 --> 00:58:22,850
包括数据可用性，

893
00:58:23,290 --> 00:58:24,350
包括数据质量，

894
00:58:25,030 --> 00:58:28,980
包括所需的计算量、模型大小，

895
00:58:28,980 --> 00:58:34,340
以及该模型在边缘设备或大型设备上运行的能力，

896
00:58:35,550 --> 00:58:39,760
你已经看到，我们的许多解决方案都是黑盒解决方案，

897
00:58:39,870 --> 00:58:41,890
有时我们有脆弱的函数，

898
00:58:42,600 --> 00:58:46,120
我们有容易受到攻击的模型，

899
00:58:47,960 --> 00:58:50,370
你也看到了一些替代模型，

900
00:58:50,450 --> 00:58:51,390
比如液态网络，

901
00:58:51,950 --> 00:58:54,930
它们试图解决其中的一些问题，

902
00:58:56,030 --> 00:59:01,740
有如此多的机会来开发改进的机器学习，

903
00:59:01,760 --> 00:59:05,160
利用现有的模型和发明新的模型，

904
00:59:05,970 --> 00:59:07,180
如果我们能做到这一点，

905
00:59:08,070 --> 00:59:10,750
我们就可以创造一个令人兴奋的世界，

906
00:59:11,130 --> 00:59:13,880
在那里机器将真正增强我们的能力，

907
00:59:13,880 --> 00:59:21,100
增强我们的认知能力和身体能力。

908
00:59:22,080 --> 00:59:22,960
所以，想象一下，

909
00:59:23,740 --> 00:59:26,430
在你的私人助理的帮助下醒来，

910
00:59:27,340 --> 00:59:29,180
这会计算出最佳的时间，

911
00:59:29,530 --> 00:59:33,980
帮助你组织一天所需的所有物品，

912
00:59:34,570 --> 00:59:36,825
然后把它们带给你，

913
00:59:36,825 --> 00:59:39,650
这样你就不必考虑你的服装是否匹配，

914
00:59:41,090 --> 00:59:43,140
当你走过一家商店时，

915
00:59:43,370 --> 00:59:49,410
商店橱窗里的图像会显示你身上的最新时尚照片，

916
00:59:51,350 --> 00:59:53,020
而在店内，

917
00:59:53,020 --> 00:59:55,270
也许你想买某一双鞋，

918
00:59:55,270 --> 00:59:58,050
AI 系统可以分析你的走路方式，

919
00:59:58,070 --> 01:00:01,620
可以分析你的尺寸，

920
01:00:02,000 --> 01:00:06,600
还可以创建一双定制的鞋子，一个专为你定制的模型，

921
01:00:07,540 --> 01:00:09,350
然后，所以的衣服，

922
01:00:09,430 --> 01:00:12,560
我们环境中的所有的物品都可以被唤醒，

923
01:00:12,700 --> 01:00:15,380
我们的衣服可能会变成机器人，

924
01:00:15,700 --> 01:00:20,810
我们的衣服可以成为监测设备，

925
01:00:20,950 --> 01:00:23,730
但它们也可以成为可编程的，

926
01:00:23,730 --> 01:00:24,945
例如，在本例中，

927
01:00:24,945 --> 01:00:29,720
你可以看到毛衣可以改变颜色，

928
01:00:30,070 --> 01:00:32,540
以便女孩可以匹配她的朋友。

929
01:00:33,040 --> 01:00:35,565
现在，这实际上并不牵强，

930
01:00:35,565 --> 01:00:37,040
我们在校园里有一个小组，

931
01:00:37,690 --> 01:00:41,295
他们提供这些可编程[光纤]，

932
01:00:41,295 --> 01:00:44,540
他们可以变更颜色，它们可以进行一些计算。

933
01:00:46,210 --> 01:00:49,050
在智能会议室内工作，

934
01:00:49,050 --> 01:00:55,940
通过监测人们的舒适度和手势，可以自动调节温度，

935
01:00:55,960 --> 01:00:57,105
假以时日，

936
01:00:57,105 --> 01:01:03,710
全息图可以用来让虚拟世界变得更加逼真，

937
01:01:04,270 --> 01:01:05,480
更加相互联系，

938
01:01:06,460 --> 01:01:12,500
他们在这里讨论一种新型飞行汽车的设计，

939
01:01:13,030 --> 01:01:15,560
假设我们有这些会飞的汽车，

940
01:01:16,120 --> 01:01:19,910
然后我们可以将这些汽车与 IT 基础设施集成在一起，

941
01:01:20,520 --> 01:01:21,965
汽车会知道你的需求，

942
01:01:21,965 --> 01:01:23,180
这样它们就可以告诉你，

943
01:01:23,180 --> 01:01:26,740
比如，你在附近买到你一直想要的植物，

944
01:01:27,060 --> 01:01:28,690
通过计算一个小小的绕道，

945
01:01:29,340 --> 01:01:33,070
回到家里，你可以第一次骑自行车，

946
01:01:33,600 --> 01:01:35,690
自行车本身就变成了一个机器人，

947
01:01:35,690 --> 01:01:40,000
它有可适应的轮子，根据你的技能水平出现和消失，

948
01:01:40,620 --> 01:01:43,040
你可以拥有帮助种植的机器人，

949
01:01:43,040 --> 01:01:44,855
你可以有送货机器人，

950
01:01:44,855 --> 01:01:48,370
还有垃圾桶会自动取出。

951
01:01:50,350 --> 01:01:51,410
而在美好的一天过后，

952
01:01:51,460 --> 01:01:53,330
到了睡前讲故事的时候，

953
01:01:54,190 --> 01:01:56,270
你就可以开始进入故事，

954
01:01:56,320 --> 01:01:58,430
控制流程，

955
01:01:58,600 --> 01:02:02,030
开始与故事中的角色互动。

956
01:02:03,580 --> 01:02:07,040
这些都是未来的一些可能性，

957
01:02:07,480 --> 01:02:12,110
机器学习、人工智能和机器人所带来的，

958
01:02:12,650 --> 01:02:15,570
我个人对这个未来感到非常兴奋，

959
01:02:15,770 --> 01:02:19,800
机器人将帮助我们进行认知和体力工作。

960
01:02:20,830 --> 01:02:25,730
但是这一未来真的取决于 非常重要的新进展，

961
01:02:26,020 --> 01:02:27,830
你们所能取得的（进展），

962
01:02:28,240 --> 01:02:33,260
我非常兴奋地看到你们在未来几年将会做些什么。

963
01:02:33,430 --> 01:02:34,490
所以非常感谢你，

964
01:02:34,750 --> 01:02:36,560
和我们一起工作吧。

