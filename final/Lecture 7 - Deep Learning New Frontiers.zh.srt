1
00:00:09,410 --> 00:00:15,000
这一节课是我绝对最喜欢的深度学习入门课程，

2
00:00:15,440 --> 00:00:20,190
重点讲述当今深度学习方法的局限性，

3
00:00:20,630 --> 00:00:24,720
以及这些局限性和突出挑战如何真正激励

4
00:00:25,010 --> 00:00:29,970
深度学习和人工智能前沿和新领域的新研究。

5
00:00:31,410 --> 00:00:32,420
在我们开始之前，

6
00:00:32,420 --> 00:00:38,650
我们有几件[后勤]方面的事情要讨论，

7
00:00:38,940 --> 00:00:44,200
首先可能是本课程中最重要的方面之一，

8
00:00:44,280 --> 00:00:48,350
我们有设计和赠送 T恤的传统，

9
00:00:48,350 --> 00:00:49,390
作为课程的一部分，

10
00:00:49,650 --> 00:00:51,245
希望你们都能看到，

11
00:00:51,245 --> 00:00:53,650
今天它们就在这里，就在前面，

12
00:00:54,090 --> 00:00:59,050
所以我们将在今天的编程结束时分发 T恤，

13
00:00:59,220 --> 00:01:03,250
所以，请留下来，如果你想要一件 T恤。

14
00:01:04,610 --> 00:01:07,375
好的，所以在我们现在的位置，

15
00:01:07,375 --> 00:01:08,370
我们有一节课，

16
00:01:09,530 --> 00:01:13,020
我要讲的是深度学习的局限性和新的领域，

17
00:01:13,460 --> 00:01:16,440
在那之后我们还会有三场讲座，

18
00:01:16,790 --> 00:01:19,890
继续我们今年的客座讲座系列。

19
00:01:21,320 --> 00:01:28,770
重要的是，我们仍然有围绕软件实验和项目推介提案的竞争，

20
00:01:29,990 --> 00:01:32,010
对于项目推介提案，

21
00:01:32,120 --> 00:01:35,100
我们要求你在今晚之前上传幻灯片，

22
00:01:35,360 --> 00:01:38,880
我们在课程大纲上有关于这方面的所有详细说明，

23
00:01:39,440 --> 00:01:43,530
如果你有兴趣提交实验竞赛，

24
00:01:43,970 --> 00:01:48,925
截止日期已经延长到明天下午 1 点，

25
00:01:48,925 --> 00:01:52,380
因此，请同时提交实验。

26
00:01:53,150 --> 00:01:59,400
希望动机不仅仅是锻炼你在深度学习中的技能和积累知识，

27
00:01:59,840 --> 00:02:03,270
而且我们还有这些令人惊叹的比赛和奖品，

28
00:02:03,410 --> 00:02:07,860
对于每个实验以及项目推介比赛。

29
00:02:08,510 --> 00:02:13,470
为了提醒你第一个关于设计神经网络以产生音乐的实验，

30
00:02:14,240 --> 00:02:18,600
我们有与音频相关的奖项悬而未决，

31
00:02:18,860 --> 00:02:20,850
比赛是公开的，

32
00:02:21,440 --> 00:02:22,615
可以是任何人的比赛，

33
00:02:22,615 --> 00:02:25,950
所以请提交你们的参赛作品。

34
00:02:27,450 --> 00:02:30,580
明天我们将举行项目推介大赛，

35
00:02:30,900 --> 00:02:32,890
每年它都是这个节目的一个亮点，

36
00:02:33,330 --> 00:02:38,020
我们听到了你关于新的深度学习算法应用程序的惊人想法，

37
00:02:38,670 --> 00:02:41,380
一种快速的[鲨鱼缸]风格，

38
00:02:41,400 --> 00:02:43,270
三分钟的演讲真的很有趣，

39
00:02:43,620 --> 00:02:47,240
不仅能站在这里，有机会向大家展示，

40
00:02:47,240 --> 00:02:52,300
还能听到你的同学、同事们令人惊叹的想法，

41
00:02:53,730 --> 00:03:02,020
同样，一些关于如何将幻灯片提交到谷歌幻灯片卡片组的后勤信息也包括在教学大纲中。

42
00:03:03,450 --> 00:03:06,605
最后，正如我们在第一节课中介绍的，

43
00:03:06,605 --> 00:03:07,840
希望你们已经意识到，

44
00:03:08,130 --> 00:03:11,590
我们有一个令人兴奋的大奖竞赛，

45
00:03:12,030 --> 00:03:17,590
围绕着实验室的可信性、深度学习、稳健性、不确定性和偏见，

46
00:03:18,090 --> 00:03:21,850
再次强调，竞争现在是完全开放的，

47
00:03:22,020 --> 00:03:23,320
请提交您的参赛作品，

48
00:03:23,970 --> 00:03:27,760
非常令人兴奋，我们期待着收到你的参赛作品。

49
00:03:29,840 --> 00:03:35,670
好的，除了这门课的那些技术部分，

50
00:03:36,290 --> 00:03:39,810
我们还有三节客座讲座，

51
00:03:39,980 --> 00:03:42,120
来结束我们的系列讲座。

52
00:03:42,410 --> 00:03:46,540
我们昨天听到了来自 Themis AI 的 Sadhana 令人惊叹的演讲，

53
00:03:46,540 --> 00:03:48,930
关于稳健和值得信赖的深度学习。

54
00:03:49,550 --> 00:03:52,770
今天，我们将听取来自 Vanguard 公司的 Ramin Hasani 的演讲，

55
00:03:53,000 --> 00:03:56,010
他将谈论统计学的新时代，

56
00:03:56,600 --> 00:03:59,130
以及这对深度学习算法意味着什么。

57
00:03:59,980 --> 00:04:02,355
明天，我们将有两场精彩的嘉宾演讲，

58
00:04:02,355 --> 00:04:04,370
来自 Google 的 Dilip Krishnan ，

59
00:04:04,780 --> 00:04:10,370
以及来自 MIT CSAIL 主管 Daniela Rus ，

60
00:04:10,630 --> 00:04:18,200
是的，一些，我们知道观众中有很多 Daniela 的粉丝，包括我们。

61
00:04:18,400 --> 00:04:22,515
所以请出席，这些应该是非常棒的演讲，

62
00:04:22,515 --> 00:04:27,710
你将听到更多关于深度学习研究的前沿。

63
00:04:28,590 --> 00:04:33,820
好了，这就是我想要宣布的所有后勤和项目公告。

64
00:04:34,380 --> 00:04:36,455
现在我们可以真正深入到有趣的东西，

65
00:04:36,455 --> 00:04:38,470
这门课的技术内容。

66
00:04:39,510 --> 00:04:42,400
到目前为止，在深度学习入门中，

67
00:04:42,540 --> 00:04:45,850
你已经了解了神经网络算法的基础，

68
00:04:46,140 --> 00:04:47,560
也领略了，

69
00:04:47,580 --> 00:04:54,580
深度学习已经开始对许多不同的研究领域和应用产生影响，

70
00:04:55,440 --> 00:04:57,800
从自动驾驶汽车的进步，

71
00:04:58,420 --> 00:05:02,030
到思考医学和医疗保健应用，

72
00:05:03,040 --> 00:05:04,395
到强化学习，

73
00:05:04,395 --> 00:05:07,430
这在改变我们思考收益和游戏的方式，

74
00:05:08,260 --> 00:05:10,460
到新的生成性建模的进步，

75
00:05:10,900 --> 00:05:11,780
到机器人技术，

76
00:05:12,250 --> 00:05:14,420
再到许多其他应用，

77
00:05:14,770 --> 00:05:18,590
比如自然语言处理、金融、安全等。

78
00:05:19,860 --> 00:05:23,270
我们真正希望你从这门课程中学到的是，

79
00:05:23,270 --> 00:05:27,100
对深度神经网络如何工作的具体理解，

80
00:05:27,660 --> 00:05:35,410
以及这些基础算法是如何真正实现这些跨众多学科的进步的。

81
00:05:36,590 --> 00:05:38,970
你们在这门课和这个课程中已经看到，

82
00:05:39,350 --> 00:05:41,910
我们把神经网络作为一种方式，

83
00:05:41,960 --> 00:05:43,500
作为一种算法方式，

84
00:05:43,670 --> 00:05:48,745
来考虑从输入数据中提取信号或测量数据的形式，

85
00:05:48,745 --> 00:05:51,270
这些数据可以从我们世界的传感器中获得，

86
00:05:52,010 --> 00:05:54,750
直接产生某种决策，

87
00:05:55,220 --> 00:05:59,250
可以是预测，比如类别标签或数值，

88
00:06:00,170 --> 00:06:03,810
也可以是动作本身，就像强化学习的情况一样。

89
00:06:05,230 --> 00:06:07,130
我们也看到了相反的情况，

90
00:06:07,570 --> 00:06:11,030
我们现在可以考虑建立神经网络算法，

91
00:06:11,290 --> 00:06:15,440
它可以从所需的预测或所需的操作，

92
00:06:15,580 --> 00:06:18,290
来尝试生成新的数据实例，

93
00:06:18,640 --> 00:06:21,320
就像生成性建模一样。

94
00:06:22,470 --> 00:06:25,370
现在，在这两种情况下退后一步，

95
00:06:25,370 --> 00:06:29,080
无论我们是从数据到决策还是从数据到决策，

96
00:06:30,020 --> 00:06:35,670
神经网络可以被认为是非常强大的函数逼近器。

97
00:06:37,600 --> 00:06:38,280
这意味着什么，

98
00:06:38,280 --> 00:06:40,520
为了更详细地了解这一点，

99
00:06:41,020 --> 00:06:46,910
我们必须回到计算机科学和神经网络理论中非常著名的一个定理，

100
00:06:47,680 --> 00:06:51,020
那就是 1989 年提出的这个定理，

101
00:06:51,550 --> 00:06:53,780
它在该领域引起了相当大的轰动，

102
00:06:54,800 --> 00:06:58,020
这个定理叫做万能逼近定理。

103
00:06:58,800 --> 00:07:00,790
它所说的是，

104
00:07:01,080 --> 00:07:03,040
如果我们从神经网络开始，

105
00:07:03,950 --> 00:07:06,000
只有一个隐藏层，

106
00:07:07,030 --> 00:07:11,870
这个单层神经网络足以逼近任何函数。

107
00:07:13,180 --> 00:07:14,460
现在，在这门课上，

108
00:07:14,460 --> 00:07:17,780
我们考虑的不是单层神经网络，

109
00:07:17,860 --> 00:07:22,580
而是将多个隐藏层堆叠在一起的深层模型，

110
00:07:23,140 --> 00:07:25,700
但是这个定理说，你甚至不需要它，

111
00:07:26,200 --> 00:07:28,860
你只需要一个层，

112
00:07:29,450 --> 00:07:30,835
如果你相信这一点，

113
00:07:30,835 --> 00:07:36,840
任何问题都可以简单地归结为通过某个函数将输入映射到输出的想法，

114
00:07:38,160 --> 00:07:40,930
这种神经网络可以存在，

115
00:07:41,720 --> 00:07:45,190
万能逼近定理指出，

116
00:07:45,270 --> 00:07:46,720
存在某种神经网络，

117
00:07:47,590 --> 00:07:53,120
有足够数量的神经元可以逼近任何任意函数。

118
00:07:54,930 --> 00:07:57,700
这是一个令人难以置信的强大的结果，

119
00:07:58,380 --> 00:08:01,540
但如果我们更仔细地观察并深入挖掘一下，

120
00:08:02,340 --> 00:08:03,405
你可以开始看到，

121
00:08:03,405 --> 00:08:05,840
我们必须牢记一些警告。

122
00:08:07,220 --> 00:08:07,920
首先，我要说的是，

123
00:08:08,580 --> 00:08:11,930
这个定理不能保证你需要多少个单元，

124
00:08:12,100 --> 00:08:15,950
在那一层有多少神经元才能解决这样的问题。

125
00:08:17,280 --> 00:08:18,370
此外，它说，

126
00:08:19,050 --> 00:08:24,310
它没有回答我们如何真正定义那个神经网络的问题，

127
00:08:24,660 --> 00:08:27,040
我们如何找到支持该体系结构的方法，

128
00:08:27,990 --> 00:08:31,390
它所宣称的就是这样一个神经网络的存在。

129
00:08:32,760 --> 00:08:33,695
但我们知道，

130
00:08:33,695 --> 00:08:37,180
使用梯度下降和我们在这节课上学到的一些工具，

131
00:08:38,160 --> 00:08:42,580
找到这些权重并不总是一个简单的问题，

132
00:08:42,960 --> 00:08:47,500
它可以是非常复杂的，非常非线性的，非常非凸的，

133
00:08:48,090 --> 00:08:51,640
并且思考我们如何真正实现这一点，

134
00:08:52,300 --> 00:08:55,770
神经网络的训练是一个非常非常困难的问题。

135
00:08:57,500 --> 00:09:01,810
其次，这个定理的真正重要之处在于，

136
00:09:01,810 --> 00:09:07,230
它不能保证网络对新任务的泛化程度，

137
00:09:08,190 --> 00:09:09,275
它所说的是，

138
00:09:09,275 --> 00:09:13,330
给定从输入到输出的所需映射，

139
00:09:13,560 --> 00:09:15,730
我们可以找到一些神经网络，

140
00:09:16,320 --> 00:09:20,710
它在其他任务或其他设置中的性能不存在保证。

141
00:09:22,500 --> 00:09:24,010
我认为这个定理，

142
00:09:24,150 --> 00:09:28,210
这个万能逼近定理的想法及其潜在的警告，

143
00:09:28,710 --> 00:09:30,160
是一个完美的例子，

144
00:09:30,330 --> 00:09:37,930
可以被认为是人工智能和深度学习中过度炒作可能产生的影响。

145
00:09:39,000 --> 00:09:42,670
现在你已经成为这个社区的一部分，

146
00:09:42,930 --> 00:09:46,840
对推动深度学习和人工智能的状态感兴趣，

147
00:09:47,460 --> 00:09:48,590
我认为，总的来说，

148
00:09:48,590 --> 00:09:52,880
我们需要非常谨慎地考虑这些算法，

149
00:09:52,880 --> 00:09:55,180
我们如何营销它们，我们如何宣传它们，

150
00:09:55,260 --> 00:09:57,730
我们如何在我们关心的问题中使用它们。

151
00:09:58,770 --> 00:10:01,480
虽然万能逼近定理告诉我们，

152
00:10:01,950 --> 00:10:05,020
神经网络可以非常、非常强大，

153
00:10:05,040 --> 00:10:07,660
并围绕这个想法产生许多兴奋点，

154
00:10:08,320 --> 00:10:14,580
在历史上，神经网络实际上给计算机科学和人工智能社区提供了错误的希望，

155
00:10:15,260 --> 00:10:21,400
认为神经网络可以解决现实世界中任何复杂的问题。

156
00:10:22,680 --> 00:10:24,650
现在，我认为不用说，

157
00:10:24,650 --> 00:10:27,610
这种过度炒作可能是极其危险的。

158
00:10:28,600 --> 00:10:34,110
事实上，这不仅在研究过程中产生了影响，

159
00:10:34,220 --> 00:10:37,350
而且可能会在整个社会产生影响。

160
00:10:39,610 --> 00:10:43,760
这就是为什么今天，在这节课剩下的时间里，

161
00:10:43,780 --> 00:10:48,530
我真的想重点关注深度学习算法的一些局限性，

162
00:10:48,790 --> 00:10:51,860
你们在这节课上已经学到的。

163
00:10:52,850 --> 00:10:56,410
我不仅想止步于此，我还想超越这一点，

164
00:10:56,410 --> 00:11:00,180
看看这些限制如何激发新的机会，

165
00:11:00,950 --> 00:11:06,630
对于新的研究，解决其中的一些问题并克服它们。

166
00:11:08,200 --> 00:11:09,110
因此，首先，

167
00:11:09,490 --> 00:11:13,820
我最喜欢的深层神经网络潜在危险的例子之一，

168
00:11:14,410 --> 00:11:15,440
来自这篇论文，

169
00:11:15,760 --> 00:11:17,270
这篇论文说，

170
00:11:18,310 --> 00:11:22,250
理解深层神经网络需要重新思考概括。

171
00:11:23,260 --> 00:11:27,380
本文提出了一个非常优雅和简单的实验，

172
00:11:27,820 --> 00:11:33,260
突出了这种泛化概念及其在深度学习模型中的局限性。

173
00:11:34,300 --> 00:11:36,750
因此，他们在这篇论文中所做的是，

174
00:11:36,750 --> 00:11:41,000
他们从一个名为 imagenet 的大型数据集中获取图像，

175
00:11:41,540 --> 00:11:45,250
这些图像中的每一张都与一个特定的标签相关联，

176
00:11:45,720 --> 00:11:47,770
狗，香蕉，狗，树，

177
00:11:48,150 --> 00:11:49,240
就像你在这里看到的。

178
00:11:50,720 --> 00:11:54,475
然后，这篇论文的作者所做的是，

179
00:11:54,475 --> 00:11:56,460
他们考虑了这些每一个图像的例子，

180
00:11:57,480 --> 00:11:59,120
对于数据中的每一幅图像，

181
00:11:59,740 --> 00:12:02,270
他们取了 k 边骰子，

182
00:12:02,890 --> 00:12:07,700
其中 k 是该数据集中可能的类别标签的数量，

183
00:12:08,490 --> 00:12:13,900
并使用骰子将新标签随机分配给这些实例中的每一个，

184
00:12:15,990 --> 00:12:19,700
现在，所有的标签都被完全打乱了，

185
00:12:19,700 --> 00:12:22,000
他们进行随机抽样，

186
00:12:22,320 --> 00:12:24,760
为这些图像分配这些全新的标签，

187
00:12:25,770 --> 00:12:27,310
这意味着，

188
00:12:27,330 --> 00:12:30,160
与图像相关联的标签，

189
00:12:30,180 --> 00:12:35,440
如你所见，相对于图像中的实际内容是完全随机的，

190
00:12:37,280 --> 00:12:38,190
然后他们所做的，

191
00:12:38,690 --> 00:12:40,080
如果你知道这里，

192
00:12:40,520 --> 00:12:42,460
你可以拥有实例，

193
00:12:42,460 --> 00:12:47,310
因为你有多个实例的重叠对应于同一个类，

194
00:12:47,870 --> 00:12:49,860
但是现在，当你引入随机性时，

195
00:12:50,180 --> 00:12:52,650
你可以拥有同一类的两个实例，

196
00:12:53,120 --> 00:12:55,470
它们现在有完全不同的标签，

197
00:12:56,090 --> 00:13:01,080
狗在这里，在一个情况下映射到香蕉，在另一个情况下映射到树，

198
00:13:01,790 --> 00:13:02,340
因此，从字面上讲，

199
00:13:02,480 --> 00:13:05,940
他们完全、完全地将标签随机化，

200
00:13:08,740 --> 00:13:09,645
有了这一点，

201
00:13:09,645 --> 00:13:13,400
他们所做的就是尝试将深度神经网络模型，

202
00:13:13,840 --> 00:13:16,940
与来自数据集 imagenet 的采样数据相匹配，

203
00:13:17,910 --> 00:13:20,645
它们的随机性程度，

204
00:13:20,645 --> 00:13:26,160
从保留下来的原始标签到完全随机，

205
00:13:27,300 --> 00:13:28,990
然后他们得到了最终的模型，

206
00:13:29,770 --> 00:13:32,220
看看它在测试集上的表现，

207
00:13:32,220 --> 00:13:33,740
这是一个独立的数据集，

208
00:13:34,060 --> 00:13:36,320
现在我们得到了新的图像，

209
00:13:36,550 --> 00:13:40,370
网络的任务是预测相关的标签，

210
00:13:41,610 --> 00:13:43,030
正如你所预料的那样，

211
00:13:43,840 --> 00:13:48,240
独立测试集上的模型的准确性会降低，

212
00:13:48,530 --> 00:13:52,650
当你在标签过程中引入越来越多的随机性时，

213
00:13:54,540 --> 00:13:56,410
然而，真正有趣的是，

214
00:13:56,640 --> 00:14:00,100
当他们现在不看测试集时看到的是什么，

215
00:14:00,660 --> 00:14:02,210
而是在训练集上，

216
00:14:02,990 --> 00:14:04,170
这就是他们的发现，

217
00:14:05,000 --> 00:14:07,990
无论他们对这些标签进行了多大程度的随机化，

218
00:14:08,400 --> 00:14:11,770
神经网络模型在得到的数据上进行训练时，

219
00:14:12,450 --> 00:14:17,200
都能够在训练集上获得 100% 的准确率，

220
00:14:19,060 --> 00:14:21,110
这意味着，

221
00:14:21,670 --> 00:14:26,570
这些神经网络模型真的可以进行这种完美的拟合，

222
00:14:26,710 --> 00:14:29,330
这种非常强大的函数逼近。

223
00:14:30,090 --> 00:14:32,380
我认为这是一个非常有力的例子，

224
00:14:32,520 --> 00:14:34,510
因为它再次表明，

225
00:14:34,920 --> 00:14:37,240
就像万能逼近定理一样，

226
00:14:37,530 --> 00:14:40,870
深度神经网络可以完美地适用于任何函数，

227
00:14:41,640 --> 00:14:46,910
即使该函数是由完全随机的标签定义的。

228
00:14:48,390 --> 00:14:49,970
现在你将注意到，

229
00:14:49,970 --> 00:14:54,190
训练集性能和测试集性能之间的差异，

230
00:14:54,630 --> 00:14:57,100
捕捉到这种泛化思想，

231
00:14:57,750 --> 00:15:01,750
神经网络在函数拟合方面的极限是什么，

232
00:15:01,860 --> 00:15:05,830
以及它在未知数据上的实际表现是什么。

233
00:15:07,760 --> 00:15:10,020
为了进一步阐明这一点，

234
00:15:10,970 --> 00:15:15,870
我们可以简单地将神经网络理解为函数逼近器，

235
00:15:16,680 --> 00:15:20,810
而万能逼近定理告诉我们的是，

236
00:15:20,810 --> 00:15:24,370
神经网络非常非常擅长做这项工作。

237
00:15:25,350 --> 00:15:27,725
所以，如果我们考虑这个例子，

238
00:15:27,725 --> 00:15:31,630
我已经在这个二维空间中展示了这些数据点，

239
00:15:33,310 --> 00:15:35,300
我们总是可以训练神经网络，

240
00:15:35,500 --> 00:15:40,760
来学习数据在这个空间中最可能的位置，

241
00:15:41,110 --> 00:15:44,300
在它以前见过的这个领域中，

242
00:15:45,380 --> 00:15:46,740
这样，如果我们得到，

243
00:15:47,150 --> 00:15:50,490
如果我们给模型一个新的紫色数据点，

244
00:15:51,260 --> 00:15:52,450
我们可以预期，

245
00:15:52,450 --> 00:15:58,650
神经网络将生成对该数据点的估计的合理预测，

246
00:15:59,600 --> 00:16:01,225
现在，挑战变成了，

247
00:16:01,225 --> 00:16:04,590
如果你现在开始超越这一领域，

248
00:16:04,880 --> 00:16:08,910
超越你拥有信息的区域，你拥有数据例子，

249
00:16:10,120 --> 00:16:15,410
我们不能保证这些地区的训练数据是什么样子的，

250
00:16:16,090 --> 00:16:21,170
这实际上是现代深度神经网络存在的巨大限制之一，

251
00:16:23,560 --> 00:16:29,180
当我们有训练数据时，它们在区域的函数逼近方面非常有效，

252
00:16:29,930 --> 00:16:34,260
但我们不能保证他们在这些地区的表现。

253
00:16:35,760 --> 00:16:37,810
这就提出了这个问题，

254
00:16:37,890 --> 00:16:40,990
在昨天的课上，

255
00:16:41,910 --> 00:16:44,770
我们如何才能得出方法，可以告诉我们，

256
00:16:45,510 --> 00:16:50,170
网络何时不知道，当它需要更多信息，需要更多例子。

257
00:16:52,530 --> 00:16:54,610
在这个想法的基础上，

258
00:16:55,170 --> 00:16:57,790
我认为经常有这样一个通用的概念，

259
00:16:58,050 --> 00:17:00,880
确实可以被媒体夸大，

260
00:17:01,380 --> 00:17:04,180
深度学习就是这个神奇的解决方案，

261
00:17:05,200 --> 00:17:05,690
炼金术，

262
00:17:06,220 --> 00:17:09,170
它可以是任何问题的最终解决方案，

263
00:17:10,750 --> 00:17:12,920
但这让人相信，

264
00:17:13,150 --> 00:17:14,990
如果我们拿一些数据例子，

265
00:17:15,130 --> 00:17:19,100
并对其应用一些训练架构，

266
00:17:19,210 --> 00:17:20,450
训练产生的模型，

267
00:17:20,770 --> 00:17:23,660
转向深度学习算法，

268
00:17:23,860 --> 00:17:27,110
它将产生一些漂亮的、优秀的结果，

269
00:17:27,430 --> 00:17:28,400
解决我们的问题，

270
00:17:29,310 --> 00:17:32,740
但这根本不是深度学习的运作方式，

271
00:17:33,560 --> 00:17:37,390
有一种通常的垃圾输入导致垃圾输出的想法，

272
00:17:37,650 --> 00:17:40,055
如果你的数据很杂乱，

273
00:17:40,055 --> 00:17:41,500
如果你没有足够的数据，

274
00:17:41,670 --> 00:17:46,450
如果你试图建立一个非常大的神经网络模型来操作数据，

275
00:17:46,980 --> 00:17:51,250
你就不能从保证性能结果。

276
00:17:52,960 --> 00:17:58,040
这激发了现代深层神经网络最恰当的失败模型之一，

277
00:17:58,480 --> 00:18:03,620
它突显了它们对训练所用的基础数据的依赖程度。

278
00:18:04,400 --> 00:18:06,450
让我们假设我们有一只狗的图像，

279
00:18:06,920 --> 00:18:10,020
我们将把它传递到 CNN 的架构中，

280
00:18:11,030 --> 00:18:14,520
我们的任务是尝试对这张图像着色，

281
00:18:14,990 --> 00:18:19,080
使用狗的黑白图像产生彩色输出，

282
00:18:21,520 --> 00:18:22,760
这可能就是结果，

283
00:18:23,580 --> 00:18:25,780
仔细观察这张生成的图像，

284
00:18:27,470 --> 00:18:30,120
有人注意到这只狗有什么不寻常的地方吗？

285
00:18:31,660 --> 00:18:32,060
是。

286
00:18:34,120 --> 00:18:35,030
耳朵是绿色的，

287
00:18:35,530 --> 00:18:37,400
很棒的观察，还有别的吗？

288
00:18:40,710 --> 00:18:41,110
是。

289
00:18:42,520 --> 00:18:44,420
是的，下巴是粉红色或紫色的。

290
00:18:44,770 --> 00:18:48,405
所以两个不同的人指出了两个不同的例子，

291
00:18:48,405 --> 00:18:50,570
这些东西并不是真正一致的，

292
00:18:51,400 --> 00:18:52,670
为什么会是这样呢。

293
00:18:53,520 --> 00:19:00,310
尤其是，如果我们看一下这个模型训练时使用的数据，

294
00:19:01,380 --> 00:19:02,830
在狗的图像中，

295
00:19:03,510 --> 00:19:05,980
许多图像可能是，

296
00:19:06,060 --> 00:19:10,360
狗伸出舌头或背景中有一些草，

297
00:19:11,100 --> 00:19:18,695
因此， CNN 的模型可能已经将下巴周围的区域映射为粉色，

298
00:19:18,695 --> 00:19:21,190
或将耳朵周围的区域映射为绿色，

299
00:19:21,840 --> 00:19:25,750
这个例子突出显示了这种差异，

300
00:19:26,040 --> 00:19:31,990
训练数据看起来像什么和预测输出可能是什么。

301
00:19:32,580 --> 00:19:33,770
这就是深度学习模式，

302
00:19:34,060 --> 00:19:35,175
它们所做的只是，

303
00:19:35,175 --> 00:19:37,160
它们建立这种表征，

304
00:19:37,600 --> 00:19:41,720
基于它们在训练过程中看到的数据。

305
00:19:43,590 --> 00:19:44,885
这就提出了一个问题，

306
00:19:44,885 --> 00:19:49,630
神经网络如何有效地处理这些实例，

307
00:19:49,800 --> 00:19:52,630
它们可能没有看到足够信息的情况，

308
00:19:53,070 --> 00:19:54,580
它们可能非常不确定，

309
00:19:55,280 --> 00:19:59,920
正如 Sadhana 在昨天的课程中提到的，列出的，

310
00:20:00,640 --> 00:20:05,160
这与现实世界的安全关键场景高度相关，

311
00:20:05,420 --> 00:20:07,650
例如，在自动驾驶的情况下，

312
00:20:08,330 --> 00:20:13,260
处于自动驾驶模式的汽车可能会或处于自动驾驶模式，

313
00:20:13,370 --> 00:20:14,970
最终可能会发生碰撞，

314
00:20:15,050 --> 00:20:20,400
结果往往是致命的或具有非常重大的后果。

315
00:20:21,870 --> 00:20:25,960
在这个特殊的情况下，真正从几年前进一步突出这一点，

316
00:20:26,880 --> 00:20:32,950
有一起自动驾驶汽车的案例，导致了一起致命的事故，

317
00:20:33,510 --> 00:20:37,805
它撞上了一个塔架，

318
00:20:37,805 --> 00:20:40,690
一个出现在路上的建筑塔架，

319
00:20:41,400 --> 00:20:42,610
事实证明，

320
00:20:42,960 --> 00:20:48,320
当他们回顾用于训练神经网络的数据时，

321
00:20:48,320 --> 00:20:49,900
这个神经网络用于控制汽车，

322
00:20:50,480 --> 00:20:52,060
在这些训练例子中，

323
00:20:52,740 --> 00:20:56,440
Google 街景图像中该特定区域的道路，

324
00:20:56,850 --> 00:21:00,580
没有那些建筑障碍物和建筑塔架，

325
00:21:00,900 --> 00:21:04,030
最终导致汽车相撞。

326
00:21:05,430 --> 00:21:10,940
因此，这种关于训练数据的差异和问题

327
00:21:10,940 --> 00:21:13,120
如何导致这些下游后果的想法，

328
00:21:13,890 --> 00:21:18,100
对于神经网络系统来说是一个非常突出的失败模型，

329
00:21:18,970 --> 00:21:22,020
正是这些类型的失败模型促使人们

330
00:21:22,020 --> 00:21:25,880
需要了解神经网络系统中的不确定性。

331
00:21:26,500 --> 00:21:30,440
这就是你们在昨天的课程中学到的，

332
00:21:30,760 --> 00:21:34,880
希望你们已经通过软件实验获得了深入的经验，

333
00:21:36,010 --> 00:21:39,890
强调开发强大的方法，

334
00:21:39,910 --> 00:21:43,130
来了解这些不确定性和风险度量的重要性，

335
00:21:43,660 --> 00:21:47,570
以及它们对于安全关键应用程序是多么重要，

336
00:21:48,040 --> 00:21:51,230
从自主到医学再到面部识别等，

337
00:21:51,460 --> 00:21:52,850
就像你正在探索的那样，

338
00:21:54,350 --> 00:21:56,280
以及这些下游后果如何从根本上

339
00:21:56,780 --> 00:22:02,370
与数据不平衡、功能不平衡和噪音问题联系在一起。

340
00:22:04,310 --> 00:22:06,330
所以，总体而言，

341
00:22:06,560 --> 00:22:12,030
我认为这些情况和这些考虑向我们指出的是，

342
00:22:12,080 --> 00:22:18,250
神经网络的敏感性，屈服于这些失败模式。

343
00:22:19,560 --> 00:22:23,195
我想要考虑和强调的最后一个失败模式是，

344
00:22:23,195 --> 00:22:25,600
这种对抗性例子的想法。

345
00:22:27,160 --> 00:22:29,850
这里的直觉和关键思想是，

346
00:22:29,850 --> 00:22:33,950
我们可以拿一个数据实例，例如一个图像，

347
00:22:34,660 --> 00:22:39,110
如果部署并输入到标准的 CNN 中，

348
00:22:39,340 --> 00:22:44,930
将以 97% 的概率预测包含一座寺庙，

349
00:22:46,430 --> 00:22:47,635
我们所能做的是，

350
00:22:47,635 --> 00:22:56,460
以随机噪声，选择随机噪声的形式对原始图像施加一些微小的扰动，

351
00:22:57,110 --> 00:22:59,820
然后产生一个扰动的图像，

352
00:23:00,410 --> 00:23:04,380
对我们人类来说，这在视觉上看起来大体相同，

353
00:23:05,190 --> 00:23:09,640
但如果你把得到的图像返回给同一个神经网络，

354
00:23:10,290 --> 00:23:16,810
它产生的预测与图像中的实际情况完全不一致，

355
00:23:17,540 --> 00:23:22,630
预测鸵鸟标签的概率为 98% 。

356
00:23:23,520 --> 00:23:28,390
这就是对抗性攻击或对抗性扰动的概念，

357
00:23:29,410 --> 00:23:31,730
这种微扰究竟在做什么，

358
00:23:33,970 --> 00:23:37,610
当我们使用梯度下降来训练神经网络时，

359
00:23:38,080 --> 00:23:44,300
任务是优化或最小化某些损失函数和目标，

360
00:23:45,450 --> 00:23:48,940
标准梯度下降的目标是，

361
00:23:49,050 --> 00:23:55,420
好的，我如何改变神经网络的权重来减少损失，从而优化这个目标，

362
00:23:57,080 --> 00:24:02,940
我们特别关注以某种方式改变这些权重 W ，以将我们的损失降至最低，

363
00:24:04,740 --> 00:24:06,100
如果你仔细看这里，

364
00:24:07,100 --> 00:24:16,540
我们只考虑相对于输入数据的权重以及对应的标签 x 和 y ，

365
00:24:17,990 --> 00:24:21,600
相比之下，在思考对抗性攻击时，

366
00:24:22,370 --> 00:24:24,420
我们现在提出了一个不同的问题，

367
00:24:25,220 --> 00:24:29,100
我们如何修改输入数据，例如图像，

368
00:24:29,570 --> 00:24:34,890
以在网络预测中增加误差，

369
00:24:36,080 --> 00:24:42,030
具体地说，输入数据 x 中微小的微扰是如何，

370
00:24:43,250 --> 00:24:45,870
导致损失最大幅度的增加，

371
00:24:46,860 --> 00:24:47,880
这意味着，

372
00:24:47,880 --> 00:24:49,130
我们可以固定权重，

373
00:24:49,480 --> 00:24:50,690
保持网络不变，

374
00:24:51,160 --> 00:24:55,220
并研究如何干扰、更改和操纵输入数据，

375
00:24:55,570 --> 00:24:57,740
以尝试增加损失函数。

376
00:24:59,910 --> 00:25:05,080
这个想法的扩展是由 MIT 的一群学生开发和提出的，

377
00:25:05,250 --> 00:25:08,380
他们利用对抗性微扰的想法，

378
00:25:09,000 --> 00:25:10,870
创造了一个算法，

379
00:25:10,890 --> 00:25:15,730
不仅可以在二维内合成对抗性实现，

380
00:25:16,110 --> 00:25:17,830
而且可以在三维内，

381
00:25:18,660 --> 00:25:20,860
使用一组不同的变换，

382
00:25:20,940 --> 00:25:24,940
比如旋转，颜色变化，其他类型的微扰，

383
00:25:26,020 --> 00:25:28,580
然后，利用这些学习的扰动，

384
00:25:29,020 --> 00:25:30,770
他们获取了这些信息，

385
00:25:31,180 --> 00:25:35,630
并使用三维打印合成了对象，

386
00:25:36,070 --> 00:25:40,610
这些被设计为神经网络的对抗性例子，

387
00:25:41,490 --> 00:25:46,210
所以他们打印了三维，打印了一堆这些乌龟的例子，

388
00:25:46,380 --> 00:25:47,860
三维物理物体，

389
00:25:48,540 --> 00:25:54,310
这样那些图像就会完全愚弄神经网络，

390
00:25:54,360 --> 00:25:57,730
看着图像，试图分类正确的标签，

391
00:25:59,050 --> 00:26:03,650
这表明，这种对抗性例子和扰动的概念

392
00:26:04,030 --> 00:26:07,070
可以扩展到现实世界中的不同领域，

393
00:26:07,390 --> 00:26:11,390
现在我们可以考虑构建这些合成例子，

394
00:26:11,800 --> 00:26:17,690
旨在探索神经网络的弱点并揭示其脆弱性。

395
00:26:20,010 --> 00:26:25,480
最后，我们昨天也讨论了很多关于这个想法的问题，

396
00:26:25,710 --> 00:26:28,210
这种算法偏差的概念，

397
00:26:28,560 --> 00:26:33,010
随着人工智能系统在社会上得到更广泛的部署，

398
00:26:33,330 --> 00:26:36,460
它们很容易受到重大偏差的影响，

399
00:26:36,510 --> 00:26:39,670
是我之前强调的许多问题造成的，

400
00:26:40,390 --> 00:26:43,670
而这些可能会导致非常真实的有害后果，

401
00:26:44,470 --> 00:26:48,920
正如我们在这门课程的实验和课程中一直在探索的那样。

402
00:26:50,990 --> 00:26:53,605
因此，我到目前为止所举的例子

403
00:26:53,605 --> 00:26:58,200
肯定不是神经网络所有局限性的详尽清单，

404
00:26:59,650 --> 00:27:02,990
但我不想把这些严格地看作是限制，

405
00:27:04,120 --> 00:27:07,770
而是对新创新的邀请和动力，

406
00:27:08,570 --> 00:27:12,150
这些创新和创造性解决方案旨在试图解决其中一些问题，

407
00:27:12,650 --> 00:27:16,860
这些问题确实是当今人工智能和深度学习研究中的悬而未决的问题，

408
00:27:17,980 --> 00:27:25,580
特别是，我们已经解决并已经考虑了解决神经网络中的偏见和不确定性问题的方法。

409
00:27:26,520 --> 00:27:28,660
在接下来的演讲中，

410
00:27:29,160 --> 00:27:34,120
我想把重点放在深度学习的一些非常令人兴奋的新领域上，

411
00:27:34,410 --> 00:27:37,930
解决引入的一些额外限制。

412
00:27:39,700 --> 00:27:41,360
第一个想法，

413
00:27:42,410 --> 00:27:46,830
源于对神经网络的规模和规模的看法。

414
00:27:47,820 --> 00:27:49,250
它们严重依赖数据，

415
00:27:49,510 --> 00:27:50,120
它们是海量的，

416
00:27:50,890 --> 00:27:54,450
因此可能很难理解，

417
00:27:54,450 --> 00:27:59,330
神经网络挑选的底层结构是什么。

418
00:28:00,060 --> 00:28:03,130
现在有一项非常非常令人兴奋的工作，

419
00:28:03,570 --> 00:28:12,100
将结构和更多来自先前人类知识的信息引入神经网络结构，

420
00:28:12,600 --> 00:28:18,880
使它们变得更小、更紧凑、更有表现力、更高效。

421
00:28:19,640 --> 00:28:22,360
我们将在今天的演讲中看到这一点，

422
00:28:22,710 --> 00:28:25,690
也会在我们接下来的 Ramin 的客座演讲中看到这一点，

423
00:28:25,920 --> 00:28:29,525
他将在那里谈到结构编码的想法，

424
00:28:29,525 --> 00:28:35,620
以及它如何产生高度表达和高效的神经网络结构。

425
00:28:37,230 --> 00:28:40,270
我要谈的第二个问题，

426
00:28:40,560 --> 00:28:45,320
与规模和结构的问题密切相关，

427
00:28:46,030 --> 00:28:52,010
我们现在如何鼓励神经网络在数据之外更有效地进行推断，

428
00:28:52,740 --> 00:28:58,340
这将非常巧妙地与我们一直在讨论的关于生成性人工智能的一些想法联系在一起，

429
00:28:58,540 --> 00:29:03,470
这将是我将在今天剩余的演讲中讨论的最后一个话题。

430
00:29:05,340 --> 00:29:08,970
好的，让我们先来关注一下这个概念，

431
00:29:09,440 --> 00:29:12,780
我们如何利用更多的人类领域知识

432
00:29:13,220 --> 00:29:16,740
来将更大的结构编码到深度学习算法中。

433
00:29:18,110 --> 00:29:21,840
到目前为止，我们已经在课程中看到了这方面的例子，

434
00:29:23,300 --> 00:29:25,530
特别是 CNN 中，

435
00:29:25,700 --> 00:29:28,650
强调的用于空间处理和视觉数据的例子。

436
00:29:29,510 --> 00:29:31,525
正如我们所看到的， CNN 被引入，

437
00:29:31,525 --> 00:29:36,540
作为一种高效的架构来捕获数据中的空间依赖关系，

438
00:29:37,370 --> 00:29:41,610
CNN 背后的核心思想是卷积的概念，

439
00:29:42,290 --> 00:29:50,575
它如何将输入图像的空间相似和相邻部分相互关联，

440
00:29:50,575 --> 00:29:52,770
通过这种有效的卷积运算。

441
00:29:53,760 --> 00:30:00,490
我们看到卷积能够有效地从视觉数据中提取局部特征，

442
00:30:01,260 --> 00:30:07,460
然后使用该局部特征提取来实现下游预测任务，

443
00:30:07,810 --> 00:30:10,880
比如分类、目标检测等。

444
00:30:13,010 --> 00:30:14,970
现在，如果不是图像，

445
00:30:15,050 --> 00:30:18,090
我们考虑更不规则的数据结构，情况会怎样，

446
00:30:19,440 --> 00:30:24,070
它可以通过二维像素网格以外的不同方式进行编码，

447
00:30:25,150 --> 00:30:29,840
图是一种编码结构信息的特别强大的方法，

448
00:30:30,340 --> 00:30:31,995
在许多情况下，

449
00:30:31,995 --> 00:30:38,180
图或网络的概念对于我们可能正在考虑的问题非常重要，

450
00:30:40,690 --> 00:30:45,200
图作为一种结构和表示数据的示例，

451
00:30:45,250 --> 00:30:46,880
确实存在于我们周围，

452
00:30:47,020 --> 00:30:48,590
跨越许多不同的应用程序，

453
00:30:49,460 --> 00:30:51,070
范围从社交网络，

454
00:30:51,660 --> 00:30:54,370
定义了我们彼此之间的联系方式，

455
00:30:55,140 --> 00:30:56,430
到状态机，

456
00:30:56,430 --> 00:31:00,830
描述一个过程如何从一个状态转换到另一个状态，

457
00:31:01,540 --> 00:31:05,750
到人类流动性、城市交通的度量和模型，

458
00:31:06,640 --> 00:31:09,800
到化学分子和生物网络。

459
00:31:10,890 --> 00:31:13,990
所有这些例子的动机是，

460
00:31:14,220 --> 00:31:21,490
如此多的真实世界数据的实例自然地落入这种网络或图形结构的概念中，

461
00:31:22,200 --> 00:31:31,030
但这种结构不容易被标准的数据编码或几何图形结合或捕获，

462
00:31:31,590 --> 00:31:34,510
因此，我们将稍微讨论一下将图作为一种结构，

463
00:31:34,800 --> 00:31:39,220
以及如何尝试将编码在图形中的信息表示出来，

464
00:31:39,480 --> 00:31:44,650
以构建能够处理这些数据类型的神经网络。

465
00:31:46,560 --> 00:31:50,170
为了看看我们如何做到这一点，并处理这个问题，

466
00:31:50,520 --> 00:31:53,470
让我们花一点时间回到 CNN ，

467
00:31:54,260 --> 00:31:58,890
在 CNN 中，我们看到了这个矩形的二维内核，

468
00:31:59,420 --> 00:32:01,465
它所做的是，

469
00:32:01,465 --> 00:32:03,090
它有效地在图像中滑动，

470
00:32:03,740 --> 00:32:09,300
关注并拾取存在于这个二维网格中的特征，

471
00:32:10,060 --> 00:32:13,380
这个函数所做的是，

472
00:32:13,380 --> 00:32:17,660
定义卷积运算的元素乘法，

473
00:32:18,450 --> 00:32:24,220
直观地说，我们可以把卷积看作滑动这个核心的可视化，

474
00:32:24,390 --> 00:32:27,790
在图像上一块一块迭代，

475
00:32:28,530 --> 00:32:36,140
继续尝试提取在这二维像素空间中捕捉到的信息特征。

476
00:32:37,680 --> 00:32:43,475
卷积在滑动中这个特征提取过滤器的核心思想是，

477
00:32:43,475 --> 00:32:49,400
我们如何将这个思想扩展到基于图形的核心，

478
00:32:50,800 --> 00:32:56,450
这激发了一种称为图卷积网络的最新类型的神经网络结构，

479
00:32:57,190 --> 00:33:00,230
其思想与标准的 CNN 非常相似，

480
00:33:00,940 --> 00:33:03,450
我们有某种权重的[核]，

481
00:33:04,120 --> 00:33:09,210
它只是一个像以前一样由神经网络权重定义的矩阵，

482
00:33:10,010 --> 00:33:15,540
现在不是在二维像素网格上滑动内核，

483
00:33:16,160 --> 00:33:19,810
内核去到图形的不同部分，

484
00:33:20,940 --> 00:33:22,690
由特定节点定义，

485
00:33:22,950 --> 00:33:26,950
它查看该节点的邻居是什么，

486
00:33:27,090 --> 00:33:29,230
它是如何连接到相邻节点的，

487
00:33:29,740 --> 00:33:35,160
而核被用来从图中的局部邻域中提取特征，

488
00:33:35,540 --> 00:33:39,390
然后捕获结构中的相关信息。

489
00:33:40,820 --> 00:33:43,015
所以我们可以在这里具体地看到这一点，

490
00:33:43,015 --> 00:33:48,960
现在不是看到二维内核查看图像的特定块，

491
00:33:49,550 --> 00:33:55,140
我要强调的是内核是如何关注节点及其本地邻居的，

492
00:33:55,970 --> 00:33:58,650
以及有效的图形卷积操作，

493
00:33:59,120 --> 00:34:05,010
它所做的是拾取特定节点的局部连通性，

494
00:34:05,570 --> 00:34:10,800
并学习与这些边相关联的权重，定义该局部连通性，

495
00:34:11,920 --> 00:34:16,070
然后，将内核迭代应用于图中的每个节点，

496
00:34:16,600 --> 00:34:19,730
试图提取有关本地连接性的信息，

497
00:34:20,560 --> 00:34:22,850
这样我们就可以四处走动，

498
00:34:23,380 --> 00:34:28,550
最后开始把所有这些信息聚集在一起，

499
00:34:29,110 --> 00:34:34,730
然后根据当地的观测结果对权重进行编码和更新。

500
00:34:36,270 --> 00:34:41,410
它的关键思想非常非常类似于标准的卷积运算，

501
00:34:42,060 --> 00:34:43,360
这种方法是核心，

502
00:34:43,440 --> 00:34:45,910
它所做的是一个特征提取过程，

503
00:34:46,410 --> 00:34:50,140
现在由这个图中的节点和边来定义，

504
00:34:50,490 --> 00:34:53,920
而不是图像中的二维像素网格。

505
00:34:55,080 --> 00:35:01,060
图形编码和图形卷积的想法非常令人兴奋和强大的，

506
00:35:01,110 --> 00:35:05,140
就我们现在如何将其扩展到现实世界的设置而言，

507
00:35:05,730 --> 00:35:08,740
它有数据集定义，

508
00:35:08,740 --> 00:35:11,940
这些数据集自然地适合于这个图结构。

509
00:35:12,830 --> 00:35:16,500
为了强调其中一些在不同领域的应用，

510
00:35:17,120 --> 00:35:22,680
我想要引起注意的第一个例子是在化学和生物科学中，

511
00:35:23,000 --> 00:35:26,490
如果我们考虑一个分子结构，

512
00:35:26,930 --> 00:35:28,740
它是由原子定义的，

513
00:35:29,150 --> 00:35:34,380
这些原子根据分子键相互连接，

514
00:35:34,900 --> 00:35:37,970
事实证明，我们可以设计图形神经网络，

515
00:35:38,350 --> 00:35:42,410
它可以有效地构建分子的表示，

516
00:35:43,470 --> 00:35:46,720
以这个图卷积的相同操作，

517
00:35:47,190 --> 00:35:51,130
建立化学结构的信息表示，

518
00:35:52,490 --> 00:35:59,610
图形卷积的这个想法最近也被应用到了药物发现的问题上，

519
00:36:00,170 --> 00:36:02,700
事实上，在 MIT 的工作中也是如此，

520
00:36:03,710 --> 00:36:07,720
事实证明，我们可以定义图形神经网络结构，

521
00:36:07,720 --> 00:36:11,010
它可以查看小分子药物的数据，

522
00:36:11,570 --> 00:36:14,020
然后查看新的数据集，

523
00:36:14,020 --> 00:36:20,040
试图预测和发现具有强大活性的新治疗化合物，

524
00:36:20,570 --> 00:36:25,050
比如杀菌和用作抗生素。

525
00:36:26,640 --> 00:36:27,670
除了这个例子，

526
00:36:28,290 --> 00:36:32,530
图神经网络最近的另一个应用是在交通预测中，

527
00:36:33,180 --> 00:36:41,320
其中的目标是将街道和十字路口视为图定义的节点和边，

528
00:36:41,760 --> 00:36:47,740
并将图神经网络应用于该结构和移动模式，

529
00:36:48,000 --> 00:36:51,760
以学习预测所产生的交通模式。

530
00:36:52,460 --> 00:36:55,200
事实上，在谷歌和 DeepMind 的工作中，

531
00:36:55,490 --> 00:37:00,090
同样的建模方法能够显著改善

532
00:37:00,620 --> 00:37:06,000
谷歌地图中 ETA 估计到达时间的预测，

533
00:37:06,620 --> 00:37:09,695
因此，当你在手机上查看谷歌地图，

534
00:37:09,695 --> 00:37:12,460
并预测你何时将到达某个位置时，

535
00:37:13,200 --> 00:37:19,600
它在后端所做的是应用这种图形神经网络体系结构来查看交通数据，

536
00:37:19,620 --> 00:37:24,520
并就这些模式如何随着时间的变化做出更有信息的预测。

537
00:37:26,180 --> 00:37:32,310
最后一个也是最近的另一个例子是在预测 COVID-19 的传播中，

538
00:37:33,020 --> 00:37:39,270
同样，基于同样的活动模式和连接，

539
00:37:39,560 --> 00:37:45,510
图神经网络被用来不仅研究 COVID 传播的空间分布，

540
00:37:45,770 --> 00:37:48,240
还结合了一个时间维度，

541
00:37:48,860 --> 00:37:55,680
以便研究人员可以有效地预测哪些地区最有可能受到 COVID-19 的影响，

542
00:37:56,210 --> 00:38:00,900
以及这些影响可能发生在什么时间尺度上。

543
00:38:02,670 --> 00:38:05,230
所以，希望这个强调，

544
00:38:05,820 --> 00:38:13,810
这样一个想法，关于数据结构可以转化为新的神经网络体系结构的简单想法，

545
00:38:13,980 --> 00:38:20,560
这些体系结构是专门为相关应用领域的特定问题设计的。

546
00:38:21,580 --> 00:38:24,620
这也自然而然地决定了，

547
00:38:24,940 --> 00:38:29,615
我们如何才能扩展从数据到图形，

548
00:38:29,615 --> 00:38:31,570
再到现在的三维数据，

549
00:38:32,740 --> 00:38:35,870
这通常被称为三维点云，

550
00:38:36,460 --> 00:38:38,865
这些都是无序的数据集，

551
00:38:38,865 --> 00:38:42,890
你可以把它们想象成散布在三维空间中，

552
00:38:43,540 --> 00:38:48,380
事实证明，你可以非常自然地扩展图形神经网络，

553
00:38:48,640 --> 00:38:52,610
以处理三维数据集，生命点云，

554
00:38:53,200 --> 00:38:55,110
其中的核心思想是，

555
00:38:55,110 --> 00:39:01,280
你可以动态计算这个三维空间中存在的网格，

556
00:39:01,690 --> 00:39:05,420
使用图形卷积和图形神经网络的相同思想。

557
00:39:08,070 --> 00:39:13,480
好的，希望通过对图形结构、图形神经网络的了解，

558
00:39:13,980 --> 00:39:16,420
你已经开始了解，

559
00:39:16,590 --> 00:39:22,660
如何使用数据基础结构的检查和我们的先验知识

560
00:39:23,100 --> 00:39:28,990
来为非常适合特定任务的新的神经网络体系结构提供信息。

561
00:39:32,510 --> 00:39:39,030
关于这个，我想把这节课剩下的时间花在我们的第二个新领域，

562
00:39:39,380 --> 00:39:41,280
重点是生成式 AI 。

563
00:39:43,450 --> 00:39:48,350
正如我们在这门课程的第一节课中所介绍的那样，

564
00:39:49,120 --> 00:39:53,330
我认为今天我们处于人工智能的转折点，

565
00:39:53,980 --> 00:39:56,250
我们看到了巨大的新功能，

566
00:39:56,750 --> 00:39:58,500
特别是生成式模型，

567
00:39:59,300 --> 00:40:02,790
使各种领域的新进展成为可能，

568
00:40:03,610 --> 00:40:07,530
我认为我们正处在这个转折点上，

569
00:40:07,530 --> 00:40:09,270
在未来的几年里，

570
00:40:09,270 --> 00:40:15,710
我们将看到生成式 AI 从根本上改变我们世界的面貌，我们社会的面貌。

571
00:40:16,640 --> 00:40:20,040
所以，今天，在新前沿课程的剩余部分，

572
00:40:20,570 --> 00:40:25,830
我们将专门关注一类新的生成式模型，扩散模型，

573
00:40:26,180 --> 00:40:30,570
正在推动生成式 AI 的一些最新进展。

574
00:40:32,130 --> 00:40:33,560
好的，首先，

575
00:40:33,560 --> 00:40:36,610
让我们回顾一下关于生成式模型的课程，

576
00:40:37,320 --> 00:40:43,540
我们主要关注两类生成式模型， VAE 和 GAN ，

577
00:40:45,540 --> 00:40:48,725
我们没有时间深入探讨的是，

578
00:40:48,725 --> 00:40:53,830
这些 VAE 和 GAN 的潜在局限性是什么。

579
00:40:55,470 --> 00:40:58,330
事实证明，这里有三个主要限制，

580
00:40:59,600 --> 00:41:02,110
第一个是我们所说的模式崩溃，

581
00:41:02,820 --> 00:41:04,720
这意味着在生成过程中，

582
00:41:04,950 --> 00:41:10,685
VAE 和 GAN 可以向下崩溃到这个模式，这个阶段，

583
00:41:10,685 --> 00:41:13,690
它们会产生很多预测，

584
00:41:13,890 --> 00:41:16,870
很多彼此非常相似的新样本，

585
00:41:17,410 --> 00:41:22,820
我们经常认为这是对平均值或最常见值的回归。

586
00:41:24,140 --> 00:41:25,510
第二个关键限制，

587
00:41:25,510 --> 00:41:28,950
就像我在我们的生成式建模课程中提到的那样，

588
00:41:29,600 --> 00:41:34,510
是这些模型真的很难生成全新的实例，

589
00:41:34,510 --> 00:41:39,310
与训练数据不同，而更加多样化。

590
00:41:40,660 --> 00:41:48,260
最后，事实证明， GAN 以及 VAE 在实践中可能很难训练，

591
00:41:48,640 --> 00:41:50,660
它们不稳定，效率低下，

592
00:41:50,800 --> 00:41:53,750
这导致在实践中出现很多问题，

593
00:41:54,100 --> 00:41:56,990
当考虑如何扩展这些模型时。

594
00:41:58,850 --> 00:42:03,660
这些限制激发了一组具体的挑战或标准，

595
00:42:03,890 --> 00:42:06,990
当我们考虑生成式模型时，

596
00:42:07,900 --> 00:42:11,300
我们希望我们的生成模型稳定、高效地进行训练，

597
00:42:12,180 --> 00:42:14,800
我们希望生成高质量的样本，

598
00:42:15,450 --> 00:42:18,160
这些样本是人工合成的，而且是新颖的多样化，

599
00:42:18,540 --> 00:42:23,050
不同于该模型以前在其训练示例中看到的。

600
00:42:24,950 --> 00:42:26,040
今天和明天，

601
00:42:26,060 --> 00:42:31,560
我们将关注两个非常令人兴奋的新的生成式模型，

602
00:42:32,000 --> 00:42:34,410
它们将正面应对这些挑战。

603
00:42:35,080 --> 00:42:38,030
今天，我将特别关注扩散模型，

604
00:42:38,740 --> 00:42:41,240
提供它们工作原理背后的直觉，

605
00:42:41,590 --> 00:42:43,020
你以前可能在哪里见过它们，

606
00:42:43,020 --> 00:42:45,740
以及它们带来了哪些进步。

607
00:42:46,500 --> 00:42:48,970
明天，在谷歌的客座演讲中，

608
00:42:49,260 --> 00:42:53,170
我们将听到 Dilip 关于另一种生成式建模方法的演讲，

609
00:42:53,400 --> 00:42:58,030
专门专注于从文本到图像生成的任务。

610
00:42:59,630 --> 00:43:01,930
好的，所以让我们开始吧。

611
00:43:03,670 --> 00:43:04,740
对于扩散模型，

612
00:43:04,740 --> 00:43:09,315
我认为它首先帮助我们再次与我们看到的模型进行比较，

613
00:43:09,315 --> 00:43:12,020
我们看到和了解的体系结构，

614
00:43:12,800 --> 00:43:15,900
对于我们在第四讲中讨论的 VAE 和 GAN ，

615
00:43:16,730 --> 00:43:20,370
任务是生成示例，合成示例，

616
00:43:20,630 --> 00:43:21,510
比如一张照片，

617
00:43:22,460 --> 00:43:23,550
在一次中，

618
00:43:23,660 --> 00:43:27,030
通过获取一些压缩或噪声的潜在空间，

619
00:43:27,560 --> 00:43:30,540
然后尝试从它解码或生成，

620
00:43:30,680 --> 00:43:34,860
在我们的原始数据空间中产生一个新的实例。

621
00:43:36,220 --> 00:43:39,680
扩散模型的工作原理与这种方法截然不同，

622
00:43:40,590 --> 00:43:43,090
不是进行这种一次的预测，

623
00:43:43,800 --> 00:43:46,070
扩散模型所做的是，

624
00:43:46,070 --> 00:43:48,580
它们迭代地产生新的样本，

625
00:43:49,110 --> 00:43:51,700
从纯粹的随机噪声开始，

626
00:43:52,410 --> 00:43:53,650
学习一个过程，

627
00:43:53,760 --> 00:43:59,350
这个过程可以迭代地从完全随机状态中删除小增量的噪声，

628
00:43:59,850 --> 00:44:04,210
直到能够生成合成示例，

629
00:44:04,650 --> 00:44:09,010
在我们开始并关心的原始数据环境中。

630
00:44:10,320 --> 00:44:12,430
这里的直觉真的很聪明，

631
00:44:12,690 --> 00:44:15,910
我认为，很强大，

632
00:44:17,040 --> 00:44:19,370
扩散模型使我们能够，

633
00:44:19,370 --> 00:44:24,070
捕捉最大的可变性，最大的信息量，

634
00:44:24,540 --> 00:44:27,700
从一个完全随机的状态开始，

635
00:44:28,990 --> 00:44:33,170
扩散模型可以分为两个关键方面，

636
00:44:33,960 --> 00:44:37,450
第一个是我们所说的前向噪声过程，

637
00:44:38,830 --> 00:44:40,605
这个想法的核心是，

638
00:44:40,605 --> 00:44:45,920
我们如何建立数据，让扩散模型查看，

639
00:44:46,120 --> 00:44:49,250
然后学习如何去噪和生成数据。

640
00:44:50,870 --> 00:44:54,420
这里的关键一步是我们从训练数据开始，

641
00:44:55,010 --> 00:44:56,580
比如说图像的例子，

642
00:44:57,460 --> 00:45:00,740
在这个前向噪声扩散过程中，

643
00:45:01,810 --> 00:45:06,530
我们所做的是逐渐增加噪音，

644
00:45:07,270 --> 00:45:11,060
这样我们就会慢慢地抹去图像中的细节，

645
00:45:11,530 --> 00:45:14,210
破坏信息，摧毁信息，

646
00:45:15,010 --> 00:45:17,900
直到我们达到纯粹的噪音状态。

647
00:45:19,070 --> 00:45:23,790
然后，我们构建神经网络的目的是，

648
00:45:23,790 --> 00:45:31,220
学习从完全噪声状态返回到原始数据空间的映射，

649
00:45:31,690 --> 00:45:33,350
我们称之为反向过程，

650
00:45:34,330 --> 00:45:36,650
学习和映射去噪声，

651
00:45:36,880 --> 00:45:39,560
从噪声返回到数据，

652
00:45:40,420 --> 00:45:42,285
这里的核心思想是，

653
00:45:42,285 --> 00:45:49,550
去噪是从完全随机的状态迭代地恢复越来越多的信息。

654
00:45:51,510 --> 00:45:53,320
这是两个核心部分，

655
00:45:53,430 --> 00:45:57,160
正向噪声过程和反向去噪过程，

656
00:45:57,720 --> 00:46:00,400
我们将看看每个过程是如何工作的，

657
00:46:00,570 --> 00:46:02,590
归结为核心直觉。

658
00:46:03,830 --> 00:46:07,140
这下面是前向噪音，第一步，

659
00:46:07,800 --> 00:46:09,260
我们得到一幅图像，

660
00:46:09,400 --> 00:46:12,710
我们想要得到一个随机的噪声样本，

661
00:46:14,890 --> 00:46:20,630
重要的是，这个过程不需要任何神经网络学习，任何训练，

662
00:46:21,370 --> 00:46:25,020
我们能够做的是定义一种固定的方式，

663
00:46:25,020 --> 00:46:29,030
一种从输入图像到噪声的确定方式，

664
00:46:29,970 --> 00:46:31,990
其核心理念非常简单，

665
00:46:32,640 --> 00:46:35,140
我们所做的就是我们有一些噪音函数，

666
00:46:35,980 --> 00:46:38,580
从第一个时间步长开始，

667
00:46:38,870 --> 00:46:40,680
我们的初始时间步长 T0 ，

668
00:46:41,540 --> 00:46:44,520
其中我们有 100% 的图像，没有添加任何噪声，

669
00:46:45,320 --> 00:46:50,730
我们所做的就是以迭代的方式逐渐增加越来越多的噪音，

670
00:46:51,670 --> 00:46:54,050
在这些单独的时间步的过程中，

671
00:46:54,900 --> 00:46:58,070
以至于结果变得越来越嘈杂。

672
00:46:59,020 --> 00:47:02,750
首先，我们从所有图像开始，更少的图像，更少的图像，

673
00:47:02,770 --> 00:47:04,790
更多的噪音，更多的噪音，所有的噪音，

674
00:47:05,020 --> 00:47:07,190
在这个明确的前进过程中，

675
00:47:07,630 --> 00:47:09,260
没有训练，没有学习，

676
00:47:09,670 --> 00:47:11,330
我们所拥有的只是一个噪音函数。

677
00:47:12,530 --> 00:47:15,640
现在，这给了我们很多例子，

678
00:47:15,640 --> 00:47:19,140
如果我们在数据集中有一堆实例，

679
00:47:19,340 --> 00:47:21,630
我们会将这种噪声应用于每个实例，

680
00:47:21,770 --> 00:47:26,820
所以我们在每个不同的噪声时间步长上都有这些切片，

681
00:47:27,950 --> 00:47:31,685
我们现在的目标是，学习神经网络，

682
00:47:31,685 --> 00:47:35,500
学习如何在这个相反的过程中去噪，

683
00:47:36,450 --> 00:47:39,340
从输入空间中的数据开始，

684
00:47:39,600 --> 00:47:42,890
噪声过程引导我们得到的是，

685
00:47:42,890 --> 00:47:46,660
这些迭代的、更有噪声的例子的时间序列。

686
00:47:48,170 --> 00:47:50,190
现在我们的任务是，

687
00:47:50,690 --> 00:47:51,450
给出一幅图像，

688
00:47:52,100 --> 00:47:57,330
我们能不能在这些时间步长上，给出一个给定的切片，比如 T ，

689
00:47:58,190 --> 00:47:59,230
我们所要问的是，

690
00:47:59,230 --> 00:48:06,950
我们能不能知道从那个时间下一个去噪的例子，

691
00:48:07,530 --> 00:48:10,030
让它变得更具体，通过它，

692
00:48:11,200 --> 00:48:14,750
一张切片上的图像，让我们称之为 T ，

693
00:48:15,820 --> 00:48:18,770
我们要求神经网络学习的是，

694
00:48:18,880 --> 00:48:24,970
在前一步 T-1 的估计图像是什么，

695
00:48:26,020 --> 00:48:27,440
让这个非常具体，

696
00:48:27,940 --> 00:48:32,390
让我们假设我们在时间 3 有这张有噪音的图像，

697
00:48:33,000 --> 00:48:35,240
然后，神经网络试图预测，

698
00:48:35,770 --> 00:48:39,260
之前一个步骤的去噪结果是什么，

699
00:48:41,800 --> 00:48:42,840
将这两者进行比较，

700
00:48:42,840 --> 00:48:43,880
让我们后退一步，

701
00:48:44,890 --> 00:48:48,500
所有这两个图像的不同之处在于噪声函数，

702
00:48:49,690 --> 00:48:56,470
所以，在训练过程中向神经网络提出的问题是，

703
00:48:57,560 --> 00:49:00,240
我们如何训练，我们如何才能学习这种区别，

704
00:49:01,770 --> 00:49:04,870
如果我们的目标是学习这个迭代去噪过程，

705
00:49:05,280 --> 00:49:07,690
我们有所有这些连续的步骤，

706
00:49:08,890 --> 00:49:10,820
从更多的噪音到更少的噪音，

707
00:49:11,380 --> 00:49:12,950
我向大家提出一个问题，

708
00:49:13,300 --> 00:49:14,480
你怎么能想到，

709
00:49:15,260 --> 00:49:19,560
定义损失，定义神经网络要学习的训练目标，

710
00:49:22,810 --> 00:49:23,570
有什么想法吗？

711
00:49:25,650 --> 00:49:26,050
好的。

712
00:49:29,820 --> 00:49:31,780
你能在这一点上做进一步的阐述吗？

713
00:49:47,310 --> 00:49:49,180
它会，所以想法是，

714
00:49:49,230 --> 00:49:50,980
我们能不能看看同样的概念，

715
00:49:51,030 --> 00:49:57,230
试图将图像中的信息编码到一个可能减少的潜在空间，

716
00:49:57,230 --> 00:49:58,300
并试图学习它，

717
00:49:59,070 --> 00:50:04,300
它得到的与网络做的事情非常相关的东西，

718
00:50:04,650 --> 00:50:06,790
但更简单的东西，

719
00:50:07,280 --> 00:50:10,420
想想我们如何才能只比较这两个图像？

720
00:50:11,630 --> 00:50:12,030
好的。

721
00:50:18,680 --> 00:50:20,520
很简单，只需要简单的思考。

722
00:50:21,020 --> 00:50:21,420
好的。

723
00:50:23,450 --> 00:50:23,850
没错，

724
00:50:24,110 --> 00:50:26,940
想法是，有多少像素是相同的，

725
00:50:27,350 --> 00:50:29,755
事实证明，我们所需要做的就是，

726
00:50:29,755 --> 00:50:33,360
看看这两个步骤之间的像素差异，

727
00:50:33,920 --> 00:50:37,435
而扩散模型背后的聪明就是，

728
00:50:37,435 --> 00:50:43,930
定义这个残余噪声作为定义损耗。

729
00:50:44,740 --> 00:50:47,070
对于这样一个模型的训练目标，

730
00:50:47,600 --> 00:50:53,550
你所要做的就是比较这些迭代噪声的连续时间步长，

731
00:50:53,810 --> 00:50:56,470
然后问，均方误差是多少，

732
00:50:56,470 --> 00:50:59,370
这两者之间的像素差异是什么。

733
00:50:59,930 --> 00:51:03,390
事实证明，这在实践中非常有效，

734
00:51:04,070 --> 00:51:08,130
我们可以训练神经网络进行迭代去噪，

735
00:51:08,180 --> 00:51:12,210
通过这种非常直观的损失概念。

736
00:51:13,990 --> 00:51:17,585
好的，希望这能给你提供空间，

737
00:51:17,585 --> 00:51:23,230
说明扩散模型如何建立对去噪过程的理解，

738
00:51:23,610 --> 00:51:29,290
并能够学习迭代地去噪例子。

739
00:51:30,310 --> 00:51:35,640
现在的任务是我们如何才能取样全新的东西，

740
00:51:35,640 --> 00:51:37,670
我们如何生成一个新实例。

741
00:51:38,360 --> 00:51:41,740
这与那个完全相反的过程非常相关，

742
00:51:42,030 --> 00:51:44,320
我们刚刚经历的去噪过程，

743
00:51:45,040 --> 00:51:45,960
我们要做的是，

744
00:51:45,960 --> 00:51:49,910
拿出一个全新的完全随机噪音的实例，

745
00:51:51,150 --> 00:51:54,790
然后用我们训练过的扩散模型，

746
00:51:55,410 --> 00:51:58,900
来问它，好的，只需预测剩余噪声差，

747
00:51:59,400 --> 00:52:03,280
可以让我们得到噪音稍微小一些的东西，

748
00:52:03,750 --> 00:52:05,530
这就是它所做的，

749
00:52:05,970 --> 00:52:08,230
在这些迭代的时间步长上重复做的所有事情，

750
00:52:08,700 --> 00:52:15,540
这样我们可以从纯粹的噪音到不那么嘈杂的东西，反复发生，

751
00:52:16,100 --> 00:52:17,790
随着这个过程的发生，

752
00:52:18,110 --> 00:52:24,060
希望你能看到反映图像相关内容的东西的出现，

753
00:52:24,890 --> 00:52:28,800
时间一步一步，迭代地做这个采样程序，

754
00:52:29,970 --> 00:52:32,620
跨越了许多世代和许多时间步调，

755
00:52:33,090 --> 00:52:36,160
这样我们就可以回到这只狗，

756
00:52:36,300 --> 00:52:37,690
正在看着我们。

757
00:52:38,730 --> 00:52:40,960
我想让你们明白的是，

758
00:52:41,370 --> 00:52:43,660
扩散模型实现的是，

759
00:52:44,520 --> 00:52:47,420
从一个完全随机的噪声状态，

760
00:52:47,950 --> 00:52:50,990
我们有最大的可变性概念，

761
00:52:52,150 --> 00:52:55,550
通过定义它并将其分解为这个迭代过程，

762
00:52:56,020 --> 00:52:59,445
在每个步骤中都有一个做出的预测，

763
00:52:59,445 --> 00:53:00,950
有一个世代做出，

764
00:53:01,210 --> 00:53:08,540
所以最大的变异性转化为生成的样本中的最大多样性。

765
00:53:09,550 --> 00:53:12,140
这样，当我们到达最终状态时，

766
00:53:12,340 --> 00:53:15,140
我们就得到了一幅图像，一个合成样本，

767
00:53:15,400 --> 00:53:19,340
在完全没有噪音的数据空间中。

768
00:53:21,180 --> 00:53:22,480
所以在它的核心，

769
00:53:22,650 --> 00:53:25,960
这是一个扩散模型如何工作的总结，

770
00:53:26,340 --> 00:53:30,130
以及它们如何能够从完全随机的噪音

771
00:53:30,480 --> 00:53:35,140
变成非常可靠的、多样化的、新产生的样本。

772
00:53:36,840 --> 00:53:38,555
正如我已经提到的，

773
00:53:38,555 --> 00:53:42,280
我认为这种方法的强大之处在于，

774
00:53:42,870 --> 00:53:46,540
我们可以从完全随机开始，

775
00:53:47,570 --> 00:53:49,950
它封装了最大的可变性，

776
00:53:50,630 --> 00:53:56,550
因此现在扩散模型能够产生非常多样化的样本，

777
00:53:57,580 --> 00:54:01,110
来自彼此根本不同的噪声样本，

778
00:54:02,220 --> 00:54:03,350
这意味着，

779
00:54:03,350 --> 00:54:05,150
现在在这个例子中，

780
00:54:05,150 --> 00:54:07,870
我们从一些噪声样本转到这张图像，

781
00:54:08,770 --> 00:54:12,150
但同样令人震惊的是，

782
00:54:12,150 --> 00:54:16,490
如果我们考虑一个完全不同的随机噪声实例，

783
00:54:17,550 --> 00:54:19,900
他们彼此之间的差异很大，

784
00:54:20,280 --> 00:54:25,630
随机噪声、随机噪声，内部最大变异性和相互比较，

785
00:54:26,530 --> 00:54:32,960
去噪的结果将是一个不同的采样图像，

786
00:54:33,250 --> 00:54:36,800
它高度多样化，与我们以前看到的非常不同。

787
00:54:37,580 --> 00:54:39,930
这就是扩散模型的能力，

788
00:54:40,160 --> 00:54:45,900
在思考我们如何产生非常不同和多样化的新样本时。

789
00:54:46,430 --> 00:54:47,520
问题，好的。

790
00:54:52,120 --> 00:54:52,520
是的。

791
00:54:54,150 --> 00:54:55,430
这是个很棒的问题，

792
00:54:55,430 --> 00:54:56,345
所以问题是，

793
00:54:56,345 --> 00:54:58,330
模型如何知道何时停止，

794
00:54:58,830 --> 00:55:00,010
当你构建这些模型时，

795
00:55:00,060 --> 00:55:02,680
你定义了一组时间步长，

796
00:55:03,150 --> 00:55:05,435
这是一个参数，

797
00:55:05,435 --> 00:55:09,460
你可以在构建和训练扩散模型时使用，

798
00:55:10,140 --> 00:55:14,290
而且有不同的研究来观察什么有效，什么无效，

799
00:55:15,690 --> 00:55:16,745
核心思想是，

800
00:55:16,745 --> 00:55:17,885
通过更多的时间步长，

801
00:55:17,885 --> 00:55:22,150
你将产生更好的世代清晰度，

802
00:55:22,800 --> 00:55:24,250
但这也是一种权衡，

803
00:55:24,420 --> 00:55:29,020
就模型的稳定性以及培训和学习的效率而言。

804
00:55:56,850 --> 00:55:59,300
所以，所以问题是，

805
00:55:59,300 --> 00:56:07,115
解释为什么扩散模型的结果中可能会有一些遗漏的细节或不正确的细节，

806
00:56:07,115 --> 00:56:10,330
特别是手的例子。

807
00:56:11,540 --> 00:56:18,190
我不知道为什么手似乎会引起问题，

808
00:56:18,630 --> 00:56:24,190
我个人还没有看到讨论这一问题的报告、例子或文献，

809
00:56:24,540 --> 00:56:28,960
但我认为，大体上是，

810
00:56:29,130 --> 00:56:31,805
是的，会有不完美的地方，

811
00:56:31,805 --> 00:56:36,075
它不会是 100% 完美或准确，

812
00:56:36,075 --> 00:56:40,520
在对真实的实例的可能方面，

813
00:56:41,350 --> 00:56:44,540
我认为现在的许多进展，

814
00:56:44,560 --> 00:56:48,050
比如思考对底层体系结构进行哪些修改，

815
00:56:48,280 --> 00:56:52,670
可以尝试缓解其中的一些细节或问题，

816
00:56:52,750 --> 00:56:54,825
但为了讲课，

817
00:56:54,825 --> 00:56:59,480
我也可以在后面进一步讨论这个特定的例子。

818
00:57:01,180 --> 00:57:01,580
好的。

819
00:57:02,620 --> 00:57:03,020
所以。

820
00:57:04,160 --> 00:57:13,310
所以，是的，确实，我认为这些模型的力量是非常非常重要的，

821
00:57:14,230 --> 00:57:20,205
回到我们在第一节课上展示的例子，

822
00:57:20,205 --> 00:57:22,790
这种能力现在可以生成一个合成图像，

823
00:57:22,810 --> 00:57:26,750
从”一张宇航员骑马的照片“的语言提示。

824
00:57:27,360 --> 00:57:32,530
事实上，这种方法的基础是一个扩散模型，

825
00:57:33,150 --> 00:57:35,780
而扩散模型所做的是，

826
00:57:35,780 --> 00:57:41,980
它能够在文本和语言之间进行转换，

827
00:57:42,450 --> 00:57:45,820
然后在两者之间运行扩散过程，

828
00:57:46,110 --> 00:57:51,460
这样图像生成就可以由特定的语言提示来引导。

829
00:57:52,560 --> 00:57:57,850
我认为，更多这种从文本到图像生成的想法

830
00:57:57,990 --> 00:58:00,850
已经在互联网上掀起了一场风暴，

831
00:58:01,470 --> 00:58:05,080
但我认为这个想法的强大之处在于，

832
00:58:05,130 --> 00:58:12,580
我们可以根据我们通过语言指定的约束来指导生成过程，

833
00:58:12,690 --> 00:58:14,020
这是非常非常强大的，

834
00:58:14,990 --> 00:58:17,430
所有的东西都是高度照片逼真的，

835
00:58:17,750 --> 00:58:22,140
或者是用特定的艺术、艺术风格产生的，

836
00:58:22,940 --> 00:58:25,320
就像我在这里展示的例子一样。

837
00:58:27,030 --> 00:58:28,150
所以，到目前为止，

838
00:58:28,800 --> 00:58:36,010
在今天关于扩散模型的部分和我们对生成模型的更广泛的讨论中，

839
00:58:36,510 --> 00:58:39,580
我们主要集中在图像中的例子，

840
00:58:40,790 --> 00:58:44,100
但是，其他数据形态、其他应用程序呢，

841
00:58:45,080 --> 00:58:47,080
我们能设计出新的生成式模型吗，

842
00:58:47,970 --> 00:58:55,210
利用这些想法在其他现实世界应用领域中设计新的合成实例。

843
00:58:56,840 --> 00:58:59,280
对我来说，我这样说肯定是有偏见的，

844
00:58:59,630 --> 00:59:04,555
但思考这个想法最令人兴奋的方面之一是，

845
00:59:04,555 --> 00:59:06,870
在分子设计的背景下，

846
00:59:07,130 --> 00:59:12,960
以及它如何与化学、生命科学和环境科学相关联。

847
00:59:13,730 --> 00:59:19,080
例如，在化学和小分子领域，

848
00:59:19,610 --> 00:59:22,710
现在不是考虑图像和像素，

849
00:59:23,120 --> 00:59:27,150
我们考虑三维空间中的原子和分子。

850
00:59:27,800 --> 00:59:31,860
最近在建立扩散模型方面已经做了很多工作，

851
00:59:32,210 --> 00:59:37,440
可以观察定义分子的原子的三维坐标，

852
00:59:37,730 --> 00:59:41,520
从三维空间中的完全随机状态

853
00:59:41,960 --> 00:59:45,150
到再次执行同样的迭代去噪过程，

854
00:59:45,800 --> 00:59:49,470
现在生成定义良好的分子结构，

855
00:59:49,760 --> 00:59:54,360
可以用于药物发现或治疗设计。

856
00:59:55,880 --> 00:59:58,080
除此之外，特别是在我的研究中，

857
00:59:58,880 --> 01:00:00,750
我们正在建立新的扩散模型，

858
01:00:00,830 --> 01:00:06,660
可以产生生物分子或生物序列，比如蛋白质，

859
01:00:07,160 --> 01:00:13,345
它们是人类生活中所有生物功能的执行者和执行者，

860
01:00:13,345 --> 01:00:15,120
跨越生命的所有领域。

861
01:00:15,910 --> 01:00:18,890
具体地说，在我的工作和我的研究团队中，

862
01:00:18,970 --> 01:00:21,140
我们开发了一种新的扩散模型，

863
01:00:21,460 --> 01:00:24,860
它能够产生全新的蛋白质结构，

864
01:00:25,600 --> 01:00:29,235
我将分享一点关于这个模式的工作原理

865
01:00:29,235 --> 01:00:32,100
和在它后面的我们的核心理念，

866
01:00:32,360 --> 01:00:33,990
来结束这一部分。

867
01:00:35,250 --> 01:00:40,055
真正推动整个领域这项工作的动机是，

868
01:00:40,055 --> 01:00:45,070
这个试图设计新的生物实体的目标，比如蛋白质，

869
01:00:45,450 --> 01:00:51,790
可以具有治疗功能或者扩大生物学的功能空间。

870
01:00:52,550 --> 01:00:55,585
在生成式 AI 领域有很多努力，

871
01:00:55,585 --> 01:00:57,720
现在都在针对这个问题，

872
01:00:58,010 --> 01:01:01,200
因为它可能会产生巨大的潜在影响。

873
01:01:02,670 --> 01:01:04,090
在我们的具体工作中，

874
01:01:04,620 --> 01:01:07,330
我们考虑蛋白质设计这个问题，

875
01:01:07,740 --> 01:01:11,530
通过回到生物学并从中获得灵感，

876
01:01:12,320 --> 01:01:17,730
如果你考虑蛋白质功能是如何决定和编码的，

877
01:01:18,530 --> 01:01:23,520
它被浓缩成，蛋白质结构在三维空间中，

878
01:01:24,020 --> 01:01:29,160
这种结构如何告知和编码特定的生物功能，

879
01:01:30,350 --> 01:01:35,550
反过来，蛋白质并不总是一开始就采用特定的三维结构，

880
01:01:35,930 --> 01:01:39,390
它们经历了我们所称的蛋白质折叠的过程，

881
01:01:39,680 --> 01:01:47,070
这定义了蛋白质中的原子链如何在三维空间中摆动，

882
01:01:47,300 --> 01:01:50,010
以采用最终定义的三维结构，

883
01:01:50,450 --> 01:01:55,320
该结构高度特定，并与特定的生物功能高度相关。

884
01:01:56,440 --> 01:01:59,985
所以我们问，我们研究了蛋白质折叠的问题，

885
01:01:59,985 --> 01:02:02,210
以及蛋白质折叠是如何导致结构的，

886
01:02:03,160 --> 01:02:08,480
以启发一种新的扩散模型方法来解决这个蛋白质结构设计问题。

887
01:02:09,260 --> 01:02:10,590
我们说，好的，

888
01:02:11,030 --> 01:02:13,950
如果蛋白质处于完全未折叠的状态，

889
01:02:14,390 --> 01:02:19,410
你可以认为这等同于图像中的随机噪声状态，

890
01:02:20,120 --> 01:02:22,710
它完全是松松垮垮的配置，

891
01:02:23,030 --> 01:02:26,190
它没有明确的结构，它是展开的。

892
01:02:26,760 --> 01:02:27,875
我们所做的是，

893
01:02:27,875 --> 01:02:35,830
把蛋白质结构被展开的概念作为扩散模型的噪声状态，

894
01:02:36,390 --> 01:02:38,200
我们设计了一个扩散模型，

895
01:02:38,550 --> 01:02:42,670
我们训练它从蛋白质的展开状态出发，

896
01:02:43,350 --> 01:02:49,450
然后产生一个关于新结构的预测，

897
01:02:49,770 --> 01:02:52,480
它将定义一个特定的三维结构。

898
01:02:53,620 --> 01:02:56,630
我们可以直观地看到这个算法是如何工作的，

899
01:02:56,860 --> 01:02:58,550
我们称之为折叠扩散，

900
01:02:59,260 --> 01:03:01,970
在这段视频中，我将在这里展示，

901
01:03:02,470 --> 01:03:04,940
在开始的初始帧中，

902
01:03:05,290 --> 01:03:08,510
我们有一个随机的噪音蛋白质链，

903
01:03:09,280 --> 01:03:16,250
完全随机的配置在 3D 空间中展开，

904
01:03:17,100 --> 01:03:21,430
现在，这段视频将展示的是去噪步骤，

905
01:03:21,720 --> 01:03:27,220
从随机噪因状态到最终生成的结构。

906
01:03:28,640 --> 01:03:29,760
正如你所看到的，

907
01:03:29,810 --> 01:03:34,140
这个过程非常类似于我用图像介绍的概念，

908
01:03:34,760 --> 01:03:38,970
我们试图从嘈杂的东西变成有结构的东西，

909
01:03:39,290 --> 01:03:42,150
现在到达最终生成的蛋白质结构。

910
01:03:43,810 --> 01:03:45,920
所以，我们的工作实际上集中在，

911
01:03:46,060 --> 01:03:52,400
为蛋白质结构设计问题引入一种新的基础性算法，

912
01:03:52,840 --> 01:03:54,890
通过扩散模型。

913
01:03:55,710 --> 01:03:56,885
但事实证明，

914
01:03:56,885 --> 01:04:02,650
在我们和其他人引入这些算法后的很短时间内，

915
01:04:02,970 --> 01:04:08,650
在缩放和扩展这些扩散模型出现了沉淀，

916
01:04:09,060 --> 01:04:13,210
以进行非常具体的可编程蛋白质设计时，

917
01:04:13,740 --> 01:04:15,910
现在我们看到了大规模的努力，

918
01:04:16,410 --> 01:04:20,620
使用这些扩散模型算法来产生蛋白质设计，

919
01:04:21,030 --> 01:04:24,820
并在物理世界中实际实验实现它们。

920
01:04:25,360 --> 01:04:27,240
所以在这张我在左边展示的图像中，

921
01:04:27,590 --> 01:04:32,400
彩色图像是扩散模型生成的输出，

922
01:04:32,810 --> 01:04:35,040
黑白图像是，

923
01:04:35,150 --> 01:04:40,740
如果你得到这个结果，在生物实验室里合成它，

924
01:04:41,030 --> 01:04:43,680
然后看着并拍下产生的蛋白质的照片，

925
01:04:44,030 --> 01:04:45,270
然后评估它的结构，

926
01:04:45,830 --> 01:04:48,090
我们看到有很强的一致性，

927
01:04:48,350 --> 01:04:53,130
这意味着我们有能力利用这些强大的扩散模型

928
01:04:53,420 --> 01:04:55,315
来产生新的蛋白质，

929
01:04:55,315 --> 01:04:57,330
这些蛋白质可以在物理世界中实现。

930
01:04:58,090 --> 01:04:59,300
这并不止步于此，

931
01:04:59,800 --> 01:05:05,240
我们现在可以考虑为非常具体的治疗应用设计蛋白质，

932
01:05:05,890 --> 01:05:08,420
例如，在这个可视化中，

933
01:05:08,830 --> 01:05:12,780
这被设计成一种新型蛋白质结合剂，

934
01:05:12,860 --> 01:05:19,410
旨在结合和阻止 COVID spike 受体的活动。

935
01:05:19,970 --> 01:05:22,140
所以你再一次看到的是，

936
01:05:22,310 --> 01:05:27,330
在最终的设计中，扩散过程的可视化，

937
01:05:27,380 --> 01:05:31,740
结合并阻止了 COVID spike 受体的顶部。

938
01:05:33,410 --> 01:05:37,080
所以，希望，这种方式描绘了

939
01:05:37,670 --> 01:05:41,700
我们今天所处的生成式 AI 的全景，

940
01:05:42,510 --> 01:05:48,610
我认为在蛋白质科学和更广泛的生物学的例子中，

941
01:05:48,900 --> 01:05:50,270
突出了这样一个事实，

942
01:05:50,270 --> 01:05:55,840
生成式 AI 现在能够实现真正有影响力的应用程序，

943
01:05:56,960 --> 01:05:58,770
不仅仅是为了创造很酷的图像，

944
01:05:59,360 --> 01:06:02,370
或者我们可以认为是人工智能合成艺术，

945
01:06:03,540 --> 01:06:09,440
生成式 AI 现在使我们能够思考现实世界问题的设计解决方案。

946
01:06:10,360 --> 01:06:12,590
我认为，还有很多悬而未决问题，

947
01:06:12,700 --> 01:06:18,860
在理解生成式 AI 方法的能力、局限性、应用上，

948
01:06:19,270 --> 01:06:21,320
以及它们如何增强我们的社会能力方面。

949
01:06:23,640 --> 01:06:27,820
作为总结，我想以一个更高层次的思考来结束。

950
01:06:29,650 --> 01:06:35,810
这就是，这种生成性设计的想法提升了这个更高层次的概念，

951
01:06:36,460 --> 01:06:38,150
关于理解意味着什么，

952
01:06:39,190 --> 01:06:43,550
我认为物理学家 Richard Feynman 的这句话完美地捕捉到了这一点，

953
01:06:43,990 --> 01:06:47,540
他说，我无法创造的东西，我无法理解，

954
01:06:49,030 --> 01:06:50,720
为了创造一些东西，

955
01:06:51,370 --> 01:06:54,110
作为人类，我们真的必须了解它是如何工作的，

956
01:06:54,940 --> 01:06:57,800
相反，为了完全理解一些东西，

957
01:06:58,300 --> 01:07:01,490
我认为我们必须创造出工程设计，

958
01:07:02,800 --> 01:07:06,810
生成式 AI 和生成性智能是这个概念的核心，

959
01:07:06,810 --> 01:07:09,350
我所认为的概念学习，

960
01:07:10,130 --> 01:07:15,780
能够提炼出一个非常复杂的问题的核心根源，

961
01:07:16,380 --> 01:07:19,940
然后在这个基础上发展到设计和创造，

962
01:07:21,860 --> 01:07:23,970
当这门课程在周一开始时，

963
01:07:24,950 --> 01:07:28,350
Alexander 介绍了什么智能的概念，

964
01:07:28,940 --> 01:07:34,830
通俗地说，是指能够获取信息，并将其用于未来决策的能力，

965
01:07:36,100 --> 01:07:42,320
人类的学习不仅限于解决具体、不同任务的能力，

966
01:07:43,490 --> 01:07:49,170
它是建立在能够理解概念的想法上，

967
01:07:49,520 --> 01:07:54,810
并使用这些概念来创造、想象、启发和梦想，

968
01:07:56,710 --> 01:08:00,620
我认为，现在我们必须真正思考，

969
01:08:01,180 --> 01:08:03,530
智能这个概念的真正含义，

970
01:08:04,400 --> 01:08:08,230
以及它的联系和区别，

971
01:08:08,670 --> 01:08:13,840
在人工智能和我们作为人类能够做和创造的东西之间。

972
01:08:15,550 --> 01:08:17,510
说到这里，我就把这个留给你们，

973
01:08:17,740 --> 01:08:20,480
我希望这能启发你们进一步思考，

974
01:08:20,650 --> 01:08:22,940
我们在课程中谈到的内容，

975
01:08:23,590 --> 01:08:28,010
以及更广泛的智能，人工智能，深度学习。

976
01:08:29,010 --> 01:08:31,960
所以，非常感谢你的关注，

977
01:08:32,460 --> 01:08:35,000
我们要稍作停顿，

978
01:08:35,000 --> 01:08:37,030
因为我们有点晚了，

979
01:08:37,260 --> 01:08:40,480
然后由 Ramin 开始我们精彩的演讲。

980
01:08:40,860 --> 01:08:41,710
非常感谢。

