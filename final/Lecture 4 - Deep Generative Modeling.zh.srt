1
00:00:09,460 --> 00:00:11,690
我对这次演讲非常兴奋，

2
00:00:11,830 --> 00:00:14,450
因为正如 Alexander 昨天介绍的那样，

3
00:00:15,580 --> 00:00:18,980
现在我们正处于一个生成性人工智能的巨大时代，

4
00:00:19,420 --> 00:00:23,630
今天我们将学习深度生成性建模的基础，

5
00:00:23,890 --> 00:00:26,000
在那里我们将讨论建立系统，

6
00:00:26,590 --> 00:00:29,180
不仅可以在数据中寻找模式，

7
00:00:29,710 --> 00:00:31,700
而且实际上可以更进一步，

8
00:00:32,050 --> 00:00:36,530
基于这些学习的模式生成全新的数据实例，

9
00:00:37,690 --> 00:00:40,830
这是一个令人难以置信的复杂和强大的想法，

10
00:00:40,830 --> 00:00:41,610
就像我提到的，

11
00:00:41,610 --> 00:00:43,610
这是深度学习的一个特殊子集，

12
00:00:43,990 --> 00:00:49,430
在过去几年里真的发生了爆炸式的增长，特别是今年。

13
00:00:50,540 --> 00:00:54,570
所以，为了演示这些算法的强大功能，

14
00:00:55,130 --> 00:00:59,370
让我向你们展示一下这三张不同的脸，

15
00:01:01,010 --> 00:01:03,060
我想让你想一想，

16
00:01:04,400 --> 00:01:06,610
想想你认为哪张脸是真的，

17
00:01:07,420 --> 00:01:09,210
如果你认为是脸 A ，请举手，

18
00:01:10,730 --> 00:01:12,000
好的，看到几个人，

19
00:01:12,560 --> 00:01:13,260
脸 B ，

20
00:01:15,340 --> 00:01:16,190
更多的人，

21
00:01:17,320 --> 00:01:18,020
脸 C ，

22
00:01:21,060 --> 00:01:21,790
第二名，

23
00:01:22,620 --> 00:01:25,540
好的，事实是你们都错了，

24
00:01:26,670 --> 00:01:28,750
这三张脸都是假的，

25
00:01:29,820 --> 00:01:31,510
这些人并不存在，

26
00:01:31,830 --> 00:01:34,960
这些图像是由深度生成模型合成的，

27
00:01:35,490 --> 00:01:37,510
基于人脸数据进行训练，

28
00:01:37,800 --> 00:01:40,660
并要求生成新的实例。

29
00:01:41,650 --> 00:01:46,580
现在，我认为这个演示展示了这些想法的力量，

30
00:01:46,900 --> 00:01:49,310
和生成性建模概念的力量。

31
00:01:49,840 --> 00:01:53,720
所以，让我们更具体地了解如何将这件事正规化。

32
00:01:54,690 --> 00:01:55,780
到目前为止，在这门课程中，

33
00:01:55,950 --> 00:01:59,650
我们一直在研究我们所说的监督学习问题，

34
00:02:00,390 --> 00:02:01,780
这意味着我们给出数据，

35
00:02:02,370 --> 00:02:05,140
并与这些数据相关联的是一组标签，

36
00:02:05,940 --> 00:02:11,180
我们的目标是学习映射该数据的标签的函数，

37
00:02:11,770 --> 00:02:14,060
现在我们在一门关于深度学习的课程，

38
00:02:14,140 --> 00:02:18,500
所以我们一直关注由深度神经网络定义的函数映射，

39
00:02:19,180 --> 00:02:21,230
但实际上，这个函数可以是任何东西，

40
00:02:21,280 --> 00:02:22,430
神经网络很强大，

41
00:02:22,510 --> 00:02:25,040
但我们也可以使用其他技术。

42
00:02:26,620 --> 00:02:30,110
相比之下，在机器学习中还有另一类问题，

43
00:02:30,580 --> 00:02:33,080
我们称之为无监督学习，

44
00:02:33,700 --> 00:02:35,690
我们获取数据，

45
00:02:36,130 --> 00:02:39,620
但现在我们只给出数据，没有标签，

46
00:02:40,060 --> 00:02:42,980
我们的目标是试图建立某种方法，

47
00:02:43,390 --> 00:02:47,420
可以理解这些数据的隐藏的底层结构，

48
00:02:48,590 --> 00:02:50,245
这使我们能够做的是，

49
00:02:50,245 --> 00:02:55,140
它让我们对数据的基本表示形式有了新的见解，

50
00:02:55,700 --> 00:02:57,060
并且正如我们稍后将看到的那样，

51
00:02:58,010 --> 00:03:00,960
使我们能够生成新的数据实例。

52
00:03:01,790 --> 00:03:03,090
现在，这类问题，

53
00:03:03,530 --> 00:03:06,210
无监督学习的定义，

54
00:03:06,710 --> 00:03:09,960
抓住了我们今天要讨论的模型类型，

55
00:03:10,250 --> 00:03:12,570
重点是生成性建模，

56
00:03:13,700 --> 00:03:15,960
这是无监督学习的一个例子，

57
00:03:16,220 --> 00:03:19,650
它与这个问题的目标相一致，

58
00:03:19,670 --> 00:03:22,530
在这个问题中，我们只得到训练集中的样本，

59
00:03:23,120 --> 00:03:24,820
我们想要学习一种模型，

60
00:03:24,900 --> 00:03:29,170
表示该模型所看到的数据的分布。

61
00:03:30,270 --> 00:03:33,700
生成式建模一般有两种形式，

62
00:03:34,650 --> 00:03:36,430
第一种是密度估计，

63
00:03:36,750 --> 00:03:38,440
第二种是样本生成。

64
00:03:39,760 --> 00:03:41,090
在密度估计中，

65
00:03:41,110 --> 00:03:44,630
任务是给出一些数据实例，

66
00:03:45,490 --> 00:03:47,570
我们的目标是训练一个模型，

67
00:03:48,040 --> 00:03:51,470
学习潜在概率分布，

68
00:03:52,180 --> 00:03:55,700
描述了数据来源。

69
00:03:57,080 --> 00:03:58,170
对于样本生成，

70
00:03:58,850 --> 00:04:00,450
想法是相似的，

71
00:04:00,560 --> 00:04:04,290
但重点更多地放在生成新实例上，

72
00:04:05,120 --> 00:04:07,560
我们生成样本的目标是，

73
00:04:07,670 --> 00:04:11,460
再次，学习这种潜在概率分布的模型，

74
00:04:12,350 --> 00:04:15,175
然后使用该模型对其进行采样，

75
00:04:15,175 --> 00:04:16,980
并生成新的实例，

76
00:04:17,330 --> 00:04:20,280
类似于我们看到的数据，

77
00:04:20,870 --> 00:04:25,560
大致下降，理想情况下，相同的真实数据分布。

78
00:04:27,060 --> 00:04:30,520
现在，在密度估计和样本生成这两种情况下，

79
00:04:31,260 --> 00:04:33,010
潜在的问题都是相同的，

80
00:04:33,750 --> 00:04:37,030
我们的学习任务是尝试建立一个模型，

81
00:04:37,170 --> 00:04:39,580
学习这种概率分布，

82
00:04:39,990 --> 00:04:44,260
尽可能接近真实的数据分布。

83
00:04:46,540 --> 00:04:50,870
好的，有了这个生成性建模的定义和概念，

84
00:04:51,430 --> 00:04:54,915
我们可以用什么方法部署生成式模型，

85
00:04:54,915 --> 00:04:58,430
在现实世界中为高影响的应用程序，

86
00:04:59,770 --> 00:05:04,185
好的，生成式模型是如此强大的部分原因是，

87
00:05:04,185 --> 00:05:09,410
它们有能力发现数据集中的底层特征，

88
00:05:09,490 --> 00:05:11,480
并以有效的方式对其进行编码。

89
00:05:12,190 --> 00:05:15,890
例如，如果我们考虑面部检测问题，

90
00:05:16,270 --> 00:05:19,250
我们得到一个包含许多不同人脸的数据集，

91
00:05:20,230 --> 00:05:22,305
而没有检查这些数据，

92
00:05:22,305 --> 00:05:26,510
我们可能不知道这个数据集中人脸的分布是什么，

93
00:05:26,710 --> 00:05:29,510
相对于我们可能关心的特征，

94
00:05:29,770 --> 00:05:35,630
例如，头部的姿势、衣服、眼镜、肤色、头发等，

95
00:05:36,860 --> 00:05:37,940
情况可能是，

96
00:05:37,940 --> 00:05:42,340
我们的训练数据可能对特定的特征非常有偏见，

97
00:05:42,690 --> 00:05:44,740
在我们甚至没有意识到的情况下，

98
00:05:45,700 --> 00:05:46,970
使用生成模型，

99
00:05:47,110 --> 00:05:51,260
我们可以识别这些潜在特征的分布，

100
00:05:51,280 --> 00:05:55,310
以一种完全自动的方式，而不需要任何标记，

101
00:05:55,980 --> 00:05:59,810
为了了解哪些特征在数据中可能被高估，

102
00:06:00,460 --> 00:06:03,200
哪些特征在数据中可能被低估。

103
00:06:04,050 --> 00:06:08,615
这是今天和明天软件实验的重点，

104
00:06:08,615 --> 00:06:12,190
将成为软件实验竞争的一部分，

105
00:06:12,930 --> 00:06:15,880
开发可以完成这项任务的生成性模型，

106
00:06:16,200 --> 00:06:19,540
并使用它来发现和诊断偏差，

107
00:06:19,680 --> 00:06:22,630
面部检测模型中可能存在的（偏差）。

108
00:06:25,050 --> 00:06:26,975
另一个真正强大的例子是，

109
00:06:26,975 --> 00:06:31,540
在异常值检测的情况下，识别罕见事件。

110
00:06:32,340 --> 00:06:36,220
所以，让我们来考虑自动驾驶汽车的例子，

111
00:06:37,040 --> 00:06:40,290
有了自动驾驶汽车，假设它正在现实世界中行驶，

112
00:06:40,790 --> 00:06:42,820
我们真的很想确保，

113
00:06:42,820 --> 00:06:46,645
这辆车能够处理所有可能的情况，

114
00:06:46,645 --> 00:06:49,110
和它可能遇到的所有可能的情况，

115
00:06:49,700 --> 00:06:53,430
包括边缘情况，比如鹿走到车前面，

116
00:06:53,660 --> 00:06:55,740
或一些意想不到的罕见事件，

117
00:06:56,300 --> 00:06:59,340
而不仅仅是典型的直线行驶，

118
00:06:59,450 --> 00:07:02,280
它可能会在大部分时间看到的。

119
00:07:03,250 --> 00:07:04,400
对于生成模型，

120
00:07:04,600 --> 00:07:07,460
我们可以使用这种密度估计的思想

121
00:07:07,630 --> 00:07:14,120
来识别训练数据中的罕见和异常事件，

122
00:07:14,260 --> 00:07:15,900
以及当它们发生时，

123
00:07:15,900 --> 00:07:17,900
就像模型第一次看到它们一样。

124
00:07:19,490 --> 00:07:25,290
希望这幅图描绘了什么是生成性建模的基本概念，

125
00:07:25,310 --> 00:07:26,815
以及几种不同的方式，

126
00:07:26,815 --> 00:07:33,060
我们可以将这些想法实际部署到强大而有效的现实世界应用程序中。

127
00:07:35,060 --> 00:07:39,550
在今天的课程中，我们将重点介绍一大类生成性模型，

128
00:07:39,550 --> 00:07:41,520
我们称之为隐变量模型，

129
00:07:42,050 --> 00:07:47,220
具体分为两种类型的隐变量模型。

130
00:07:48,740 --> 00:07:52,140
首先，我已经引入了隐变量这个术语，

131
00:07:52,700 --> 00:07:57,030
但是我还没有告诉你们或者描述它到底是什么，

132
00:07:58,350 --> 00:07:59,660
我认为一个很好的例子，

133
00:07:59,660 --> 00:08:02,830
也是我在整个课程中最喜欢的一个例子，

134
00:08:03,600 --> 00:08:05,980
获得隐变量的思想，

135
00:08:06,450 --> 00:08:09,220
就是这个来自柏拉图的理想国的小故事，

136
00:08:09,960 --> 00:08:12,340
这就是众所周知的洞穴寓言，

137
00:08:13,410 --> 00:08:14,200
在这个寓言中，

138
00:08:14,340 --> 00:08:15,880
有一群囚犯，

139
00:08:16,380 --> 00:08:18,125
作为惩罚的一部分，

140
00:08:18,125 --> 00:08:20,080
他们被迫面对一堵墙，

141
00:08:20,970 --> 00:08:24,700
现在囚犯们唯一能观察到的东西是，

142
00:08:24,810 --> 00:08:30,220
在他们身后的火堆前经过的物体的阴影，

143
00:08:30,600 --> 00:08:35,440
他们观察这个洞穴墙上的阴影的投射，

144
00:08:36,780 --> 00:08:37,610
对于囚犯来说，

145
00:08:37,610 --> 00:08:39,800
这些阴影是他们唯一看到的东西，

146
00:08:39,800 --> 00:08:41,960
他们的观察，他们可以测量他们，

147
00:08:41,960 --> 00:08:42,910
他们可以给他们起名字，

148
00:08:43,410 --> 00:08:45,280
因为对他们来说，这就是他们的现实，

149
00:08:46,380 --> 00:08:50,810
但他们不能直接看到潜在的物体，

150
00:08:50,810 --> 00:08:54,070
真实的因素本身，这些因素投射那些阴影，

151
00:08:55,180 --> 00:08:59,150
这里的这些对象就像机器学习中的隐变量，

152
00:09:00,030 --> 00:09:01,690
它们不是直接可见的，

153
00:09:02,160 --> 00:09:06,580
但它们是真正的潜在特征或解释因素，

154
00:09:06,720 --> 00:09:12,880
它们创造了我们可以看到和观察到的差异和变量。

155
00:09:15,030 --> 00:09:18,640
这就达到了生成性建模的目标，

156
00:09:18,930 --> 00:09:22,330
即找到方法让我们能够真正学习这些隐藏的特征，

157
00:09:22,440 --> 00:09:24,340
这些潜在的隐变量，

158
00:09:24,660 --> 00:09:28,750
即使我们只得到了观察到的数据。

159
00:09:31,530 --> 00:09:35,240
所以，让我们从讨论一个非常简单的生成性模型开始，

160
00:09:35,240 --> 00:09:39,640
试图通过对数据输入进行编码来实现这一点，

161
00:09:40,320 --> 00:09:43,220
我们将要讨论的模型被称为自编码器，

162
00:09:44,050 --> 00:09:46,940
为了看看自编码器是如何工作的，

163
00:09:47,290 --> 00:09:48,710
我们将一步一步地来，

164
00:09:49,330 --> 00:09:53,750
从获取一些原始输入数据开始，

165
00:09:54,430 --> 00:09:57,860
并将其传递给一系列神经网络层，

166
00:09:58,820 --> 00:10:06,630
现在，这第一步的输出就是我们所说的低维隐空间，

167
00:10:07,070 --> 00:10:10,980
它是这些基本特征的编码表示，

168
00:10:11,780 --> 00:10:15,640
这就是我们试图训练这个模型并预测这些特征的目标，

169
00:10:17,120 --> 00:10:20,790
这样的模型之所以被称为编码器或自编码器，

170
00:10:21,080 --> 00:10:26,970
是因为它将数据 x 映射到隐变量 z 的向量中。

171
00:10:27,950 --> 00:10:29,845
现在，让我们问自己一个问题，

172
00:10:29,845 --> 00:10:30,840
让我们暂停一下，

173
00:10:31,720 --> 00:10:38,670
为什么我们可能会关心这个隐变量向量 z 在低维空间，

174
00:10:40,620 --> 00:10:41,650
有谁有什么想法吗？

175
00:10:49,870 --> 00:10:53,690
好的，也许有一些想法，是的。

176
00:10:57,070 --> 00:10:58,760
建议是，这样做效率更高，

177
00:10:58,810 --> 00:11:00,975
是的，这就是问题所在，

178
00:11:00,975 --> 00:11:02,360
这个问题的核心。

179
00:11:02,920 --> 00:11:05,985
拥有低维隐空间的想法是，

180
00:11:05,985 --> 00:11:11,385
它是对丰富的、高维数据的一种非常有效、紧凑的编码，

181
00:11:11,385 --> 00:11:12,500
我们可以从它开始。

182
00:11:13,410 --> 00:11:16,175
正如你所指出的，这意味着，

183
00:11:16,175 --> 00:11:21,100
我们能够将数据压缩为向量的小特征表示形式，

184
00:11:22,380 --> 00:11:25,390
这抓住了这种紧凑性和丰富性，

185
00:11:25,890 --> 00:11:28,990
而不需要如此多的内存或存储空间。

186
00:11:30,250 --> 00:11:34,670
那么，我们如何训练网络来学习这个隐变量向量呢，

187
00:11:36,270 --> 00:11:37,895
由于我们没有训练数据，

188
00:11:37,895 --> 00:11:41,110
我们不能显式地观察这些潜在的变量 z ，

189
00:11:41,730 --> 00:11:43,930
我们需要做一些更聪明的事情，

190
00:11:44,770 --> 00:11:46,725
自编码器所做的是，

191
00:11:46,725 --> 00:11:54,650
它建立一种方法，将这个隐变量向量解码回到原始数据空间，

192
00:11:55,300 --> 00:12:00,830
试图从压缩的、高效的隐编码中重建原始图像。

193
00:12:01,480 --> 00:12:05,030
再次，我们可以使用一系列神经网络层，

194
00:12:05,350 --> 00:12:08,270
比如卷积层，完全连接层，

195
00:12:08,590 --> 00:12:14,510
但现在要从低维空间向上映射回输入空间，

196
00:12:15,880 --> 00:12:20,870
这会生成重建的输出，我们可以将其表示为 x hat ，

197
00:12:21,400 --> 00:12:25,730
因为它是原始输入数据的不完美重建。

198
00:12:27,070 --> 00:12:28,250
要训练这个网络，

199
00:12:28,990 --> 00:12:30,380
我们所要做的就是，

200
00:12:30,490 --> 00:12:34,610
比较输出的重建数据和原始的输入数据，

201
00:12:34,900 --> 00:12:38,180
然后说，我们如何使它们尽可能地相似，

202
00:12:38,350 --> 00:12:43,400
我们可以最小化输入和重构输出之间的距离。

203
00:12:44,080 --> 00:12:45,380
例如，对于一幅图像，

204
00:12:45,850 --> 00:12:52,190
我们可以比较输入数据和重建输出之间的像素差异，

205
00:12:52,450 --> 00:12:56,510
只需将图像彼此相减并对差异平方，

206
00:12:56,830 --> 00:13:02,510
捕捉输入和重建之间的像素差异。

207
00:13:04,990 --> 00:13:07,290
我希望你能注意和理解的是，

208
00:13:07,290 --> 00:13:08,840
在损失的定义中，

209
00:13:09,790 --> 00:13:11,930
它不需要任何标签，

210
00:13:11,950 --> 00:13:14,670
损失的唯一组成部分是，

211
00:13:14,670 --> 00:13:19,460
原始输入数据 x 和重建的输出 x hat 。

212
00:13:21,280 --> 00:13:24,120
所以我现在已经简化了这个图，

213
00:13:24,120 --> 00:13:27,570
通过抽象出那些单独的神经网络层，

214
00:13:27,570 --> 00:13:31,040
在这个的编码器和解码器组件中。

215
00:13:32,000 --> 00:13:38,730
再次，这个不需要任何标签的想法回到了无监督学习的想法，

216
00:13:39,290 --> 00:13:45,180
因为我们所做的是我们能够学习一个编码量，我们的隐变量，

217
00:13:45,650 --> 00:13:49,110
我们无法在没有任何显式标签的情况下观察到，

218
00:13:49,310 --> 00:13:51,840
我们从原始数据本身开始。

219
00:13:53,790 --> 00:13:57,460
事实证明，只要问题和答案出来，

220
00:13:57,660 --> 00:14:02,075
隐空间的维度就会产生巨大的影响，

221
00:14:02,075 --> 00:14:08,860
对生成的重建的质量和信息瓶颈的压缩程度。

222
00:14:10,020 --> 00:14:12,040
自动编码是压缩的一种形式，

223
00:14:12,300 --> 00:14:15,520
因此隐空间的维度越低，

224
00:14:15,990 --> 00:14:19,270
我们的重建效果就越差，

225
00:14:19,920 --> 00:14:21,980
但维度越高，

226
00:14:22,000 --> 00:14:25,730
编码的效率就越低。

227
00:14:27,440 --> 00:14:29,760
所以总结第一部分，

228
00:14:29,990 --> 00:14:35,400
这个自编码器的想法是利用这个瓶颈，压缩的，隐藏的隐层，

229
00:14:35,750 --> 00:14:41,160
来试图降低网络，以学习数据的紧凑、高效的表示，

230
00:14:42,000 --> 00:14:44,000
我们不需要任何标签，

231
00:14:44,000 --> 00:14:45,610
这完全是无监管的，

232
00:14:46,080 --> 00:14:47,240
所以，通过这种方式，

233
00:14:47,240 --> 00:14:52,180
我们能够自动对数据本身中的信息进行编码，

234
00:14:52,200 --> 00:14:58,440
以了解这个隐空间，自编码信息，自编码数据。

235
00:14:59,960 --> 00:15:02,460
这是一个非常简单的模型，

236
00:15:03,200 --> 00:15:10,170
事实证明，在实践中，自编码或自动编码的想法有一点扭曲，

237
00:15:10,520 --> 00:15:13,950
使我们能够实际生成新的示例，

238
00:15:14,090 --> 00:15:17,790
这些示例不仅仅是输入数据本身的重建。

239
00:15:18,400 --> 00:15:22,910
这就引出了变分自编码器的概念，或 VAE ，

240
00:15:24,620 --> 00:15:27,330
对于我们刚刚看到的传统自编码器，

241
00:15:27,950 --> 00:15:31,140
如果我们更仔细地关注隐层，

242
00:15:31,580 --> 00:15:34,230
它显示为橙色三文鱼色，

243
00:15:35,150 --> 00:15:39,000
这个隐层只是神经网络中的一个普通层，

244
00:15:39,350 --> 00:15:41,340
这完全是决定的，

245
00:15:41,850 --> 00:15:44,290
这意味着，一旦我们训练了网络，

246
00:15:44,370 --> 00:15:45,670
一旦设置了权重，

247
00:15:46,110 --> 00:15:48,700
任何时候我们传递给定的输入，

248
00:15:49,110 --> 00:15:52,600
并通过潜在层返回，解码返回，

249
00:15:52,800 --> 00:15:55,300
我们将得到完全相同的重建，

250
00:15:55,650 --> 00:15:56,800
权重没有改变，

251
00:15:57,030 --> 00:15:58,270
这是确定性的。

252
00:15:59,230 --> 00:15:59,900
相比之下，

253
00:16:01,060 --> 00:16:06,895
变分自编码器 VAE 引入了随机性的元素，

254
00:16:06,895 --> 00:16:10,680
这是对自编码思想的概率扭曲，

255
00:16:11,240 --> 00:16:18,040
这将允许我们生成新图像或新数据实例，

256
00:16:18,330 --> 00:16:20,140
与输入数据相似，

257
00:16:20,610 --> 00:16:23,770
但不强制进行严格重建的实例。

258
00:16:24,870 --> 00:16:27,520
在实践中，使用变分自编码器，

259
00:16:27,870 --> 00:16:33,460
我们用随机采样操作取代了单一确定层，

260
00:16:35,380 --> 00:16:39,320
现在，我们不是直接学习隐变量本身，

261
00:16:39,760 --> 00:16:44,240
我们为每个隐变量定义平均值和标准差，

262
00:16:44,680 --> 00:16:48,680
以捕获隐变量的概率分布。

263
00:16:50,440 --> 00:16:51,240
我们所做的是，

264
00:16:51,240 --> 00:16:53,900
我们已经从隐变量 z 的单一向量，

265
00:16:54,340 --> 00:16:59,930
变成了均值 μ 的向量和标准差 σ 的向量，

266
00:17:00,160 --> 00:17:04,790
它将这些隐变量周围的概率分布参数化。

267
00:17:06,520 --> 00:17:08,085
这将允许我们现在做的是，

268
00:17:08,085 --> 00:17:13,010
使用这个随机性元素，这个概率元素进行采样，

269
00:17:13,510 --> 00:17:18,350
然后获得隐空间本身的概率表示。

270
00:17:19,590 --> 00:17:21,005
希望你能看出来，

271
00:17:21,005 --> 00:17:24,400
这与自编码器本身非常相似，

272
00:17:24,840 --> 00:17:27,400
但我们刚刚增加了这个概率扭曲，

273
00:17:27,720 --> 00:17:30,280
我们可以在中间空间采样，

274
00:17:30,930 --> 00:17:33,550
得到这些隐变量的样本。

275
00:17:36,020 --> 00:17:41,930
好的，现在，为了更深入地了解这是如何学习的，

276
00:17:41,930 --> 00:17:43,270
这是如何训练的，

277
00:17:44,220 --> 00:17:45,610
有了定义 VAE ，

278
00:17:46,230 --> 00:17:48,790
我们已经消除了这种确定性的性质，

279
00:17:49,380 --> 00:17:53,680
现在这些编码器和解码器都是概率的，

280
00:17:54,420 --> 00:18:00,730
编码器计算的隐变量 z 的概率分布，

281
00:18:00,990 --> 00:18:02,830
给定输入数据 x ，

282
00:18:03,680 --> 00:18:06,130
而解码器进行相反的操作，

283
00:18:06,510 --> 00:18:11,530
试图学习输入数据空间中的概率分布，

284
00:18:11,970 --> 00:18:14,290
给定隐变量 z ，

285
00:18:15,200 --> 00:18:19,350
并且我们定义了单独的权重集合 Φ 和 θ ，

286
00:18:19,430 --> 00:18:25,290
以定义用于 v 的编码器和解码器组件的网络权重。

287
00:18:27,370 --> 00:18:29,990
好的，我们现在，

288
00:18:30,070 --> 00:18:34,520
如何优化和学习 VAE 中的网络权重，

289
00:18:35,440 --> 00:18:37,710
第一步是定义损失函数，

290
00:18:37,710 --> 00:18:40,370
这是训练神经网络的核心要素，

291
00:18:41,490 --> 00:18:46,990
我们的损失将是数据的函数和神经网络权重的函数，

292
00:18:47,010 --> 00:18:47,770
就像以前一样，

293
00:18:48,730 --> 00:18:53,390
但我们有这两个组成部分，两个条目定义了我们的 VAE 损失，

294
00:18:54,270 --> 00:18:57,220
首先，我们看到重建损失，像以前一样，

295
00:18:57,840 --> 00:19:02,920
目标是捕捉我们的输入数据和重建输出之间的差异，

296
00:19:03,680 --> 00:19:07,440
现在对于 VAE ，我们引入了损失的第二个项，

297
00:19:07,970 --> 00:19:10,260
我们称之为正则化项，

298
00:19:11,210 --> 00:19:14,790
通常，你甚至可能会看到这被称为 VAE 损失，

299
00:19:15,680 --> 00:19:23,190
我们将进入描述这个正则化术语的含义和它在做什么。

300
00:19:25,420 --> 00:19:29,660
要做到这一点，并理解，记住，

301
00:19:30,100 --> 00:19:32,210
在所有神经网络操作中，

302
00:19:32,380 --> 00:19:35,900
我们的目标是尝试优化网络权重，

303
00:19:36,130 --> 00:19:41,360
相对于数据，从而将这种客观损失降至最低，

304
00:19:42,140 --> 00:19:46,250
所以这里我们关注的是网络权重 Φ 和 θ ，

305
00:19:46,250 --> 00:19:49,930
它们定义了编码器和解码器的权重。

306
00:19:51,300 --> 00:19:52,870
我们考虑这两个项目，

307
00:19:53,550 --> 00:19:55,300
首先是重建损失，

308
00:19:56,160 --> 00:20:00,190
再次，重建损失非常相似，和以前一样，

309
00:20:00,810 --> 00:20:03,730
你可以认为它是误差或可能性，

310
00:20:03,900 --> 00:20:08,920
有效地捕捉到你的输入和输出之间的差异，

311
00:20:09,240 --> 00:20:12,220
同样，我们可以以一种无监督的方式进行训练，

312
00:20:12,420 --> 00:20:16,990
不需要任何标签来迫使隐空间和网络

313
00:20:17,070 --> 00:20:20,200
学习如何有效地重建输入数据。

314
00:20:21,880 --> 00:20:23,835
第二个条目，正规化条目，

315
00:20:23,835 --> 00:20:26,145
现在是事情变得更有趣的地方，

316
00:20:26,145 --> 00:20:29,630
所以，让我们更详细地讨论这个问题。

317
00:20:31,000 --> 00:20:33,050
因为我们有这个概率分布，

318
00:20:34,390 --> 00:20:39,380
我们试图计算这个编码，然后再解码回来，

319
00:20:40,320 --> 00:20:42,730
作为正规化的一部分，

320
00:20:43,470 --> 00:20:47,680
我们希望对隐分布采取这种推断，

321
00:20:48,360 --> 00:20:51,010
并约束它表现良好，如果你愿意的话，

322
00:20:51,660 --> 00:20:53,390
我们这样做的方法是，

323
00:20:53,390 --> 00:20:56,320
我们把我们所说的先验放在隐分布上，

324
00:20:57,090 --> 00:21:03,880
这是一些关于隐变量空间可能是什么样子的初始假设或猜测，

325
00:21:04,530 --> 00:21:08,960
这有助于我们和网络实施一个隐空间，

326
00:21:08,960 --> 00:21:11,680
尝试遵循这种先前的分布，

327
00:21:13,050 --> 00:21:16,420
这个先验信息表示为 p(z) ，

328
00:21:17,470 --> 00:21:21,590
那项 D ，就是正则化项，

329
00:21:22,090 --> 00:21:26,270
它捕捉到了我们对隐变量的编码

330
00:21:26,530 --> 00:21:32,090
和我们先前关于隐空间结构应该是什么样子的假设之间的距离，

331
00:21:32,830 --> 00:21:34,275
所以，在训练过程中，

332
00:21:34,275 --> 00:21:36,270
我们努力执行这一点，

333
00:21:36,270 --> 00:21:44,240
这些隐变量中的每一个都适应一个类似于先验的概率分布。

334
00:21:46,790 --> 00:21:51,600
在训练 VAE 和开发这些模型时，一个常见的选择是，

335
00:21:51,920 --> 00:21:57,780
将隐变量强制为大致标准的正态分布，

336
00:21:58,670 --> 00:22:02,070
这意味着它们以均值零为中心，

337
00:22:02,180 --> 00:22:04,170
并且它们的标准差为一，

338
00:22:05,300 --> 00:22:06,895
这让我们能够做的是，

339
00:22:06,895 --> 00:22:13,770
鼓励编码者将隐变量大致放在一个中心空间周围，

340
00:22:14,540 --> 00:22:16,500
平稳地分布编码，

341
00:22:16,910 --> 00:22:22,380
这样我们就不会从这个平滑的空间中得到太多的偏离，

342
00:22:23,240 --> 00:22:28,620
如果网络试图作弊并试图简单地记住数据，就会发生这种情况，

343
00:22:30,240 --> 00:22:34,300
通过将高斯标准正态先验放在隐空间上，

344
00:22:34,890 --> 00:22:37,840
我们可以定义一个具体的数学术语，

345
00:22:38,190 --> 00:22:45,130
捕捉编码的隐变量与该先验之间的距离，

346
00:22:45,930 --> 00:22:48,940
这称为 KL 散度，

347
00:22:49,860 --> 00:22:52,090
当我们的前项是标准正态时，

348
00:22:52,680 --> 00:22:57,790
KL 散度采用我在屏幕上显示的方程的形式，

349
00:22:58,710 --> 00:23:03,490
但我想让你们真正[逃脱]的是，

350
00:23:03,840 --> 00:23:10,330
试着让事情变得平滑，捕捉这种差异的概念，

351
00:23:10,500 --> 00:23:13,570
以及先验编码和隐编码之间的差异，

352
00:23:13,650 --> 00:23:16,150
这就是 KL 术语试图捕捉的全部。

353
00:23:17,680 --> 00:23:21,080
所以这有点数学，我承认这一点，

354
00:23:21,310 --> 00:23:23,805
但我接下来想要探讨的是，

355
00:23:23,805 --> 00:23:27,650
这个正规化操作背后的直觉是什么，

356
00:23:28,420 --> 00:23:29,805
我们为什么要这样做，

357
00:23:29,805 --> 00:23:33,860
为什么正常的先验，特别是对 VAE 。

358
00:23:34,960 --> 00:23:39,800
所以，让我们考虑我们希望隐空间采用什么性质，

359
00:23:39,940 --> 00:23:41,990
以及为了实现这种正规化，

360
00:23:43,040 --> 00:23:45,600
首先是这个连续性的目标，

361
00:23:46,280 --> 00:23:49,890
我们没有，我们所说的连续性的意思是，

362
00:23:50,120 --> 00:23:53,340
如果在隐空间中有一些点很接近，

363
00:23:54,170 --> 00:23:56,160
理想情况下是在解码之后，

364
00:23:56,360 --> 00:24:00,060
我们应该恢复两个内容相似的重建，

365
00:24:00,440 --> 00:24:02,310
这有意义的，它们离得很近。

366
00:24:03,310 --> 00:24:06,500
第二个关键属性是这种完整性的概念，

367
00:24:07,340 --> 00:24:09,690
我们不希望在隐空间中有空隙，

368
00:24:09,980 --> 00:24:13,675
我们希望能够从隐空间解码和采样，

369
00:24:13,675 --> 00:24:16,350
以一种流畅和连接的方式。

370
00:24:17,940 --> 00:24:19,120
更具体地说，

371
00:24:20,760 --> 00:24:25,360
让我们问一问，如果根本不把我们的隐空间正规化，会有什么后果，

372
00:24:26,100 --> 00:24:28,390
如果我们不正规化，

373
00:24:28,590 --> 00:24:30,305
我们可能会得到这样的例子，

374
00:24:30,305 --> 00:24:33,130
在隐空间中有一些接近的点，

375
00:24:33,510 --> 00:24:37,960
但最终不会得到类似的解码或类似的重建，

376
00:24:38,790 --> 00:24:44,140
同样，我们可能会有一些根本不会导致有意义的重建的点，

377
00:24:44,430 --> 00:24:45,680
它们不知何故被编码了，

378
00:24:45,680 --> 00:24:47,800
但我们无法有效地解码。

379
00:24:49,440 --> 00:24:54,850
正规化使我们能够认识到最终接近隐空间的点，

380
00:24:55,320 --> 00:24:59,920
也可以进行类似的重建和有意义的重建。

381
00:25:02,460 --> 00:25:05,830
好的，继续这个例子，

382
00:25:05,970 --> 00:25:10,445
我在那里展示的例子，我没有深入到细节，

383
00:25:10,445 --> 00:25:13,300
展示了这些形状，这些不同颜色的形状，

384
00:25:13,560 --> 00:25:17,320
试图在一些低维空间中编码，

385
00:25:19,090 --> 00:25:20,300
有了正规化，

386
00:25:21,610 --> 00:25:27,660
我们能够通过尝试最小化正则化项来实现这一点，

387
00:25:27,660 --> 00:25:35,240
仅仅利用重建损失来实现这种连续性和完整性是不够的，

388
00:25:36,730 --> 00:25:39,470
因为没有正规化，

389
00:25:39,550 --> 00:25:46,400
仅仅编码和重构并不能保证连续性和完备性，

390
00:25:47,970 --> 00:25:49,210
我们克服了这一点，

391
00:25:50,650 --> 00:25:57,870
这些问题具有潜在的指向性分布，具有不连续，具有不同的平均值，

392
00:25:57,870 --> 00:26:01,820
最终可能在没有正则化影响的隐空间中结束，

393
00:26:02,880 --> 00:26:03,935
我们克服这个，

394
00:26:03,935 --> 00:26:13,420
根据正态先验正则化编码的潜在分布的均值和方差，

395
00:26:14,440 --> 00:26:21,980
这使得学习到的那些隐变量的分布在隐空间中有效地重叠，

396
00:26:22,390 --> 00:26:29,000
因为根据这个先验，所有的东西都被规则化为平均为零，标准差为一，

397
00:26:29,650 --> 00:26:31,070
这使平均值居中，

398
00:26:31,940 --> 00:26:37,570
使每个独立隐变量分布的方差正则化。

399
00:26:38,930 --> 00:26:43,150
总而言之，这种正则化在网络中的效果是，

400
00:26:43,150 --> 00:26:46,590
我们可以在隐空间中实现连续性和完备性，

401
00:26:47,480 --> 00:26:53,760
接近的点和距离应该与我们得到的类似重建相对应。

402
00:26:55,040 --> 00:26:58,560
所以希望这能得到一些直觉，

403
00:26:58,790 --> 00:27:02,820
在 VAE 后的思想，正则化的思想，

404
00:27:03,140 --> 00:27:07,500
并试图在隐空间上强制结构化的正态先验。

405
00:27:09,500 --> 00:27:10,470
有了这一点，

406
00:27:10,640 --> 00:27:12,720
随着我们损失函数的两个组件，

407
00:27:13,370 --> 00:27:19,260
重建输入正规化，学习以尝试实现连续性和完整性，

408
00:27:19,760 --> 00:27:23,880
我们现在可以考虑如何定义通过网络的前向传递，

409
00:27:24,350 --> 00:27:26,220
从输入示例开始，

410
00:27:26,780 --> 00:27:31,770
并能够从潜在变量解码和采样以查看新示例。

411
00:27:32,970 --> 00:27:34,505
我们的最后一个关键步骤是

412
00:27:34,505 --> 00:27:39,760
如何定义实际的反向传播训练算法，以及如何实现这一点，

413
00:27:41,060 --> 00:27:45,870
正如我在 VAE 中介绍的那样，关键是抽样随机性，

414
00:27:46,220 --> 00:27:52,290
我们通过定义每个隐变量的概率分布而引入的，

415
00:27:53,390 --> 00:27:55,375
这给我们带来的问题是，

416
00:27:55,375 --> 00:28:00,960
我们不能直接通过任何含有抽样元素的东西来反向传播，

417
00:28:01,280 --> 00:28:03,630
任何带有随机元素的东西，

418
00:28:05,220 --> 00:28:10,180
反向传播要求完全确定的节点，确定的层，

419
00:28:10,290 --> 00:28:16,030
能够成功地应用梯度下降和反向传播算法。

420
00:28:17,800 --> 00:28:22,845
使 VAE 能够被完全端到端训练的突破性想法是

421
00:28:22,845 --> 00:28:28,010
这个在采样层内重新参数化的想法，

422
00:28:28,860 --> 00:28:32,435
我会给你关于这个操作如何运作的关键想法，

423
00:28:32,435 --> 00:28:35,080
它是相当聪明的，

424
00:28:36,270 --> 00:28:40,720
所以，就像我说的，当我们有了概率的随机性概念时，

425
00:28:41,310 --> 00:28:44,140
我们不能直接通过这一层进行采样，

426
00:28:45,800 --> 00:28:47,610
取而代之的是，利用重新参数化，

427
00:28:47,960 --> 00:28:53,400
我们所做的是重新定义如何对隐变量向量进行采样，

428
00:28:54,040 --> 00:28:57,570
作为固定的确定性平均值 μ ，

429
00:28:58,430 --> 00:29:02,100
和固定的标准偏差 σ 的向量，

430
00:29:02,570 --> 00:29:11,010
现在的诀窍是我们把所有的随机性，所有的抽样，都转移到一个随机常数 ε 上，

431
00:29:11,480 --> 00:29:14,370
它是从正态分布中提取出来的，

432
00:29:15,470 --> 00:29:17,730
所以平均值本身是固定的，

433
00:29:18,170 --> 00:29:19,560
标准差也是固定的，

434
00:29:19,850 --> 00:29:24,510
所有的随机性和抽样都是根据这个 ε 常数进行的，

435
00:29:25,330 --> 00:29:29,990
然后，我们可以通过该随机常数来调整平均值和标准差，

436
00:29:30,490 --> 00:29:35,450
以重新实现隐变量本身内的采样操作。

437
00:29:37,050 --> 00:29:38,390
这是什么样子的，

438
00:29:38,390 --> 00:29:45,700
一个打破这种再参数化和发散概念的插图，如下所示。

439
00:29:46,550 --> 00:29:48,120
所以看这里，

440
00:29:48,350 --> 00:29:49,420
我展示的是，

441
00:29:49,420 --> 00:29:56,730
蓝色的完全确定的步骤和橙色的采样随机步骤，

442
00:29:57,650 --> 00:30:04,470
如果我们的隐变量是有效地捕捉随机性的东西，样本本身，

443
00:30:05,150 --> 00:30:06,205
我们就有这个问题，

444
00:30:06,205 --> 00:30:08,080
因为我们不能反向传播，

445
00:30:08,080 --> 00:30:13,620
我们不能直接通过任何具有随机性的东西进行训练，

446
00:30:14,680 --> 00:30:18,240
重新参数化允许我们做的是，

447
00:30:18,240 --> 00:30:19,820
它移动了这张图，

448
00:30:20,170 --> 00:30:24,980
现在我们已经完全将采样操作转移到一边，

449
00:30:25,510 --> 00:30:27,260
到这个常数 ε ，

450
00:30:27,670 --> 00:30:29,510
它是从正常的先验绘制的，

451
00:30:30,220 --> 00:30:33,050
现在当我们回过头来看我们的隐变量，

452
00:30:33,370 --> 00:30:37,370
它是关于采样操作的确定性的，

453
00:30:38,580 --> 00:30:39,695
这意味着，

454
00:30:39,695 --> 00:30:44,230
我们可以完全端到端地反向传播来更新我们的网络权重，

455
00:30:45,210 --> 00:30:51,760
而不必担心那些隐变量 z 内的直接随机性，

456
00:30:52,960 --> 00:30:54,770
这个技巧非常强大，

457
00:30:54,910 --> 00:31:03,050
因为它能够通过反向传播算法完全端到端地训练这些 VAEs 。

458
00:31:05,070 --> 00:31:10,030
好的，至此，我们已经了解了 VAE 的核心架构，

459
00:31:10,530 --> 00:31:12,730
我们已经介绍了这两个项目的损失，

460
00:31:12,780 --> 00:31:14,860
我们已经看到了如何对其进行端到端的训练。

461
00:31:15,640 --> 00:31:19,245
现在，让我们考虑一下这些隐变量实际上捕捉到了什么，

462
00:31:19,245 --> 00:31:20,060
以及它们代表了什么，

463
00:31:22,510 --> 00:31:25,490
当我们施加这种分布先验时，

464
00:31:26,290 --> 00:31:30,230
它允许我们做的是从隐空间有效地采样，

465
00:31:30,820 --> 00:31:35,870
慢慢地扰动单个隐变量的值，

466
00:31:36,460 --> 00:31:37,760
保持其他变量不变，

467
00:31:38,650 --> 00:31:40,335
你可以观察到的，

468
00:31:40,335 --> 00:31:42,285
你可以在这里看到的是，

469
00:31:42,285 --> 00:31:46,220
通过扰动，调整隐变量的值，

470
00:31:47,280 --> 00:31:50,440
我们可以每次运行 VAE 的解码器，

471
00:31:50,730 --> 00:31:53,710
每次进行调整时重建输出，

472
00:31:54,300 --> 00:31:59,060
在这个脸部的例子中，你会看到的是，

473
00:31:59,060 --> 00:32:05,680
一个单独的隐变量捕捉到了一些语义上的信息，一些有意义的东西，

474
00:32:06,480 --> 00:32:09,430
我们看到通过这个扰动，通过这个调整，

475
00:32:10,110 --> 00:32:14,450
在这个例子中，希望你能欣赏到这一点，面部正在改变，

476
00:32:14,450 --> 00:32:15,700
姿势在改变，

477
00:32:15,930 --> 00:32:20,350
所有这一切都是由单一隐变量的扰动驱动的，

478
00:32:21,280 --> 00:32:23,240
调整隐变量的值，

479
00:32:23,500 --> 00:32:26,870
看看这对解码后的重建有什么影响。

480
00:32:28,160 --> 00:32:34,620
网络实际上能够学习这些不同的编码特征，这些不同的隐变量，

481
00:32:35,210 --> 00:32:38,910
这样通过分别扰动它们的值，

482
00:32:39,500 --> 00:32:45,030
我们可以理解这些隐变量的含义和它们所代表的意义。

483
00:32:46,750 --> 00:32:48,620
为了更具体，

484
00:32:49,720 --> 00:32:53,895
我们甚至可以同时考虑多个隐变量，

485
00:32:53,895 --> 00:32:55,370
将一个与另一个进行比较，

486
00:32:55,930 --> 00:33:00,740
理想情况下，我们希望这些隐特征尽可能独立，

487
00:33:01,270 --> 00:33:07,760
以便获得最紧凑、最丰富的表示和紧凑编码。

488
00:33:08,320 --> 00:33:10,590
所以这里，在这个人脸的例子中，

489
00:33:11,030 --> 00:33:13,020
我们沿着两个轴行走，

490
00:33:13,940 --> 00:33:15,880
头部姿势在 x 轴上，

491
00:33:16,260 --> 00:33:21,070
看起来像是微笑的概念在 y 轴上。

492
00:33:21,650 --> 00:33:22,740
你可以看到，

493
00:33:22,910 --> 00:33:24,360
通过这些重建，

494
00:33:24,680 --> 00:33:26,670
我们可以扰乱这些特征，

495
00:33:27,080 --> 00:33:32,760
从而能够扰乱重建空间中的末端效应。

496
00:33:34,450 --> 00:33:37,070
所以，最终，对于 VAV ，

497
00:33:37,210 --> 00:33:43,040
我们的目标是尝试强制执行尽可能多的信息，以便在该编码中捕获，

498
00:33:43,750 --> 00:33:48,920
我们希望这些隐特征是独立的，理想情况下是分开的。

499
00:33:50,120 --> 00:33:54,085
事实证明，有一种非常聪明和简单的方法，

500
00:33:54,085 --> 00:33:58,920
来鼓励这种独立和这种分开，

501
00:33:59,980 --> 00:34:04,040
虽然这在数学上可能看起来有点复杂，有点可怕，

502
00:34:04,570 --> 00:34:06,590
但我将分解这个，

503
00:34:06,700 --> 00:34:15,260
使用一个非常简单的概念如何实施这种独立的隐编码和这种解耦的想法。

504
00:34:16,200 --> 00:34:19,450
所有这一项都是损失的两个组成部分，

505
00:34:19,920 --> 00:34:22,750
重建项，正则化项，

506
00:34:22,980 --> 00:34:24,520
这就是我想要你关注的。

507
00:34:25,700 --> 00:34:33,540
隐空间解耦的想法是从 β-VAE 的这个概念中产生的，

508
00:34:34,310 --> 00:34:36,325
β-VAE 所做的是，

509
00:34:36,325 --> 00:34:38,460
引入这个参数 β ，

510
00:34:38,960 --> 00:34:41,280
它是等待常数，

511
00:34:41,990 --> 00:34:49,470
等待常数控制正则化项在 VAE 的总体损失中有多强大，

512
00:34:50,560 --> 00:34:53,880
事实证明，通过增加 β 的值，

513
00:34:54,290 --> 00:34:59,830
你可以尝试鼓励更大的解耦，更有效的编码，

514
00:34:59,830 --> 00:35:04,440
以强制这些隐变量彼此不相关。

515
00:35:05,530 --> 00:35:12,765
现在，如果你对数学上为什么 β-VAE 强制这种解耦感兴趣，

516
00:35:12,765 --> 00:35:15,690
有很多文献和证明，

517
00:35:15,690 --> 00:35:17,510
以及关于为什么会发生这种情况的讨论，

518
00:35:17,740 --> 00:35:19,790
我们可以向你指出这些方向，

519
00:35:20,500 --> 00:35:23,930
但为了了解这会对下游产生什么影响，

520
00:35:24,430 --> 00:35:28,490
当我们将人脸重建视为一项感兴趣的任务时，

521
00:35:29,020 --> 00:35:33,470
我们使用标准的 VAE ，没有 β 项，或者更确切地说， β 项为 1 ，

522
00:35:34,730 --> 00:35:36,450
希望你能理解，

523
00:35:37,070 --> 00:35:40,020
头部的旋转，

524
00:35:40,190 --> 00:35:42,600
姿势和头部的旋转，

525
00:35:42,770 --> 00:35:50,460
也与微笑和面部，嘴部的表情相关，

526
00:35:51,110 --> 00:35:53,370
因为，随着头部姿势的改变，

527
00:35:53,750 --> 00:35:57,330
明显的微笑或嘴巴的位置也在改变，

528
00:35:58,570 --> 00:36:03,320
但通过 β-VAE ，我们可以观察到，

529
00:36:03,880 --> 00:36:07,610
通过施加比 1 大得多的 β 值，

530
00:36:08,020 --> 00:36:11,090
我们可以尝试实施更大的解耦，

531
00:36:11,410 --> 00:36:15,440
现在我们只能考虑一个隐变量头部姿势，

532
00:36:15,850 --> 00:36:21,020
这些图像中的微笑和嘴巴的位置更恒定，

533
00:36:21,700 --> 00:36:23,930
与标准的的 VAE 相比。

534
00:36:26,210 --> 00:36:31,410
好的，这就是所有的核心数学，核心运算，

535
00:36:31,520 --> 00:36:33,520
VAE 的核心架构，

536
00:36:33,520 --> 00:36:37,500
我们将在今天的课程中讨论。

537
00:36:38,750 --> 00:36:42,235
在这节课结束之前。作为最后一个注解，

538
00:36:42,235 --> 00:36:45,000
我想提醒大家回到鼓舞人心的例子，

539
00:36:45,440 --> 00:36:49,050
我在本课开始时介绍的，面部检测。

540
00:36:49,750 --> 00:36:55,160
现在，希望你已经理解了隐变量学习和编码的概念，

541
00:36:55,300 --> 00:36:59,060
以及这对于面部检测这样的任务是多么有用，

542
00:36:59,380 --> 00:37:04,610
我们可能想要了解数据中隐特征的分布，

543
00:37:05,320 --> 00:37:09,800
实际上，你将在软件实验中亲身实践，

544
00:37:10,630 --> 00:37:13,245
建立变化自编码器，

545
00:37:13,245 --> 00:37:18,820
可以自动发现面部检测数据集的隐特征，

546
00:37:19,170 --> 00:37:23,980
并使用这一点来了解潜在和隐藏的偏见，

547
00:37:24,120 --> 00:37:26,920
可能来自这些数据和模型。

548
00:37:27,890 --> 00:37:29,280
而且它并不只是止步于此，

549
00:37:29,510 --> 00:37:32,490
明天我们将有一个非常令人兴奋的客座讲座，

550
00:37:33,170 --> 00:37:35,520
关于稳健和值得信赖的深度学习，

551
00:37:35,870 --> 00:37:38,040
它将使这个概念更进一步，

552
00:37:38,540 --> 00:37:43,530
认识到我们如何使用生成模型和隐变量学习的想法，

553
00:37:43,910 --> 00:37:47,100
不仅发现和诊断偏见，

554
00:37:47,420 --> 00:37:52,350
而且实际上解决和减轻这些偏见的一些有害影响，

555
00:37:52,520 --> 00:37:56,310
在用于面部检测和其他应用的神经网络中。

556
00:37:57,830 --> 00:38:01,680
好的，简单总结一下 VAE 的要点，

557
00:38:02,240 --> 00:38:08,400
我们已经了解了它们是如何将数据压缩到这个紧凑的编码表示中的，

558
00:38:09,140 --> 00:38:10,435
从这个表示中，

559
00:38:10,435 --> 00:38:15,240
我们可以以完全无监督的方式生成输入的重构，

560
00:38:17,160 --> 00:38:21,850
我们可以使用重新参数化技巧对它们进行端到端的训练，

561
00:38:22,680 --> 00:38:29,740
我们可以通过扰动个体隐变量的值来理解它们的语义解释，

562
00:38:30,630 --> 00:38:34,210
最后，我们可以从潜在空间中进行采样，

563
00:38:34,230 --> 00:38:38,680
通过解码器向上传递来生成新的示例。

564
00:38:40,050 --> 00:38:46,570
所以 VAE 把隐变量编码和密度估计作为他们的核心问题，

565
00:38:47,550 --> 00:38:51,665
如果现在我们只关注生成样本的质量，

566
00:38:51,665 --> 00:38:54,640
这是我们更关心的任务。

567
00:38:55,690 --> 00:38:58,910
所以，我们将过渡到一种新型的生成性模型，

568
00:38:59,470 --> 00:39:02,690
称为生成性对抗网络，或称 GAN ，

569
00:39:04,040 --> 00:39:06,475
对于 GAN，我们的目标是，

570
00:39:06,475 --> 00:39:12,900
更多地关心如何生成与现有数据相似的新实例，

571
00:39:13,610 --> 00:39:18,930
这意味着我们希望尝试从潜在非常复杂的分布中进行采样，

572
00:39:19,610 --> 00:39:22,140
模型试图近似的（分布），

573
00:39:23,370 --> 00:39:27,670
直接学习这种分布可能非常困难，

574
00:39:27,840 --> 00:39:30,250
因为它很复杂，维度很高，

575
00:39:30,450 --> 00:39:33,790
我们希望能够绕过这种复杂性，

576
00:39:34,510 --> 00:39:36,320
GAN 所做的是，他们会说，

577
00:39:36,340 --> 00:39:39,675
好的，如果我们从超级简单的东西开始，

578
00:39:39,675 --> 00:39:43,190
就像它可以得到完全随机的噪音一样简单，

579
00:39:43,900 --> 00:39:46,160
我们能否建立一种神经网络体系结构，

580
00:39:46,930 --> 00:39:52,790
能够学习从完全随机的噪音中生成合成样本。

581
00:39:54,210 --> 00:39:57,070
这就是 GAN 的基本概念，

582
00:39:57,810 --> 00:40:01,060
目标是训练这个生成器网络，

583
00:40:01,500 --> 00:40:06,970
学习从噪声到训练数据分布的转换，

584
00:40:07,840 --> 00:40:13,290
目标是使生成的示例尽可能接近真实的[] 。

585
00:40:14,910 --> 00:40:18,370
对于 GAN ，这里的突破性想法是，

586
00:40:18,630 --> 00:40:23,260
将这两个神经网络连接在一起，

587
00:40:24,060 --> 00:40:27,850
一个是生成器，一个是判别器，

588
00:40:28,480 --> 00:40:31,850
而这两个组成部分，生成器和判别器，

589
00:40:32,140 --> 00:40:34,730
处于战争之中，相互竞争，

590
00:40:35,650 --> 00:40:39,195
具体地说，生成器网络的目标是，

591
00:40:39,195 --> 00:40:40,700
观察随机噪声，

592
00:40:41,170 --> 00:40:45,920
并试图产生尽可能接近真实数据的模拟数据，

593
00:40:47,100 --> 00:40:53,530
然后，判别器获取生成器的输出以及一些真实的数据示例，

594
00:40:54,090 --> 00:40:57,670
并尝试学习分类决策，

595
00:40:58,750 --> 00:41:00,740
辨别真假，

596
00:41:01,480 --> 00:41:02,965
在 GAN 中，

597
00:41:02,965 --> 00:41:06,810
这两个部分相互竞争，

598
00:41:07,520 --> 00:41:12,600
试图迫使判别器更好地区分真假，

599
00:41:13,250 --> 00:41:21,060
而生成器试图愚弄和超越判别器进行分类的能力。

600
00:41:22,780 --> 00:41:25,490
所以这就是最重要的概念，

601
00:41:26,080 --> 00:41:29,900
但真正让我兴奋的是下一个例子，

602
00:41:29,920 --> 00:41:34,850
这是我在这门课上最喜欢的插图和演练之一，

603
00:41:35,230 --> 00:41:37,910
它得到了 GAN 背后的直觉，

604
00:41:38,230 --> 00:41:40,280
它们是如何工作的，以及潜在的概念。

605
00:41:42,800 --> 00:41:44,760
我们来看一个一维例子，

606
00:41:44,960 --> 00:41:46,240
直线上的点，

607
00:41:46,240 --> 00:41:48,180
这就是我们处理的数据，

608
00:41:48,760 --> 00:41:52,070
同样，生成器从随机噪声开始，

609
00:41:52,390 --> 00:41:53,715
产生一些虚假数据，

610
00:41:53,715 --> 00:41:56,750
它们会落在这条一维线上的某个地方。

611
00:41:57,870 --> 00:42:01,930
现在下一步是判别器看到这些点，

612
00:42:02,920 --> 00:42:05,420
它还看到了一些真实的数据，

613
00:42:06,460 --> 00:42:11,450
判别器的目标是训练它输出概率，

614
00:42:11,770 --> 00:42:15,890
它所看到的实例是真的还是假的。

615
00:42:17,130 --> 00:42:19,630
最初，在训练之前，

616
00:42:20,070 --> 00:42:21,340
它没有得到训练，

617
00:42:21,600 --> 00:42:23,650
所以，它的预测可能不是很好，

618
00:42:24,240 --> 00:42:25,955
但在训练的过程中，

619
00:42:25,955 --> 00:42:27,100
你将训练它，

620
00:42:27,480 --> 00:42:33,370
它有望开始增加那些真实例子的概率，

621
00:42:33,870 --> 00:42:37,990
减少那些虚假例子的可能性，

622
00:42:38,750 --> 00:42:41,610
总体目标是预测什么是真实的，

623
00:42:42,600 --> 00:42:46,025
直到最终判别器达到这一点，

624
00:42:46,025 --> 00:42:51,880
它有一个完美的分离，完美的真假分类。

625
00:42:53,040 --> 00:42:55,600
好的，在这一点上，判别器认为，

626
00:42:55,680 --> 00:42:56,830
好的，我已经完成了我的工作，

627
00:42:57,330 --> 00:42:59,560
现在我们回到生成器，

628
00:43:00,180 --> 00:43:03,070
它会看到真实数据所在的示例，

629
00:43:04,140 --> 00:43:08,510
而且，它可能会被迫开始将其生成的虚假数据，

630
00:43:09,100 --> 00:43:12,980
越来越接近真实数据，

631
00:43:14,310 --> 00:43:16,295
然后我们可以回到判别器，

632
00:43:16,295 --> 00:43:20,110
它从生成器接收这些新合成的示例，

633
00:43:20,670 --> 00:43:26,980
并重复相同的过程来估计任何给定点为真实的概率，

634
00:43:28,220 --> 00:43:32,250
学习增加真实例的概率，

635
00:43:32,990 --> 00:43:35,250
减少假点的概率，

636
00:43:36,580 --> 00:43:39,230
适应在它的训练过程中。

637
00:43:40,530 --> 00:43:45,130
最后，我们可以回到生成器上，最后一次重复，

638
00:43:45,390 --> 00:43:51,540
生成器开始将这些假点移动得越来越接近真实数据，

639
00:43:52,070 --> 00:43:56,460
以至于假数据几乎都在跟随真实数据的分布。

640
00:43:58,180 --> 00:44:03,765
在这一点上，判别器很难区分，

641
00:44:03,765 --> 00:44:06,080
什么是真的，什么是假的，

642
00:44:06,800 --> 00:44:12,670
生成器将继续尝试创建虚假数据点来愚弄判别器，

643
00:44:14,000 --> 00:44:15,750
这确实是关键概念，

644
00:44:15,800 --> 00:44:21,870
GAN 组件如何本质上相互竞争的基本直觉，

645
00:44:22,220 --> 00:44:26,610
在生成器和判别器之间来回移动，

646
00:44:27,690 --> 00:44:33,040
事实上，这就是 GAN 是如何在实践中被训练的直观概念，

647
00:44:33,870 --> 00:44:38,080
生成器首先试图合成新的例子，

648
00:44:38,370 --> 00:44:41,200
合成例子来愚弄判别器，

649
00:44:41,730 --> 00:44:47,140
而判别器的目标是同时获取虚假实例和真实数据，

650
00:44:47,340 --> 00:44:50,440
以尝试识别合成实例。

651
00:44:51,980 --> 00:44:55,260
在训练中，这意味着目标是，

652
00:44:56,240 --> 00:45:01,000
生成器和判别器的损失必须是彼此不一致的，

653
00:45:01,260 --> 00:45:02,500
它们是对抗性的，

654
00:45:02,550 --> 00:45:08,620
这就是生成对抗性网络中对抗性成分的原因，

655
00:45:09,960 --> 00:45:14,860
然后将这些对抗性目标放在一起，

656
00:45:14,970 --> 00:45:19,210
以定义达到稳定的全局最优意味着什么，

657
00:45:19,680 --> 00:45:24,730
在这种情况下，生成器能够产生真实数据分布，

658
00:45:25,200 --> 00:45:27,730
将会完全欺骗判别器，

659
00:45:30,440 --> 00:45:34,740
具体地说，这可以在数学上定义为损失目标，

660
00:45:35,510 --> 00:45:38,850
再次，虽然我在展示数学，

661
00:45:39,110 --> 00:45:40,990
我们可以把这些提炼出来，

662
00:45:40,990 --> 00:45:47,910
通过每个术语所反映的核心直观概念和概念概念，

663
00:45:48,110 --> 00:45:51,960
希望这个一维例子能传达出来。

664
00:45:53,140 --> 00:45:57,440
所以我们首先考虑判别者的观点 D ，

665
00:45:58,210 --> 00:46:01,700
它的目标是最大化它的决定的概率，

666
00:46:02,920 --> 00:46:03,795
在它的决定中，

667
00:46:03,795 --> 00:46:08,510
真实的数据被归类为真实的，假数据被归类为假的，

668
00:46:09,330 --> 00:46:14,950
所以，这里的第一项 G(z) 是生成器的输出，

669
00:46:15,420 --> 00:46:23,110
D(G(z)) 是判别器对生成器的输出为假的估计，

670
00:46:25,200 --> 00:46:27,550
D(x) ， x 是真实的数据，

671
00:46:27,810 --> 00:46:33,040
所以 D(x) 是真实实例为假的概率的估计，

672
00:46:33,780 --> 00:46:38,570
1-D(x) 是真实实例是真实的估计，

673
00:46:39,580 --> 00:46:41,955
因此，在这两种情况下，

674
00:46:41,955 --> 00:46:46,910
判别器都在做出关于虚假数据和真实数据的决定，

675
00:46:47,200 --> 00:46:53,810
它想要一起努力最大化它得到正确答案的可能性。

676
00:46:56,360 --> 00:46:59,730
现在，对于生成器，我们有相同的条目，

677
00:47:00,320 --> 00:47:04,580
但请记住，生成器永远不能影响，

678
00:47:04,720 --> 00:47:08,600
判别器决策做的任何事情，

679
00:47:09,160 --> 00:47:11,840
除了生成新的数据示例之外。

680
00:47:12,900 --> 00:47:15,830
因此，对于生成器来说，其目标只是，

681
00:47:16,570 --> 00:47:22,460
将生成的数据被识别为虚假数据的概率降至最低。

682
00:47:24,800 --> 00:47:28,200
总之，我们想把这些放在一起，

683
00:47:28,220 --> 00:47:32,970
定义生成器合成假图像意味着什么，

684
00:47:33,320 --> 00:47:35,580
希望能愚弄判别者。

685
00:47:37,030 --> 00:47:39,260
总而言之，这个项目，

686
00:47:40,030 --> 00:47:43,370
除了数学，除了这个定义的特殊性，

687
00:47:44,140 --> 00:47:48,110
我想让你们从这个关于 GAN 的这一节中得到的，

688
00:47:48,790 --> 00:47:52,340
是我们有这个双重竞争的目标，

689
00:47:52,990 --> 00:47:57,770
生成器试图合成这些合成例子，

690
00:47:58,270 --> 00:48:02,330
尽可能地愚弄最好的判别者，

691
00:48:03,100 --> 00:48:06,560
在这样做的过程中，目标是建立一个网络，

692
00:48:07,810 --> 00:48:11,240
通过这种对抗性训练，这种对抗性竞争，

693
00:48:11,740 --> 00:48:14,630
使用生成器来创建新数据，

694
00:48:14,740 --> 00:48:20,810
最能模拟真实数据分布及其完全合成的新实例。

695
00:48:23,120 --> 00:48:27,150
实际上，在培训过程之后，

696
00:48:27,230 --> 00:48:30,270
你可以只查看生成器组件，

697
00:48:30,860 --> 00:48:34,830
然后使用它来创建新的数据实例。

698
00:48:37,220 --> 00:48:40,920
所有这些都是从随机噪声开始，

699
00:48:41,390 --> 00:48:47,130
并试图学习从随机噪声到真实数据分布的模型。

700
00:48:47,990 --> 00:48:51,120
实际上， GAN 在做的是学习一种函数，

701
00:48:51,530 --> 00:48:56,250
将随机噪声的分布转化为某个目标，

702
00:48:57,450 --> 00:48:59,735
这种映射的作用是，

703
00:48:59,735 --> 00:49:05,080
它允许我们对噪声空间中的噪声进行特定的观察，

704
00:49:05,760 --> 00:49:10,840
并将其映射到某些输出，目标数据空间中的特定输出，

705
00:49:12,130 --> 00:49:15,800
反过来，如果我们考虑一些其他随机的噪声样本，

706
00:49:16,210 --> 00:49:18,380
如果我们把它送入 GAN 的生成器，

707
00:49:18,580 --> 00:49:21,110
它将产生一个全新的实例，

708
00:49:21,730 --> 00:49:25,670
落在真正的数据分布流形上的其他地方，

709
00:49:26,380 --> 00:49:29,210
事实上，我们实际上可以做的是，

710
00:49:29,650 --> 00:49:34,010
在噪声空间中的轨迹之间进行内插和遍历，

711
00:49:34,540 --> 00:49:41,540
然后映射到目标数据空间中的遍历和内插，

712
00:49:42,240 --> 00:49:43,785
这真的很酷，

713
00:49:43,785 --> 00:49:47,780
因为现在你可以考虑一个起始点和一个目标点，

714
00:49:48,220 --> 00:49:50,390
所以步骤将会让你，

715
00:49:50,770 --> 00:49:55,730
在目标数据分布中合成和处理这些图像。

716
00:49:57,740 --> 00:50:06,510
希望这能让我们对用于创建新数据实例的生成性建模的概念有一个了解。

717
00:50:07,200 --> 00:50:10,910
而这种插补和数据转换的概念

718
00:50:11,800 --> 00:50:16,430
很好地引领了 GAN 最近的一些进步和应用，

719
00:50:17,400 --> 00:50:21,065
其中一个特别常用的想法是，

720
00:50:21,065 --> 00:50:27,130
尝试再次迭代 GAN 以获得更多更详细的图像生成，

721
00:50:28,080 --> 00:50:31,450
在训练过程中逐步添加层，

722
00:50:31,620 --> 00:50:35,770
然后改进由生成器生成的示例。

723
00:50:37,020 --> 00:50:43,720
这就是用来生成那些合成脸部的图像的方法，

724
00:50:43,860 --> 00:50:45,880
我在这节课的开头展示的图像，

725
00:50:46,380 --> 00:50:52,810
使用迭代细化的 GAN 来产生更高分辨率的图像。

726
00:50:55,100 --> 00:50:57,900
我们可以扩展这一概念的另一种方法是，

727
00:50:57,950 --> 00:51:02,850
扩展 GAN 架构以考虑特定任务，

728
00:51:03,320 --> 00:51:06,600
并将进一步的结构强加于网络本身，

729
00:51:07,490 --> 00:51:09,480
一个特别的想法是，

730
00:51:09,560 --> 00:51:13,830
好的，如果我们有一个特定的标签或一些因素，

731
00:51:13,850 --> 00:51:16,740
我们想要以此为条件来产生，

732
00:51:17,820 --> 00:51:18,820
我们称为 c ，

733
00:51:18,960 --> 00:51:22,330
它同时提供给生成器和判别器，

734
00:51:23,590 --> 00:51:30,470
这将使我们能够实现不同类型数据之间的配对转换，

735
00:51:30,820 --> 00:51:35,660
例如，我们可以有街景的图像，

736
00:51:35,800 --> 00:51:39,650
我们可以有街景的分割图像，

737
00:51:40,120 --> 00:51:46,460
我们可以重新构建可以在街景图像和分割之间直接转换。

738
00:51:47,300 --> 00:51:50,760
让我们通过考虑一些特定的例子来使这一点更加具体，

739
00:51:51,680 --> 00:51:57,300
所以我刚才描述的就是从一个分割标签变成一个街头场景，

740
00:51:58,040 --> 00:52:02,700
我们还可以将卫星视图，航空卫星图像转换为

741
00:52:03,140 --> 00:52:08,400
相当于该航空卫星图像的路线图，

742
00:52:09,170 --> 00:52:14,100
或将建筑物图像的特定注释或标签转换为

743
00:52:14,390 --> 00:52:19,470
建筑物的实际视觉实现和视觉立面，

744
00:52:20,280 --> 00:52:23,980
我们可以在不同的照明条件之间进行转换，白天到晚上，

745
00:52:25,010 --> 00:52:26,310
从黑白到彩色，

746
00:52:26,690 --> 00:52:30,510
轮廓再到彩色照片。

747
00:52:31,440 --> 00:52:32,320
所有这些案例，

748
00:52:32,790 --> 00:52:37,580
特别是对我来说最有趣和最有影响力的是，

749
00:52:37,580 --> 00:52:40,660
街景和鸟瞰之间的转换，

750
00:52:41,010 --> 00:52:44,075
这是用来考虑，

751
00:52:44,075 --> 00:52:47,140
例如，如果你有来自谷歌地图的数据，

752
00:52:47,400 --> 00:52:53,290
你如何在地图的街景和航拍图像之间移动。

753
00:52:55,250 --> 00:53:02,340
最后，再次扩展一个域到另一个域转换的概念，

754
00:53:03,200 --> 00:53:06,960
另一个想法是完全不成对的转换，

755
00:53:07,730 --> 00:53:11,610
这使用了一种特定 GAN 体系结构，称为 CycleGAN 。

756
00:53:12,650 --> 00:53:14,700
在我在这里展示的这段视频中，

757
00:53:14,960 --> 00:53:19,050
模型将一个域中的一组图像作为输入，

758
00:53:19,700 --> 00:53:24,180
它并不一定要在另一个目标域中有对应的图像，

759
00:53:24,920 --> 00:53:33,690
但它被训练成试图在该目标域中生成大致对应于源域的示例，

760
00:53:34,340 --> 00:53:39,030
将源的样式转移到目标上，反之亦然。

761
00:53:39,620 --> 00:53:46,950
所以这个例子展示了从马域到斑马域的图像转换，

762
00:53:48,020 --> 00:53:51,600
这里的概念是循环依赖，

763
00:53:51,710 --> 00:53:56,760
你有两个 GAN 通过这个循环损失连接在一起，

764
00:53:57,470 --> 00:54:00,000
在一个域和另一个域之间转换。

765
00:54:01,340 --> 00:54:05,400
像我们到目前为止在这节课上看到的所有例子一样，

766
00:54:05,840 --> 00:54:08,730
直觉是分布变换的想法，

767
00:54:09,950 --> 00:54:13,110
通常情况下，在 GAN 的作用下，你会在循环中从噪声转向某个目标，

768
00:54:13,820 --> 00:54:18,330
在 CycleGAN 下，你试着从某些源分布，

769
00:54:18,830 --> 00:54:24,540
从某些数据流形 X 到目标分布另一个数据流形 Y ，

770
00:54:25,420 --> 00:54:28,160
这真的不仅很酷，

771
00:54:28,210 --> 00:54:35,000
而且在思考我们如何灵活地转换这些不同的分布时也很强大。

772
00:54:35,850 --> 00:54:40,780
事实上，这让我们不仅可以对图像进行转换，

773
00:54:41,280 --> 00:54:43,390
还可以对语音和音频进行转换，

774
00:54:44,210 --> 00:54:46,080
所以在语音和音频的情况下，

775
00:54:46,610 --> 00:54:53,000
事实证明你可以利用声波，使用一个压缩语谱图表示，

776
00:54:53,500 --> 00:54:54,950
然后使用 CycleGAN ，

777
00:54:55,180 --> 00:55:05,460
将语音从一个域中的一个人的声音转换到另一个域中的另一个人的声音，

778
00:55:05,460 --> 00:55:08,780
这是我们定义的两个独立的数据分布。

779
00:55:09,700 --> 00:55:12,530
也许你已经明白我在暗示什么，也许不是，

780
00:55:12,940 --> 00:55:21,140
但这正是我们开发模型来合成奥巴马声音背后的音频的方式，

781
00:55:21,430 --> 00:55:24,260
我们在昨天的入门课中看到的，

782
00:55:25,210 --> 00:55:26,130
我们所做的是，

783
00:55:26,130 --> 00:55:27,800
我们训练一个 CycleGAN ，

784
00:55:28,210 --> 00:55:30,890
获取 Alexander 声音中的数据，

785
00:55:31,480 --> 00:55:36,380
并将其转换为奥巴马声音中的数据，

786
00:55:37,520 --> 00:55:44,340
所以我们可以直观地看到 Alexander 的声音和奥巴马的声音的频谱图波形是什么样的，

787
00:55:44,450 --> 00:55:48,660
这是完全使用 CycleGAN 方法合成的。

788
00:55:59,130 --> 00:56:00,760
在 MIT 。

789
00:56:03,260 --> 00:56:05,670
我重放了一遍，

790
00:56:05,810 --> 00:56:08,370
好的，但基本上我们所做的就是，

791
00:56:08,540 --> 00:56:12,600
Alexander 说了昨天播放的那个短语，

792
00:56:13,430 --> 00:56:15,480
我们有训练 CycleGAN 模型，

793
00:56:15,950 --> 00:56:18,420
然后我们可以把它部署到那个精确的音频上，

794
00:56:18,830 --> 00:56:23,970
把它从 Alexander 的声音域转换到奥巴马的声音域，

795
00:56:24,600 --> 00:56:28,550
生成为该视频剪辑播放的合成音频。

796
00:56:32,260 --> 00:56:35,640
好的，在我不小心再次播放之前，

797
00:56:35,640 --> 00:56:38,590
我跳到总结幻灯片。

798
00:56:39,240 --> 00:56:40,660
今天，在这节课中，

799
00:56:40,680 --> 00:56:42,820
我们学习了深入生成模型，

800
00:56:42,900 --> 00:56:49,540
主要讨论了隐变量模型，自编码器，变分自编码器，

801
00:56:49,680 --> 00:56:54,040
我们的目标是学习数据的低维潜在编码，

802
00:56:54,780 --> 00:56:57,580
以及生成对抗网络，

803
00:56:57,930 --> 00:57:01,960
我们有这些相互竞争的生成器和判别器组件，

804
00:57:02,370 --> 00:57:05,770
试图合成合成例子。

805
00:57:07,510 --> 00:57:10,640
我们已经讨论了这些核心的基本生成方法，

806
00:57:11,500 --> 00:57:15,440
但事实证明，就像我在讲座开始时提到的那样，

807
00:57:16,270 --> 00:57:18,470
特别是在过去的一年里，

808
00:57:18,700 --> 00:57:22,820
我们看到了生成建模的真正的巨大进步，

809
00:57:23,380 --> 00:57:26,990
其中许多并不是来自这两种方法，

810
00:57:27,400 --> 00:57:29,780
我们描述的两种基本方法，

811
00:57:30,500 --> 00:57:33,780
而是一种名为扩散模型的新方法。

812
00:57:34,700 --> 00:57:42,360
扩散模型是推动生成性人工智能巨大进步的驱动工具，

813
00:57:42,500 --> 00:57:44,850
特别是在过去的一年里，

814
00:57:46,130 --> 00:57:50,460
这些 GAN ，它们正在学习这些转换，这些编码，

815
00:57:50,750 --> 00:57:59,790
但它们在很大程度上被限制在生成类似于他们以前见过的数据空间的例子，

816
00:58:01,270 --> 00:58:03,320
扩散模型现在有这种能力，

817
00:58:03,340 --> 00:58:09,200
可以幻觉、想象和想象全新的物体和实例，

818
00:58:09,340 --> 00:58:13,370
作为人类，我们可能没有见过，甚至没有想过，

819
00:58:14,050 --> 00:58:18,260
设计空间的部分没有没有被训练数据覆盖。

820
00:58:18,940 --> 00:58:21,920
这里的一个例子是人工智能生成的艺术，

821
00:58:22,240 --> 00:58:23,750
如果你愿意的话，它是艺术，

822
00:58:25,000 --> 00:58:27,140
它是由扩散模型创建的，

823
00:58:27,700 --> 00:58:35,010
我认为这不仅是为了达到这些强大模型的一些限制和功能，

824
00:58:35,390 --> 00:58:41,100
但也有关于创建新实例意味着什么的问题，

825
00:58:41,360 --> 00:58:44,100
这些模型的极限和界限是什么，

826
00:58:44,480 --> 00:58:45,540
它们如何，

827
00:58:45,920 --> 00:58:51,120
我们如何看待它们在人类能力和人类智力方面的进步。

828
00:58:52,540 --> 00:58:54,465
所以，我非常兴奋，

829
00:58:54,465 --> 00:58:58,460
在周四的第七讲，深度学习的新领域，

830
00:58:58,810 --> 00:59:01,700
我们将深入研究扩散模型，

831
00:59:02,110 --> 00:59:03,650
讨论它们的基本原理，

832
00:59:04,030 --> 00:59:06,410
不仅讨论图像的应用，

833
00:59:06,820 --> 00:59:08,240
也讨论其他领域，

834
00:59:08,470 --> 00:59:13,070
我们看到这些模型真正开始取得变革性的进展，

835
00:59:13,570 --> 00:59:19,370
因为它们确实处于当今生成性人工智能的前沿和新前沿。

836
00:59:20,470 --> 00:59:22,070
好的，有了这些，

837
00:59:23,180 --> 00:59:28,530
梳理并希望为周四的第七讲做好准备，

838
00:59:29,300 --> 00:59:33,370
结束并提醒你们，

839
00:59:33,370 --> 00:59:37,075
我们现在大约有一个小时的开放办公时间，

840
00:59:37,075 --> 00:59:39,840
是你在软件实验工作的时间，

841
00:59:39,980 --> 00:59:42,870
请到我们这里来，提出任何你可能有的问题，

842
00:59:43,730 --> 00:59:46,530
以及助教们也在这里。

843
00:59:47,030 --> 00:59:47,880
非常感谢。

