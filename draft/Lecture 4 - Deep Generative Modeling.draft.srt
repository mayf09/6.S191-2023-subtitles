1
00:00:09,460 --> 00:00:11,025
0,380 380,560 560,860 860,1235 1235,1565
I'm really, really excited about
我对这次演讲非常兴奋，

2
00:00:11,025 --> 00:00:13,040
0,300 300,665 805,1170 1170,1535 1615,2015
this lecture,| because as Alexander
|因为正如 Alexander 昨天介绍的那样，

3
00:00:13,150 --> 00:00:16,380
0,400 900,1300 2430,2735 2735,2990 2990,3230
introduced yesterday,| right now we're
|现在我们正处于一个生成性人工智能的巨大时代，

4
00:00:16,380 --> 00:00:17,955
0,90 90,300 300,650 820,1215 1215,1575
in this tremendous age of

5
00:00:17,955 --> 00:00:20,310
0,645 645,1025 1465,1770 1770,2055 2055,2355
generative AI,| and today we're
|今天我们将学习深度生成性建模的基础，

6
00:00:20,310 --> 00:00:21,650
0,135 135,330 330,600 600,840 840,1340
going to learn the foundations

7
00:00:21,850 --> 00:00:24,180
0,380 380,710 710,1220 1220,1780 2040,2330
of deep generative modeling,| where
|在那里我们将讨论建立系统，

8
00:00:24,180 --> 00:00:25,290
0,225 225,375 375,555 555,810 810,1110
we're going to talk about

9
00:00:25,290 --> 00:00:27,210
0,315 315,710 1300,1590 1590,1740 1740,1920
building systems,| that can not
|不仅可以在数据中寻找模式，

10
00:00:27,210 --> 00:00:28,890
0,315 315,660 660,975 975,1340 1390,1680
only look for patterns in

11
00:00:28,890 --> 00:00:30,630
0,290 820,1095 1095,1335 1335,1590 1590,1740
data,| but can actually go
|而且实际上可以更进一步，

12
00:00:30,630 --> 00:00:32,355
0,150 150,405 405,720 720,1070 1420,1725
a step beyond this| to
|基于这些学习的模式生成全新的数据实例，

13
00:00:32,355 --> 00:00:34,970
0,305 475,855 855,1235 1375,1770 1770,2615
generate brand new data instances

14
00:00:35,080 --> 00:00:36,530
0,380 380,650 650,845 845,1100 1100,1450
based on those learned patterns,|
|

15
00:00:37,690 --> 00:00:39,645
0,350 350,560 560,820 960,1360 1590,1955
this is an incredibly complex
这是一个令人难以置信的复杂和强大的想法，

16
00:00:39,645 --> 00:00:41,220
0,270 270,575 835,1185 1185,1425 1425,1575
and powerful idea,| and as
|就像我提到的，

17
00:00:41,220 --> 00:00:42,440
0,135 135,390 390,675 675,855 855,1220
I mentioned,| it's a particular
|这是深度学习的一个特殊子集，

18
00:00:42,490 --> 00:00:44,295
0,470 470,620 620,815 815,1120 1500,1805
subset of deep learning,| that
|在过去几年里真的发生了爆炸式的增长，特别是今年。

19
00:00:44,295 --> 00:00:47,040
0,305 685,1050 1050,1415 1615,2255 2485,2745
has actually really exploded in

20
00:00:47,040 --> 00:00:48,075
0,150 150,390 390,645 645,825 825,1035
the past couple of years

21
00:00:48,075 --> 00:00:49,430
0,240 240,465 465,720 720,1005 1005,1355
and this year in particular.|
|

22
00:00:50,540 --> 00:00:52,350
0,400 750,1040 1040,1280 1280,1520 1520,1810
So to start, and to
所以，为了演示这些算法的强大功能，

23
00:00:52,430 --> 00:00:54,250
0,350 350,650 650,995 995,1385 1385,1820
demonstrate how powerful these algorithms

24
00:00:54,250 --> 00:00:56,130
0,320 880,1155 1155,1350 1350,1575 1575,1880
are,| let me show you
|让我向你们展示一下这三张不同的脸，

25
00:00:57,290 --> 00:01:00,300
0,400 720,1120 1320,1700 1700,2080
these three different faces,|
|

26
00:01:01,010 --> 00:01:01,870
0,275 275,425 425,575 575,725 725,860
I want you to take
我想让你想一想，

27
00:01:01,870 --> 00:01:04,400
0,105 105,350 790,1190
a minute think,|
|

28
00:01:04,400 --> 00:01:05,780
0,210 210,450 450,720 720,1070 1090,1380
think about which face you
想想你认为哪张脸是真的，

29
00:01:05,780 --> 00:01:07,420
0,225 225,495 495,830
think is real,|
|

30
00:01:07,420 --> 00:01:08,365
0,240 240,405 405,615 615,825 825,945
raise your hand, if you
如果你认为是脸 A ，请举手，

31
00:01:08,365 --> 00:01:09,210
0,120 120,210 210,300 300,510 510,845
think {it's -} face A,|
|

32
00:01:10,730 --> 00:01:11,725
0,380 380,590 590,680 680,830 830,995
okay, see a couple of
好的，看到几个人，

33
00:01:11,725 --> 00:01:14,080
0,275 835,1185 1185,1535
people,| Face B,|
|脸 B ，|

34
00:01:15,340 --> 00:01:17,060
0,305 305,530 530,850
many more people,|
更多的人，|

35
00:01:17,320 --> 00:01:18,860
0,350 350,700
face C,|
脸 C ，|

36
00:01:21,060 --> 00:01:23,345
0,365 365,730 1560,1865 1865,2075 2075,2285
second place,| well, the truth
第二名，|好的，事实是你们都错了，

37
00:01:23,345 --> 00:01:24,905
0,285 285,665 865,1140 1140,1290 1290,1560
is that all of you

38
00:01:24,905 --> 00:01:27,350
0,315 315,635 1765,2115 2115,2325 2325,2445
are wrong,| {all,three -} of
|这三张脸都是假的，

39
00:01:27,350 --> 00:01:29,600
0,180 180,500 730,1065 1065,1400
these faces are fake,|
|

40
00:01:29,820 --> 00:01:31,510
0,380 380,740 740,1010 1010,1295 1295,1690
these people do not exist,|
这些人并不存在，|

41
00:01:31,830 --> 00:01:33,965
0,320 320,640 750,1070 1070,1780 1800,2135
{these,images -} were synthesized by
这些图像是由深度生成模型合成的，

42
00:01:33,965 --> 00:01:36,320
0,255 255,705 705,995 1525,1925 2035,2355
deep generative models,| trained on
|基于人脸数据进行训练，

43
00:01:36,320 --> 00:01:38,105
0,300 300,570 570,825 825,1190 1480,1785
data of human faces| and
|并要求生成新的实例。

44
00:01:38,105 --> 00:01:40,660
0,305 535,885 885,1235 1465,1785 1785,2555
asked to produce new instances.|
|

45
00:01:41,650 --> 00:01:42,855
0,305 305,515 515,725 725,920 920,1205
Now, I think that this
现在，我认为这个演示展示了这些想法的力量，

46
00:01:42,855 --> 00:01:45,240
0,720 720,975 975,1110 1110,1775 2065,2385
demonstration kind of demonstrates the

47
00:01:45,240 --> 00:01:47,160
0,320 370,660 660,945 945,1340 1660,1920
power of these ideas| and
|和生成性建模概念的力量。

48
00:01:47,160 --> 00:01:48,105
0,150 150,345 345,510 510,675 675,945
the power of this notion

49
00:01:48,105 --> 00:01:50,580
0,240 240,660 660,1205 1735,2115 2115,2475
of generative modeling.| {So,let's -}
|所以，让我们更具体地了解如何将这件事正规化。

50
00:01:50,580 --> 00:01:52,005
0,150 150,300 300,465 465,770 1060,1425
get a little more concrete

51
00:01:52,005 --> 00:01:53,370
0,315 315,570 570,735 735,900 900,1365
about how we can formalize

52
00:01:53,370 --> 00:01:54,340
0,350
this.|
|

53
00:01:54,690 --> 00:01:55,780
0,290 290,455 455,590 590,770 770,1090
So far in this course,|
到目前为止，在这门课程中，|

54
00:01:55,950 --> 00:01:57,170
0,335 335,485 485,755 755,1025 1025,1220
we've been looking at what
我们一直在研究我们所说的监督学习问题，

55
00:01:57,170 --> 00:01:59,230
0,195 195,465 465,830 940,1340 1630,2060
we call problems of supervised

56
00:01:59,250 --> 00:02:01,445
0,400 1140,1505 1505,1760 1760,2000 2000,2195
learning,| meaning that we're given
|这意味着我们给出数据，

57
00:02:01,445 --> 00:02:03,755
0,335 925,1305 1305,1685 1825,2115 2115,2310
data| and associated with that
|并与这些数据相关联的是一组标签，

58
00:02:03,755 --> 00:02:04,655
0,270 270,495 495,630 630,765 765,900
data is a set of

59
00:02:04,655 --> 00:02:07,025
0,485 1285,1620 1620,1920 1920,2190 2190,2370
labels,| {our,goal -} is to
|我们的目标是学习映射该数据的标签的函数，

60
00:02:07,025 --> 00:02:08,780
0,240 240,510 510,815 1105,1440 1440,1755
learn a function that maps

61
00:02:08,780 --> 00:02:10,080
0,285 285,590
that data

62
00:02:10,080 --> 00:02:12,690
0,380 400,645 645,1100 1690,2090 2290,2610
to the labels,| {Now\,,we're -}
|现在我们在一门关于深度学习的课程，

63
00:02:12,690 --> 00:02:13,770
0,60 60,180 180,470 520,855 855,1080
in a course on deep

64
00:02:13,770 --> 00:02:15,330
0,290 370,690 690,945 945,1185 1185,1560
learning,| so we've been concerned
|所以我们一直关注由深度神经网络定义的函数映射，

65
00:02:15,330 --> 00:02:17,085
0,345 345,945 945,1365 1365,1530 1530,1755
with functional mappings that are

66
00:02:17,085 --> 00:02:18,500
0,330 330,645 645,900 900,1155 1155,1415
defined by deep neural networks,|
|

67
00:02:19,180 --> 00:02:20,610
0,275 275,530 530,830 830,1150 1170,1430
but really, that function could
但实际上，这个函数可以是任何东西，

68
00:02:20,610 --> 00:02:22,140
0,240 240,620 670,1035 1035,1275 1275,1530
{be,anything -},| neural networks are
|神经网络很强大，

69
00:02:22,140 --> 00:02:23,745
0,290 370,770 1030,1305 1305,1440 1440,1605
powerful,| but we could use
|但我们也可以使用其他技术。

70
00:02:23,745 --> 00:02:25,600
0,255 255,605 685,990 990,1295
other techniques as well.|
|

71
00:02:26,620 --> 00:02:28,530
0,335 335,670 900,1295 1295,1595 1595,1910
In contrast, there's another class
相比之下，在机器学习中还有另一类问题，

72
00:02:28,530 --> 00:02:30,110
0,225 225,530 670,1020 1020,1275 1275,1580
of problems in machine learning,|
|

73
00:02:30,580 --> 00:02:31,875
0,305 305,470 470,730 750,1040 1040,1295
that we refer to as
我们称之为无监督学习，

74
00:02:31,875 --> 00:02:35,180
0,870 870,1205 1825,2225 2665,2985 2985,3305
unsupervised learning,| where we take
|我们获取数据，

75
00:02:35,290 --> 00:02:37,575
0,400 840,1145 1145,1450 1740,2075 2075,2285
data,| but now we're given
|但现在我们只给出数据，没有标签，

76
00:02:37,575 --> 00:02:40,350
0,345 345,725 1255,1560 1560,2045 2485,2775
only data, no labels,| and
|我们的目标是试图建立某种方法，

77
00:02:40,350 --> 00:02:41,720
0,240 240,555 555,840 840,1065 1065,1370
our goal is to try

78
00:02:41,740 --> 00:02:43,665
0,365 365,695 695,950 950,1240 1650,1925
to build some method,| that
|可以理解这些数据的隐藏的底层结构，

79
00:02:43,665 --> 00:02:45,920
0,275 505,905 925,1260 1260,1595 1855,2255
can understand the hidden underlying

80
00:02:46,300 --> 00:02:48,100
0,380 380,635 635,815 815,1120
structure of that data,|
|

81
00:02:48,590 --> 00:02:49,780
0,305 305,545 545,800 800,1025 1025,1190
what this allows us to
这使我们能够做的是，

82
00:02:49,780 --> 00:02:50,920
0,225 225,465 465,630 630,855 855,1140
do is,| it gives us
|它让我们对数据的基本表示形式有了新的见解，

83
00:02:50,920 --> 00:02:53,040
0,350 490,945 945,1250 1330,1605 1605,2120
new insights into the foundational

84
00:02:53,060 --> 00:02:56,065
0,940 1350,1655 1655,1820 1820,2080 2640,3005
representation of the data| and
|并且正如我们稍后将看到的那样，

85
00:02:56,065 --> 00:02:58,285
0,270 270,525 525,690 690,995 1945,2220
as we'll see later,| actually
|使我们能够生成新的数据实例。

86
00:02:58,285 --> 00:02:59,905
0,390 390,660 660,855 855,1145 1285,1620
enables us to generate new

87
00:02:59,905 --> 00:03:01,540
0,285 285,1055
data instances.|
|

88
00:03:01,790 --> 00:03:03,090
0,290 290,530 530,785 785,995 995,1300
Now, this class of problems,|
现在，这类问题，|

89
00:03:03,530 --> 00:03:06,210
0,400 630,1030 1290,1580 1580,2345 2345,2680
this definition of unsupervised learning,|
无监督学习的定义，|

90
00:03:06,710 --> 00:03:08,485
0,710 710,1055 1055,1325 1325,1505 1505,1775
captures the types of models
抓住了我们今天要讨论的模型类型，

91
00:03:08,485 --> 00:03:09,415
0,270 270,465 465,585 585,750 750,930
that we're going to talk

92
00:03:09,415 --> 00:03:10,920
0,225 225,545 835,1110 1110,1245 1245,1505
about today| in the focus
|重点是生成性建模，

93
00:03:10,970 --> 00:03:13,040
0,400 540,1100 1100,1600
on generative modeling,|
|

94
00:03:13,700 --> 00:03:14,950
0,275 275,410 410,635 635,995 995,1250
which is an example of
这是无监督学习的一个例子，

95
00:03:14,950 --> 00:03:17,290
0,690 690,1010 1270,1620 1620,1965 1965,2340
unsupervised learning| and is united
|它与这个问题的目标相一致，

96
00:03:17,290 --> 00:03:19,360
0,270 270,495 495,830 1510,1845 1845,2070
by this goal of the

97
00:03:19,360 --> 00:03:20,880
0,290 310,630 630,885 885,1125 1125,1520
problem,| where we're given only
|在这个问题中，我们只得到训练集中的样本，

98
00:03:20,900 --> 00:03:22,530
0,605 605,830 830,980 980,1250 1250,1630
samples from a training set,|
|

99
00:03:23,120 --> 00:03:24,260
0,300 300,495 495,690 690,900 900,1140
and we want to learn
我们想要学习一种模型，

100
00:03:24,260 --> 00:03:26,315
0,255 255,560 640,945 945,1250 1660,2055
a model,| that represents the
|表示该模型所看到的数据的分布。

101
00:03:26,315 --> 00:03:28,175
0,395 835,1140 1140,1320 1320,1545 1545,1860
distribution of the data that

102
00:03:28,175 --> 00:03:29,900
0,225 225,435 435,690 690,995
the model is seeing.|
|

103
00:03:30,270 --> 00:03:33,305
0,545 545,1060 1230,1630 2220,2620 2640,3035
Generative modeling takes two general
生成式建模一般有两种形式，

104
00:03:33,305 --> 00:03:37,115
0,395 1345,1745 1975,2685 2685,3125 3445,3810
forms,| first, density estimation| and
|第一种是密度估计，|第二种是样本生成。

105
00:03:37,115 --> 00:03:39,520
0,365 535,930 930,1325
second, sample generation.|
|

106
00:03:39,760 --> 00:03:41,720
0,290 290,890 890,1330 1350,1655 1655,1960
In density estimation,| the task
在密度估计中，|任务是给出一些数据实例，

107
00:03:41,740 --> 00:03:44,630
0,400 1080,1475 1475,1870 2070,2470 2490,2890
is given some data examples,|
|

108
00:03:45,490 --> 00:03:47,115
0,335 335,670 690,1055 1055,1370 1370,1625
{our,goal -} is to train
我们的目标是训练一个模型，

109
00:03:47,115 --> 00:03:49,170
0,180 180,455 925,1290 1290,1800 1800,2055
a model,| that learns an
|学习潜在概率分布，

110
00:03:49,170 --> 00:03:53,390
0,350 970,1670 1900,2300 3010,3410 3490,4220
underlying probability distribution,| that describes
|描述了数据来源。

111
00:03:54,040 --> 00:03:55,700
0,335 335,545 545,820 990,1325 1325,1660
where the data came from.|
|

112
00:03:57,080 --> 00:03:59,530
0,350 350,695 695,1090 1770,2135 2135,2450
With sample generation,| the idea
对于样本生成，|想法是相似的，

113
00:03:59,530 --> 00:04:01,350
0,350 520,920 1030,1350 1350,1545 1545,1820
is similar,| but the focus
|但重点更多地放在生成新实例上，

114
00:04:01,370 --> 00:04:03,280
0,320 320,605 605,970 1110,1415 1415,1910
is more on actually generating

115
00:04:03,280 --> 00:04:05,890
0,270 270,1010 1840,2145 2145,2370 2370,2610
new instances,| {our,goal -} with
|我们生成样本的目标是，

116
00:04:05,890 --> 00:04:08,070
0,315 315,710 1090,1380 1380,1670 1780,2180
sample generation is to,| again,
|再次，学习这种潜在概率分布的模型，

117
00:04:08,300 --> 00:04:09,595
0,320 320,560 560,860 860,1115 1115,1295
learn this model of this

118
00:04:09,595 --> 00:04:12,835
0,305 655,1325 1465,1865 2755,3030 3030,3240
underlying probability distribution,| but then
|然后使用该模型对其进行采样，

119
00:04:12,835 --> 00:04:14,670
0,270 270,525 525,845 1135,1485 1485,1835
use that model to sample

120
00:04:14,720 --> 00:04:16,180
0,275 275,455 455,680 680,1000 1140,1460
from it| and generate new
|并生成新的实例，

121
00:04:16,180 --> 00:04:19,090
0,800 1150,1500 1500,1845 1845,2240 2560,2910
instances,| that are similar to
|类似于我们看到的数据，

122
00:04:19,090 --> 00:04:20,280
0,210 210,420 420,660 660,915 915,1190
the data that we've seen,|
|

123
00:04:20,870 --> 00:04:23,635
0,400 660,1060 1260,1660 1830,2495 2495,2765
approximately falling along, ideally that
大致下降，理想情况下，相同的真实数据分布。|

124
00:04:23,635 --> 00:04:26,180
0,365 565,965 1045,1445 1525,1925
same real data distribution.|
|

125
00:04:27,060 --> 00:04:28,300
0,290 290,470 470,680 680,920 920,1240
Now, in both these cases
现在，在密度估计和样本生成这两种情况下，

126
00:04:28,320 --> 00:04:30,125
0,305 305,860 860,1265 1265,1520 1520,1805
of density estimation and sample

127
00:04:30,125 --> 00:04:32,585
0,395 1135,1410 1410,1685 1885,2235 2235,2460
generation,| the underlying question is
|潜在的问题都是相同的，

128
00:04:32,585 --> 00:04:34,870
0,150 150,425 1165,1515 1515,1865 1885,2285
the same,| {our,learning -} task
|我们的学习任务是尝试建立一个模型，

129
00:04:34,920 --> 00:04:36,575
0,335 335,605 605,935 935,1330 1350,1655
is to try to build

130
00:04:36,575 --> 00:04:38,120
0,180 180,455 595,930 930,1350 1350,1545
a model,| that learns this
|学习这种概率分布，

131
00:04:38,120 --> 00:04:40,730
0,590 1060,1460 1870,2145 2145,2325 2325,2610
probability distribution,| that is as
|尽可能接近真实的数据分布。

132
00:04:40,730 --> 00:04:42,800
0,285 285,510 510,830 1510,1830 1830,2070
close as possible to the

133
00:04:42,800 --> 00:04:45,060
0,320 400,800 1060,1460
true data distribution.|
|

134
00:04:46,540 --> 00:04:48,680
0,400 870,1270 1350,1640 1640,1835 1835,2140
Okay, so with this definition
好的，有了这个生成性建模的定义和概念，

135
00:04:48,820 --> 00:04:50,325
0,275 275,470 470,770 770,1055 1055,1505
and this concept of generative

136
00:04:50,325 --> 00:04:52,485
0,545 1105,1365 1365,1545 1545,1845 1845,2160
modeling,| what are some ways
|我们可以用什么方法部署生成式模型，

137
00:04:52,485 --> 00:04:54,045
0,195 195,315 315,575 625,1025 1225,1560
that we can actually deploy

138
00:04:54,045 --> 00:04:56,085
0,450 450,870 870,1205 1585,1875 1875,2040
generative modeling| forward in the
|在现实世界中为高影响的应用程序，

139
00:04:56,085 --> 00:04:57,740
0,210 210,525 525,825 825,1145 1255,1655
real world for high impact

140
00:04:58,030 --> 00:05:01,170
0,400 1740,2140 2610,2885 2885,3020 3020,3140
applications,| well, part of the
|好的，生成式模型是如此强大的部分原因是，

141
00:05:01,170 --> 00:05:03,060
0,240 240,510 510,945 945,1220 1570,1890
reason that generative models are

142
00:05:03,060 --> 00:05:04,530
0,255 255,590 850,1125 1125,1275 1275,1470
so powerful is,| that they
|它们有能力发现数据集中的底层特征，

143
00:05:04,530 --> 00:05:06,870
0,255 255,585 585,980 1390,1680 1680,2340
have this ability to uncover

144
00:05:06,870 --> 00:05:08,835
0,180 180,470 790,1190 1540,1815 1815,1965
the underlying features in a

145
00:05:08,835 --> 00:05:10,560
0,575 655,945 945,1425 1425,1560 1560,1725
dataset| and encode it in
|并以有效的方式对其进行编码。

146
00:05:10,560 --> 00:05:12,060
0,225 225,555 555,920
an efficient way.|
|

147
00:05:12,190 --> 00:05:13,590
0,275 275,425 425,700 780,1070 1070,1400
So, for example, if we're
例如，如果我们考虑面部检测问题，

148
00:05:13,590 --> 00:05:15,315
0,285 285,510 510,800 850,1230 1230,1725
considering the problem of facial

149
00:05:15,315 --> 00:05:17,085
0,575 955,1245 1245,1440 1440,1590 1590,1770
detection| and we're given a
|我们得到一个包含许多不同人脸的数据集，

150
00:05:17,085 --> 00:05:18,615
0,225 225,555 555,930 930,1260 1260,1530
{dataset -} with many, many

151
00:05:18,615 --> 00:05:21,200
0,285 285,635 1615,1965 1965,2250 2250,2585
different faces,| starting out without
|而没有检查这些数据，

152
00:05:21,340 --> 00:05:22,785
0,515 515,695 695,965 965,1235 1235,1445
inspecting this data,| we may
|我们可能不知道这个数据集中人脸的分布是什么，

153
00:05:22,785 --> 00:05:24,620
0,240 240,575 925,1230 1230,1485 1485,1835
not know what the distribution

154
00:05:24,640 --> 00:05:25,935
0,275 275,550 630,890 890,1055 1055,1295
of faces in this {dataset

155
00:05:25,935 --> 00:05:27,750
0,255 255,575 775,1170 1170,1515 1515,1815
-} is,| with respect to
|相对于我们可能关心的特征，

156
00:05:27,750 --> 00:05:29,220
0,330 330,585 585,750 750,1020 1020,1470
features we may be caring

157
00:05:29,220 --> 00:05:30,930
0,290 550,840 840,1130 1180,1455 1455,1710
about,| {for,example, -} the pose
|例如，头部的姿势、衣服、眼镜、肤色、头发等，

158
00:05:30,930 --> 00:05:33,170
0,210 210,345 345,620 1210,1610 1840,2240
of the head, clothing, glasses,

159
00:05:33,490 --> 00:05:36,860
0,335 335,670 930,1330 1590,2140
skin tone, hair, etc,|
|

160
00:05:36,860 --> 00:05:37,715
0,105 105,300 300,540 540,705 705,855
and it can be the
情况可能是，

161
00:05:37,715 --> 00:05:39,080
0,225 225,465 465,705 705,1020 1020,1365
case,| that our training data
|我们的训练数据可能对特定的特征非常有偏见，

162
00:05:39,080 --> 00:05:40,690
0,255 255,450 450,720 720,1020 1020,1610
may be very, very biased

163
00:05:41,010 --> 00:05:43,490
0,400 480,880 930,1330 1680,2080 2130,2480
towards particular features| without us
|在我们甚至没有意识到的情况下，

164
00:05:43,490 --> 00:05:45,400
0,330 330,855 855,1250
even realizing this,|
|

165
00:05:45,700 --> 00:05:47,600
0,395 395,980 980,1270 1410,1655 1655,1900
using generative models,| we can
使用生成模型，|我们可以识别这些潜在特征的分布，

166
00:05:47,680 --> 00:05:50,085
0,400 930,1235 1235,1520 1520,2050 2130,2405
actually identify the distributions of

167
00:05:50,085 --> 00:05:51,765
0,180 180,485 775,1175 1195,1440 1440,1680
these underlying features| in a
|以一种完全自动的方式，而不需要任何标记，

168
00:05:51,765 --> 00:05:54,690
0,395 1045,1425 1425,1805 2155,2555 2575,2925
completely automatic way without any

169
00:05:54,690 --> 00:05:55,980
0,620
labeling,|
|

170
00:05:55,980 --> 00:05:57,570
0,165 165,465 465,830 880,1275 1275,1590
in order to understand what
为了了解哪些特征在数据中可能被高估，

171
00:05:57,570 --> 00:05:59,400
0,315 315,585 585,750 750,1605 1605,1830
features may be overrepresented in

172
00:05:59,400 --> 00:06:01,455
0,150 150,410 1060,1380 1380,1700 1780,2055
the data,| what features may
|哪些特征在数据中可能被低估。

173
00:06:01,455 --> 00:06:03,200
0,210 210,1110 1110,1350 1350,1485 1485,1745
be underrepresented in the data.|
|

174
00:06:04,050 --> 00:06:05,350
0,290 290,575 575,845 845,1010 1010,1300
And this is the focus
这是今天和明天软件实验的重点，

175
00:06:05,520 --> 00:06:07,990
0,400 780,1160 1160,1490 1490,2105 2105,2470
of today and tomorrow's software

176
00:06:08,040 --> 00:06:09,875
0,575 575,830 830,1150 1320,1655 1655,1835
labs,| which are going to
|将成为软件实验竞争的一部分，

177
00:06:09,875 --> 00:06:11,240
0,245 265,570 570,750 750,1005 1005,1365
be part of the software

178
00:06:11,240 --> 00:06:14,690
0,380 550,950 1690,2090 2620,3180 3180,3450
lab competition,| developing generative models
|开发可以完成这项任务的生成性模型，

179
00:06:14,690 --> 00:06:15,880
0,255 255,435 435,645 645,870 870,1190
that can do this task|
|

180
00:06:16,200 --> 00:06:17,810
0,305 305,575 575,815 815,950 950,1610
and using it to uncover
并使用它来发现和诊断偏差，

181
00:06:17,810 --> 00:06:20,210
0,195 195,800 940,1730 1870,2145 2145,2400
and diagnose biases,| that can
|面部检测模型中可能存在的（偏差）。

182
00:06:20,210 --> 00:06:22,630
0,380 580,945 945,1520 1690,2145 2145,2420
exist within facial detection models.|
|

183
00:06:25,050 --> 00:06:26,975
0,350 350,650 650,1000 1170,1570 1590,1925
Another really powerful example is,|
另一个真正强大的例子是，|

184
00:06:26,975 --> 00:06:28,685
0,210 210,375 375,630 630,975 975,1710
in the case of outlier
在异常值检测的情况下，识别罕见事件。

185
00:06:28,685 --> 00:06:32,645
0,605 1375,2115 2115,2475 2475,2855 3655,3960
detection, identifying rare events.| {So\,,let's
|所以，让我们来考虑自动驾驶汽车的例子，

186
00:06:32,645 --> 00:06:34,330
0,375 375,645 645,885 885,1205 1285,1685
-} consider the example of

187
00:06:34,440 --> 00:06:36,940
0,335 335,670 720,1445 1445,1780
self driving autonomous cars,|
|

188
00:06:37,040 --> 00:06:38,695
0,260 260,380 380,890 890,1180 1320,1655
with an autonomous car, let's
有了自动驾驶汽车，假设它正在现实世界中行驶，

189
00:06:38,695 --> 00:06:39,670
0,90 90,255 255,480 480,780 780,975
say it's driving out in

190
00:06:39,670 --> 00:06:41,440
0,120 120,300 300,620 1120,1470 1470,1770
the real world,| {we,really, -}
|我们真的很想确保，

191
00:06:41,440 --> 00:06:42,610
0,285 285,540 540,705 705,900 900,1170
really want to make sure

192
00:06:42,610 --> 00:06:44,155
0,210 210,465 465,830 1090,1380 1380,1545
that,| that car can be
|这辆车能够处理所有可能的情况，

193
00:06:44,155 --> 00:06:45,655
0,255 255,525 525,815 955,1290 1290,1500
able to handle all the

194
00:06:45,655 --> 00:06:47,215
0,275 355,990 990,1170 1170,1380 1380,1560
possible scenarios| and all the
|和它可能遇到的所有可能的情况，

195
00:06:47,215 --> 00:06:49,110
0,275 355,735 735,990 990,1245 1245,1895
possible cases it may encounter,|
|

196
00:06:49,700 --> 00:06:51,835
0,400 420,770 770,1120 1260,1660 1830,2135
including edge cases like a
包括边缘情况，比如鹿走到车前面，

197
00:06:51,835 --> 00:06:52,960
0,360 360,600 600,795 795,975 975,1125
deer coming in front of

198
00:06:52,960 --> 00:06:54,630
0,165 165,470 700,1005 1005,1290 1290,1670
the car| or some unexpected
|或一些意想不到的罕见事件，

199
00:06:55,010 --> 00:06:57,220
0,365 365,730 1290,1625 1625,1955 1955,2210
rare events,| not just, you
|而不仅仅是典型的直线行驶，

200
00:06:57,220 --> 00:06:59,020
0,150 150,330 330,620 820,1220 1300,1800
know, the typical straight freeway

201
00:06:59,020 --> 00:07:00,810
0,320 430,830 910,1185 1185,1425 1425,1790
driving,| that it may see
|它可能会在大部分时间看到的。

202
00:07:01,070 --> 00:07:02,280
0,245 245,490 510,785 785,935 935,1210
the majority of the time.|
|

203
00:07:03,250 --> 00:07:05,055
0,350 350,860 860,1150 1350,1625 1625,1805
With generative models,| we can
对于生成模型，|我们可以使用这种密度估计的思想

204
00:07:05,055 --> 00:07:06,975
0,225 225,540 540,885 885,1215 1215,1920
use this idea of density

205
00:07:06,975 --> 00:07:08,840
0,485 655,930 930,1095 1095,1385 1465,1865
estimation| to be able to
|来识别训练数据中的罕见和异常事件，

206
00:07:09,520 --> 00:07:12,110
0,400 690,1090 1140,1445 1445,2225 2225,2590
identify rare and anomalous events

207
00:07:12,820 --> 00:07:14,660
0,400 420,695 695,935 935,1300 1440,1840
within the training data| and
|以及当它们发生时，

208
00:07:14,800 --> 00:07:16,260
0,320 320,695 695,1100 1100,1310 1310,1460
as they're occurring,| as the
|就像模型第一次看到它们一样。

209
00:07:16,260 --> 00:07:17,340
0,240 240,510 510,750 750,960 960,1080
model sees them, for the

210
00:07:17,340 --> 00:07:18,460
0,210 210,560
first time.|
|

211
00:07:19,490 --> 00:07:21,115
0,400 420,800 800,1130 1130,1460 1460,1625
So hopefully this paints this
希望这幅图描绘了什么是生成性建模的基本概念，

212
00:07:21,115 --> 00:07:23,755
0,305 385,785 1405,1770 1770,2280 2280,2640
picture of what generative modeling

213
00:07:23,755 --> 00:07:25,660
0,150 150,425 655,1055 1135,1535 1555,1905
the underlying concept is,| and
|以及几种不同的方式，

214
00:07:25,660 --> 00:07:26,815
0,225 225,420 420,615 615,840 840,1155
a couple of different ways|
|

215
00:07:26,815 --> 00:07:28,110
0,240 240,405 405,555 555,815 895,1295
in which we can actually
我们可以将这些想法实际部署到强大而有效的现实世界应用程序中。

216
00:07:28,340 --> 00:07:30,660
0,320 320,620 620,1000 1260,1660 1920,2320
deploy these ideas for powerful

217
00:07:30,770 --> 00:07:33,060
0,365 365,965 965,1280 1280,1600 1890,2290
and impactful real world applications.|
|

218
00:07:35,060 --> 00:07:36,400
0,305 305,710 710,935 935,1220 1220,1340
In today's lecture, we're going
在今天的课程中，我们将重点介绍一大类生成性模型，

219
00:07:36,400 --> 00:07:38,280
0,180 180,470 580,980 1270,1575 1575,1880
to focus on a broad

220
00:07:38,300 --> 00:07:39,820
0,350 350,590 590,995 995,1250 1250,1520
class of generative models,| that
|我们称之为隐变量模型，

221
00:07:39,820 --> 00:07:41,520
0,195 195,450 450,915 915,1220 1300,1700
we call latent variable models|
|

222
00:07:42,050 --> 00:07:44,130
0,320 320,640 930,1430 1430,1715 1715,2080
and specifically distill down into
具体分为两种类型的隐变量模型。

223
00:07:44,210 --> 00:07:46,770
0,400 930,1700 1700,1940 1940,2285 2285,2560
two subtypes of latent variable

224
00:07:46,820 --> 00:07:48,480
0,400
models.|
|

225
00:07:48,740 --> 00:07:50,965
0,320 320,560 560,880 1410,1925 1925,2225
First things, first, I've introduced
首先，我已经引入了隐变量这个术语，

226
00:07:50,965 --> 00:07:53,005
0,240 240,525 525,900 900,1175 1735,2040
this term latent variable,| but
|但是我还没有告诉你们或者描述它到底是什么，

227
00:07:53,005 --> 00:07:54,670
0,210 210,695 865,1200 1200,1410 1410,1665
I haven't told you or

228
00:07:54,670 --> 00:07:56,400
0,380 490,720 720,950 1060,1395 1395,1730
described to you what that

229
00:07:56,420 --> 00:07:57,680
0,305 305,610
actually is,|
|

230
00:07:58,350 --> 00:07:59,660
0,305 305,485 485,665 665,965 965,1310
I think a great example,|
我认为一个很好的例子，|

231
00:07:59,660 --> 00:08:00,670
0,240 240,390 390,525 525,705 705,1010
and one of my favorite
也是我在整个课程中最喜欢的一个例子，

232
00:08:00,690 --> 00:08:02,830
0,400 780,1085 1085,1370 1370,1745 1745,2140
examples throughout this entire course,|
|

233
00:08:03,600 --> 00:08:05,090
0,400 540,845 845,1025 1025,1265 1265,1490
that gets at this idea
获得隐变量的思想，

234
00:08:05,090 --> 00:08:06,740
0,135 135,285 285,615 615,890 1360,1650
of the latent variable,| is
|就是这个来自柏拉图的理想国的小故事，

235
00:08:06,740 --> 00:08:08,770
0,210 210,510 510,890 910,1275 1275,2030
this little story from Plato's

236
00:08:08,820 --> 00:08:09,920
0,400
republic,|
|

237
00:08:09,960 --> 00:08:11,465
0,335 335,590 590,860 860,1210 1230,1505
which is known as the
这就是众所周知的洞穴寓言，

238
00:08:11,465 --> 00:08:13,180
0,225 225,435 435,585 585,875
myth of the cave,|
|

239
00:08:13,410 --> 00:08:14,735
0,290 290,485 485,790 930,1190 1190,1325
in this myth,| there is
在这个寓言中，|有一群囚犯，

240
00:08:14,735 --> 00:08:16,685
0,165 165,375 375,615 615,1145 1645,1950
a group of prisoners,| and
|作为惩罚的一部分，

241
00:08:16,685 --> 00:08:18,125
0,225 225,435 435,600 600,810 810,1440
as part of their punishment,|
|

242
00:08:18,125 --> 00:08:19,820
0,345 345,875 985,1320 1320,1545 1545,1695
they're constrained to face a
他们被迫面对一堵墙，

243
00:08:19,820 --> 00:08:22,475
0,260 1150,1550 1750,2025 2025,2295 2295,2655
wall,| {now,the -} only things
|现在囚犯们唯一能观察到的东西是，

244
00:08:22,475 --> 00:08:24,700
0,285 285,750 750,1050 1050,1415 1825,2225
that prisoners can observe are|
|

245
00:08:24,810 --> 00:08:27,520
0,640 660,1060 1230,1630 2070,2390 2390,2710
shadows of objects that are
在他们身后的火堆前经过的物体的阴影，

246
00:08:27,660 --> 00:08:28,895
0,400 420,725 725,935 935,1085 1085,1235
passing in front of a

247
00:08:28,895 --> 00:08:30,875
0,300 300,720 720,990 990,1325 1705,1980
fire that's behind them,| and
|他们观察这个洞穴墙上的阴影的投射，

248
00:08:30,875 --> 00:08:32,690
0,330 330,695 865,1170 1170,1680 1680,1815
they're observing the casting of

249
00:08:32,690 --> 00:08:34,655
0,150 150,620 910,1310 1420,1710 1710,1965
the shadows on the wall

250
00:08:34,655 --> 00:08:36,080
0,240 240,450 450,785
of this cave,|
|

251
00:08:36,780 --> 00:08:38,390
0,245 245,365 365,830 830,1115 1115,1610
to the prisoners,| those shadows
对于囚犯来说，|这些阴影是他们唯一看到的东西，

252
00:08:38,390 --> 00:08:39,485
0,195 195,330 330,570 570,855 855,1095
are the only things they

253
00:08:39,485 --> 00:08:41,465
0,315 315,690 690,1235 1555,1830 1830,1980
see,| their observations, they can
|他们的观察，他们可以测量他们，

254
00:08:41,465 --> 00:08:42,455
0,225 225,495 495,675 675,825 825,990
measure them,| {they,can -} give
|他们可以给他们起名字，

255
00:08:42,455 --> 00:08:44,170
0,165 165,455 955,1275 1275,1455 1455,1715
them names,| because to them
|因为对他们来说，这就是他们的现实，

256
00:08:44,220 --> 00:08:47,255
0,500 500,740 740,1060 2160,2560 2580,3035
that's their reality,| but they're
|但他们不能直接看到潜在的物体，

257
00:08:47,255 --> 00:08:49,220
0,345 345,675 675,1025 1165,1565 1675,1965
unable to directly see the

258
00:08:49,220 --> 00:08:51,700
0,290 1210,1590 1590,1905 1905,2160 2160,2480
underlying objects,| the true factors
|真实的因素本身，这些因素投射那些阴影，

259
00:08:51,960 --> 00:08:53,570
0,380 380,620 620,845 845,1400 1400,1610
themselves, that are casting those

260
00:08:53,570 --> 00:08:54,620
0,500
shadows,|
|

261
00:08:55,180 --> 00:08:57,350
0,400 600,995 995,1370 1370,1750 1770,2170
those objects here are like
这里的这些对象就像机器学习中的隐变量，

262
00:08:57,460 --> 00:08:59,150
0,455 455,920 920,1160 1160,1400 1400,1690
latent variables in machine learning,|
|

263
00:09:00,030 --> 00:09:02,510
0,335 335,545 545,910 960,1660 2130,2480
they're not directly observable,| but
它们不是直接可见的，|但它们是真正的潜在特征或解释因素，

264
00:09:02,510 --> 00:09:03,910
0,210 210,465 465,795 795,1065 1065,1400
they are the true underlying

265
00:09:04,200 --> 00:09:07,120
0,400 690,1090 1320,2045 2045,2380 2520,2920
features or explanatory factors,| that
|它们创造了我们可以看到和观察到的差异和变量。

266
00:09:07,560 --> 00:09:10,240
0,365 365,710 710,1090 1110,1510 2280,2680
create the observed differences and

267
00:09:10,320 --> 00:09:12,200
0,650 650,1000 1110,1385 1385,1580 1580,1880
variables that we can see

268
00:09:12,200 --> 00:09:13,920
0,330 330,680
and observe.|
|

269
00:09:15,030 --> 00:09:17,030
0,290 290,580 1050,1340 1340,1630 1710,2000
And this gets at the
这就达到了生成性建模的目标，

270
00:09:17,030 --> 00:09:19,220
0,290 340,645 645,1095 1095,1610 1900,2190
goal of generative modeling,| which
|即找到方法让我们能够真正学习这些隐藏的特征，

271
00:09:19,220 --> 00:09:20,360
0,165 165,330 330,585 585,915 915,1140
is to find ways that

272
00:09:20,360 --> 00:09:21,680
0,120 120,380 430,765 765,1050 1050,1320
we can actually learn these

273
00:09:21,680 --> 00:09:23,750
0,285 285,650 760,1080 1080,1400 1630,2070
hidden features,| these underlying latent
|这些潜在的隐变量，

274
00:09:23,750 --> 00:09:25,805
0,590 910,1290 1290,1575 1575,1815 1815,2055
variables,| even when we're only
|即使我们只得到了观察到的数据。

275
00:09:25,805 --> 00:09:28,330
0,395 745,1295 1495,1860 1860,2175 2175,2525
given observations of the observed

276
00:09:28,350 --> 00:09:29,380
0,400
data.|
|

277
00:09:31,530 --> 00:09:33,620
0,400 720,1160 1160,1370 1370,1690 1710,2090
So let's start by discussing
所以，让我们从讨论一个非常简单的生成性模型开始，

278
00:09:33,620 --> 00:09:35,240
0,285 285,570 570,885 885,1365 1365,1620
a very simple generative model,|
|

279
00:09:35,240 --> 00:09:36,490
0,330 330,645 645,870 870,990 990,1250
that tries to do this
试图通过对数据输入进行编码来实现这一点，

280
00:09:36,810 --> 00:09:38,590
0,275 275,515 515,815 815,1055 1055,1780
through the idea of encoding

281
00:09:38,610 --> 00:09:40,320
0,275 275,550 630,1030
the data input,|
|

282
00:09:40,320 --> 00:09:41,100
0,120 120,360 360,570 570,645 645,780
the models {we're -} going
我们将要讨论的模型被称为自编码器，

283
00:09:41,100 --> 00:09:42,255
0,150 150,345 345,600 600,855 855,1155
to talk about are called

284
00:09:42,255 --> 00:09:43,860
0,300 300,965
{autoencoders -},|
|

285
00:09:44,050 --> 00:09:45,030
0,305 305,470 470,605 605,755 755,980
and to take a look
为了看看自编码器是如何工作的，

286
00:09:45,030 --> 00:09:46,665
0,315 315,600 600,810 810,1065 1065,1635
at how an {autoencoder -}

287
00:09:46,665 --> 00:09:48,180
0,275 625,930 930,1095 1095,1320 1320,1515
works,| we'll go through step
|我们将一步一步地来，

288
00:09:48,180 --> 00:09:50,690
0,210 210,530 1150,1550 1570,1970 2110,2510
by step,| starting with the
|从获取一些原始输入数据开始，

289
00:09:51,160 --> 00:09:52,740
0,350 350,635 635,905 905,1235 1235,1580
first step of taking some

290
00:09:52,740 --> 00:09:55,170
0,350 430,720 720,1010 1690,2070 2070,2430
raw input data| and passing
|并将其传递给一系列神经网络层，

291
00:09:55,170 --> 00:09:56,745
0,285 285,525 525,825 825,1185 1185,1575
it through a series of

292
00:09:56,745 --> 00:09:58,380
0,345 345,585 585,1115
neural network layers,|
|

293
00:09:58,820 --> 00:10:00,990
0,400 570,970 1110,1505 1505,1835 1835,2170
now, the output of this,
现在，这第一步的输出就是我们所说的低维隐空间，

294
00:10:01,760 --> 00:10:03,670
0,275 275,550 630,980 980,1330 1560,1910
of this first step is

295
00:10:03,670 --> 00:10:04,855
0,225 225,375 375,650 700,1005 1005,1185
what we refer to as

296
00:10:04,855 --> 00:10:06,630
0,165 165,390 390,1065 1065,1455 1455,1775
a low dimensional latent space,|
|

297
00:10:07,070 --> 00:10:09,730
0,320 320,440 440,1240 1440,2345 2345,2660
it's an encoded representation of
它是这些基本特征的编码表示，

298
00:10:09,730 --> 00:10:11,780
0,255 255,590 850,1250
those underlying features,|
|

299
00:10:11,780 --> 00:10:12,710
0,270 270,420 420,525 525,675 675,930
and {that's -} our goal
这就是我们试图训练这个模型并预测这些特征的目标，

300
00:10:12,710 --> 00:10:14,165
0,315 315,645 645,990 990,1260 1260,1455
in trying to train this

301
00:10:14,165 --> 00:10:15,640
0,270 270,540 540,825 825,1140 1140,1475
model and predict those features,|
|

302
00:10:17,120 --> 00:10:18,175
0,260 260,455 455,635 635,815 815,1055
the reason a model like
这样的模型之所以被称为编码器或自编码器，

303
00:10:18,175 --> 00:10:19,705
0,195 195,390 390,585 585,870 870,1530
this is called an encoder

304
00:10:19,705 --> 00:10:21,415
0,60 60,195 195,435 435,1085 1375,1710
or an {autoencoder -}| is
|是因为它将数据 x 映射到隐变量 z 的向量中。

305
00:10:21,415 --> 00:10:22,950
0,330 330,705 705,1125 1125,1275 1275,1535
that it's mapping the data

306
00:10:23,240 --> 00:10:25,405
0,400 870,1220 1220,1490 1490,1865 1865,2165
X into this vector of

307
00:10:25,405 --> 00:10:27,880
0,390 390,995 1165,1565
latent variables z.|
|

308
00:10:27,950 --> 00:10:29,470
0,400 690,920 920,995 995,1235 1235,1520
Now, {let's -} ask ourselves
现在，让我们问自己一个问题，

309
00:10:29,470 --> 00:10:30,355
0,150 150,375 375,555 555,660 660,885
a question,| let's pause for
|让我们暂停一下，

310
00:10:30,355 --> 00:10:31,720
0,165 165,240 240,485
a moment,|
|

311
00:10:31,720 --> 00:10:33,510
0,285 285,585 585,885 885,1250 1390,1790
why may we care about
为什么我们可能会关心这个隐变量向量 z 在低维空间，

312
00:10:34,070 --> 00:10:36,060
0,400 420,800 800,1250 1250,1540 1560,1990
having this latent variable vector

313
00:10:36,080 --> 00:10:37,645
0,400 570,950 950,1190 1190,1340 1340,1565
z be in a low

314
00:10:37,645 --> 00:10:39,900
0,690 690,1025
dimensional space,|
|

315
00:10:40,620 --> 00:10:42,480
0,275 275,410 410,650 650,1030
anyone have any ideas?|
有谁有什么想法吗？|

316
00:10:49,870 --> 00:10:50,760
0,400
Alright,
好的，也许有一些想法，是的。

317
00:10:51,070 --> 00:10:52,460
0,335 335,530 530,695 695,995 995,1390
maybe there are some ideas,

318
00:10:53,290 --> 00:10:54,680
0,400
yes.|
|

319
00:10:57,070 --> 00:10:58,200
0,305 305,680 680,800 800,920 920,1130
The suggestion was that it's
建议是，这样做效率更高，

320
00:10:58,200 --> 00:11:00,630
0,210 210,560 610,1010 1120,1640 2140,2430
more efficient,| {yes\,,that -} gets
|是的，这就是问题所在，

321
00:11:00,630 --> 00:11:01,455
0,150 150,345 345,540 540,690 690,825
at it,| the heart of
|这个问题的核心。

322
00:11:01,455 --> 00:11:03,270
0,210 210,480 480,645 645,905 1465,1815
the, of {the,question -}.| The
|拥有低维隐空间的想法是，

323
00:11:03,270 --> 00:11:04,425
0,210 210,390 390,660 660,930 930,1155
idea of having that low

324
00:11:04,425 --> 00:11:05,985
0,555 555,900 900,1170 1170,1410 1410,1560
dimensional latent space is that,|
|

325
00:11:05,985 --> 00:11:07,995
0,165 165,300 300,605 685,1085 1405,2010
it's a very efficient, compact
它是对丰富的、高维数据的一种非常有效、紧凑的编码，

326
00:11:07,995 --> 00:11:10,635
0,725 985,1350 1350,1620 1620,1925 2335,2640
encoding of the rich, high

327
00:11:10,635 --> 00:11:11,925
0,525 525,750 750,945 945,1080 1080,1290
dimensional data,| that we may
|我们可以从它开始。

328
00:11:11,925 --> 00:11:13,080
0,255 255,575
start with.|
|

329
00:11:13,410 --> 00:11:15,410
0,350 350,605 605,845 845,1180 1710,2000
As you pointed out, what
正如你所指出的，这意味着，

330
00:11:15,410 --> 00:11:16,400
0,195 195,420 420,615 615,765 765,990
this means is that,| we're
|我们能够将数据压缩为向量的小特征表示形式，

331
00:11:16,400 --> 00:11:18,380
0,240 240,480 480,960 960,1310 1630,1980
able to compress data into

332
00:11:18,380 --> 00:11:20,765
0,315 315,680 1030,1380 1380,2145 2145,2385
this small feature representation of

333
00:11:20,765 --> 00:11:22,000
0,335
vector,|
|

334
00:11:22,380 --> 00:11:24,830
0,400 840,1400 1400,1595 1595,2240 2240,2450
that captures this compactness and
这抓住了这种紧凑性和丰富性，

335
00:11:24,830 --> 00:11:27,740
0,560 1060,1460 2020,2450 2470,2730 2730,2910
richness| without requiring so much
|而不需要如此多的内存或存储空间。

336
00:11:27,740 --> 00:11:28,990
0,300 300,585 585,750 750,930 930,1250
memory or so much storage.|
|

337
00:11:30,250 --> 00:11:31,785
0,400 480,800 800,965 965,1205 1205,1535
So how do we actually
那么，我们如何训练网络来学习这个隐变量向量呢，

338
00:11:31,785 --> 00:11:33,405
0,255 255,435 435,695 925,1305 1305,1620
train the network to learn

339
00:11:33,405 --> 00:11:35,980
0,240 240,570 570,840 840,1265
this latent variable vector,|
|

340
00:11:36,270 --> 00:11:37,565
0,320 320,515 515,770 770,980 980,1295
since we don't have training
由于我们没有训练数据，

341
00:11:37,565 --> 00:11:39,680
0,330 330,615 615,1065 1065,1715 1735,2115
data,| we can't explicitly observe
|我们不能显式地观察这些潜在的变量 z ，

342
00:11:39,680 --> 00:11:42,050
0,300 300,645 645,1110 1110,1430 2050,2370
these latent variables x,| we
|我们需要做一些更聪明的事情，

343
00:11:42,050 --> 00:11:43,610
0,240 240,420 420,615 615,950 1240,1560
need to do something more

344
00:11:43,610 --> 00:11:44,760
0,320
clever,|
|

345
00:11:44,770 --> 00:11:46,280
0,305 305,470 470,665 665,1220 1220,1510
what the {autoencoder -} does
自编码器所做的是，

346
00:11:46,330 --> 00:11:48,870
0,395 395,790 1230,1930 1980,2270 2270,2540
is| it builds a way
|它建立一种方法，将这个隐变量向量解码回到原始数据空间，

347
00:11:48,870 --> 00:11:51,320
0,315 315,1100 1240,1640 1660,2160 2160,2450
to decode this latent variable

348
00:11:51,340 --> 00:11:53,475
0,430 990,1370 1370,1730 1730,1955 1955,2135
vector back up to the

349
00:11:53,475 --> 00:11:55,950
0,335 385,780 780,1175 1825,2190 2190,2475
original data space,| trying to
|试图从压缩的、高效的隐编码中重建原始图像。

350
00:11:55,950 --> 00:11:58,395
0,720 720,1035 1035,1350 1350,1730 2140,2445
reconstruct the original image from

351
00:11:58,395 --> 00:12:00,830
0,240 240,785 925,1325 1345,1800 1800,2435
that compressed, efficient latent encoding.|
|

352
00:12:01,480 --> 00:12:02,865
0,305 305,560 560,910 930,1205 1205,1385
And once again, we can
再次，我们可以使用一系列神经网络层，

353
00:12:02,865 --> 00:12:04,170
0,270 270,525 525,795 795,1050 1050,1305
use a series of neural

354
00:12:04,170 --> 00:12:06,555
0,255 255,860 1180,1455 1455,1650 1650,2385
network layers,| such as convolutional
|比如卷积层，完全连接层，

355
00:12:06,555 --> 00:12:08,880
0,450 450,825 825,1185 1185,1715 2035,2325
layers, fully connected layers,| but
|但现在要从低维空间向上映射回输入空间，

356
00:12:08,880 --> 00:12:10,755
0,290 310,630 630,950 970,1370 1570,1875
now to map back from

357
00:12:10,755 --> 00:12:12,780
0,210 210,480 480,1110 1110,1445 1675,2025
that lower dimensional space back

358
00:12:12,780 --> 00:12:14,510
0,590 610,885 885,1125 1125,1410 1410,1730
upwards to the input space,|
|

359
00:12:15,880 --> 00:12:18,890
0,400 750,1340 1340,1610 1610,2560 2610,3010
this generates a reconstructed output,
这会生成重建的输出，我们可以将其表示为 x hat ，

360
00:12:18,910 --> 00:12:20,175
0,305 305,470 470,620 620,950 950,1265
which we can denote as

361
00:12:20,175 --> 00:12:22,220
0,345 345,695 1225,1560 1560,1815 1815,2045
x hat,| since it's an
|因为它是原始输入数据的不完美重建。

362
00:12:22,270 --> 00:12:24,920
0,580 630,1390 1410,1760 1760,2110 2250,2650
imperfect reconstruction of our original

363
00:12:25,120 --> 00:12:26,420
0,305 305,610
input data.|
|

364
00:12:27,070 --> 00:12:29,295
0,305 305,545 545,830 830,1180 1920,2225
To train this network,| all
要训练这个网络，|我们所要做的就是，

365
00:12:29,295 --> 00:12:30,380
0,180 180,360 360,525 525,735 735,1085
we have to do is|
|

366
00:12:30,490 --> 00:12:33,360
0,400 660,1060 1230,1610 1610,2290 2550,2870
compare the outputted reconstruction and
比较输出的重建数据和原始的输入数据，

367
00:12:33,360 --> 00:12:35,205
0,225 225,530 670,960 960,1250 1540,1845
the original input data| and
|然后说，我们如何使它们尽可能地相似，

368
00:12:35,205 --> 00:12:36,765
0,305 835,1125 1125,1245 1245,1365 1365,1560
say, how do we make

369
00:12:36,765 --> 00:12:38,180
0,225 225,495 495,810 810,1095 1095,1415
these as similar as possible,|
|

370
00:12:38,350 --> 00:12:39,830
0,275 275,455 455,980 980,1205 1205,1480
we can minimize the distance
我们可以最小化输入和重构输出之间的距离。

371
00:12:40,540 --> 00:12:42,090
0,290 290,580 630,980 980,1280 1280,1550
between that input and our

372
00:12:42,090 --> 00:12:43,820
0,890 910,1310
reconstructed output.|
|

373
00:12:44,080 --> 00:12:45,120
0,290 290,470 470,740 740,950 950,1040
So, for example, for an
例如，对于一幅图像，

374
00:12:45,120 --> 00:12:47,025
0,260 730,1005 1005,1280 1300,1650 1650,1905
image,| we can compare the
|我们可以比较输入数据和重建输出之间的像素差异，

375
00:12:47,025 --> 00:12:49,800
0,405 405,675 675,1055 2185,2505 2505,2775
pixel wise difference between the

376
00:12:49,800 --> 00:12:51,795
0,240 240,530 790,1080 1080,1230 1230,1995
input data and the reconstructed

377
00:12:51,795 --> 00:12:53,930
0,395 655,1005 1005,1755 1755,1890 1890,2135
output,| just subtracting the images
|只需将图像彼此相减并对差异平方，

378
00:12:53,980 --> 00:12:55,935
0,320 320,560 560,880 1230,1595 1595,1955
from one another and squaring

379
00:12:55,935 --> 00:12:58,260
0,270 270,575 895,1295 1555,1955 2035,2325
that difference| to capture the
|捕捉输入和重建之间的像素差异。

380
00:12:58,260 --> 00:13:00,960
0,420 420,710 970,1760 2110,2415 2415,2700
pixel wise divergence between the

381
00:13:00,960 --> 00:13:03,940
0,380 490,810 810,975 975,1550
input and the reconstruction.|
|

382
00:13:04,990 --> 00:13:06,285
0,395 395,695 695,890 890,1115 1115,1295
What I hope you'll notice
我希望你能注意和理解的是，

383
00:13:06,285 --> 00:13:07,695
0,270 270,605 685,1005 1005,1215 1215,1410
and appreciate is| in that
|在损失的定义中，

384
00:13:07,695 --> 00:13:10,080
0,305 445,720 720,870 870,1145 2095,2385
definition of the loss,| it
|它不需要任何标签，

385
00:13:10,080 --> 00:13:12,240
0,465 465,855 855,1230 1230,1850 1870,2160
doesn't require any labels,| {the,only
|损失的唯一组成部分是，

386
00:13:12,240 --> 00:13:14,090
0,290 610,1005 1005,1305 1305,1530 1530,1850
-} components of that loss

387
00:13:14,380 --> 00:13:15,900
0,290 290,500 500,820 990,1280 1280,1520
are| the original input data
|原始输入数据 x 和重建的输出 x hat 。

388
00:13:15,900 --> 00:13:18,560
0,350 850,1185 1185,1380 1380,2210 2260,2660
x and the reconstructed output

389
00:13:18,760 --> 00:13:20,100
0,350 350,700
x hat.|
|

390
00:13:21,280 --> 00:13:23,655
0,400 1050,1460 1460,1940 1940,2165 2165,2375
So I've simplified now this
所以我现在已经简化了这个图，

391
00:13:23,655 --> 00:13:25,760
0,465 465,785 1015,1545 1545,1785 1785,2105
diagram| by abstracting away those
|通过抽象出那些单独的神经网络层，

392
00:13:26,020 --> 00:13:27,825
0,380 380,725 725,1000 1020,1550 1550,1805
individual neural network layers| in
|在这个的编码器和解码器组件中。

393
00:13:27,825 --> 00:13:30,180
0,300 300,585 585,1200 1200,1505 1675,2355
both the encoder and decoder

394
00:13:30,180 --> 00:13:31,740
0,315 315,570 570,860
components of this.|
|

395
00:13:32,000 --> 00:13:34,000
0,320 320,640 960,1360 1410,1745 1745,2000
And again, this idea of
再次，这个不需要任何标签的想法回到了无监督学习的想法，

396
00:13:34,000 --> 00:13:35,845
0,320 340,750 750,1065 1065,1560 1560,1845
not requiring any labels gets

397
00:13:35,845 --> 00:13:37,380
0,300 300,510 510,750 750,1140 1140,1535
back to the idea of

398
00:13:37,490 --> 00:13:40,255
0,905 905,1240 1800,2200 2250,2525 2525,2765
unsupervised learning,| since what we've
|因为我们所做的是我们能够学习一个编码量，我们的隐变量，

399
00:13:40,255 --> 00:13:41,425
0,210 210,465 465,720 720,870 870,1170
done is we've been able

400
00:13:41,425 --> 00:13:43,620
0,270 270,540 540,855 855,1500 1500,2195
to learn an encoded quantity,

401
00:13:43,970 --> 00:13:46,135
0,305 305,650 650,1210 1680,1970 1970,2165
our latent variables,| that we
|我们无法在没有任何显式标签的情况下观察到，

402
00:13:46,135 --> 00:13:48,625
0,305 445,845 985,1385 1495,1895 2005,2490
cannot observe without any explicit

403
00:13:48,625 --> 00:13:50,395
0,485 685,990 990,1200 1200,1455 1455,1770
labels,| {all,we -} started from
|我们从原始数据本身开始。

404
00:13:50,395 --> 00:13:51,840
0,255 255,420 420,630 630,965 1045,1445
was the raw data itself.|
|

405
00:13:53,790 --> 00:13:55,150
0,305 305,530 530,785 785,1040 1040,1360
It turns out that as
事实证明，只要问题和答案出来，

406
00:13:55,260 --> 00:13:56,470
0,320 320,515 515,725 725,935 935,1210
as the question and answer

407
00:13:56,880 --> 00:13:59,135
0,290 290,580 780,1085 1085,2060 2060,2255
got out,| that dimensionality of
|隐空间的维度就会产生巨大的影响，

408
00:13:59,135 --> 00:14:00,875
0,165 165,525 525,845 1135,1470 1470,1740
the latent space has a

409
00:14:00,875 --> 00:14:02,860
0,335 865,1200 1200,1425 1425,1650 1650,1985
huge impact| on the quality
|对生成的重建的质量和信息瓶颈的压缩程度。

410
00:14:02,970 --> 00:14:05,560
0,305 305,500 500,790 930,1900 2190,2590
of the generated reconstructions and

411
00:14:05,640 --> 00:14:08,570
0,400 810,1400 1400,1780 1890,2285 2285,2930
how compressed that information bottleneck

412
00:14:08,570 --> 00:14:09,520
0,290
is.|
|

413
00:14:10,020 --> 00:14:11,420
0,335 335,905 905,1085 1085,1235 1235,1400
{Autoencoding -} is a form
自动编码是压缩的一种形式，

414
00:14:11,420 --> 00:14:13,160
0,180 180,620 880,1200 1200,1485 1485,1740
of compression,| and so the
|因此隐空间的维度越低，

415
00:14:13,160 --> 00:14:14,900
0,270 270,495 495,1380 1380,1575 1575,1740
lower the dimensionality of the

416
00:14:14,900 --> 00:14:17,165
0,300 300,620 1090,1395 1395,1700 1900,2265
latent space,| the less good
|我们的重建效果就越差，

417
00:14:17,165 --> 00:14:19,025
0,315 315,1205 1225,1515 1515,1710 1710,1860
our reconstructions are going to

418
00:14:19,025 --> 00:14:19,920
0,245
be,|
|

419
00:14:19,920 --> 00:14:21,980
0,195 195,390 390,680 880,1125 1125,2060
but the higher the dimensionality,|
但维度越高，|

420
00:14:22,000 --> 00:14:24,225
0,275 275,550 1320,1595 1595,1865 1865,2225
the more, the less efficient
编码的效率就越低。

421
00:14:24,225 --> 00:14:25,470
0,255 255,720 720,900 900,1095 1095,1245
that encoding is going to

422
00:14:25,470 --> 00:14:26,540
0,260
be.|
|

423
00:14:27,440 --> 00:14:29,425
0,335 335,545 545,920 920,1240 1650,1985
So to summarize this first
所以总结第一部分，

424
00:14:29,425 --> 00:14:30,880
0,335 565,945 945,1185 1185,1305 1305,1455
part,| this idea of an
|这个自编码器的想法是利用这个瓶颈，压缩的，隐藏的隐层，

425
00:14:30,880 --> 00:14:32,530
0,225 225,765 765,975 975,1305 1305,1650
{autoencoder -} is using this

426
00:14:32,530 --> 00:14:35,400
0,800 1000,1610 1810,2175 2175,2580 2580,2870
bottleneck, compressed, hidden latent layer|
|

427
00:14:35,750 --> 00:14:37,105
0,305 305,560 560,875 875,1160 1160,1355
to try to bring the
来试图降低网络，以学习数据的紧凑、高效的表示，

428
00:14:37,105 --> 00:14:38,485
0,270 270,645 645,900 900,1110 1110,1380
network down to learn a

429
00:14:38,485 --> 00:14:40,900
0,605 985,1335 1335,2070 2070,2280 2280,2415
compact, efficient representation of the

430
00:14:40,900 --> 00:14:41,900
0,260
data,|
|

431
00:14:42,000 --> 00:14:44,000
0,290 290,760 930,1265 1265,1550 1550,2000
we don't require any labels,|
我们不需要任何标签，|

432
00:14:44,000 --> 00:14:46,385
0,180 180,420 420,750 750,1610 2080,2385
{this,is -} completely unsupervised,| and
这完全是无监管的，|所以，通过这种方式，

433
00:14:46,385 --> 00:14:47,495
0,225 225,435 435,630 630,855 855,1110
so in this way,| we're
|我们能够自动对数据本身中的信息进行编码，

434
00:14:47,495 --> 00:14:49,780
0,210 210,575 685,1085 1135,1865 1885,2285
able to automatically encode information

435
00:14:50,820 --> 00:14:52,535
0,365 365,590 590,850 960,1360 1380,1715
within the data itself| to
|以了解这个隐空间，自编码信息，自编码数据。

436
00:14:52,535 --> 00:14:54,580
0,330 330,630 630,1035 1035,1355
learn this latent space,

437
00:14:55,010 --> 00:14:58,150
0,350 350,1030 1950,2315 2315,2600 2600,3140
{autoencoding -} information, {autoencoding -}

438
00:14:58,150 --> 00:14:59,240
0,290
data.|
|

439
00:14:59,960 --> 00:15:01,540
0,365 365,605 605,880 1020,1325 1325,1580
Now, this is a pretty
这是一个非常简单的模型，

440
00:15:01,540 --> 00:15:03,805
0,350 520,920 1660,1920 1920,2070 2070,2265
simple model,| and it turns
|事实证明，在实践中，自编码或自动编码的想法有一点扭曲，

441
00:15:03,805 --> 00:15:05,340
0,255 255,510 510,735 735,1055 1135,1535
out that in practice, this

442
00:15:05,540 --> 00:15:07,420
0,320 320,640 840,1130 1130,1700 1700,1880
idea of self encoding or

443
00:15:07,420 --> 00:15:09,010
0,225 225,920 1000,1305 1305,1470 1470,1590
{autoencoding -} has a bit

444
00:15:09,010 --> 00:15:10,170
0,105 105,270 270,600 600,870 870,1160
of a twist on it|
|

445
00:15:10,520 --> 00:15:11,935
0,335 335,590 590,800 800,1085 1085,1415
to allow us to actually
使我们能够实际生成新的示例，

446
00:15:11,935 --> 00:15:14,545
0,335 1165,1565 1615,2015 2155,2430 2430,2610
generate new examples,| that are
|这些示例不仅仅是输入数据本身的重建。

447
00:15:14,545 --> 00:15:16,855
0,240 240,575 895,1875 1875,2100 2100,2310
not only reconstructions of the

448
00:15:16,855 --> 00:15:18,360
0,210 210,485 535,935
input data itself.|
|

449
00:15:18,400 --> 00:15:19,515
0,290 290,485 485,680 680,920 920,1115
And this leads us to
这就引出了变分自编码器的概念，或 VAE ，

450
00:15:19,515 --> 00:15:21,285
0,135 135,425 535,840 840,1415 1435,1770
the concept of variational {autoencoders

451
00:15:21,285 --> 00:15:24,020
0,630 630,990 990,1305 1305,1625
-} or {VAEs -},|
|

452
00:15:24,620 --> 00:15:26,485
0,305 305,560 560,905 905,1220 1220,1865
with the traditional {autoencoder -}
对于我们刚刚看到的传统自编码器，

453
00:15:26,485 --> 00:15:28,240
0,195 195,360 360,540 540,845 1465,1755
that we just saw,| if
|如果我们更仔细地关注隐层，

454
00:15:28,240 --> 00:15:30,400
0,210 210,530 580,980 1420,1820 1900,2160
we pay closer attention to

455
00:15:30,400 --> 00:15:32,065
0,135 135,480 480,740 1180,1470 1470,1665
the latent layer,| which is
|它显示为橙色三文鱼色，

456
00:15:32,065 --> 00:15:33,925
0,240 240,450 450,725 955,1355 1405,1860
shown in that orange salmon

457
00:15:33,925 --> 00:15:36,385
0,305 1225,1545 1545,1935 1935,2190 2190,2460
color,| that latent layer is
|这个隐层只是神经网络中的一个普通层，

458
00:15:36,385 --> 00:15:38,350
0,210 210,435 435,755 1225,1625 1675,1965
just a normal layer in

459
00:15:38,350 --> 00:15:40,290
0,150 150,375 375,650 1000,1520 1540,1940
the neural network,| it's completely
|这完全是决定的，

460
00:15:40,490 --> 00:15:41,840
0,850
deterministic,|
|

461
00:15:41,850 --> 00:15:43,325
0,290 290,470 470,695 695,1030 1110,1475
what that means is once
这意味着，一旦我们训练了网络，

462
00:15:43,325 --> 00:15:44,705
0,360 360,540 540,705 705,965 1045,1380
we've trained the network,| once
|一旦设置了权重，

463
00:15:44,705 --> 00:15:46,790
0,195 195,465 465,645 645,965 1405,2085
the weights are set,| anytime
|任何时候我们传递给定的输入，

464
00:15:46,790 --> 00:15:48,410
0,240 240,560 670,945 945,1220 1330,1620
we pass a given input

465
00:15:48,410 --> 00:15:50,405
0,290 700,1100 1150,1440 1440,1725 1725,1995
in| and go back through
|并通过潜在层返回，解码返回，

466
00:15:50,405 --> 00:15:52,295
0,135 135,450 450,725 1195,1725 1725,1890
the latent layer, decode back

467
00:15:52,295 --> 00:15:53,645
0,305 505,855 855,1035 1035,1200 1200,1350
out,| we're going to get
|我们将得到完全相同的重建，

468
00:15:53,645 --> 00:15:55,925
0,210 210,515 595,945 945,1655 2005,2280
the same exact reconstruction,| {the,weights
|权重没有改变，

469
00:15:55,925 --> 00:15:58,270
0,285 285,585 585,875 1105,1575 1575,2345
-} aren't changing,| it's deterministic.|
|这是确定性的。|

470
00:15:59,230 --> 00:16:01,060
0,335 335,670
In contrast,|
相比之下，|

471
00:16:01,060 --> 00:16:04,780
0,590 610,945 945,1550 1600,2330 3340,3720
variational {autoencoders -}, VAEs introduce
变分自编码器 VAE 引入了随机性的元素，

472
00:16:04,780 --> 00:16:07,135
0,380 790,1185 1185,1470 1470,2115 2115,2355
an element of randomness,| a
|这是对自编码思想的概率扭曲，

473
00:16:07,135 --> 00:16:09,625
0,900 900,1265 1585,1905 1905,2220 2220,2490
probabilistic twist on this idea

474
00:16:09,625 --> 00:16:11,240
0,165 165,375 375,1055
of {autoencoding -},|
|

475
00:16:11,240 --> 00:16:12,455
0,285 285,480 480,705 705,975 975,1215
what this will allow us
这将允许我们生成新图像或新数据实例，

476
00:16:12,455 --> 00:16:13,535
0,165 165,360 360,570 570,810 810,1080
to do is to actually

477
00:16:13,535 --> 00:16:16,190
0,305 505,825 825,1145 1885,2285 2365,2655
generate new images, similar to

478
00:16:16,190 --> 00:16:18,605
0,285 285,680 760,1080 1080,1850 2140,2415
or new data instances,| that
|与输入数据相似，

479
00:16:18,605 --> 00:16:19,850
0,225 225,575 595,840 840,1020 1020,1245
are similar to the input

480
00:16:19,850 --> 00:16:22,070
0,290 760,1160 1240,1635 1635,1995 1995,2220
data,| but not forced to
|但不强制进行严格重建的实例。

481
00:16:22,070 --> 00:16:24,340
0,260 280,680 730,1700
be strict reconstructions.|
|

482
00:16:24,870 --> 00:16:26,555
0,320 320,640 840,1100 1100,1220 1220,1685
In practice, with a variational
在实践中，使用变分自编码器，

483
00:16:26,555 --> 00:16:28,985
0,300 300,965 1315,1815 1815,2130 2130,2430
{autoencoder -},| we've replaced that
|我们用随机采样操作取代了单一确定层，

484
00:16:28,985 --> 00:16:31,820
0,365 685,1395 1395,1685 1945,2345 2515,2835
single deterministic layer with a

485
00:16:31,820 --> 00:16:34,260
0,320 490,1160 1240,1640
random sampling operation,|
|

486
00:16:35,380 --> 00:16:37,440
0,400 810,1130 1130,1370 1370,1690 1710,2060
now, instead of learning just
现在，我们不是直接学习隐变量本身，

487
00:16:37,440 --> 00:16:39,320
0,210 210,495 495,975 975,1310 1480,1880
the latent variables directly themselves,|
|

488
00:16:39,760 --> 00:16:41,685
0,320 320,590 590,965 965,1240 1560,1925
for each latent variable, we
我们为每个隐变量定义平均值和标准差，

489
00:16:41,685 --> 00:16:43,335
0,315 315,555 555,845 1045,1395 1395,1650
define a mean and a

490
00:16:43,335 --> 00:16:45,735
0,285 285,905 1345,1665 1665,2175 2175,2400
standard deviation,| that captures a
|以捕获隐变量的概率分布。

491
00:16:45,735 --> 00:16:48,405
0,605 775,1175 1435,1835 2005,2310 2310,2670
probability distribution over that latent

492
00:16:48,405 --> 00:16:49,440
0,275
variable.|
|

493
00:16:50,440 --> 00:16:51,465
0,275 275,485 485,620 620,800 800,1025
What we've done is,| we've
我们所做的是，|我们已经从隐变量 z 的单一向量，

494
00:16:51,465 --> 00:16:52,605
0,165 165,330 330,480 480,780 780,1140
gone from a single vector

495
00:16:52,605 --> 00:16:54,660
0,195 195,510 510,785 895,1295 1735,2055
of latent variable z| to
|变成了均值 μ 的向量和标准差 σ 的向量，

496
00:16:54,660 --> 00:16:56,240
0,210 210,495 495,720 720,1010 1180,1580
a vector of means μ

497
00:16:56,650 --> 00:16:58,365
0,350 350,575 575,880 900,1300 1320,1715
and a vector of standard

498
00:16:58,365 --> 00:17:01,590
0,635 985,1565 1795,2160 2160,2955 2955,3225
deviations σ,| that parameterize the
|它将这些隐变量周围的概率分布参数化。

499
00:17:01,590 --> 00:17:04,245
0,560 700,1250 1480,1845 1845,2210 2230,2655
probability distributions around those latent

500
00:17:04,245 --> 00:17:05,360
0,545
variables.|
|

501
00:17:06,520 --> 00:17:07,605
0,290 290,470 470,680 680,890 890,1085
what this will allow us
这将允许我们现在做的是，

502
00:17:07,605 --> 00:17:08,840
0,165 165,300 300,480 480,785 835,1235
to do is| now sample
|使用这个随机性元素，这个概率元素进行采样，

503
00:17:09,580 --> 00:17:11,685
0,380 380,760 840,1240 1260,1535 1535,2105
using this element of randomness,

504
00:17:11,685 --> 00:17:13,785
0,195 195,480 480,765 765,1325 1825,2100
this element of probability,| to
|然后获得隐空间本身的概率表示。

505
00:17:13,785 --> 00:17:16,640
0,275 445,1050 1050,1320 1320,2115 2115,2855
then obtain a probabilistic representation

506
00:17:16,690 --> 00:17:18,350
0,395 395,650 650,980 980,1265 1265,1660
of the latent space itself.|
|

507
00:17:19,590 --> 00:17:20,765
0,305 305,470 470,695 695,965 965,1175
As you hopefully can tell,
希望你能看出来，

508
00:17:20,765 --> 00:17:22,070
0,240 240,435 435,660 660,1005 1005,1305
right,| this is very, very,
|这与自编码器本身非常相似，

509
00:17:22,070 --> 00:17:23,465
0,270 270,620 730,1035 1035,1200 1200,1395
very similar to the {autoencoder

510
00:17:23,465 --> 00:17:25,685
0,585 585,935 1375,1740 1740,2070 2070,2220
-} itself,| but we've just
|但我们刚刚增加了这个概率扭曲，

511
00:17:25,685 --> 00:17:28,010
0,285 285,585 585,1365 1365,1715 2035,2325
added this probabilistic twist,| where
|我们可以在中间空间采样，

512
00:17:28,010 --> 00:17:29,195
0,165 165,375 375,710 760,1020 1020,1185
we can sample in that

513
00:17:29,195 --> 00:17:32,045
0,750 750,1085 1735,2135 2335,2640 2640,2850
intermediate space| to get these
|得到这些隐变量的样本。

514
00:17:32,045 --> 00:17:34,100
0,435 435,630 630,945 945,1505
samples of latent variables.|
|

515
00:17:36,020 --> 00:17:37,220
0,400
Okay,
好的，现在，为了更深入地了解这是如何学习的，

516
00:17:37,350 --> 00:17:39,170
0,400 960,1265 1265,1460 1460,1625 1625,1820
now, to get a little

517
00:17:39,170 --> 00:17:40,510
0,225 225,495 495,735 735,975 975,1340
more into the depth of

518
00:17:40,620 --> 00:17:41,930
0,305 305,485 485,740 740,1025 1025,1310
how this is actually learned,|
|

519
00:17:41,930 --> 00:17:43,270
0,270 270,435 435,675 675,990 990,1340
how this is actually trained,|
这是如何训练的，|

520
00:17:44,220 --> 00:17:46,745
0,400 420,935 935,1100 1100,1390 2010,2525
with defining the VAE,| we've
有了定义 VAE ，|我们已经消除了这种确定性的性质，

521
00:17:46,745 --> 00:17:49,655
0,585 585,965 1015,1740 1740,2045 2635,2910
eliminated this deterministic nature| to
|现在这些编码器和解码器都是概率的，

522
00:17:49,655 --> 00:17:51,365
0,270 270,585 585,825 825,1485 1485,1710
now have these encoders and

523
00:17:51,365 --> 00:17:54,120
0,570 570,810 810,1145 1285,2315
decoders that are probabilistic,|
|

524
00:17:54,420 --> 00:17:57,215
0,290 290,1030 1200,1600 2130,2585 2585,2795
the encoder is computing a
编码器计算的隐变量 z 的概率分布，

525
00:17:57,215 --> 00:17:59,930
0,605 715,1115 1885,2205 2205,2385 2385,2715
probability distribution of the latent

526
00:17:59,930 --> 00:18:02,450
0,290 400,800 1060,1460 1960,2250 2250,2520
variable z,| given input data
|给定输入数据 x ，

527
00:18:02,450 --> 00:18:03,680
0,380
x,|
|

528
00:18:03,680 --> 00:18:05,320
0,225 225,420 420,1035 1035,1320 1320,1640
while the decoder is doing
而解码器进行相反的操作，

529
00:18:05,460 --> 00:18:07,295
0,245 245,670 1050,1385 1385,1595 1595,1835
the inverse,| trying to learn
|试图学习输入数据空间中的概率分布，

530
00:18:07,295 --> 00:18:10,310
0,270 270,875 985,1385 2365,2745 2745,3015
a probability distribution back in

531
00:18:10,310 --> 00:18:12,370
0,270 270,540 540,825 825,1220 1660,2060
the input data space,| given
|给定隐变量 z ，

532
00:18:12,690 --> 00:18:14,820
0,260 260,605 605,1205 1205,1600
the latent variables z,|
|

533
00:18:15,200 --> 00:18:16,765
0,260 260,485 485,845 845,1240 1260,1565
and we define separate sets
并且我们定义了单独的权重集合 Φ 和 θ ，

534
00:18:16,765 --> 00:18:19,350
0,195 195,665 1165,1655 1765,2085 2085,2585
of weights Φ and θ,|
|

535
00:18:19,430 --> 00:18:21,460
0,365 365,695 695,1060 1080,1480 1530,2030
to define the network weights
以定义用于 v 的编码器和解码器组件的网络权重。

536
00:18:21,460 --> 00:18:23,520
0,210 210,375 375,1095 1095,1395 1395,2060
for the encoder and decoder

537
00:18:23,900 --> 00:18:26,120
0,400 600,920 920,1115 1115,1390
components of the v.|
|

538
00:18:27,370 --> 00:18:29,400
0,245 245,490 960,1360 1530,1835 1835,2030
All right, so when we
好的，我们现在，

539
00:18:29,400 --> 00:18:30,830
0,240 240,590 670,960 960,1140 1140,1430
get now| to how we
|如何优化和学习 VAE 中的网络权重，

540
00:18:30,850 --> 00:18:33,180
0,400 750,1220 1220,1520 1520,1840 2070,2330
actually optimize and learn the

541
00:18:33,180 --> 00:18:34,520
0,255 255,690 690,855 855,990 990,1340
network weights in the VAE,|
|

542
00:18:35,440 --> 00:18:36,750
0,320 320,575 575,815 815,1055 1055,1310
first step is to define
第一步是定义损失函数，

543
00:18:36,750 --> 00:18:37,995
0,180 180,375 375,690 690,960 960,1245
a loss function, right,| that's
|这是训练神经网络的核心要素，

544
00:18:37,995 --> 00:18:39,675
0,180 180,485 655,1055 1135,1440 1440,1680
the core element to training

545
00:18:39,675 --> 00:18:41,040
0,210 210,435 435,695
a neural network,|
|

546
00:18:41,490 --> 00:18:42,725
0,320 320,620 620,890 890,1085 1085,1235
our loss is going to
我们的损失将是数据的函数和神经网络权重的函数，

547
00:18:42,725 --> 00:18:43,730
0,165 165,375 375,630 630,870 870,1005
be a function of the

548
00:18:43,730 --> 00:18:45,740
0,260 550,930 930,1185 1185,1460 1720,2010
data and a function of

549
00:18:45,740 --> 00:18:47,300
0,165 165,375 375,650 700,1250 1270,1560
the neural network weights,| just
|就像以前一样，

550
00:18:47,300 --> 00:18:48,440
0,180 180,470
like before,|
|

551
00:18:48,730 --> 00:18:50,270
0,380 380,665 665,860 860,1145 1145,1540
but we have these two
但我们有这两个组成部分，两个条目定义了我们的 VAE 损失，

552
00:18:50,440 --> 00:18:52,215
0,400 510,875 875,1175 1175,1460 1460,1775
components, these two terms that

553
00:18:52,215 --> 00:18:54,260
0,270 270,480 480,825 825,1175
define our VAE loss,|
|

554
00:18:54,270 --> 00:18:55,940
0,380 380,665 665,860 860,1025 1025,1670
first, we see the reconstruction
首先，我们看到重建损失，像以前一样，

555
00:18:55,940 --> 00:18:58,130
0,380 490,795 795,990 990,1280 1900,2190
loss just like before,| where
|目标是捕捉我们的输入数据和重建输出之间的差异，

556
00:18:58,130 --> 00:18:59,110
0,150 150,300 300,480 480,675 675,980
the goal is to capture

557
00:18:59,130 --> 00:19:00,695
0,260 260,520 750,1025 1025,1295 1295,1565
the difference between our input

558
00:19:00,695 --> 00:19:02,920
0,275 565,870 870,1035 1035,1775 1825,2225
data and the reconstructed output,|
|

559
00:19:03,680 --> 00:19:05,215
0,320 320,640 690,965 965,1100 1100,1535
and now for the VAE,
现在对于 VAE ，我们引入了损失的第二个项，

560
00:19:05,215 --> 00:19:06,910
0,395 445,795 795,1050 1050,1355 1375,1695
we've introduced a second term

561
00:19:06,910 --> 00:19:08,470
0,180 180,285 285,530 1060,1335 1335,1560
to the loss,| what we
|我们称之为正则化项，

562
00:19:08,470 --> 00:19:10,940
0,315 315,555 555,1340 1390,1790
call the regularization term,|
|

563
00:19:11,210 --> 00:19:12,715
0,380 380,755 755,980 980,1280 1280,1505
often, you'll maybe even see
通常，你甚至可能会看到这被称为 VAE 损失，

564
00:19:12,715 --> 00:19:14,110
0,270 270,615 615,870 870,1140 1140,1395
this referred to as a

565
00:19:14,110 --> 00:19:16,315
0,390 390,680 1570,1845 1845,2040 2040,2205
VAE loss| and we'll go
|我们将进入描述这个正则化术语的含义和它在做什么。

566
00:19:16,315 --> 00:19:18,745
0,305 1015,1350 1350,1470 1470,1745 1885,2430
into, we'll go into describing

567
00:19:18,745 --> 00:19:22,110
0,255 255,465 465,2375 2395,2795 2965,3365
what this regularization term means

568
00:19:22,190 --> 00:19:23,190
0,305 305,470 470,575 575,710 710,1000
and what {it's -} doing.|
|

569
00:19:25,420 --> 00:19:26,420
0,260 260,380 380,530 530,710 710,1000
To do that and to
要做到这一点，并理解，记住，

570
00:19:26,440 --> 00:19:29,250
0,400 450,850 870,1270 1320,1720 2550,2810
understand, remember and, and keep

571
00:19:29,250 --> 00:19:30,945
0,135 135,410 850,1185 1185,1425 1425,1695
in mind,| that in all
|在所有神经网络操作中，

572
00:19:30,945 --> 00:19:33,020
0,315 315,575 865,1265 1435,1755 1755,2075
neural network operations,| our goal
|我们的目标是尝试优化网络权重，

573
00:19:33,130 --> 00:19:34,785
0,305 305,530 530,770 770,1090 1140,1655
is to try to optimize

574
00:19:34,785 --> 00:19:36,950
0,240 240,485 565,1115 1345,1745 1765,2165
the network weights| with respect
|相对于数据，从而将这种客观损失降至最低，

575
00:19:37,420 --> 00:19:39,405
0,350 350,560 560,820 1260,1655 1655,1985
to the data, with respect

576
00:19:39,405 --> 00:19:41,360
0,255 255,815 835,1215 1215,1575 1575,1955
to minimizing this objective loss,|
|

577
00:19:42,140 --> 00:19:43,415
0,195 195,390 390,630 630,990 990,1275
and so here we're concerned
所以这里我们关注的是网络权重 Φ 和 θ ，

578
00:19:43,415 --> 00:19:45,515
0,210 210,345 345,605 625,1205 1645,2100
with the network weights Φ

579
00:19:45,515 --> 00:19:47,150
0,315 315,735 735,1020 1020,1335 1335,1635
and θ,| that define the
|它们定义了编码器和解码器的权重。

580
00:19:47,150 --> 00:19:49,000
0,390 390,570 570,735 735,1455 1455,1850
weights of the encoder and

581
00:19:49,080 --> 00:19:50,480
0,260 260,850
the decoder.|
|

582
00:19:51,300 --> 00:19:52,870
0,400 420,755 755,1025 1025,1265 1265,1570
We consider these two terms,|
我们考虑这两个项目，|

583
00:19:53,550 --> 00:19:56,560
0,400 510,770 770,1370 1370,1750 2610,3010
{first\,,the -} reconstruction loss,| again
首先是重建损失，|再次，重建损失非常相似，和以前一样，

584
00:19:56,610 --> 00:19:58,640
0,260 260,860 860,1190 1190,1540 1680,2030
the reconstruction loss is very,

585
00:19:58,640 --> 00:20:00,190
0,300 300,650 790,1080 1080,1260 1260,1550
very similar, same as before,|
|

586
00:20:00,810 --> 00:20:01,700
0,260 260,410 410,590 590,740 740,890
you can think of it
你可以认为它是误差或可能性，

587
00:20:01,700 --> 00:20:03,035
0,285 285,555 555,830 880,1170 1170,1335
as the error or the

588
00:20:03,035 --> 00:20:05,750
0,695 865,1245 1245,1625 1855,2505 2505,2715
likelihood,| that effectively captures the
|有效地捕捉到你的输入和输出之间的差异，

589
00:20:05,750 --> 00:20:08,000
0,290 580,930 930,1280 1300,1700 1900,2250
difference between your input and

590
00:20:08,000 --> 00:20:09,965
0,350 400,920 1240,1560 1560,1785 1785,1965
your outputs,| and again, we
|同样，我们可以以一种无监督的方式进行训练，

591
00:20:09,965 --> 00:20:11,105
0,195 195,450 450,750 750,990 990,1140
can train this in an

592
00:20:11,105 --> 00:20:13,550
0,765 765,1115 1315,1710 1710,2115 2115,2445
unsupervised way,| not requiring any
|不需要任何标签来迫使隐空间和网络

593
00:20:13,550 --> 00:20:15,530
0,560 850,1140 1140,1395 1395,1635 1635,1980
labels to force the latent

594
00:20:15,530 --> 00:20:17,345
0,320 790,1080 1080,1215 1215,1460 1540,1815
space and the network| to
|学习如何有效地重建输入数据。

595
00:20:17,345 --> 00:20:19,330
0,240 240,495 495,720 720,1055 1225,1985
learn how to effectively reconstruct

596
00:20:19,350 --> 00:20:20,680
0,350 350,575 575,850
the input data.|
|

597
00:20:21,880 --> 00:20:23,580
0,275 275,530 530,830 830,1010 1010,1700
The second term, the regularization
第二个条目，正规化条目，

598
00:20:23,580 --> 00:20:24,750
0,255 255,435 435,675 675,915 915,1170
term,| is now where things
|现在是事情变得更有趣的地方，

599
00:20:24,750 --> 00:20:26,145
0,270 270,465 465,630 630,920 1030,1395
get a bit more interesting,|
|

600
00:20:26,145 --> 00:20:28,080
0,300 300,825 825,945 945,1205 1585,1935
{so,let's -} go on into
所以，让我们更详细地讨论这个问题。

601
00:20:28,080 --> 00:20:28,965
0,285 285,480 480,600 600,735 735,885
this in a little bit

602
00:20:28,965 --> 00:20:30,560
0,270 270,665
more detail.|
|

603
00:20:31,000 --> 00:20:32,480
0,305 305,500 500,695 695,920 920,1480
Because we have this probability
因为我们有这个概率分布，

604
00:20:32,650 --> 00:20:35,595
0,400 1740,2120 2120,2465 2465,2705 2705,2945
distribution| and we're trying to
|我们试图计算这个编码，然后再解码回来，

605
00:20:35,595 --> 00:20:38,385
0,515 1525,1830 1830,2415 2415,2580 2580,2790
compute this encoding and then

606
00:20:38,385 --> 00:20:40,280
0,510 510,690 690,995
decode back up,|
|

607
00:20:40,320 --> 00:20:43,760
0,365 365,620 620,910 1650,2410 3150,3440
as part of regularizing,| we
作为正规化的一部分，|我们希望对隐分布采取这种推断，

608
00:20:43,760 --> 00:20:45,395
0,210 210,465 465,765 765,1050 1050,1635
want to take that inference

609
00:20:45,395 --> 00:20:48,665
0,395 655,930 930,1445 1885,2285 2965,3270
over the latent distribution| and
|并约束它表现良好，如果你愿意的话，

610
00:20:48,665 --> 00:20:50,260
0,510 510,705 705,870 870,1245 1245,1595
constrain it to behave nicely,

611
00:20:50,340 --> 00:20:52,085
0,275 275,410 410,670 1320,1580 1580,1745
if you will,| {the,way -}
|我们这样做的方法是，

612
00:20:52,085 --> 00:20:53,660
0,195 195,485 595,990 990,1305 1305,1575
we do that is| we
|我们把我们所说的先验放在隐分布上，

613
00:20:53,660 --> 00:20:54,755
0,285 285,495 495,660 660,855 855,1095
place what we call a

614
00:20:54,755 --> 00:20:56,320
0,335 385,675 675,825 825,1215 1215,1565
prior on the latent distribution,|
|

615
00:20:57,090 --> 00:20:58,250
0,305 305,500 500,665 665,875 875,1160
and what this is is
这是一些关于隐变量空间可能是什么样子的初始假设或猜测，

616
00:20:58,250 --> 00:21:00,610
0,350 430,810 810,1740 1740,2025 2025,2360
some initial hypothesis or guess

617
00:21:01,080 --> 00:21:02,560
0,350 350,620 620,845 845,1205 1205,1480
about what that latent variable

618
00:21:02,670 --> 00:21:04,320
0,395 395,680 680,890 890,1210
space may look like,|
|

619
00:21:04,530 --> 00:21:06,170
0,335 335,650 650,1030 1110,1415 1415,1640
this helps us and helps
这有助于我们和网络实施一个隐空间，

620
00:21:06,170 --> 00:21:08,405
0,180 180,440 850,1250 1270,1670 1960,2235
the network to enforce a

621
00:21:08,405 --> 00:21:09,995
0,330 330,555 555,810 810,1145 1255,1590
latent space| that roughly tries
|尝试遵循这种先前的分布，

622
00:21:09,995 --> 00:21:11,680
0,240 240,540 540,885 885,1235 1285,1685
to follow this prior distribution,|
|

623
00:21:13,050 --> 00:21:14,510
0,290 290,500 500,785 785,1055 1055,1460
and this prior is denoted
这个先验信息表示为 p(z) ，

624
00:21:14,510 --> 00:21:16,420
0,380 460,810 810,1065 1065,1370 1510,1910
as p of z, right,|
|

625
00:21:17,470 --> 00:21:20,090
0,365 365,730 810,1210 1710,2270 2270,2620
that term D, that's effectively
那项 D ，就是正则化项，

626
00:21:20,200 --> 00:21:22,965
0,260 260,970 990,1390 1890,2300 2300,2765
the regularization term,| it's capturing
|它捕捉到了我们对隐变量的编码

627
00:21:22,965 --> 00:21:25,050
0,165 165,455 775,1125 1125,1395 1395,2085
a distance between our encoding

628
00:21:25,050 --> 00:21:26,930
0,195 195,330 330,645 645,1220 1480,1880
of the latent variables| and
|和我们先前关于隐空间结构应该是什么样子的假设之间的距离，

629
00:21:26,950 --> 00:21:29,390
0,400 480,860 860,1760 1760,2090 2090,2440
our prior hypothesis about what

630
00:21:29,770 --> 00:21:31,185
0,305 305,605 605,875 875,1040 1040,1415
the structure of that latent

631
00:21:31,185 --> 00:21:32,640
0,240 240,435 435,600 600,905
space should look like,|
|

632
00:21:32,830 --> 00:21:33,975
0,290 290,530 530,755 755,935 935,1145
so over the course of
所以，在训练过程中，

633
00:21:33,975 --> 00:21:35,540
0,300 300,660 660,840 840,1145 1165,1565
training,| we're trying to enforce
|我们努力执行这一点，

634
00:21:35,950 --> 00:21:37,185
0,320 320,530 530,695 695,875 875,1235
that,| {each,of -} those latent
|这些隐变量中的每一个都适应一个类似于先验的概率分布。

635
00:21:37,185 --> 00:21:39,900
0,605 775,1440 1440,1605 1605,1895 2125,2715
variables adapts a {}, adapts

636
00:21:39,900 --> 00:21:42,740
0,150 150,740 910,1310 1750,2330 2440,2840
a probability distribution that's similar

637
00:21:43,300 --> 00:21:44,980
0,320 320,590 590,940
to that prior.|
|

638
00:21:46,790 --> 00:21:49,330
0,320 320,640 660,1040 1040,1420 2160,2540
A common choice when training
在训练 VAE 和开发这些模型时，一个常见的选择是，

639
00:21:49,330 --> 00:21:51,090
0,450 450,765 765,1100 1150,1455 1455,1760
VAEs and developing these models

640
00:21:51,200 --> 00:21:53,590
0,400 720,1120 1440,1835 1835,2105 2105,2390
is| to enforce the latent
|将隐变量强制为大致标准的正态分布，

641
00:21:53,590 --> 00:21:55,980
0,525 525,735 735,1010 1030,1430 1990,2390
variables to be roughly standard

642
00:21:56,150 --> 00:21:59,370
0,400 510,1145 1145,1630 2520,2870 2870,3220
normal gouging distributions,| meaning that
|这意味着它们以均值零为中心，

643
00:21:59,720 --> 00:22:01,675
0,260 260,455 455,1030 1110,1510 1560,1955
they are centered around mean

644
00:22:01,675 --> 00:22:02,920
0,395 505,780 780,945 945,1095 1095,1245
zero| and they have a
|并且它们的标准差为一，

645
00:22:02,920 --> 00:22:04,700
0,255 255,720 720,945 945,1250
standard deviation of one,|
|

646
00:22:05,300 --> 00:22:06,460
0,305 305,530 530,770 770,995 995,1160
what this allows us to
这让我们能够做的是，

647
00:22:06,460 --> 00:22:08,580
0,195 195,435 435,740 1180,1580 1720,2120
do is| to encourage the
|鼓励编码者将隐变量大致放在一个中心空间周围，

648
00:22:08,720 --> 00:22:10,330
0,710 710,905 905,1100 1100,1265 1265,1610
encoder to put the latent

649
00:22:10,330 --> 00:22:13,230
0,590 910,1310 1480,1880 1930,2295 2295,2900
variables roughly around a centered

650
00:22:13,370 --> 00:22:16,500
0,400 1170,1670 1670,1865 1865,2495 2495,3130
space,| distributing the encoding smoothly,|
|平稳地分布编码，|

651
00:22:16,910 --> 00:22:19,360
0,400 660,1060 1650,1925 1925,2225 2225,2450
so that we don't get
这样我们就不会从这个平滑的空间中得到太多的偏离，

652
00:22:19,360 --> 00:22:21,310
0,240 240,495 495,1340 1360,1710 1710,1950
too much divergence away from

653
00:22:21,310 --> 00:22:23,040
0,290 340,705 705,1070
that smooth space,|
|

654
00:22:23,240 --> 00:22:24,715
0,335 335,670 750,1085 1085,1310 1310,1475
which can occur if the
如果网络试图作弊并试图简单地记住数据，就会发生这种情况，

655
00:22:24,715 --> 00:22:26,455
0,275 295,630 630,870 870,1235 1405,1740
network tries to cheat and

656
00:22:26,455 --> 00:22:28,360
0,240 240,420 420,695 1075,1695 1695,1905
try to simply memorize the

657
00:22:28,360 --> 00:22:29,760
0,260
data,|
|

658
00:22:30,240 --> 00:22:32,255
0,350 350,785 785,920 920,1450 1620,2015
by placing the gaussian standard
通过将高斯标准正态先验放在隐空间上，

659
00:22:32,255 --> 00:22:33,995
0,395 415,815 895,1230 1230,1425 1425,1740
normal prior on the latent

660
00:22:33,995 --> 00:22:36,100
0,305 895,1170 1170,1410 1410,1740 1740,2105
space,| we can define a
|我们可以定义一个具体的数学术语，

661
00:22:36,300 --> 00:22:39,080
0,380 380,1220 1220,1540 1890,2225 2225,2780
concrete mathematical term,| that captures
|捕捉编码的隐变量与该先验之间的距离，

662
00:22:39,080 --> 00:22:42,365
0,320 850,1250 1570,1845 1845,2540 2980,3285
the distance the divergence between

663
00:22:42,365 --> 00:22:44,570
0,300 300,1020 1020,1380 1380,1905 1905,2205
our encoded latent variables and

664
00:22:44,570 --> 00:22:45,860
0,240 240,560
this prior,|
|

665
00:22:45,930 --> 00:22:47,465
0,395 395,665 665,875 875,1205 1205,1535
and this is called the
这称为 KL 散度，

666
00:22:47,465 --> 00:22:50,660
0,270 270,605 655,1475 2395,2795 2815,3195
{KL -} divergence,| when our
|当我们的前项是标准正态时，

667
00:22:50,660 --> 00:22:52,090
0,360 360,615 615,780 780,1050 1050,1430
prior is a standard normal,|
|

668
00:22:52,680 --> 00:22:54,905
0,400 780,1190 1190,1775 1775,2030 2030,2225
{the,KL -} divergence takes the
KL 散度采用我在屏幕上显示的方程的形式，

669
00:22:54,905 --> 00:22:56,135
0,240 240,525 525,750 750,1020 1020,1230
form of the equation that

670
00:22:56,135 --> 00:22:57,485
0,210 210,485 745,1020 1020,1170 1170,1350
I'm showing up on the

671
00:22:57,485 --> 00:23:00,095
0,305 1225,1625 1975,2235 2235,2385 2385,2610
screen,| but what I want
|但我想让你们真正[逃脱]的是，

672
00:23:00,095 --> 00:23:01,810
0,210 210,345 345,605 1105,1410 1410,1715
you to really get away

673
00:23:02,100 --> 00:23:03,490
0,320 320,545 545,800 800,1070 1070,1390
come away with is that,|
|

674
00:23:03,840 --> 00:23:05,830
0,305 305,610 810,1210 1230,1610 1610,1990
the concept of trying to
试着让事情变得平滑，捕捉这种差异的概念，

675
00:23:06,240 --> 00:23:07,840
0,335 335,605 605,920 920,1250 1250,1600
smooth things out and to

676
00:23:08,910 --> 00:23:10,985
0,400 420,710 710,1420 1590,1880 1880,2075
capture this divergence| and this
|以及先验编码和隐编码之间的差异，

677
00:23:10,985 --> 00:23:12,485
0,305 505,780 780,960 960,1245 1245,1500
difference between the prior and

678
00:23:12,485 --> 00:23:14,210
0,135 135,450 450,1085 1165,1485 1485,1725
the latent encoding,| is all
|这就是 KL 术语试图捕捉的全部。

679
00:23:14,210 --> 00:23:15,680
0,240 240,645 645,945 945,1230 1230,1470
this kl term is trying

680
00:23:15,680 --> 00:23:17,080
0,195 195,470
to capture.|
|

681
00:23:17,680 --> 00:23:18,585
0,335 335,590 590,665 665,770 770,905
So it's a bit of
所以这有点数学，我承认这一点，

682
00:23:18,585 --> 00:23:20,700
0,270 270,665 775,1175 1195,1595 1735,2115
math, and I I acknowledge

683
00:23:20,700 --> 00:23:22,620
0,380 610,1010 1180,1485 1485,1665 1665,1920
that,| {but,what -} I want
|但我接下来想要探讨的是，

684
00:23:22,620 --> 00:23:23,805
0,285 285,540 540,750 750,945 945,1185
to next go into is|
|

685
00:23:23,805 --> 00:23:25,340
0,270 270,555 555,765 765,915 915,1535
really what is the intuition
这个正规化操作背后的直觉是什么，

686
00:23:25,600 --> 00:23:28,820
0,335 335,590 590,1420 1650,2050 2820,3220
behind this regularization operation,| why
|我们为什么要这样做，

687
00:23:29,020 --> 00:23:30,060
0,260 260,395 395,560 560,785 785,1040
do we do this| and
|为什么正常的先验，特别是对 VAE 。

688
00:23:30,060 --> 00:23:31,350
0,255 255,465 465,615 615,890 940,1290
why does the normal prior,

689
00:23:31,350 --> 00:23:33,570
0,315 315,680 1000,1380 1380,1760 1930,2220
in particular, work effectively for

690
00:23:33,570 --> 00:23:34,600
0,290
VAEs.|
|

691
00:23:34,960 --> 00:23:36,530
0,275 275,650 650,965 965,1235 1235,1570
So let's consider what properties
所以，让我们考虑我们希望隐空间采用什么性质，

692
00:23:36,730 --> 00:23:38,630
0,305 305,610 840,1190 1190,1580 1580,1900
we want our latent space

693
00:23:38,650 --> 00:23:40,680
0,400 570,1150 1290,1640 1640,1865 1865,2030
to adopt| and for this
|以及为了实现这种正规化，

694
00:23:40,680 --> 00:23:42,480
0,690 690,990 990,1310
regularization to achieve,|
|

695
00:23:43,040 --> 00:23:44,410
0,275 275,550 600,890 890,1100 1100,1370
the first is this goal
首先是这个连续性的目标，

696
00:23:44,410 --> 00:23:47,875
0,350 490,1190 1870,2175 2175,2660 3160,3465
of continuity,| {we,don't and -}
|我们没有，我们所说的连续性的意思是，

697
00:23:47,875 --> 00:23:49,405
0,210 210,390 390,555 555,845 895,1530
what we mean by continuity

698
00:23:49,405 --> 00:23:50,755
0,195 195,485 715,1005 1005,1155 1155,1350
is that,| if there are
|如果在隐空间中有一些点很接近，

699
00:23:50,755 --> 00:23:51,990
0,270 270,465 465,585 585,915 915,1235
points in the latent space

700
00:23:52,190 --> 00:23:54,970
0,245 245,425 425,755 755,1150 1980,2780
that are close together,| ideally
|理想情况下是在解码之后，

701
00:23:54,970 --> 00:23:57,360
0,350 400,1190 1390,1695 1695,1995 1995,2390
after decoding,| we should recover
|我们应该恢复两个内容相似的重建，

702
00:23:57,380 --> 00:23:59,425
0,290 290,1100 1100,1355 1355,1670 1670,2045
two reconstructions that are similar

703
00:23:59,425 --> 00:24:01,240
0,315 315,635 1015,1320 1320,1560 1560,1815
in content,| that make sense
|这有意义的，它们离得很近。

704
00:24:01,240 --> 00:24:02,880
0,210 210,450 450,690 690,1070
that they're close together.|
|

705
00:24:03,310 --> 00:24:04,965
0,290 290,580 600,920 920,1240 1380,1655
The second key property is
第二个关键属性是这种完整性的概念，

706
00:24:04,965 --> 00:24:07,220
0,275 295,600 600,905 925,1535
this idea of completeness,|
|

707
00:24:07,340 --> 00:24:08,290
0,260 260,470 470,605 605,800 800,950
we don't want there to
我们不希望在隐空间中有空隙，

708
00:24:08,290 --> 00:24:09,325
0,165 165,465 465,705 705,825 825,1035
be gaps in the latent

709
00:24:09,325 --> 00:24:10,795
0,365 655,945 945,1155 1155,1320 1320,1470
space,| {we,want -} to be
|我们希望能够从隐空间解码和采样，

710
00:24:10,795 --> 00:24:12,730
0,255 255,480 480,1050 1050,1325 1555,1935
able to decode and sample

711
00:24:12,730 --> 00:24:13,855
0,270 270,420 420,645 645,945 945,1125
from the latent space| in
|以一种流畅和连接的方式。

712
00:24:13,855 --> 00:24:14,820
0,105 105,270 270,435 435,630 630,965
a way that is smooth

713
00:24:15,050 --> 00:24:16,000
0,275 275,410 410,590 590,755 755,950
and a way that is

714
00:24:16,000 --> 00:24:17,120
0,350
connected.|
|

715
00:24:17,940 --> 00:24:21,095
0,260 260,395 395,670 780,1180 2820,3155
To get more concrete,| let's
更具体地说，|让我们问一问，如果根本不把我们的隐空间正规化，会有什么后果，

716
00:24:21,095 --> 00:24:22,145
0,225 225,540 540,735 735,870 870,1050
ask what could be the

717
00:24:22,145 --> 00:24:24,380
0,305 685,1005 1005,1260 1260,1950 1950,2235
consequences of not regularizing our

718
00:24:24,380 --> 00:24:26,500
0,345 345,555 555,720 720,980 1720,2120
latent space at all,| well,
|如果我们不正规化，

719
00:24:27,120 --> 00:24:28,850
0,305 305,470 470,695 695,1270 1470,1730
if we don't regularize,| we
|我们可能会得到这样的例子，

720
00:24:28,850 --> 00:24:30,305
0,150 150,330 330,510 510,705 705,1455
can end up with instances,|
|

721
00:24:30,305 --> 00:24:31,730
0,365 535,795 795,975 975,1230 1230,1425
where there are points that
在隐空间中有一些接近的点，

722
00:24:31,730 --> 00:24:32,825
0,210 210,480 480,660 660,765 765,1095
are close in the latent

723
00:24:32,825 --> 00:24:35,180
0,305 685,1085 1465,1860 1860,2055 2055,2355
space,| but don't end up
|但最终不会得到类似的解码或类似的重建，

724
00:24:35,180 --> 00:24:37,025
0,315 315,675 675,1320 1320,1560 1560,1845
with similar decodings or similar

725
00:24:37,025 --> 00:24:38,500
0,935
reconstructions,|
|

726
00:24:38,790 --> 00:24:40,970
0,1100 1100,1295 1295,1445 1445,1720 1800,2180
similarly, we could have points
同样，我们可能会有一些根本不会导致有意义的重建的点，

727
00:24:40,970 --> 00:24:42,550
0,380 580,960 960,1125 1125,1305 1305,1580
that don't lead to meaningful

728
00:24:42,960 --> 00:24:45,005
0,785 785,920 920,1180 1470,1820 1820,2045
reconstructions at all,| they're somehow
|它们不知何故被编码了，

729
00:24:45,005 --> 00:24:47,420
0,675 675,995 1135,1425 1425,1800 1800,2415
encoded,| but we can't decode
|但我们无法有效地解码。

730
00:24:47,420 --> 00:24:49,060
0,380
effectively.|
|

731
00:24:49,440 --> 00:24:52,180
0,910 1440,1775 1775,2045 2045,2360 2360,2740
Regularization allows us to realize
正规化使我们能够认识到最终接近隐空间的点，

732
00:24:52,560 --> 00:24:53,900
0,350 350,620 620,815 815,1055 1055,1340
points that end up close

733
00:24:53,900 --> 00:24:56,300
0,240 240,645 645,950 1420,1820 2050,2400
into latent space| and also
|也可以进行类似的重建和有意义的重建。

734
00:24:56,300 --> 00:24:59,045
0,300 300,900 900,1760 1810,2145 2145,2745
are similarly reconstructed and meaningfully

735
00:24:59,045 --> 00:25:00,280
0,875
reconstructed.|
|

736
00:25:02,460 --> 00:25:05,465
0,400 780,1180 2190,2540 2540,2765 2765,3005
Okay, so continuing with this
好的，继续这个例子，

737
00:25:05,465 --> 00:25:07,040
0,365 505,825 825,1110 1110,1365 1365,1575
example,| the example that I
|我在那里展示的例子，我没有深入到细节，

738
00:25:07,040 --> 00:25:08,290
0,240 240,510 510,705 705,855 855,1250
showed there and I didn't

739
00:25:09,540 --> 00:25:11,090
0,275 275,550 570,905 905,1220 1220,1550
get into details,| was showing
|展示了这些形状，这些不同颜色的形状，

740
00:25:11,090 --> 00:25:12,725
0,315 315,710 880,1200 1200,1455 1455,1635
these shapes, these shapes of

741
00:25:12,725 --> 00:25:14,195
0,225 225,575 835,1110 1110,1275 1275,1470
different colors| and that were
|试图在一些低维空间中编码，

742
00:25:14,195 --> 00:25:15,500
0,240 240,420 420,540 540,1110 1110,1305
trying to be encoded in

743
00:25:15,500 --> 00:25:17,880
0,320 460,825 825,1485 1485,1820
some lower dimensional space,|
|

744
00:25:19,090 --> 00:25:22,545
0,350 350,1210 2520,2885 2885,3155 3155,3455
with regularization,| we are able
有了正规化，|我们能够通过尝试最小化正则化项来实现这一点，

745
00:25:22,545 --> 00:25:24,465
0,345 345,615 615,915 915,1295 1585,1920
to achieve this by trying

746
00:25:24,465 --> 00:25:27,660
0,225 225,720 720,1085 2005,2880 2880,3195
to minimize that regularization term,|
|

747
00:25:27,660 --> 00:25:29,210
0,300 300,590 610,1005 1005,1275 1275,1550
it's not sufficient to just
仅仅利用重建损失来实现这种连续性和完整性是不够的，

748
00:25:29,380 --> 00:25:31,820
0,400 600,860 860,1505 1505,1900 2040,2440
employ the reconstruction loss alone

749
00:25:32,110 --> 00:25:34,365
0,350 350,650 650,1000 1260,1955 1955,2255
to achieve this continuity and

750
00:25:34,365 --> 00:25:35,740
0,300 300,875
this completeness,|
|

751
00:25:36,730 --> 00:25:38,030
0,320 320,515 515,665 665,920 920,1300
because of the fact that
因为没有正规化，

752
00:25:38,200 --> 00:25:40,950
0,400 420,1270 1350,1745 1745,2510 2510,2750
without regularization,| just encoding and
|仅仅编码和重构并不能保证连续性和完备性，

753
00:25:40,950 --> 00:25:43,545
0,920 1180,1485 1485,1740 1740,2090 2290,2595
reconstructing does not guarantee the

754
00:25:43,545 --> 00:25:46,400
0,305 595,995 1045,1745 1855,2255 2275,2855
properties of continuity and completeness,|
|

755
00:25:47,970 --> 00:25:50,380
0,320 320,640 840,1240
we overcome this,|
我们克服了这一点，|

756
00:25:50,650 --> 00:25:53,210
0,320 320,640 780,1180 1470,1870 2160,2560
these issues of having potentially
这些问题具有潜在的指向性分布，具有不连续，具有不同的平均值，

757
00:25:53,320 --> 00:25:56,990
0,400 510,1030 1590,1955 1955,3010 3270,3670
pointed distributions, having discontinuities, having

758
00:25:57,040 --> 00:25:58,650
0,575 575,830 830,1100 1100,1355 1355,1610
disparate means,| that could end
|最终可能在没有正则化影响的隐空间中结束，

759
00:25:58,650 --> 00:25:59,730
0,225 225,390 390,510 510,825 825,1080
up in the latent space

760
00:25:59,730 --> 00:26:01,820
0,345 345,660 660,870 870,1160 1210,2090
without the effect of regularization,|
|

761
00:26:02,880 --> 00:26:04,540
0,290 290,580 720,1055 1055,1325 1325,1660
we overcome this,| by now
我们克服这个，|根据正态先验正则化编码的潜在分布的均值和方差，

762
00:26:04,980 --> 00:26:07,175
0,790 1140,1430 1430,1715 1715,2015 2015,2195
regularizing the mean and the

763
00:26:07,175 --> 00:26:10,610
0,540 540,905 1195,1595 2305,3030 3030,3435
variance of the encoded latent

764
00:26:10,610 --> 00:26:13,025
0,470 1120,1520 1630,1905 1905,2100 2100,2415
distributions according to that normal

765
00:26:13,025 --> 00:26:14,240
0,395
prior,|
|

766
00:26:14,440 --> 00:26:16,035
0,290 290,515 515,850 1080,1400 1400,1595
what this allows is for
这使得学习到的那些隐变量的分布在隐空间中有效地重叠，

767
00:26:16,035 --> 00:26:18,030
0,165 165,455 835,1355 1435,1755 1755,1995
the learned distributions of those

768
00:26:18,030 --> 00:26:20,910
0,360 360,950 1180,1545 1545,1910 2110,2880
latent variables to effectively overlap

769
00:26:20,910 --> 00:26:22,790
0,270 270,420 420,750 750,1070 1480,1880
in the latent space,| because
|因为根据这个先验，所有的东西都被规则化为平均为零，标准差为一，

770
00:26:23,020 --> 00:26:24,770
0,305 305,515 515,1175 1175,1445 1445,1750
everything is regularized to have,

771
00:26:25,450 --> 00:26:26,850
0,380 380,620 620,800 800,1100 1100,1400
according to this prior, of

772
00:26:26,850 --> 00:26:29,000
0,270 270,620 850,1230 1230,1755 1755,2150
mean zero, standard deviation one,|
|

773
00:26:29,650 --> 00:26:31,070
0,290 290,545 545,905 905,1160 1160,1420
and that centers the means,|
这使平均值居中，|

774
00:26:31,940 --> 00:26:34,355
0,765 765,1020 1020,1730 1780,2100 2100,2415
regularizes the variances for each
使每个独立隐变量分布的方差正则化。

775
00:26:34,355 --> 00:26:36,790
0,345 345,695 1105,1505 1585,2115 2115,2435
of those independent latent variable

776
00:26:37,050 --> 00:26:38,480
0,520
distributions.|
|

777
00:26:38,930 --> 00:26:40,825
0,400 600,965 965,1330 1410,1700 1700,1895
Together, the effect of this
总而言之，这种正则化在网络中的效果是，

778
00:26:40,825 --> 00:26:43,150
0,785 805,1140 1140,1475 1885,2160 2160,2325
regularization in net is that|
|

779
00:26:43,150 --> 00:26:45,055
0,165 165,420 420,800 1030,1620 1620,1905
we can achieve continuity and
我们可以在隐空间中实现连续性和完备性，

780
00:26:45,055 --> 00:26:46,590
0,525 525,750 750,885 885,1215 1215,1535
completeness in the latent space,|
|

781
00:26:47,480 --> 00:26:48,910
0,350 350,590 590,1145 1145,1265 1265,1430
{points,and -} distances that are
接近的点和距离应该与我们得到的类似重建相对应。

782
00:26:48,910 --> 00:26:51,205
0,320 940,1260 1260,1580 1720,2010 2010,2295
close should correspond to similar

783
00:26:51,205 --> 00:26:53,760
0,965 1165,1565 1705,1980 1980,2205 2205,2555
reconstructions that we get out.|
|

784
00:26:55,040 --> 00:26:57,400
0,400 540,940 1050,1450 1830,2135 2135,2360
So hopefully this gets at
所以希望这能得到一些直觉，

785
00:26:57,400 --> 00:26:59,155
0,270 270,510 510,645 645,1160 1390,1755
some of the intuition| behind
|在 VAE 后的思想，正则化的思想，

786
00:26:59,155 --> 00:27:00,540
0,315 315,570 570,735 735,870 870,1385
the idea of the VAE,

787
00:27:00,920 --> 00:27:02,065
0,320 320,605 605,860 860,1025 1025,1145
behind the idea of the

788
00:27:02,065 --> 00:27:04,440
0,755 1075,1425 1425,1725 1725,2025 2025,2375
regularization| and trying to enforce
|并试图在隐空间上强制结构化的正态先验。

789
00:27:04,490 --> 00:27:06,700
0,320 320,910 990,1390 1470,1870 1920,2210
the structured normal prior on

790
00:27:06,700 --> 00:27:08,160
0,165 165,495 495,800
the latent space.|
|

791
00:27:09,500 --> 00:27:10,945
0,305 305,500 500,680 680,970 1140,1445
With this in hand,| with
有了这一点，|随着我们损失函数的两个组件，

792
00:27:10,945 --> 00:27:12,130
0,225 225,510 510,840 840,1050 1050,1185
the two components of our

793
00:27:12,130 --> 00:27:15,000
0,240 240,590 1240,2100 2100,2340 2340,2870
loss function| reconstructing the inputs,
|重建输入正规化，学习以尝试实现连续性和完整性，

794
00:27:15,230 --> 00:27:17,305
0,850 900,1300 1350,1640 1640,1820 1820,2075
regularizing learning to try to

795
00:27:17,305 --> 00:27:20,035
0,365 505,1110 1110,1380 1380,1955 2455,2730
achieve continuity and completeness,| we
|我们现在可以考虑如何定义通过网络的前向传递，

796
00:27:20,035 --> 00:27:21,865
0,150 150,425 745,1065 1065,1385 1525,1830
can now think about how

797
00:27:21,865 --> 00:27:23,290
0,270 270,555 555,780 780,1080 1080,1425
we define a forward pass

798
00:27:23,290 --> 00:27:25,120
0,210 210,330 330,590 1060,1455 1455,1830
through the network,| going from
|从输入示例开始，

799
00:27:25,120 --> 00:27:27,310
0,380 400,750 750,1100 1660,1965 1965,2190
an input example| and being
|并能够从潜在变量解码和采样以查看新示例。

800
00:27:27,310 --> 00:27:28,885
0,285 285,540 540,1065 1065,1260 1260,1575
able to decode and sample

801
00:27:28,885 --> 00:27:30,625
0,285 285,435 435,720 720,1295 1465,1740
from the latent variables to

802
00:27:30,625 --> 00:27:32,480
0,195 195,450 450,765 765,1145
look at new examples.|
|

803
00:27:32,970 --> 00:27:34,505
0,305 305,575 575,940 960,1295 1295,1535
Our last critical step is|
我们的最后一个关键步骤是|

804
00:27:34,505 --> 00:27:36,280
0,225 225,540 540,855 855,1140 1140,1775
how the actual back propagation
如何定义实际的反向传播训练算法，以及如何实现这一点，

805
00:27:36,690 --> 00:27:38,765
0,400 600,1055 1055,1355 1355,1720 1770,2075
training algorithm is defined and

806
00:27:38,765 --> 00:27:40,420
0,210 210,450 450,690 690,995
how we achieve this,|
|

807
00:27:41,060 --> 00:27:42,550
0,320 320,620 620,875 875,1150 1170,1490
the key, as I introduce
正如我在 VAE 中介绍的那样，关键是抽样随机性，

808
00:27:42,550 --> 00:27:44,010
0,225 225,675 675,930 930,1140 1140,1460
with VAEs, is this notion

809
00:27:44,030 --> 00:27:46,525
0,335 335,965 965,1235 1235,1840 2190,2495
of randomness of sampling| that
|我们通过定义每个隐变量的概率分布而引入的，

810
00:27:46,525 --> 00:27:48,475
0,285 285,665 745,1110 1110,1455 1455,1950
we have introduced by defining

811
00:27:48,475 --> 00:27:51,130
0,210 210,785 955,1475 1945,2345 2365,2655
these probability distributions over each

812
00:27:51,130 --> 00:27:52,880
0,150 150,270 270,600 600,1160
of the latent variables,|
|

813
00:27:53,390 --> 00:27:54,870
0,320 320,635 635,950 950,1175 1175,1480
the problem this gives us
这给我们带来的问题是，

814
00:27:54,890 --> 00:27:56,725
0,290 290,485 485,740 740,1090 1470,1835
is that,| we cannot back
|我们不能直接通过任何含有抽样元素的东西来反向传播，

815
00:27:56,725 --> 00:27:59,350
0,635 745,1145 1585,1985 2155,2445 2445,2625
propagate directly through anything that

816
00:27:59,350 --> 00:28:00,960
0,240 240,480 480,765 765,1065 1065,1610
has an element of sampling,|
|

817
00:28:01,280 --> 00:28:02,520
0,275 275,440 440,710 710,965 965,1240
anything that has an element
任何带有随机元素的东西，

818
00:28:02,690 --> 00:28:04,240
0,275 275,940
of randomness,|
|

819
00:28:05,220 --> 00:28:08,510
0,335 335,970 1200,1600 1800,2200 2520,3290
back propagation requires completely deterministic
反向传播要求完全确定的节点，确定的层，

820
00:28:08,510 --> 00:28:10,685
0,510 510,1185 1185,1670 1780,2025 2025,2175
nodes, deterministic layers| to be
|能够成功地应用梯度下降和反向传播算法。

821
00:28:10,685 --> 00:28:13,550
0,305 385,785 1075,1475 1945,2345 2365,2865
able to successfully apply gradient

822
00:28:13,550 --> 00:28:15,340
0,440 550,840 840,990 990,1185 1185,1790
descent and the back propagation

823
00:28:15,480 --> 00:28:16,580
0,550
algorithm.|
|

824
00:28:17,800 --> 00:28:19,580
0,290 290,830 830,1145 1145,1340 1340,1780
The breakthrough idea that enabled
使 VAE 能够被完全端到端训练的突破性想法是

825
00:28:19,660 --> 00:28:21,390
0,455 455,695 695,875 875,1210 1350,1730
VAE to be trained completely

826
00:28:21,390 --> 00:28:23,145
0,285 285,450 450,710 1150,1455 1455,1755
end to end was| this
|这个在采样层内重新参数化的想法，

827
00:28:23,145 --> 00:28:26,955
0,360 360,725 865,1265 1285,2285 3445,3810
idea of {reparameterriization -} within

828
00:28:26,955 --> 00:28:28,740
0,285 285,750 750,1055
that sampling layer,|
|

829
00:28:28,860 --> 00:28:30,125
0,400 480,845 845,950 950,1085 1085,1265
and I'll give you the
我会给你关于这个操作如何运作的关键想法，

830
00:28:30,125 --> 00:28:31,780
0,305 325,725 775,1110 1110,1350 1350,1655
key idea about how this

831
00:28:31,800 --> 00:28:33,290
0,350 350,635 635,995 995,1235 1235,1490
operation works,| it's actually really
|它是相当聪明的，

832
00:28:33,290 --> 00:28:34,400
0,255 255,560
quite level,

833
00:28:34,400 --> 00:28:35,940
0,315 315,680
quite clever.|
|

834
00:28:36,270 --> 00:28:37,970
0,400 540,830 830,1010 1010,1300 1410,1700
So, as I said, when
所以，就像我说的，当我们有了概率的随机性概念时，

835
00:28:37,970 --> 00:28:39,485
0,180 180,465 465,860 940,1290 1290,1515
we have a notion of

836
00:28:39,485 --> 00:28:42,065
0,510 510,705 705,1235 1825,2175 2175,2580
randomness of probability,| we can't
|我们不能直接通过这一层进行采样，

837
00:28:42,065 --> 00:28:44,140
0,305 445,845 1015,1410 1410,1740 1740,2075
sample directly through that layer,|
|

838
00:28:45,800 --> 00:28:48,415
0,395 395,790 870,1810 2160,2435 2435,2615
instead, with reparameterriization,| what we
取而代之的是，利用重新参数化，|我们所做的是重新定义如何对隐变量向量进行采样，

839
00:28:48,415 --> 00:28:51,040
0,305 685,990 990,1200 1200,1865 2245,2625
do is we redefine how

840
00:28:51,040 --> 00:28:52,870
0,255 255,570 570,860 970,1400 1540,1830
a latent variable vector is

841
00:28:52,870 --> 00:28:54,040
0,530
sampled,|
|

842
00:28:54,040 --> 00:28:55,225
0,255 255,495 495,765 765,1005 1005,1185
as a sum of a
作为固定的确定性平均值 μ ，

843
00:28:55,225 --> 00:28:58,750
0,305 895,1695 1695,1980 1980,2345 3205,3525
fixed deterministic mean μ,| a
|和固定的标准偏差 σ 的向量，

844
00:28:58,750 --> 00:29:01,410
0,320 1000,1455 1455,1785 1785,2085 2085,2660
fixed vector of standard deviation

845
00:29:01,490 --> 00:29:03,870
0,610 1080,1385 1385,1685 1685,2030 2030,2380
σ,| {and,now -} the trick
|现在的诀窍是我们把所有的随机性，所有的抽样，都转移到一个随机常数 ε 上，

846
00:29:04,130 --> 00:29:06,370
0,305 305,500 500,790 1230,1900 1920,2240
is that we divert all

847
00:29:06,370 --> 00:29:08,130
0,180 180,735 735,990 990,1200 1200,1760
the randomness, all the sampling

848
00:29:08,450 --> 00:29:11,010
0,305 305,545 545,880 1020,1420 1800,2560
to a random constant ε,|
|

849
00:29:11,480 --> 00:29:13,620
0,440 440,760 840,1240 1290,1690 1740,2140
that's drawn from a normal
它是从正态分布中提取出来的，

850
00:29:13,970 --> 00:29:17,425
0,400 1500,1900 2190,2590 2640,3040 3150,3455
distribution,| so mean itself is
|所以平均值本身是固定的，

851
00:29:17,425 --> 00:29:19,560
0,305 745,1110 1110,1590 1590,1830 1830,2135
fixed,| standard deviation is fixed,|
|标准差也是固定的，|

852
00:29:19,850 --> 00:29:21,475
0,320 320,500 500,1150 1200,1475 1475,1625
all the randomness and the
所有的随机性和抽样都是根据这个 ε 常数进行的，

853
00:29:21,475 --> 00:29:23,575
0,545 745,1145 1285,1680 1680,1920 1920,2100
sampling occurs according to that

854
00:29:23,575 --> 00:29:25,160
0,600 600,935
ε constant,|
|

855
00:29:25,330 --> 00:29:27,285
0,275 275,440 440,730 1170,1570 1680,1955
we can then scale the
然后，我们可以通过该随机常数来调整平均值和标准差，

856
00:29:27,285 --> 00:29:29,085
0,180 180,405 405,725 865,1445 1465,1800
mean and standard deviation by

857
00:29:29,085 --> 00:29:31,100
0,225 225,510 510,905 1405,1710 1710,2015
that random constant| to {re-achieve
|以重新实现隐变量本身内的采样操作。

858
00:29:31,510 --> 00:29:33,930
0,400 720,1040 1040,1600 1680,2060 2060,2420
-} the sampling operation within

859
00:29:33,930 --> 00:29:36,160
0,240 240,525 525,1070 1120,1520
the latent variables themselves.|
|

860
00:29:37,050 --> 00:29:38,390
0,305 305,590 590,875 875,1100 1100,1340
What this actually looks like,
这实际上是什么样子，以及一个打破这种再灌输和发散概念的插图，如下所示。

861
00:29:38,390 --> 00:29:41,015
0,210 210,405 405,990 990,1370 2260,2625
and an illustration that breaks

862
00:29:41,015 --> 00:29:43,240
0,300 300,555 555,875 895,1200 1200,2225
down this concept of reperatization

863
00:29:43,470 --> 00:29:45,700
0,290 290,970 1230,1595 1595,1895 1895,2230
and divergence, is as follows.|
|

864
00:29:46,550 --> 00:29:48,120
0,275 275,550 570,935 935,1235 1235,1570
So looking, looking here, right,
所以看，看这里，对了，我展示的是蓝色的完全确定的步骤和橙色的采样随机步骤。如果我们的潜变量是有效地捕捉随机性的东西，样本本身，我们就有这个问题，因为我们不能反向传播。我们不能直接通过任何具有随机性和随机性的东西进行训练。

865
00:29:48,350 --> 00:29:49,770
0,275 275,500 500,740 740,1070 1070,1420
what I've shown is these

866
00:29:50,210 --> 00:29:52,410
0,400 540,1280 1280,1565 1565,1865 1865,2200
completely deterministic steps in blue

867
00:29:52,970 --> 00:29:56,200
0,400 750,1150 1740,2390 2390,2770 2850,3230
and the sampling random steps

868
00:29:56,200 --> 00:29:59,125
0,255 255,530 1450,1850 2380,2700 2700,2925
in orange originally. {If,our -}

869
00:29:59,125 --> 00:30:01,380
0,330 330,825 825,1145 1285,1685 1855,2255
latent variables are what effectively

870
00:30:01,430 --> 00:30:03,235
0,335 335,905 905,1070 1070,1595 1595,1805
are capturing the randomness, the

871
00:30:03,235 --> 00:30:05,875
0,545 835,1235 1915,2205 2205,2400 2400,2640
sampling themselves, we have this

872
00:30:05,875 --> 00:30:07,360
0,330 330,615 615,855 855,1125 1125,1485
problem in that we can't

873
00:30:07,360 --> 00:30:09,030
0,225 225,720 720,960 960,1365 1365,1670
back {propagate.,We -} can't train

874
00:30:09,380 --> 00:30:11,190
0,400 480,880 990,1295 1295,1505 1505,1810
directly through anything that has

875
00:30:11,330 --> 00:30:14,060
0,1115 1115,1400 1400,1610 1610,2290
stochasticity that has randomness.|
|

876
00:30:14,680 --> 00:30:17,100
0,350 350,620 620,1540 1860,2195 2195,2420
What Rep parameterriization allows us
Rep参数恐惧化允许我们做的是，它移动了这张图，现在我们已经完全将采样操作转移到一边，到这个恒定的epsilon，它是从正常的先验绘制的。现在当我们回过头来看我们的潜变量，它是关于采样操作的确定性的。

877
00:30:17,100 --> 00:30:19,020
0,165 165,440 790,1140 1140,1490 1510,1920
to do is it shifts

878
00:30:19,020 --> 00:30:21,210
0,195 195,800 1150,1470 1470,1740 1740,2190
this diagram where now we've

879
00:30:21,210 --> 00:30:23,510
0,375 375,1095 1095,1305 1305,1905 1905,2300
completely diverted that sampling operation

880
00:30:24,040 --> 00:30:25,800
0,350 350,545 545,665 665,940 1470,1760
off to the side, to

881
00:30:25,800 --> 00:30:28,140
0,225 225,560 700,1460 1870,2145 2145,2340
this constant epsilon, which is

882
00:30:28,140 --> 00:30:29,510
0,300 300,540 540,675 675,950 970,1370
drawn from a normal prior.

883
00:30:30,220 --> 00:30:31,920
0,320 320,640 1020,1310 1310,1490 1490,1700
{And,now -} when we look

884
00:30:31,920 --> 00:30:33,050
0,225 225,390 390,525 525,840 840,1130
back at our latent variable,

885
00:30:33,370 --> 00:30:35,750
0,275 275,550 570,1450 1530,1930 1980,2380
it is deterministic with respect

886
00:30:35,830 --> 00:30:37,960
0,260 260,455 455,1060 1140,1540
to that sampling operation.|
|

887
00:30:38,580 --> 00:30:39,695
0,290 290,485 485,725 725,935 935,1115
What this means is that
这意味着我们可以完全端到端地反向传播来更新我们的网络权重，而不必担心那些潜在变量c内的直接随机性、直接随机性。

888
00:30:39,695 --> 00:30:41,290
0,165 165,345 345,615 615,1215 1215,1595
we can back propagate to

889
00:30:41,340 --> 00:30:43,520
0,290 290,470 470,755 755,1330 1800,2180
update our network weights completely

890
00:30:43,520 --> 00:30:46,100
0,285 285,450 450,710 1690,2090 2200,2580
end to end, without having

891
00:30:46,100 --> 00:30:48,605
0,255 255,530 670,1070 1300,1700 1780,2505
to worry about direct randomness,

892
00:30:48,605 --> 00:30:51,005
0,285 285,1245 1245,1535 1765,2055 2055,2400
direct stochasticity within those latent

893
00:30:51,005 --> 00:30:52,220
0,450 450,755
variables c.|
|

894
00:30:52,960 --> 00:30:54,435
0,335 335,605 605,905 905,1205 1205,1475
This trick is really, really
这个技巧非常非常强大，因为它能够通过反向传播算法完全端到端地训练这些VAS。

895
00:30:54,435 --> 00:30:56,430
0,335 475,735 735,855 855,1295 1675,1995
powerful because it enabled the

896
00:30:56,430 --> 00:30:58,040
0,320 400,705 705,930 930,1185 1185,1610
ability to train these vas

897
00:30:58,570 --> 00:31:00,170
0,380 380,665 665,830 830,1090 1200,1600
completely end to end in

898
00:31:01,300 --> 00:31:04,280
0,290 290,515 515,1090 1230,1750
through back propagation algorithm.|
|

899
00:31:05,070 --> 00:31:07,355
0,260 260,520 1080,1480 1800,2090 2090,2285
All right. {So,at -} this
好的。至此，我们已经了解了VAS的核心架构。我们已经介绍了这两个术语的损失。我们已经看到了如何对其进行端到端的训练。

900
00:31:07,355 --> 00:31:08,660
0,300 300,660 660,870 870,1110 1110,1305
point, we've gone through the

901
00:31:08,660 --> 00:31:11,050
0,225 225,530 700,990 990,1370 1870,2390
core architecture of vas. We've

902
00:31:11,070 --> 00:31:12,305
0,305 305,545 545,800 800,1040 1040,1235
introduced these two terms of

903
00:31:12,305 --> 00:31:13,595
0,150 150,425 475,855 855,1080 1080,1290
the loss. We've seen how

904
00:31:13,595 --> 00:31:14,435
0,150 150,330 330,510 510,660 660,840
we can train it end

905
00:31:14,435 --> 00:31:15,480
0,165 165,425
to end.|
|

906
00:31:15,640 --> 00:31:16,950
0,335 335,500 500,695 695,1025 1025,1310
Now {let's -} consider what
现在，让我们考虑一下这些潜在变量实际上捕捉到了什么，以及它们代表了什么。

907
00:31:16,950 --> 00:31:18,795
0,270 270,645 645,1110 1110,1400 1540,1845
these latent variables are actually

908
00:31:18,795 --> 00:31:20,060
0,450 450,615 615,795 795,975 975,1265
capturing and what they represent.|
|

909
00:31:22,510 --> 00:31:25,140
0,320 320,640 690,1360 1410,1810 1920,2630
When we impose this distributional
当我们施加这种分布先验时，它允许我们做的是从潜在空间有效地采样，实际上慢慢地扰动单个潜在变量的值，保持其他变量不变。你可以观察到的，你可以在这里看到的是，通过微扰，调整潜变量的值。

910
00:31:25,140 --> 00:31:27,315
0,350 1150,1455 1455,1710 1710,1965 1965,2175
prior, what it allows us

911
00:31:27,315 --> 00:31:28,310
0,150 150,300 300,495 495,690 690,995
to do is to sample

912
00:31:28,510 --> 00:31:30,230
0,400 570,890 890,1070 1070,1415 1415,1720
effectively from the latent space

913
00:31:30,820 --> 00:31:33,450
0,400 690,1090 1170,1570 1650,2435 2435,2630
and actually slowly perturb the

914
00:31:33,450 --> 00:31:35,870
0,290 670,1020 1020,1350 1350,1800 1800,2420
value of single latent variables,

915
00:31:36,460 --> 00:31:37,760
0,350 350,545 545,710 710,965 965,1300
keeping the other ones fixed.

916
00:31:38,650 --> 00:31:40,335
0,400 780,1055 1055,1190 1190,1415 1415,1685
{And,what -} you can observe,

917
00:31:40,335 --> 00:31:41,265
0,195 195,360 360,495 495,675 675,930
and what you can see

918
00:31:41,265 --> 00:31:42,795
0,335 535,825 825,1020 1020,1260 1260,1530
here is that by doing

919
00:31:42,795 --> 00:31:44,565
0,255 255,795 795,1170 1170,1620 1620,1770
that perturbation that tuning of

920
00:31:44,565 --> 00:31:45,615
0,150 150,390 390,615 615,750 750,1050
the value of the latent

921
00:31:45,615 --> 00:31:47,180
0,605
variables.|
|

922
00:31:47,280 --> 00:31:48,845
0,275 275,455 455,695 695,905 905,1565
We can run the decoder
我们可以每次运行VA的解码器，每次进行调整时重建输出。在这个脸部的例子中，你会看到，一个个体的潜变量捕捉到了一些语义上的信息，一些有意义的东西。我们通过这个微扰，通过这个调谐，看到了这一点。

923
00:31:48,845 --> 00:31:50,440
0,300 300,480 480,840 840,1200 1200,1595
of the va every time,

924
00:31:50,730 --> 00:31:52,595
0,740 740,1025 1025,1280 1280,1550 1550,1865
reconstruct the output every time

925
00:31:52,595 --> 00:31:54,700
0,225 225,390 390,615 615,1115 1705,2105
we do that tuning. {And,what

926
00:31:55,020 --> 00:31:56,870
0,275 275,515 515,790 1110,1510 1590,1850
-} you'll see hopefully with

927
00:31:56,870 --> 00:31:58,630
0,260 700,1080 1080,1335 1335,1485 1485,1760
this example with the face

928
00:31:58,650 --> 00:32:00,320
0,275 275,410 410,670 840,1220 1220,1670
is that an individual latent

929
00:32:00,320 --> 00:32:04,000
0,290 760,1125 1125,1785 1785,2120 2710,3680
variable is capturing something semantically

930
00:32:04,290 --> 00:32:06,920
0,635 635,995 995,1390 2190,2450 2450,2630
informative, something {meaningful.,And -} we

931
00:32:06,920 --> 00:32:08,320
0,210 210,420 420,630 630,840 840,1400
see that by this perturbation,

932
00:32:08,340 --> 00:32:09,940
0,335 335,620 620,1090
by this tuning.|
|

933
00:32:10,110 --> 00:32:11,830
0,275 275,515 515,880 1140,1430 1430,1720
In this example, the face,
在这个例子中，面孔正在改变，希望你能欣赏到这一点。姿势在改变，所有这一切都是由单一潜变量的扰动驱动的。

934
00:32:11,850 --> 00:32:13,240
0,290 290,470 470,740 740,1055 1055,1390
as you hopefully can appreciate,

935
00:32:13,620 --> 00:32:15,185
0,350 350,830 830,1025 1025,1310 1310,1565
is shifting. {The,pose -} is

936
00:32:15,185 --> 00:32:16,985
0,515 745,1065 1065,1320 1320,1560 1560,1800
shifting, and all this is

937
00:32:16,985 --> 00:32:18,610
0,300 300,600 600,825 825,1020 1020,1625
driven by is the perturbation

938
00:32:18,630 --> 00:32:20,350
0,260 260,425 425,730 930,1415 1415,1720
of a single latent variable.|
|

939
00:32:21,280 --> 00:32:22,605
0,500 500,680 680,920 920,1160 1160,1325
Tuning the value of that
调整潜变量的值，看看这对解码后的重建有什么影响。

940
00:32:22,605 --> 00:32:24,465
0,345 345,635 895,1245 1245,1575 1575,1860
latent variable and seeing how

941
00:32:24,465 --> 00:32:26,870
0,270 270,785 805,1080 1080,1680 1680,2405
that affects the decoded reconstruction.|
|

942
00:32:28,160 --> 00:32:30,510
0,290 290,580 720,1120 1290,1690 1950,2350
The network is actually able
网络实际上能够学习这些不同的编码特征，这些不同的潜在变量，这样通过分别扰动它们的值，我们可以解释和理解这些潜在变量的含义和它们所代表的意义。

943
00:32:30,530 --> 00:32:32,545
0,290 290,530 530,815 815,1150 1320,2015
to learn these different encoded

944
00:32:32,545 --> 00:32:34,620
0,305 355,705 705,1035 1035,1455 1455,2075
features, these different latent variables,

945
00:32:35,210 --> 00:32:37,375
0,335 335,635 635,950 950,1750 1890,2165
such that by perturbing the

946
00:32:37,375 --> 00:32:39,775
0,275 325,600 600,875 955,1535 2125,2400
values of them individually, we

947
00:32:39,775 --> 00:32:41,845
0,275 355,755 1195,1485 1485,1725 1725,2070
can interpret and make sense

948
00:32:41,845 --> 00:32:43,525
0,330 330,585 585,810 810,1170 1170,1680
of what those latent variables

949
00:32:43,525 --> 00:32:45,030
0,335 535,840 840,1050 1050,1230 1230,1505
mean and what they represent.|
|

950
00:32:46,750 --> 00:32:48,620
0,260 260,425 425,730 1050,1450 1470,1870
To make this more concrete,
为了更具体，我们甚至可以同时考虑多个潜在变量，将一个与另一个进行比较，理想情况下，我们希望这些潜在特征尽可能独立，以便获得最紧凑、最丰富的表示和紧凑编码。

951
00:32:49,720 --> 00:32:51,230
0,275 275,530 530,830 830,1130 1130,1510
we can consider even multiple

952
00:32:51,700 --> 00:32:54,590
0,455 455,1030 1080,2195 2195,2540 2540,2890
latent variables simultaneously, compare one

953
00:32:54,610 --> 00:32:56,910
0,335 335,515 515,760 1320,1625 1625,2300
against the other, and ideally,

954
00:32:56,910 --> 00:32:58,400
0,195 195,435 435,690 690,1125 1125,1490
we want those latent features

955
00:32:58,510 --> 00:33:00,420
0,260 260,455 455,790 1170,1570 1590,1910
to be as independent as

956
00:33:00,420 --> 00:33:02,550
0,320 850,1170 1170,1490 1540,1890 1890,2130
possible in order to get

957
00:33:02,550 --> 00:33:04,515
0,180 180,345 345,620 1060,1695 1695,1965
at the most compact and

958
00:33:04,515 --> 00:33:07,760
0,480 480,1355 1675,2070 2070,2565 2565,3245
richest representation and compact encoding.|
|

959
00:33:08,320 --> 00:33:09,775
0,285 285,590 730,1035 1035,1215 1215,1455
So here again, in this
所以在这里，在这个人脸的例子中，我们沿着两个轴行走。

960
00:33:09,775 --> 00:33:11,350
0,315 315,540 540,815 1255,1485 1485,1575
example of faces, {we're -}

961
00:33:11,350 --> 00:33:13,940
0,260 370,770 790,1095 1095,1670
walking along two axes.|
|

962
00:33:13,940 --> 00:33:15,275
0,270 270,615 615,930 930,1125 1125,1335
Head pose on the X
头部在X轴上的姿势和在Y轴上看起来像是微笑的概念。

963
00:33:15,275 --> 00:33:17,630
0,605 985,1350 1350,1715 1765,2130 2130,2355
axis and what appears to

964
00:33:17,630 --> 00:33:19,160
0,260 670,945 945,1065 1065,1245 1245,1530
be kind of a notion

965
00:33:19,160 --> 00:33:20,270
0,195 195,375 375,690 690,960 960,1110
of a smile on the

966
00:33:20,270 --> 00:33:21,640
0,195 195,800
y axis.|
|

967
00:33:21,650 --> 00:33:22,740
0,275 275,410 410,575 575,785 785,1090
And you can see that
你可以看到，通过这些重建，我们实际上可以扰乱这些特征，从而能够扰乱重建空间中的末端效应。

968
00:33:22,910 --> 00:33:25,260
0,305 305,515 515,1450 1770,2060 2060,2350
with these reconstructions, we can

969
00:33:25,310 --> 00:33:27,325
0,305 305,800 800,1010 1010,1360 1770,2015
actually perturb these features to

970
00:33:27,325 --> 00:33:29,335
0,150 150,455 655,960 960,1685 1735,2010
be able to perturb the

971
00:33:29,335 --> 00:33:32,425
0,270 270,665 895,1230 1230,1565 2155,3090
end effect in the reconstructed

972
00:33:32,425 --> 00:33:34,020
0,335
space.|
|

973
00:33:34,450 --> 00:33:36,660
0,305 305,610 870,1270 1350,1750 1920,2210
And so ultimately, with with
因此，最终，对于AV，我们的目标是尝试强制执行尽可能多的信息，以便在该编码中捕获。我们希望这些潜在的特征是独立的，理想情况下是分开的。

974
00:33:36,810 --> 00:33:38,385
0,260 400,735 735,1065 1065,1365 1365,1575
AV, our goal is to

975
00:33:38,385 --> 00:33:40,280
0,240 240,540 540,905 1195,1545 1545,1895
try to enforce as much

976
00:33:40,390 --> 00:33:41,925
0,400 480,725 725,890 890,1210 1260,1535
information to be captured in

977
00:33:41,925 --> 00:33:44,055
0,150 150,675 675,825 825,1115 1825,2130
that encoding as possible. {We,want

978
00:33:44,055 --> 00:33:45,615
0,255 255,540 540,945 945,1235 1285,1560
-} these latent features to

979
00:33:45,615 --> 00:33:48,920
0,275 475,875 1195,1500 1500,2225 2275,3305
be independent and ideally disentangled.|
|

980
00:33:50,120 --> 00:33:51,265
0,290 290,500 500,740 740,965 965,1145
It turns out that there
事实证明，有一种非常聪明和简单的方法来鼓励这种独立和这种解脱。

981
00:33:51,265 --> 00:33:53,440
0,150 150,315 315,605 1615,1935 1935,2175
is a very clever and

982
00:33:53,440 --> 00:33:55,200
0,300 300,645 645,975 975,1340 1360,1760
simple way to try to

983
00:33:55,850 --> 00:33:57,985
0,400 690,1090 1170,1570 1620,1925 1925,2135
encourage this independence and this

984
00:33:57,985 --> 00:33:59,600
0,935
disentanglement.|
|

985
00:33:59,980 --> 00:34:00,990
0,305 305,500 500,695 695,875 875,1010
While this may look a
虽然这在数学上可能看起来有点复杂，有点可怕，但我将用一个非常简单的概念如何实施这种独立的潜在编码和这种解缠的想法来分解这一点。

986
00:34:00,990 --> 00:34:02,820
0,210 210,560 790,1190 1330,1590 1590,1830
little complicated with the math

987
00:34:02,820 --> 00:34:04,950
0,380 400,660 660,870 870,1220 1750,2130
and a bit scary, I

988
00:34:04,950 --> 00:34:07,035
0,380 820,1125 1125,1335 1335,1640 1750,2085
will break this down with

989
00:34:07,035 --> 00:34:09,165
0,315 315,675 675,1055 1255,1655 1825,2130
the idea of how a

990
00:34:09,165 --> 00:34:11,900
0,255 255,555 555,905 1585,2415 2415,2735
very simple concept enforces this

991
00:34:12,160 --> 00:34:14,280
0,400 750,1190 1190,1685 1685,1850 1850,2120
independent latent encoding and this

992
00:34:14,280 --> 00:34:15,680
0,980
disentanglement.|
|

993
00:34:16,200 --> 00:34:17,380
0,305 305,515 515,695 695,875 875,1180
All this term is showing
所有这一项都是损失的两个组成部分，重建项，正则化项。这就是我想要你关注的。

994
00:34:17,430 --> 00:34:19,055
0,350 350,650 650,980 980,1355 1355,1625
is those two components of

995
00:34:19,055 --> 00:34:21,190
0,135 135,395 865,1125 1125,1740 1740,2135
the loss, the reconstruction term,

996
00:34:21,390 --> 00:34:23,465
0,260 260,965 965,1360 1590,1985 1985,2075
the regularization term. That's what

997
00:34:23,465 --> 00:34:24,185
0,120 120,255 255,390 390,525 525,720
I want you to focus

998
00:34:24,185 --> 00:34:25,100
0,335
on.|
|

999
00:34:25,700 --> 00:34:28,120
0,380 380,760 780,1180 1620,2120 2120,2420
The idea of latent space
潜在空间解缠的想法确实是从Beta Beta Ves的这个概念中产生的。

1000
00:34:28,120 --> 00:34:30,280
0,960 960,1260 1260,1800 1800,1965 1965,2160
disentanglement really arose with this

1001
00:34:30,280 --> 00:34:33,540
0,320 640,1035 1035,1640 2020,2565 2565,3260
concept of Beta Beta ves.|
|

1002
00:34:34,310 --> 00:34:36,325
0,290 290,635 635,970 1020,1420 1710,2015
What Beta vas do is
他们所做的是引入这个参数，以及它是等待常数，等待常数控制，正则化项在v的总体损失中有多强大。

1003
00:34:36,325 --> 00:34:38,460
0,305 415,750 750,1020 1020,1590 1590,2135
they introduce this parameter Beta

1004
00:34:38,960 --> 00:34:40,380
0,350 350,560 560,680 680,905 905,1420
and what it is it's

1005
00:34:40,490 --> 00:34:42,930
0,485 485,790 1500,1790 1790,2150 2150,2440
awaiting constant, the awaiting constant

1006
00:34:43,940 --> 00:34:46,530
0,400 570,950 950,1330 1530,1820 1820,2590
controls, how powerful that regularization

1007
00:34:46,550 --> 00:34:48,240
0,400 600,995 995,1265 1265,1415 1415,1690
term is in the overall

1008
00:34:48,410 --> 00:34:50,560
0,380 380,635 635,785 785,1060
loss of the v.|
|

1009
00:34:50,560 --> 00:34:51,745
0,180 180,360 360,615 615,915 915,1185
And it turns out that
事实证明，通过增加Beta的值，你可以尝试鼓励更大的解缠，更有效的编码，以强制这些潜在变量彼此不相关。

1010
00:34:51,745 --> 00:34:53,380
0,305 475,870 870,1125 1125,1365 1365,1635
by increasing the value of

1011
00:34:53,380 --> 00:34:55,500
0,500 910,1185 1185,1410 1410,1740 1740,2120
Beta, you can try to

1012
00:34:55,670 --> 00:34:59,230
0,400 990,1370 1370,2440 2850,3245 3245,3560
encourage greater disentanglement, more efficient

1013
00:34:59,230 --> 00:35:01,600
0,600 600,920 1270,1670 1690,2010 2010,2370
encoding to enforce these latent

1014
00:35:01,600 --> 00:35:03,970
0,525 525,720 720,980 1300,2130 2130,2370
variables to be uncorrelated with

1015
00:35:03,970 --> 00:35:05,160
0,180 180,470
each other.|
|

1016
00:35:05,530 --> 00:35:07,730
0,400 750,1055 1055,1450 1530,1865 1865,2200
Now, if you're interested in
现在，如果你对数学上为什么Beta v强制这种解缠感兴趣，有很多文献和证明，以及关于为什么会发生这种情况的讨论，我们可以向你指出这些方向，但为了了解这实际上会对下游产生什么影响，当我们将人脸重建视为一项感兴趣的任务时，我们使用标准的v非Beta项，或者更确切地说，Beta项为1。

1017
00:35:08,080 --> 00:35:11,520
0,670 810,1210 2070,2495 2495,2770 3090,3440
mathematically why Beta v enforce

1018
00:35:11,520 --> 00:35:13,310
0,330 330,1245 1245,1380 1380,1500 1500,1790
this disentanglement there are many

1019
00:35:13,510 --> 00:35:15,050
0,395 395,650 650,770 770,1030 1140,1540
papers in the literature and

1020
00:35:15,220 --> 00:35:16,740
0,470 470,725 725,1070 1070,1355 1355,1520
proofs and discussions as to

1021
00:35:16,740 --> 00:35:18,165
0,165 165,420 420,770 1000,1290 1290,1425
why this occurs, and we

1022
00:35:18,165 --> 00:35:19,455
0,135 135,330 330,635 805,1080 1080,1290
can point you in those

1023
00:35:19,455 --> 00:35:21,600
0,335 1045,1445 1615,1890 1890,2025 2025,2145
directions, but to get a

1024
00:35:21,600 --> 00:35:22,845
0,180 180,375 375,570 570,885 885,1245
sense of what this actually

1025
00:35:22,845 --> 00:35:25,155
0,420 420,1085 1585,1875 1875,2070 2070,2310
affects downstream when we look

1026
00:35:25,155 --> 00:35:27,570
0,335 715,1065 1065,1725 1725,2105 2125,2415
at face reconstruction as a

1027
00:35:27,570 --> 00:35:29,475
0,225 225,540 540,920 1450,1725 1725,1905
task of interest with a

1028
00:35:29,475 --> 00:35:31,700
0,305 355,755 1165,1515 1515,1935 1935,2225
standard v no Beta term,

1029
00:35:31,900 --> 00:35:33,165
0,365 365,650 650,845 845,1115 1115,1265
or rather a Beta of

1030
00:35:33,165 --> 00:35:34,420
0,305
one.|
|

1031
00:35:34,730 --> 00:35:36,450
0,275 275,425 425,700 840,1240 1320,1720
You can hopefully appreciate that
希望你能理解，头部的旋转，姿势和旋转的特征实际上也与嘴巴位置的微笑和面部，嘴部的表情相关。因为，随着头部姿势的改变，明显的微笑或嘴巴的位置也在改变。

1032
00:35:37,070 --> 00:35:39,355
0,400 480,880 1020,1420 1560,1835 1835,2285
the features of the rotation

1033
00:35:39,355 --> 00:35:40,720
0,240 240,390 390,665 835,1095 1095,1365
of the head, the pose

1034
00:35:40,720 --> 00:35:42,325
0,225 225,500 760,1260 1260,1455 1455,1605
and the rotation of the

1035
00:35:42,325 --> 00:35:44,290
0,275 445,845 985,1385 1465,1755 1755,1965
head is also actually ends

1036
00:35:44,290 --> 00:35:46,795
0,240 240,560 760,1370 1630,2030 2140,2505
up being correlated with smile

1037
00:35:46,795 --> 00:35:48,750
0,240 240,390 390,875 1435,1695 1695,1955
and the facial, the mouth

1038
00:35:49,040 --> 00:35:50,460
0,365 365,605 605,725 725,970 1020,1420
expression in the mouth position.

1039
00:35:51,110 --> 00:35:52,660
0,290 290,580 840,1175 1175,1385 1385,1550
{In,that -} as the head

1040
00:35:52,660 --> 00:35:54,420
0,210 210,405 405,710 1090,1425 1425,1760
pose is changing, the apparent

1041
00:35:54,500 --> 00:35:56,050
0,400 630,920 920,1130 1130,1370 1370,1550
smile or the position of

1042
00:35:56,050 --> 00:35:57,330
0,120 120,345 345,705 705,990 990,1280
the mouth is also changing.|
|

1043
00:35:58,570 --> 00:36:02,145
0,400 720,1100 1100,1565 1565,2080 2760,3575
But with Beta ves empirically
但通过Beta Ves的经验，我们可以观察到，通过施加比1大得多的Beta值，我们可以尝试实施更大的解缠，现在我们只能考虑一个潜在的可变头部姿势和微笑，与标准的v相比，这些图像中嘴巴的位置更恒定。

1044
00:36:02,145 --> 00:36:04,215
0,150 150,405 405,780 780,1175 1735,2070
we can observe that with

1045
00:36:04,215 --> 00:36:06,180
0,765 765,975 975,1335 1335,1595 1645,1965
imposing these Beta values much,

1046
00:36:06,180 --> 00:36:07,610
0,255 255,540 540,855 855,1125 1125,1430
much, much greater than one,

1047
00:36:08,020 --> 00:36:09,470
0,275 275,485 485,770 770,1085 1085,1450
we can try to enforce

1048
00:36:09,700 --> 00:36:12,225
0,380 380,1390 1710,2030 2030,2300 2300,2525
greater disentanglement, where now we

1049
00:36:12,225 --> 00:36:13,620
0,270 270,600 600,870 870,1125 1125,1395
can consider only a single

1050
00:36:13,620 --> 00:36:16,250
0,405 405,680 1180,1470 1470,1820 2230,2630
latent variable head pose and

1051
00:36:16,840 --> 00:36:18,510
0,350 350,700 870,1220 1220,1490 1490,1670
the smile, the position of

1052
00:36:18,510 --> 00:36:19,670
0,120 120,345 345,615 615,840 840,1160
the mouth in these images

1053
00:36:20,020 --> 00:36:22,620
0,335 335,635 635,1000 1680,2080 2340,2600
is more constant compared to

1054
00:36:22,620 --> 00:36:24,600
0,260 310,710 910,1310
the standard v.|
|

1055
00:36:26,210 --> 00:36:27,760
0,260 260,520 870,1175 1175,1370 1370,1550
All right, so this is
好的，这就是所有的核心数学，核心运算，a的核心架构，我们将在今天的课程中讨论，在这节课结束之前。作为最后一个注解，我想提醒大家回到我在本课开始时介绍的鼓舞人心的例子，面部检测。

1056
00:36:27,760 --> 00:36:29,610
0,285 285,600 600,840 840,1160 1450,1850
really all the core math,

1057
00:36:29,720 --> 00:36:32,005
0,305 305,610 1290,1690 1800,2090 2090,2285
the core operations, the core

1058
00:36:32,005 --> 00:36:33,835
0,305 595,995 1015,1260 1260,1515 1515,1830
architecture of the a that

1059
00:36:33,835 --> 00:36:35,260
0,330 330,480 480,645 645,905 1105,1425
we're going to cover in

1060
00:36:35,260 --> 00:36:36,535
0,390 390,615 615,885 885,1065 1065,1275
today's lecture and in this

1061
00:36:36,535 --> 00:36:39,370
0,335 385,675 675,965 2215,2565 2565,2835
class in general to close

1062
00:36:39,370 --> 00:36:41,680
0,225 225,530 760,1160 1840,2130 2130,2310
this section. {And,as -} a

1063
00:36:41,680 --> 00:36:42,850
0,255 255,555 555,780 780,960 960,1170
final note, I want to

1064
00:36:42,850 --> 00:36:44,005
0,255 255,465 465,735 735,1005 1005,1155
remind you back to the

1065
00:36:44,005 --> 00:36:46,450
0,545 595,995 1435,1785 1785,2135 2155,2445
motivating example that I introduced

1066
00:36:46,450 --> 00:36:47,350
0,150 150,330 330,555 555,735 735,900
at the beginning of this

1067
00:36:47,350 --> 00:36:49,440
0,290 670,1170 1170,1700
lecture, facial detection.|
|

1068
00:36:49,750 --> 00:36:51,860
0,305 305,610 660,1060 1110,1630 1710,2110
Where now hopefully you've understood
现在，希望你已经理解了潜变量学习和编码的概念，以及这对于面部检测这样的任务是多么有用，我们可能想要了解数据中潜在特征的分布，实际上，你将在软件实验室中亲身实践，建立可以自动发现的变化自动编码器。

1069
00:36:51,970 --> 00:36:53,690
0,335 335,670 810,1115 1115,1445 1445,1720
this concept of latent variable

1070
00:36:53,770 --> 00:36:55,860
0,400 450,710 710,1390 1530,1850 1850,2090
learning and encoding, and how

1071
00:36:55,860 --> 00:36:57,180
0,240 240,435 435,585 585,860 1000,1320
this may be useful for

1072
00:36:57,180 --> 00:36:59,060
0,225 225,525 525,855 855,1305 1305,1880
a task like facial detection,

1073
00:36:59,380 --> 00:37:00,660
0,290 290,485 485,740 740,1025 1025,1280
where we may want to

1074
00:37:00,660 --> 00:37:02,925
0,320 640,1040 1060,1610 1810,2100 2100,2265
learn those distributions of the

1075
00:37:02,925 --> 00:37:04,610
0,275 625,1020 1020,1290 1290,1425 1425,1685
underlying features in the data,

1076
00:37:05,320 --> 00:37:06,495
0,365 365,635 635,875 875,1025 1025,1175
and indeed, you're going to

1077
00:37:06,495 --> 00:37:08,370
0,165 165,420 420,690 690,1025 1495,1875
get hands on practice in

1078
00:37:08,370 --> 00:37:11,360
0,380 400,795 795,1430 2260,2625 2625,2990
the software labs to build

1079
00:37:11,770 --> 00:37:13,760
0,635 635,950 950,1475 1475,1685 1685,1990
variational auto encoders that can

1080
00:37:13,870 --> 00:37:15,780
0,400 570,1390
automatically uncover.|
|

1081
00:37:16,170 --> 00:37:18,470
0,400 480,880 1140,1670 1670,2075 2075,2300
Features underlying facial detection data
面部检测数据集的潜在特征，并使用这一点来实际了解这些数据和模型可能存在的潜在和隐藏的偏见。

1082
00:37:18,470 --> 00:37:20,380
0,350 700,1050 1050,1350 1350,1605 1605,1910
sets and use this to

1083
00:37:20,460 --> 00:37:23,210
0,400 840,1240 1440,1840 2130,2450 2450,2750
actually understand underlying and hidden

1084
00:37:23,210 --> 00:37:25,325
0,770 910,1215 1215,1520 1570,1905 1905,2115
biases that may exist with

1085
00:37:25,325 --> 00:37:26,615
0,195 195,515 685,960 960,1110 1110,1290
those data and with those

1086
00:37:26,615 --> 00:37:27,640
0,305
models.|
|

1087
00:37:27,890 --> 00:37:28,945
0,245 245,350 350,635 635,815 815,1055
And it doesn't just stop
而且它并不只是止步于此。明天，我们将有一个非常非常令人兴奋的客座讲座，关于稳健和值得信赖的深度学习，它将使这个概念更进一步，认识到我们如何使用生成模型和潜在变量学习的想法，不仅发现和诊断偏见，而且实际上解决和减轻这些偏见的一些有害影响。

1088
00:37:28,945 --> 00:37:30,655
0,335 565,960 960,1275 1275,1485 1485,1710
there. {Tomorrow,we'll -} have a

1089
00:37:30,655 --> 00:37:32,490
0,225 225,555 555,935 1165,1500 1500,1835
very, very exciting guest lecture

1090
00:37:33,170 --> 00:37:35,215
0,400 450,830 830,1175 1175,1850 1850,2045
on robust and trustworthy deep

1091
00:37:35,215 --> 00:37:36,850
0,305 655,960 960,1170 1170,1395 1395,1635
learning, which will take this

1092
00:37:36,850 --> 00:37:38,920
0,320 340,630 630,855 855,1190 1690,2070
concept a step further to

1093
00:37:38,920 --> 00:37:40,390
0,380 550,870 870,1050 1050,1215 1215,1470
realize how we can use

1094
00:37:40,390 --> 00:37:41,850
0,330 330,555 555,690 690,1155 1155,1460
this idea of generative models

1095
00:37:42,140 --> 00:37:44,170
0,305 305,680 680,970 990,1390 1770,2030
and latent variable learning to

1096
00:37:44,170 --> 00:37:46,375
0,150 150,440 700,1425 1425,1635 1635,2205
not only uncover and diagnose

1097
00:37:46,375 --> 00:37:49,360
0,725 1045,1445 1615,2015 2245,2645 2665,2985
biases, but actually solve and

1098
00:37:49,360 --> 00:37:51,055
0,540 540,780 780,930 930,1155 1155,1695
mitigate some of those harmful

1099
00:37:51,055 --> 00:37:52,920
0,270 270,480 480,660 660,1295 1465,1865
effects of those biases in.|
|

1100
00:37:53,380 --> 00:37:55,195
0,270 270,530 610,885 885,1275 1275,1815
Neural networks for facial detection
用于面部检测和其他应用的神经网络。

1101
00:37:55,195 --> 00:37:57,520
0,270 270,545 715,1115
and other applications.|
|

1102
00:37:57,830 --> 00:37:59,875
0,275 275,550 930,1325 1325,1610 1610,2045
All right, so to summarize
好了，简单地总结一下VAS的要点，我们已经了解了它们是如何将数据压缩到这个紧凑的编码表示中的。从这个表示中，我们可以以完全无监督的方式生成输入的重构。

1103
00:37:59,875 --> 00:38:01,300
0,300 300,570 570,795 795,1115 1135,1425
quickly the key points of

1104
00:38:01,300 --> 00:38:03,535
0,380 940,1320 1320,1560 1560,1920 1920,2235
vas we've gone through how

1105
00:38:03,535 --> 00:38:05,665
0,360 360,635 745,1145 1225,1785 1785,2130
they're able to compress data

1106
00:38:05,665 --> 00:38:08,400
0,390 390,705 705,1265 1405,1995 1995,2735
into this compact encoded representation.

1107
00:38:09,140 --> 00:38:10,855
0,320 320,530 530,1295 1295,1535 1535,1715
{From,this -} representation, we can

1108
00:38:10,855 --> 00:38:12,840
0,305 355,1230 1230,1395 1395,1620 1620,1985
generate reconstructions of the input

1109
00:38:13,340 --> 00:38:15,240
0,260 260,485 485,800 800,1550 1550,1900
in a completely unsupervised fashion.|
|

1110
00:38:17,160 --> 00:38:19,325
0,290 290,580 1410,1730 1730,1955 1955,2165
We can train them end
我们可以使用参数化技巧对它们进行端到端的训练。我们可以通过扰动个体潜变量的值来理解它们的语义解释。最后，我们可以从潜在空间中进行采样，通过解码器向上传递来生成新的示例。

1111
00:38:19,325 --> 00:38:21,400
0,165 165,420 420,795 795,1140 1140,2075
to end using the parameterurization

1112
00:38:21,450 --> 00:38:23,975
0,400 1230,1490 1490,1750 1950,2300 2300,2525
trick. {We,can -} understand the

1113
00:38:23,975 --> 00:38:27,500
0,635 1465,2165 2215,2615 2725,3105 3105,3525
semantic interpretation of individual latent

1114
00:38:27,500 --> 00:38:29,740
0,510 510,830 1000,1740 1740,1950 1950,2240
variables by perturbing their {values.,And

1115
00:38:30,630 --> 00:38:32,080
0,305 305,610 660,935 935,1130 1130,1450
-} finally, we can sample

1116
00:38:32,250 --> 00:38:34,520
0,400 630,1030 1140,1640 1640,1960 1980,2270
from the latent space to

1117
00:38:34,520 --> 00:38:36,640
0,290 610,990 990,1370 1450,1785 1785,2120
generate new examples by passing

1118
00:38:36,660 --> 00:38:38,680
0,350 350,700 930,1235 1235,1415 1415,2020
back up through the decoder.|
|

1119
00:38:40,050 --> 00:38:41,990
0,400 600,1000 1140,1460 1460,1715 1715,1940
So v are looking at
所以v把潜变量编码和密度估计作为他们的核心问题。如果现在我们只关注生成样本的质量，这是我们更关心的任务。

1120
00:38:41,990 --> 00:38:43,445
0,285 285,600 600,840 840,1215 1215,1455
this idea of latent variable

1121
00:38:43,445 --> 00:38:45,740
0,660 660,855 855,1455 1455,1895 1975,2295
encoding and density estimation as

1122
00:38:45,740 --> 00:38:48,005
0,240 240,495 495,830 1810,2085 2085,2265
their core problem. {What,if -}

1123
00:38:48,005 --> 00:38:49,445
0,225 225,450 450,750 750,1140 1140,1440
now we only focus on

1124
00:38:49,445 --> 00:38:50,980
0,240 240,575 715,1035 1035,1245 1245,1535
the quality of the generated

1125
00:38:51,120 --> 00:38:52,880
0,545 545,755 755,1265 1265,1535 1535,1760
samples, and that's the task

1126
00:38:52,880 --> 00:38:54,640
0,225 225,390 390,680 880,1280 1360,1760
that we care more about.|
|

1127
00:38:55,690 --> 00:38:56,790
0,290 290,515 515,770 770,905 905,1100
For that, we're going to
为此，我们将过渡到一种新型的生成性模型，称为生成性对抗网络，或称GAM。

1128
00:38:56,790 --> 00:38:58,050
0,290 460,690 690,825 825,1050 1050,1260
transition to a new type

1129
00:38:58,050 --> 00:39:00,060
0,165 165,585 585,860 1420,1770 1770,2010
of generative model called a

1130
00:39:00,060 --> 00:39:02,690
0,540 540,1320 1320,1610 1750,2150 2200,2630
generative adversarial network, or gam.|
|

1131
00:39:04,040 --> 00:39:05,620
0,305 305,530 530,880 930,1265 1265,1580
Where with gans, our goal
对于Gans，我们的目标实际上是更多地关心如何生成与现有数据相似的新实例，这意味着我们希望尝试从模型试图近似的潜在非常复杂的分布中进行采样。

1132
00:39:05,620 --> 00:39:07,015
0,330 330,630 630,855 855,1080 1080,1395
is really that we care

1133
00:39:07,015 --> 00:39:08,875
0,365 685,1050 1050,1365 1365,1635 1635,1860
more about how well we

1134
00:39:08,875 --> 00:39:11,245
0,305 505,825 825,1595 1885,2160 2160,2370
generate new instances that are

1135
00:39:11,245 --> 00:39:12,900
0,335 445,720 720,945 945,1275 1275,1655
similar to the existing data,

1136
00:39:13,610 --> 00:39:15,190
0,365 365,695 695,980 980,1280 1280,1580
meaning that we want to

1137
00:39:15,190 --> 00:39:17,170
0,255 255,555 555,920 1060,1460 1630,1980
try to sample from a

1138
00:39:17,170 --> 00:39:19,960
0,350 430,830 940,1340 1360,1760 2440,2790
potentially very complex distribution that

1139
00:39:19,960 --> 00:39:21,445
0,210 210,465 465,795 795,1110 1110,1485
the model is trying to

1140
00:39:21,445 --> 00:39:22,600
0,695
approximate.|
|

1141
00:39:23,370 --> 00:39:25,060
0,260 260,365 365,610 780,1180 1290,1690
It can be extremely, extremely
直接了解这种分布可能非常非常困难，因为它很复杂，维度很高，我们希望能够绕过这种复杂性。

1142
00:39:25,230 --> 00:39:27,160
0,305 305,515 515,820 990,1390 1530,1930
difficult to learn that distribution

1143
00:39:27,270 --> 00:39:29,375
0,400 570,935 935,1385 1385,1760 1760,2105
directly because it's complex, it's

1144
00:39:29,375 --> 00:39:31,160
0,180 180,875 1075,1350 1350,1515 1515,1785
high dimensional and we want

1145
00:39:31,160 --> 00:39:32,600
0,380 640,900 900,1080 1080,1260 1260,1440
to be able to get

1146
00:39:32,600 --> 00:39:34,440
0,225 225,495 495,1190
around that complexity.|
|

1147
00:39:34,510 --> 00:39:36,015
0,305 305,725 725,1025 1025,1310 1310,1505
What ganss do is they
甘斯所做的是，他们会说，好吧，如果我们从超级，超级简单的东西开始呢？就像它可以得到完全随机的噪音一样简单吗？我们能否建立一种神经网络体系结构，能够学习从完全随机的噪音中生成合成样本？

1148
00:39:36,015 --> 00:39:37,740
0,305 325,725 1105,1380 1380,1530 1530,1725
say, okay, what if we

1149
00:39:37,740 --> 00:39:39,330
0,225 225,435 435,740 880,1260 1260,1590
start from something super, super

1150
00:39:39,330 --> 00:39:40,800
0,345 345,735 735,1080 1080,1320 1320,1470
simple? As simple as it

1151
00:39:40,800 --> 00:39:43,190
0,150 150,440 1150,1550 1570,1970 1990,2390
can get completely random noise?

1152
00:39:43,900 --> 00:39:45,150
0,290 290,470 470,695 695,920 920,1250
Could we build a neural

1153
00:39:45,150 --> 00:39:47,720
0,290 610,1010 1780,2055 2055,2250 2250,2570
network architecture that can learn

1154
00:39:48,070 --> 00:39:50,780
0,305 305,610 960,1600 1680,2080 2310,2710
to generate synthetic examples from

1155
00:39:50,860 --> 00:39:53,360
0,400 870,1270 1530,1930
complete random noise?|
|

1156
00:39:54,210 --> 00:39:55,420
0,290 290,530 530,800 800,965 965,1210
And this is the underlying
这就是GANS的基本概念，目标是训练这个生成器网络，学习从噪声到训练数据分布的转换。

1157
00:39:55,680 --> 00:39:58,925
0,400 570,965 965,1390 2130,2530 2970,3245
concept of gans, where the

1158
00:39:58,925 --> 00:40:00,095
0,225 225,465 465,690 690,945 945,1170
goal is to train this

1159
00:40:00,095 --> 00:40:02,710
0,545 565,965 1405,1740 1740,2250 2250,2615
generator network that learns a

1160
00:40:02,730 --> 00:40:05,795
0,400 1050,1355 1355,1660 2310,2710 2760,3065
transformation from noise to the

1161
00:40:05,795 --> 00:40:07,840
0,285 285,665 775,1175
training data distribution.|
|

1162
00:40:07,840 --> 00:40:09,145
0,270 270,420 420,630 630,960 960,1305
With the goal of making
目标是使生成的示例尽可能接近真实的deo。

1163
00:40:09,145 --> 00:40:11,425
0,240 240,515 745,1145 1585,1965 1965,2280
the generated examples as close

1164
00:40:11,425 --> 00:40:12,940
0,180 180,285 285,480 480,965 1165,1515
to the real deo as

1165
00:40:12,940 --> 00:40:14,640
0,350
possible.|
|

1166
00:40:14,910 --> 00:40:17,720
0,335 335,700 1560,1880 1880,2480 2480,2810
With gans, the breakthrough idea
对于甘斯，这里的突破性想法是将这两个神经网络连接在一起，一个是生成器，一个是鉴别器。

1167
00:40:17,720 --> 00:40:21,695
0,300 300,650 910,1310 2860,3600 3600,3975
here was to interface these

1168
00:40:21,695 --> 00:40:24,455
0,330 330,675 675,965 1165,1565 2365,2760
two neural networks together, one

1169
00:40:24,455 --> 00:40:26,465
0,315 315,510 510,1025 1405,1725 1725,2010
being a generator and one

1170
00:40:26,465 --> 00:40:28,440
0,365 385,660 660,1385
being a discriminator.|
|

1171
00:40:28,480 --> 00:40:30,030
0,305 305,560 560,860 860,1210 1260,1550
And these two components, the
而这两个组成部分，生成器和鉴别器，正处于战争之中，相互竞争。

1172
00:40:30,030 --> 00:40:32,625
0,510 510,890 1000,1820 2110,2385 2385,2595
generator and discriminator, are at

1173
00:40:32,625 --> 00:40:34,440
0,335 535,935 1105,1440 1440,1650 1650,1815
war, at competition with each

1174
00:40:34,440 --> 00:40:35,460
0,290
other.|
|

1175
00:40:35,650 --> 00:40:37,425
0,400 930,1220 1220,1430 1430,1625 1625,1775
Specifically, the goal of the
具体地说，生成器网络的目标是观察随机噪声，并试图产生尽可能接近真实数据的模拟数据。

1176
00:40:37,425 --> 00:40:39,855
0,450 450,815 1375,1770 1770,2160 2160,2430
generator network is to look

1177
00:40:39,855 --> 00:40:41,790
0,165 165,450 450,845 1315,1665 1665,1935
at random noise and try

1178
00:40:41,790 --> 00:40:43,365
0,285 285,600 600,840 840,1365 1365,1575
to produce an imitation of

1179
00:40:43,365 --> 00:40:45,015
0,135 135,395 655,1110 1110,1350 1350,1650
the data that's as close

1180
00:40:45,015 --> 00:40:46,760
0,195 195,345 345,570 570,905
to real as possible.|
|

1181
00:40:47,100 --> 00:40:49,450
0,290 290,1000 1380,1745 1745,2030 2030,2350
The discriminator then takes the
然后，鉴别器获取生成器的输出以及一些真实的数据示例，并尝试学习分类决策。

1182
00:40:49,530 --> 00:40:51,980
0,400 630,965 965,1190 1190,1720 2100,2450
output of the generator as

1183
00:40:51,980 --> 00:40:53,150
0,240 240,435 435,645 645,870 870,1170
well as some real data

1184
00:40:53,150 --> 00:40:55,190
0,380 940,1290 1290,1590 1590,1815 1815,2040
examples, and tries to learn

1185
00:40:55,190 --> 00:40:58,340
0,270 270,1820 2080,2480
a classification decision.|
|

1186
00:40:58,750 --> 00:41:01,480
0,760 870,1270 1380,1685 1685,1990
Distinguishing real from fake.|
辨别真假。|

1187
00:41:01,480 --> 00:41:02,965
0,345 345,710 820,1110 1110,1275 1275,1485
And effectively in the g,
在G中，这两个部分相互竞争，试图迫使鉴别者更好地区分真假，而生成器试图愚弄和超越鉴别者进行分类的能力。

1188
00:41:02,965 --> 00:41:05,110
0,315 315,695 865,1265 1585,1890 1890,2145
these two components are going

1189
00:41:05,110 --> 00:41:06,520
0,255 255,450 450,740 880,1245 1245,1410
back and forth, competing each

1190
00:41:06,520 --> 00:41:08,665
0,290 1000,1335 1335,1545 1545,1785 1785,2145
other, trying to force the

1191
00:41:08,665 --> 00:41:10,555
0,810 810,1080 1080,1365 1365,1665 1665,1890
discriminator to better learn this

1192
00:41:10,555 --> 00:41:12,600
0,485 985,1305 1305,1530 1530,1740 1740,2045
distinction between real and fake,

1193
00:41:13,250 --> 00:41:15,325
0,400 750,1055 1055,1520 1520,1805 1805,2075
while the generator is trying

1194
00:41:15,325 --> 00:41:17,920
0,335 415,815 925,1230 1230,2015 2215,2595
to fool and outperform the

1195
00:41:17,920 --> 00:41:19,735
0,380 430,705 705,840 840,1520 1540,1815
ability of the discriminator to

1196
00:41:19,735 --> 00:41:21,740
0,225 225,575 655,1325
make that classification.|
|

1197
00:41:22,780 --> 00:41:25,490
0,400 1170,1610 1610,1715 1715,2230 2310,2710
So that's the overlying concept.
所以这就是最重要的概念。但真正让我兴奋的是下一个例子，这是我在这门课上最喜欢的插图和演练之一。它得到了甘斯背后的直觉，他们是如何工作的，以及潜在的概念。

1198
00:41:26,080 --> 00:41:27,915
0,400 420,695 695,1060 1140,1505 1505,1835
{But,what -} I'm really excited

1199
00:41:27,915 --> 00:41:29,900
0,300 300,635 1015,1290 1290,1565 1585,1985
about is the next example,

1200
00:41:29,920 --> 00:41:30,980
0,275 275,455 455,635 635,785 785,1060
which is one of my

1201
00:41:31,120 --> 00:41:34,230
0,395 395,790 960,1690 1860,2260 2460,3110
absolute favorite illustrations and walkthroughs

1202
00:41:34,230 --> 00:41:35,640
0,135 135,315 315,620 1000,1260 1260,1410
in this {class.,And -} it

1203
00:41:35,640 --> 00:41:37,530
0,255 255,525 525,705 705,1280 1540,1890
gets at the intuition behind

1204
00:41:37,530 --> 00:41:39,345
0,380 700,990 990,1170 1170,1460 1540,1815
gans, how they work and

1205
00:41:39,345 --> 00:41:42,760
0,120 120,365 535,935
the underlying concept.|
|

1206
00:41:42,800 --> 00:41:43,735
0,320 320,440 440,605 605,755 755,935
We're going to look at
我们来看一个例子，直线上的点，对吗？这就是我们正在处理的数据。

1207
00:41:43,735 --> 00:41:45,280
0,195 195,420 420,690 690,1025 1225,1545
a one d example, points

1208
00:41:45,280 --> 00:41:46,735
0,180 180,300 300,560 580,960 960,1455
on a line, right? That's

1209
00:41:46,735 --> 00:41:47,800
0,210 210,420 420,630 630,840 840,1065
the data that we're working

1210
00:41:47,800 --> 00:41:48,720
0,380
with.|
|

1211
00:41:48,760 --> 00:41:50,870
0,335 335,670 840,1130 1130,1690 1710,2110
And again, the generator starts
同样，生成器从随机噪声开始，产生一些虚假数据。它们会落在这条一维线上的某个地方。

1212
00:41:50,890 --> 00:41:53,175
0,400 450,815 815,1180 1500,1985 1985,2285
from random noise, produces some

1213
00:41:53,175 --> 00:41:54,375
0,255 255,540 540,840 840,990 990,1200
fake data. They're going to

1214
00:41:54,375 --> 00:41:55,620
0,270 270,630 630,900 900,1065 1065,1245
fall somewhere on this one

1215
00:41:55,620 --> 00:41:57,460
0,680 730,1130
dimensional line.|
|

1216
00:41:57,870 --> 00:41:59,960
0,400 630,905 905,1130 1130,1480 1740,2090
Now the next step is
现在下一步是鉴别器，然后看到这些点。

1217
00:41:59,960 --> 00:42:01,610
0,225 225,885 885,1185 1185,1425 1425,1650
the discriminator then sees these

1218
00:42:01,610 --> 00:42:02,720
0,320
points.|
|

1219
00:42:02,920 --> 00:42:04,760
0,400 450,850 870,1175 1175,1460 1460,1840
And it also sees some
它还看到了一些真实的数据。

1220
00:42:04,810 --> 00:42:06,160
0,305 305,610
real data.|
|

1221
00:42:06,460 --> 00:42:07,880
0,275 275,455 455,635 635,770 770,1420
The goal of the discriminator
鉴别器的目标是训练它输出它所看到的实例是真是假的概率。

1222
00:42:08,050 --> 00:42:10,370
0,400 720,995 995,1190 1190,1510 1920,2320
is to be trained to

1223
00:42:10,450 --> 00:42:12,530
0,260 260,425 425,1000 1320,1700 1700,2080
output a probability that an

1224
00:42:12,760 --> 00:42:14,720
0,290 290,500 500,820 1290,1625 1625,1960
instance it sees is real

1225
00:42:15,250 --> 00:42:16,560
0,320 320,640
or fake.|
|

1226
00:42:17,130 --> 00:42:18,770
0,365 365,730 840,1130 1130,1340 1340,1640
And initially in the beginning
而且最初在训练之前，它没有得到正确的训练。因此，它的预测可能不是很好。但在训练的过程中，你将训练它，它有望开始增加那些真实例子的概率，减少那些虚假例子的可能性。

1227
00:42:18,770 --> 00:42:20,975
0,380 460,860 1300,1695 1695,1905 1905,2205
before training, it's not trained

1228
00:42:20,975 --> 00:42:22,745
0,365 625,885 885,1050 1050,1575 1575,1770
right. {So,its -} predictions may

1229
00:42:22,745 --> 00:42:24,640
0,165 165,330 330,555 555,905 1495,1895
not be very {good.,But -}

1230
00:42:24,750 --> 00:42:25,955
0,335 335,545 545,710 710,905 905,1205
over the course of training,

1231
00:42:25,955 --> 00:42:27,100
0,330 330,480 480,675 675,855 855,1145
you're going to train it

1232
00:42:27,480 --> 00:42:29,200
0,290 290,580 630,1025 1025,1370 1370,1720
and it hopefully will start

1233
00:42:29,430 --> 00:42:32,090
0,380 380,635 635,1180 2040,2360 2360,2660
increasing the probability for those

1234
00:42:32,090 --> 00:42:34,270
0,380 520,780 780,960 960,1280 1780,2180
examples that are real and

1235
00:42:34,470 --> 00:42:36,520
0,440 440,575 575,1120 1470,1760 1760,2050
decreasing the probability for those

1236
00:42:36,540 --> 00:42:38,560
0,400 720,980 980,1145 1145,1450
examples that are fake.|
|

1237
00:42:38,750 --> 00:42:40,860
0,400 480,880 1170,1550 1550,1820 1820,2110
Overall goal is to predict
总体目标是预测什么是真实的。

1238
00:42:40,880 --> 00:42:42,260
0,260 260,425 425,730
what is real.|
|

1239
00:42:42,600 --> 00:42:45,560
0,400 1140,1540 1590,1850 1850,2525 2525,2960
Until eventually the discriminator reaches
直到最终鉴别器达到这一点，它有一个完美的分离，完美的真假分类。

1240
00:42:45,560 --> 00:42:46,685
0,195 195,465 465,765 765,975 975,1125
this point where it has

1241
00:42:46,685 --> 00:42:49,180
0,180 180,450 450,1085 1465,1830 1830,2495
a perfect separation, perfect classification

1242
00:42:49,860 --> 00:42:52,480
0,400 570,970 1260,1640 1640,2020
of real versus fake.|
|

1243
00:42:53,040 --> 00:42:54,395
0,400 600,845 845,980 980,1160 1160,1355
Okay, so at this point,
好的，在这一点上，鉴别器认为，好的，我已经完成了我的工作，现在我们回到生成器，它会看到真实数据所在的示例。

1244
00:42:54,395 --> 00:42:56,195
0,165 165,825 825,1205 1285,1545 1545,1800
the discriminator thinks, okay, I've

1245
00:42:56,195 --> 00:42:58,385
0,150 150,345 345,635 1135,1535 1915,2190
done my job, now we

1246
00:42:58,385 --> 00:42:59,560
0,165 165,390 390,570 570,690 690,1175
go back to the generator

1247
00:43:00,180 --> 00:43:01,540
0,260 260,455 455,725 725,1010 1010,1360
and it sees the examples

1248
00:43:01,560 --> 00:43:02,705
0,290 290,485 485,665 665,860 860,1145
of where the real data

1249
00:43:02,705 --> 00:43:04,140
0,365
lie.|
|

1250
00:43:04,140 --> 00:43:06,015
0,320 730,1005 1005,1260 1260,1575 1575,1875
And it can be forced
而且，它可能会被迫开始将其生成的虚假数据越来越接近真实数据。

1251
00:43:06,015 --> 00:43:07,670
0,270 270,525 525,875 955,1305 1305,1655
to start moving its generated

1252
00:43:07,870 --> 00:43:10,520
0,320 320,640 1230,1630 1890,2270 2270,2650
fake data closer and closer,

1253
00:43:10,900 --> 00:43:12,660
0,400 540,940 1200,1445 1445,1565 1565,1760
increasingly closer to the real

1254
00:43:12,660 --> 00:43:13,640
0,320
data.|
|

1255
00:43:14,310 --> 00:43:15,350
0,260 260,395 395,590 590,800 800,1040
We can then go back
然后我们可以回到鉴别器，它从生成器接收这些新合成的示例，并重复相同的过程来估计任何给定点为真实的概率。

1256
00:43:15,350 --> 00:43:17,000
0,210 210,330 330,945 945,1230 1230,1650
to the discriminator, which receives

1257
00:43:17,000 --> 00:43:19,445
0,285 285,795 795,1430 1630,2030 2140,2445
these newly synthesized examples from

1258
00:43:19,445 --> 00:43:21,650
0,180 180,665 1225,1625 1645,2010 2010,2205
the generator, and repeats that

1259
00:43:21,650 --> 00:43:24,140
0,255 255,590 1240,1640 1780,2250 2250,2490
same process of estimating the

1260
00:43:24,140 --> 00:43:26,170
0,560 820,1140 1140,1395 1395,1680 1680,2030
probability that any given point

1261
00:43:26,370 --> 00:43:27,680
0,305 305,610
is real.|
|

1262
00:43:28,220 --> 00:43:30,175
0,400 660,1040 1040,1420 1470,1775 1775,1955
And learning to increase the
学习增加真实例的概率，减少假点的概率。

1263
00:43:30,175 --> 00:43:31,885
0,540 540,900 900,1200 1200,1425 1425,1710
probability of the true real

1264
00:43:31,885 --> 00:43:34,450
0,365 1105,1410 1410,1590 1590,2135 2185,2565
examples decrease the probability of

1265
00:43:34,450 --> 00:43:36,080
0,255 255,465 465,800
the fake points.|
|

1266
00:43:36,580 --> 00:43:38,580
0,635 635,1240 1260,1595 1595,1820 1820,2000
Adjusting, adjusting over the course
适应，适应在它的训练过程中。

1267
00:43:38,580 --> 00:43:40,020
0,150 150,330 330,650
of its training.|
|

1268
00:43:40,530 --> 00:43:41,660
0,290 290,575 575,830 830,965 965,1130
And finally, we can go
最后，我们可以回到生成器上，最后一次重复。发电机开始将这些假点移动得更近。

1269
00:43:41,660 --> 00:43:43,010
0,240 240,590 640,1020 1020,1245 1245,1350
back and repeat to the

1270
00:43:43,010 --> 00:43:45,130
0,480 480,860 1180,1485 1485,1755 1755,2120
generator again one last time.

1271
00:43:45,390 --> 00:43:47,150
0,290 290,770 770,1115 1115,1445 1445,1760
{The,generator -} starts moving those

1272
00:43:47,150 --> 00:43:49,480
0,285 285,620 910,1310
fake points closer.|
|

1273
00:43:49,490 --> 00:43:51,010
0,400 510,845 845,1175 1175,1415 1415,1520
Closer and closer to the
越来越接近真实数据，以至于假数据几乎都在跟随真实数据的分布。

1274
00:43:51,010 --> 00:43:52,780
0,195 195,530 1060,1395 1395,1620 1620,1770
real data, such that the

1275
00:43:52,780 --> 00:43:54,540
0,195 195,525 525,920 1060,1410 1410,1760
fake data is almost following

1276
00:43:54,590 --> 00:43:56,140
0,350 350,700 930,1220 1220,1370 1370,1550
the distribution of the real

1277
00:43:56,140 --> 00:43:57,660
0,320
data.|
|

1278
00:43:58,180 --> 00:43:59,940
0,305 305,545 545,860 860,1240 1380,1760
At this point, it becomes
在这一点上，鉴别者很难区分真假。

1279
00:43:59,940 --> 00:44:01,800
0,345 345,660 660,1010 1360,1680 1680,1860
very, very hard for the

1280
00:44:01,800 --> 00:44:04,470
0,710 1000,1395 1395,1965 1965,2360 2410,2670
discriminator to distinguish between what

1281
00:44:04,470 --> 00:44:05,775
0,165 165,470 670,975 975,1140 1140,1305
is real and what is

1282
00:44:05,775 --> 00:44:06,800
0,305
fake.|
|

1283
00:44:06,800 --> 00:44:08,405
0,225 225,450 450,915 915,1245 1245,1605
While the generator will continue
而生成器将继续尝试创建虚假数据点来愚弄鉴别者。

1284
00:44:08,405 --> 00:44:10,145
0,300 300,600 600,965 1045,1425 1425,1740
to try to create fake

1285
00:44:10,145 --> 00:44:11,975
0,285 285,635 985,1320 1320,1605 1605,1830
data points to fool the

1286
00:44:11,975 --> 00:44:13,400
0,695
discriminator.|
|

1287
00:44:14,000 --> 00:44:15,355
0,290 290,500 500,755 755,1025 1025,1355
This is really the key
这确实是关键概念，是GAN组件如何本质上相互竞争的基本直觉，在生成器和鉴别器之间来回移动。

1288
00:44:15,355 --> 00:44:17,815
0,395 445,705 705,965 1195,1865 2065,2460
concept, the underlying intuition behind

1289
00:44:17,815 --> 00:44:19,510
0,285 285,540 540,905 1225,1530 1530,1695
how the components of the

1290
00:44:19,510 --> 00:44:21,430
0,290 460,825 825,1190 1390,1785 1785,1920
Gan are essentially competing with

1291
00:44:21,430 --> 00:44:23,020
0,150 150,440 790,1140 1140,1395 1395,1590
each other, going back and

1292
00:44:23,020 --> 00:44:25,440
0,290 580,980 1120,1410 1410,1970 2020,2420
forth between the generator and

1293
00:44:25,670 --> 00:44:27,320
0,260 260,940
the discriminator.|
|

1294
00:44:27,690 --> 00:44:28,900
0,260 260,410 410,665 665,920 920,1210
And in fact, this is
事实上，这就是GAN是如何在实践中被训练的直观概念，生成器首先试图合成新的例子，合成例子来愚弄鉴别者。而鉴别器的目标是同时获取虚假实例和真实数据，以尝试识别合成实例。

1295
00:44:29,220 --> 00:44:31,115
0,335 335,935 935,1240 1320,1640 1640,1895
this intuitive concept is how

1296
00:44:31,115 --> 00:44:32,735
0,195 195,465 465,795 795,1145 1315,1620
the Gan is trained in

1297
00:44:32,735 --> 00:44:35,435
0,305 1135,1425 1425,1590 1590,2105 2335,2700
practice, where the generator first

1298
00:44:35,435 --> 00:44:38,080
0,345 345,615 615,1235 1645,2045 2245,2645
tries to synthesize new examples,

1299
00:44:38,370 --> 00:44:40,520
0,580 630,1030 1470,1760 1760,1970 1970,2150
synthetic examples to fool the

1300
00:44:40,520 --> 00:44:42,605
0,680 1210,1500 1500,1680 1680,1890 1890,2085
discriminator. {And,the -} goal of

1301
00:44:42,605 --> 00:44:44,135
0,135 135,825 825,1110 1110,1305 1305,1530
the discriminator is to take

1302
00:44:44,135 --> 00:44:46,490
0,335 685,975 975,1260 1260,1655 2035,2355
both the fake examples and

1303
00:44:46,490 --> 00:44:47,900
0,180 180,345 345,650 850,1170 1170,1410
the real data to try

1304
00:44:47,900 --> 00:44:50,440
0,320 580,945 945,1200 1200,1785 1785,2540
to identify the synthesized instances.|
|

1305
00:44:51,980 --> 00:44:53,875
0,350 350,700 1050,1370 1370,1595 1595,1895
In training, what this means
在训练中，这意味着目标是。

1306
00:44:53,875 --> 00:44:56,240
0,285 285,575 625,1005 1005,1385
is that the objective.|
|

1307
00:44:56,240 --> 00:44:57,830
0,195 195,470 580,900 900,1095 1095,1590
The loss for the generator
发电机和鉴别器的损失必须是彼此不一致的。它们是对抗性的，这就是生成对抗性网络中对抗性成分的原因。

1308
00:44:57,830 --> 00:44:59,660
0,285 285,1010 1150,1470 1470,1650 1650,1830
and discriminator have to be

1309
00:44:59,660 --> 00:45:01,000
0,270 270,750 750,900 900,1050 1050,1340
at odds with each other.

1310
00:45:01,260 --> 00:45:03,290
0,335 335,1240 1290,1580 1580,1805 1805,2030
They're adversarial, and that is

1311
00:45:03,290 --> 00:45:04,730
0,180 180,405 405,740 790,1140 1140,1440
what gives rise to the

1312
00:45:04,730 --> 00:45:07,520
0,345 345,740 1090,2040 2040,2310 2310,2790
component of adversarial in generative

1313
00:45:07,520 --> 00:45:09,340
0,750 750,1100
adversarial network.|
|

1314
00:45:09,960 --> 00:45:14,110
0,380 380,1550 1550,1930 2640,3040 3750,4150
These adversarial objectives are then
然后将这些对抗性目标放在一起，以定义达到稳定的全局最优意味着什么，在这种情况下，生成器能够产生完全欺骗鉴别器的真实数据分布。

1315
00:45:14,160 --> 00:45:15,965
0,350 350,700 810,1115 1115,1420 1440,1805
put together to then define

1316
00:45:15,965 --> 00:45:17,240
0,225 225,360 360,600 600,930 930,1275
what it means to arrive

1317
00:45:17,240 --> 00:45:19,210
0,225 225,390 390,710 850,1250 1450,1970
at a stable global optimum,

1318
00:45:19,680 --> 00:45:22,000
0,400 990,1265 1265,1715 1715,1985 1985,2320
where the generator is capable

1319
00:45:22,080 --> 00:45:24,280
0,400 420,820 1080,1430 1430,1780 1800,2200
of producing the true data

1320
00:45:24,330 --> 00:45:26,740
0,400 870,1205 1205,1540 1560,1960 2010,2410
distribution that would completely fool

1321
00:45:26,790 --> 00:45:28,300
0,275 275,940
the discriminator.|
|

1322
00:45:30,440 --> 00:45:31,810
0,485 485,695 695,860 860,1070 1070,1370
Concretely, this can be defined
具体地说，这可以根据亏损目标在数学上反复定义。

1323
00:45:31,810 --> 00:45:33,880
0,650 850,1170 1170,1470 1470,1815 1815,2070
mathematically in terms of a

1324
00:45:33,880 --> 00:45:37,040
0,290 460,860 1630,1950 1950,2270
loss objective and again.|
|

1325
00:45:37,370 --> 00:45:38,850
0,290 290,635 635,920 920,1145 1145,1480
Though I'm, I'm showing math,
虽然我在展示数学，但我可以，我们可以把这些提炼出来，通过每个术语所反映的核心直观概念和概念概念，希望这个例子能传达出来。

1326
00:45:39,110 --> 00:45:40,480
0,275 275,550 570,830 830,965 965,1370
I can, we can distill

1327
00:45:40,480 --> 00:45:41,880
0,240 240,510 510,765 765,1035 1035,1400
this down and go through

1328
00:45:41,990 --> 00:45:43,200
0,305 305,500 500,665 665,875 875,1210
what each of these terms

1329
00:45:43,670 --> 00:45:45,055
0,380 380,680 680,920 920,1115 1115,1385
reflect in terms of that

1330
00:45:45,055 --> 00:45:47,530
0,315 315,990 990,1385 1495,1800 1800,2475
core intuitive idea and conceptual

1331
00:45:47,530 --> 00:45:50,560
0,380 580,980 1030,1430 1510,1910 2710,3030
idea that hopefully that one

1332
00:45:50,560 --> 00:45:52,520
0,300 300,680 700,1400
d example conveyed.|
|

1333
00:45:53,140 --> 00:45:55,455
0,275 275,515 515,790 870,1270 1920,2315
So we'll first consider the
所以我们首先考虑鉴别者的观点，它的目标是最大化它的决定的概率，在它的决定中，真实的数据被归类为真实的假数据，被归类为假的。

1334
00:45:55,455 --> 00:45:57,440
0,360 360,600 600,750 750,1415 1585,1985
perspective of the discriminator d

1335
00:45:58,210 --> 00:46:00,105
0,350 350,695 695,995 995,1205 1205,1895
its goal is to maximize

1336
00:46:00,105 --> 00:46:03,180
0,645 645,915 915,1200 1200,1595 2815,3075
probability that its decisions in

1337
00:46:03,180 --> 00:46:04,730
0,255 255,615 615,930 930,1215 1215,1550
its decisions that real data

1338
00:46:04,900 --> 00:46:07,220
0,305 305,815 815,1150 1650,1985 1985,2320
are classified real fake data

1339
00:46:07,330 --> 00:46:09,060
0,620 620,860 860,1180
classified as fake.|
|

1340
00:46:09,330 --> 00:46:11,410
0,320 320,640 1170,1460 1460,1715 1715,2080
So here the first term
这里，z的第一项g是生成器的输出，g的d是鉴别器对生成的输出为假的估计。

1341
00:46:12,360 --> 00:46:13,535
0,260 260,455 455,725 725,965 965,1175
g of z is the

1342
00:46:13,535 --> 00:46:16,715
0,905 1015,1415 1885,2285 2455,2835 2835,3180
generator's output, and d of

1343
00:46:16,715 --> 00:46:18,305
0,270 270,510 510,845 1105,1410 1410,1590
g of z is the

1344
00:46:18,305 --> 00:46:21,520
0,935 1135,1535 1975,2375 2515,2865 2865,3215
discriminator's estimate of that generated

1345
00:46:21,930 --> 00:46:23,760
0,305 305,530 530,815 815,1180
output as being fake.|
|

1346
00:46:25,200 --> 00:46:26,870
0,245 245,380 380,670 1110,1445 1445,1670
D of X X is
X的D是真实的数据，所以X的D是真实实例为假的概率的估计。

1347
00:46:26,870 --> 00:46:28,310
0,165 165,360 360,680 940,1230 1230,1440
the real data, and so

1348
00:46:28,310 --> 00:46:29,440
0,165 165,315 315,585 585,840 840,1130
d of X is the

1349
00:46:29,490 --> 00:46:31,190
0,275 275,440 440,620 620,1180 1440,1700
estimate of the probability that

1350
00:46:31,190 --> 00:46:33,040
0,165 165,470 940,1290 1290,1545 1545,1850
a real instance is fake.|
|

1351
00:46:33,780 --> 00:46:35,120
0,300 300,645 645,885 885,1035 1035,1340
One minus d of X
X的一个减去d是该真实实例是真实的估计。

1352
00:46:35,680 --> 00:46:37,005
0,290 290,580 630,920 920,1100 1100,1325
is the estimate that that

1353
00:46:37,005 --> 00:46:39,280
0,335 565,945 945,1245 1245,1565
real instance is real.|
|

1354
00:46:39,580 --> 00:46:41,655
0,400 660,1060 1320,1625 1625,1850 1850,2075
So here in both these
因此，在这两种情况下，鉴别者都在做出关于虚假数据和真实数据的决定。它想要一起努力最大化它得到正确答案的可能性，对吗？

1355
00:46:41,655 --> 00:46:43,560
0,300 300,570 570,1200 1200,1535 1555,1905
cases, the discriminator is producing

1356
00:46:43,560 --> 00:46:45,740
0,285 285,620 1060,1460 1540,1860 1860,2180
a decision about fake data,

1357
00:46:46,240 --> 00:46:48,540
0,335 335,670 960,1325 1325,1690 2010,2300
real data. {And,together -} it

1358
00:46:48,540 --> 00:46:50,190
0,240 240,465 465,690 690,990 990,1650
wants to try to maximize

1359
00:46:50,190 --> 00:46:52,140
0,240 240,795 795,1095 1095,1520 1600,1950
the probability that it's getting

1360
00:46:52,140 --> 00:46:54,320
0,350 700,1100 1270,1670
answers correct, right?|
|

1361
00:46:56,360 --> 00:46:58,315
0,400 780,1070 1070,1235 1235,1700 1700,1955
Now with the generator, we
现在，对于发电机，我们有相同的术语，但请记住，发电机永远不会。

1362
00:46:58,315 --> 00:46:59,730
0,195 195,420 420,735 735,1065 1065,1415
have those same exact terms,

1363
00:47:00,320 --> 00:47:01,435
0,305 305,485 485,635 635,875 875,1115
but keep in mind the

1364
00:47:01,435 --> 00:47:03,540
0,435 435,690 690,1025
generator is never.|
|

1365
00:47:03,540 --> 00:47:05,310
0,345 345,690 690,1040 1180,1485 1485,1770
Able to affect anything that
能够影响鉴别器决策实际正在做的任何事情，除了生成新的数据示例之外。

1366
00:47:05,310 --> 00:47:08,295
0,380 520,1500 1500,1850 2050,2450 2680,2985
the discriminator decision is actually

1367
00:47:08,295 --> 00:47:11,420
0,305 865,1265 1705,2310 2310,2705 2725,3125
doing, besides generating new data

1368
00:47:11,440 --> 00:47:12,900
0,400
examples.|
|

1369
00:47:12,900 --> 00:47:14,570
0,255 255,375 375,495 495,1040 1270,1670
So for the generator, its
因此，对于生成器来说，其目标只是将生成的数据被识别为虚假数据的概率降至最低。

1370
00:47:14,590 --> 00:47:17,930
0,400 540,890 890,1240 1980,2380 2670,3340
objective is simply to minimize

1371
00:47:17,980 --> 00:47:20,030
0,320 320,910 1260,1550 1550,1745 1745,2050
the probability that the generated

1372
00:47:20,200 --> 00:47:22,460
0,400 570,970 1260,1625 1625,1925 1925,2260
data is identified as fake.|
|

1373
00:47:24,800 --> 00:47:26,760
0,400 630,935 935,1220 1220,1580 1580,1960
Together, we want to then
总之，我们想把这些放在一起，定义生成器合成假图像意味着什么，希望这些假图像能愚弄鉴别者。

1374
00:47:27,350 --> 00:47:29,245
0,275 275,500 500,850 870,1270 1500,1895
put this together to define

1375
00:47:29,245 --> 00:47:30,685
0,255 255,390 390,665 985,1275 1275,1440
what it means for the

1376
00:47:30,685 --> 00:47:32,970
0,515 595,995 1015,1695 1695,1965 1965,2285
generator to synthesize fake images

1377
00:47:33,320 --> 00:47:35,580
0,320 320,640 930,1330 1350,1610 1610,2260
that hopefully fool the discriminator.|
|

1378
00:47:37,030 --> 00:47:38,750
0,290 290,440 440,700 870,1270 1320,1720
All in all, right, this
总而言之，对了，这个术语，除了数学，除了这个定义的特殊性，我想让你们从这个关于甘斯的这一节中得到的是，我们有这个双重竞争的目标，生成器试图合成这些理想的愚弄最好的鉴别者的合成例子。

1379
00:47:38,860 --> 00:47:41,340
0,400 1170,1505 1505,1700 1700,1960 2100,2480
term, besides the math, besides

1380
00:47:41,340 --> 00:47:43,370
0,380 580,1395 1395,1575 1575,1740 1740,2030
the particularities of this definition,

1381
00:47:44,140 --> 00:47:45,105
0,275 275,425 425,635 635,845 845,965
what I want you to

1382
00:47:45,105 --> 00:47:47,085
0,165 165,485 715,1035 1035,1355 1705,1980
get away from this, from

1383
00:47:47,085 --> 00:47:49,080
0,165 165,405 405,675 675,1025 1705,1995
this section on gans, is

1384
00:47:49,080 --> 00:47:51,080
0,195 195,420 420,705 705,1070 1450,2000
that we have this dual

1385
00:47:51,190 --> 00:47:54,255
0,520 750,1150 1800,2200 2310,2600 2600,3065
competing objective where the generator

1386
00:47:54,255 --> 00:47:55,880
0,255 255,510 510,720 720,1275 1275,1625
is trying to synthesize these

1387
00:47:56,770 --> 00:48:00,140
0,550 600,1000 1500,1900 2130,2950 2970,3370
synthetic examples that ideally fool

1388
00:48:00,460 --> 00:48:02,940
0,350 350,680 680,1475 1475,1870
the best discriminator possible.|
|

1389
00:48:03,100 --> 00:48:04,710
0,275 275,455 455,725 725,1090 1320,1610
And in doing so, the
在这样做的过程中，目标是建立一个网络。

1390
00:48:04,710 --> 00:48:06,165
0,270 270,585 585,920 940,1260 1260,1455
goal is to build up

1391
00:48:06,165 --> 00:48:07,680
0,135 135,395
a network.|
|

1392
00:48:07,810 --> 00:48:09,855
0,380 380,650 650,1460 1460,1760 1760,2045
VIA this adversarial training, this
通过这种对抗性训练，这种对抗性竞争使用生成器来创建最能模拟真实数据分布及其完全合成的新实例的新数据。

1393
00:48:09,855 --> 00:48:12,465
0,875 985,1385 1885,2160 2160,2385 2385,2610
adversarial competition to use the

1394
00:48:12,465 --> 00:48:14,630
0,515 895,1230 1230,1545 1545,1845 1845,2165
generator to create new data

1395
00:48:14,740 --> 00:48:16,875
0,335 335,670 690,1210 1620,1925 1925,2135
that best mimics the true

1396
00:48:16,875 --> 00:48:18,825
0,305 325,725 1015,1290 1290,1560 1560,1950
data distribution and its completely

1397
00:48:18,825 --> 00:48:21,420
0,545 865,1200 1200,1985
synthetic new instances.|
|

1398
00:48:23,120 --> 00:48:24,850
0,400 690,1010 1010,1295 1295,1535 1535,1730
What this amounts to in
实际上，在培训过程之后，您可以只查看生成器组件，然后使用它来创建新的数据实例。

1399
00:48:24,850 --> 00:48:26,455
0,320 460,735 735,930 930,1250 1300,1605
practice is that after the

1400
00:48:26,455 --> 00:48:27,960
0,300 300,695 775,1035 1035,1200 1200,1505
training process, you can look

1401
00:48:28,160 --> 00:48:30,270
0,680 680,920 920,1130 1130,1660 1710,2110
exclusively at the generator component

1402
00:48:30,860 --> 00:48:32,400
0,400 540,860 860,1100 1100,1280 1280,1540
and use it to then

1403
00:48:32,630 --> 00:48:35,740
0,400 600,1000 1020,1400 1400,2200
create new data instances.|
|

1404
00:48:37,220 --> 00:48:38,970
0,305 305,500 500,680 680,970 1350,1750
All this is done by
所有这些都是从随机噪声开始，并试图学习从随机噪声到真实数据分布的模型。实际上，Gans正在做的是学习一种函数，将随机噪声的分布转化为某个目标。

1405
00:48:39,440 --> 00:48:41,755
0,400 420,725 725,1030 1080,1480 1950,2315
starting from random noise and

1406
00:48:41,755 --> 00:48:43,375
0,330 330,570 570,845 1045,1335 1335,1620
trying to learn a model

1407
00:48:43,375 --> 00:48:44,880
0,300 300,525 525,765 765,1085 1105,1505
that goes from random noise

1408
00:48:45,350 --> 00:48:47,130
0,395 395,680 680,935 935,1300 1380,1780
to the real data distribution.

1409
00:48:47,990 --> 00:48:49,645
0,350 350,700 870,1175 1175,1430 1430,1655
{And,effectively -} what gans are

1410
00:48:49,645 --> 00:48:51,120
0,285 285,600 600,915 915,1185 1185,1475
doing is learning a function

1411
00:48:51,530 --> 00:48:53,910
0,400 660,1145 1145,1400 1400,1780 1980,2380
that transforms that distribution of

1412
00:48:53,930 --> 00:48:56,250
0,395 395,790 1320,1655 1655,1955 1955,2320
random noise to some target.|
|

1413
00:48:57,450 --> 00:48:59,735
0,305 305,515 515,1060 1620,2020 2070,2285
What this mapping does is
这种映射的作用是允许我们对噪声空间中的噪声进行特定的观察，并将其映射到某些输出，目标数据空间中的特定输出。

1414
00:48:59,735 --> 00:49:01,235
0,210 210,585 585,900 900,1215 1215,1500
it allows us to take

1415
00:49:01,235 --> 00:49:03,550
0,300 300,695 1285,1685 1735,2025 2025,2315
a particular observation of noise

1416
00:49:03,900 --> 00:49:06,160
0,290 290,515 515,815 815,1180 1860,2260
in that noise space and

1417
00:49:06,420 --> 00:49:08,210
0,335 335,605 605,845 845,1150 1470,1790
map it to some output,

1418
00:49:08,210 --> 00:49:09,800
0,255 255,590 820,1140 1140,1365 1365,1590
a particular output in our

1419
00:49:09,800 --> 00:49:11,640
0,300 300,660 660,1040
target data space.|
|

1420
00:49:12,130 --> 00:49:13,635
0,290 290,500 500,820 960,1250 1250,1505
And in turn, if we
反过来，如果我们考虑一些其他随机的噪声样本，如果我们把它送入g的生成器，它将产生一个全新的实例，落在真正的数据分布流形上的其他地方。

1421
00:49:13,635 --> 00:49:15,270
0,315 315,585 585,870 870,1235 1285,1635
consider some other random sample

1422
00:49:15,270 --> 00:49:16,875
0,240 240,530 940,1215 1215,1410 1410,1605
of noise, if we feed

1423
00:49:16,875 --> 00:49:18,030
0,165 165,345 345,510 510,900 900,1155
it through the generator of

1424
00:49:18,030 --> 00:49:19,515
0,350 550,915 915,1035 1035,1245 1245,1485
g, it's going to produce

1425
00:49:19,515 --> 00:49:22,130
0,270 270,635 655,1055 1195,1595 2215,2615
a completely new instance falling

1426
00:49:22,180 --> 00:49:23,960
0,400 420,815 815,1145 1145,1430 1430,1780
somewhere else on that true

1427
00:49:24,010 --> 00:49:26,280
0,400 480,880 960,1660
data distribution manifold.|
|

1428
00:49:26,380 --> 00:49:28,040
0,395 395,790 990,1265 1265,1400 1400,1660
And indeed, what we can
事实上，我们实际上可以做的是在噪声空间中的轨迹之间进行内插和遍历，然后映射到目标数据空间中的遍历和内插。

1429
00:49:28,150 --> 00:49:30,780
0,320 320,640 660,1060 1500,2170 2280,2630
actually do is interpolate and

1430
00:49:30,780 --> 00:49:33,375
0,560 760,1160 1390,2160 2160,2415 2415,2595
traverse between trajectories in the

1431
00:49:33,375 --> 00:49:35,600
0,255 255,635 1165,1515 1515,1845 1845,2225
noise space that then map

1432
00:49:35,980 --> 00:49:40,110
0,400 990,1810 2160,2540 2540,3310 3780,4130
to traversals and interpolations in

1433
00:49:40,110 --> 00:49:42,240
0,300 300,650 670,1050 1050,1430
the target data space.|
|

1434
00:49:42,240 --> 00:49:43,440
0,270 270,435 435,630 630,900 900,1200
And this is really, really
这真的，真的很酷，因为现在你可以考虑一个起始点和一个目标点，以及在目标数据分布中合成和处理这些图像所需的所有步骤。

1435
00:49:43,440 --> 00:49:44,640
0,345 345,630 630,840 840,1035 1035,1200
cool because now you can

1436
00:49:44,640 --> 00:49:46,280
0,210 210,530 580,930 930,1260 1260,1640
think about an initial point

1437
00:49:46,660 --> 00:49:48,510
0,275 275,455 455,740 740,1120 1560,1850
and a target point and

1438
00:49:48,510 --> 00:49:49,455
0,225 225,450 450,660 660,825 825,945
all the steps that are

1439
00:49:49,455 --> 00:49:51,170
0,195 195,405 405,615 615,935 1315,1715
going to take you to

1440
00:49:51,370 --> 00:49:53,535
0,710 710,1090 1170,1570 1620,1925 1925,2165
synthesize and go between those

1441
00:49:53,535 --> 00:49:55,250
0,335 475,765 765,990 990,1320 1320,1715
images in that target data

1442
00:49:55,330 --> 00:49:56,880
0,400
distribution.|
|

1443
00:49:57,740 --> 00:50:00,685
0,400 630,1025 1025,1420 2490,2780 2780,2945
So hopefully this gives a
因此，希望这能让我们对用于创建新数据实例的生成性建模的概念有一个了解。

1444
00:50:00,685 --> 00:50:02,260
0,270 270,555 555,810 810,1175 1285,1575
sense of this concept of

1445
00:50:02,260 --> 00:50:03,930
0,435 435,915 915,1185 1185,1380 1380,1670
generative modeling for the purpose

1446
00:50:04,010 --> 00:50:06,510
0,400 600,1000 1140,1460 1460,1715 1715,2500
of creating new data instances.|
|

1447
00:50:07,200 --> 00:50:09,860
0,380 640,1040 1360,1725 1725,1965 1965,2660
And that notion of interpolation
而这种插补和数据转换的概念很好地导致了Gans最近的一些进步和应用。

1448
00:50:09,910 --> 00:50:12,480
0,320 320,620 620,1000 1890,2255 2255,2570
and data transformation leads very

1449
00:50:12,480 --> 00:50:13,725
0,350 430,735 735,915 915,1065 1065,1245
nicely to some of the

1450
00:50:13,725 --> 00:50:16,110
0,305 505,1205 1255,1655 1705,2100 2100,2385
recent advances and applications of

1451
00:50:16,110 --> 00:50:17,300
0,320
gans.|
|

1452
00:50:17,400 --> 00:50:20,110
0,350 350,700 1080,1480 1650,2050 2310,2710
Where one particularly commonly employed
其中一个特别常用的想法是尝试再次迭代增长以获得更多更详细的图像生成，在训练过程中逐步添加层，然后改进由生成器生成的示例。

1453
00:50:20,160 --> 00:50:22,060
0,400 600,905 905,1145 1145,1480 1500,1900
idea is to try to

1454
00:50:22,260 --> 00:50:24,830
0,800 800,1150 1170,1570 2010,2315 2315,2570
iteratively grow again to get

1455
00:50:24,830 --> 00:50:26,620
0,300 300,570 570,890 1060,1425 1425,1790
more and more detailed image

1456
00:50:26,730 --> 00:50:30,530
0,400 1350,2195 2195,2500 2790,3370 3450,3800
generations, progressively adding layers over

1457
00:50:30,530 --> 00:50:31,895
0,225 225,390 390,600 600,920 1090,1365
the course of training to

1458
00:50:31,895 --> 00:50:34,420
0,255 255,825 825,1205 1315,1715 2125,2525
then refine the examples generated

1459
00:50:34,590 --> 00:50:36,360
0,400 420,680 680,1180
by the generator.|
|

1460
00:50:37,020 --> 00:50:38,135
0,275 275,425 425,575 575,800 800,1115
And this is the approach
这就是用来生成那些合成的，那些合成面孔的图像的方法，我在这节课的开头展示了这个想法，使用迭代细化的g来产生更高分辨率的图像。

1461
00:50:38,135 --> 00:50:40,480
0,255 255,480 480,815 1225,1625 1945,2345
that was used to generate

1462
00:50:40,560 --> 00:50:42,755
0,335 335,850 1170,1520 1520,1870 1920,2195
those synthetic, those images of

1463
00:50:42,755 --> 00:50:44,450
0,180 180,615 615,965 1105,1425 1425,1695
those synthetic faces that I

1464
00:50:44,450 --> 00:50:45,440
0,240 240,405 405,585 585,810 810,990
showed at the beginning of

1465
00:50:45,440 --> 00:50:47,380
0,150 150,440 940,1320 1320,1620 1620,1940
this lecture, this idea of

1466
00:50:47,610 --> 00:50:49,960
0,400 450,850 1200,1490 1490,1760 1760,2350
using g that is refined

1467
00:50:50,040 --> 00:50:52,150
0,770 770,1085 1085,1445 1445,1760 1760,2110
iteratively to produce higher resolution

1468
00:50:52,410 --> 00:50:54,300
0,400
images.|
|

1469
00:50:55,100 --> 00:50:56,380
0,350 350,605 605,770 770,995 995,1280
Another way we can extend
我们可以扩展这一概念的另一种方法是扩展GAN架构以考虑特定任务，并将进一步的结构强加于网络本身。一个特别的想法是，好的，如果我们有一个特定的标签或一些因素，我们想要以此为条件来产生。

1470
00:50:56,380 --> 00:50:59,155
0,225 225,530 1120,1520 1570,1970 2470,2775
this concept is to extend

1471
00:50:59,155 --> 00:51:01,530
0,180 180,420 420,755 1255,1655 1975,2375
the Gan architecture to consider

1472
00:51:01,670 --> 00:51:04,345
0,400 780,1180 1650,1910 1910,2315 2315,2675
particular tasks and impose further

1473
00:51:04,345 --> 00:51:06,600
0,395 715,1020 1020,1185 1185,1445 1855,2255
structure on the network itself.

1474
00:51:07,490 --> 00:51:09,190
0,380 380,760 840,1175 1175,1460 1460,1700
{One,particular -} idea is to

1475
00:51:09,190 --> 00:51:10,855
0,290 370,770 1090,1350 1350,1500 1500,1665
say, okay, what if we

1476
00:51:10,855 --> 00:51:12,690
0,150 150,390 390,755 805,1205 1435,1835
have a particular label or

1477
00:51:13,160 --> 00:51:14,560
0,335 335,670 690,980 980,1160 1160,1400
some factor that we want

1478
00:51:14,560 --> 00:51:16,740
0,350 640,1020 1020,1275 1275,1550 1780,2180
to condition the generation on.|
|

1479
00:51:17,820 --> 00:51:19,265
0,290 290,470 470,680 680,1000 1140,1445
We call the c and
我们调用c，它同时提供给生成器和鉴别器。

1480
00:51:19,265 --> 00:51:20,810
0,240 240,705 705,930 930,1205 1225,1545
it supplied to both the

1481
00:51:20,810 --> 00:51:23,180
0,495 495,705 705,855 855,1520
generator and the discriminator.|
|

1482
00:51:23,590 --> 00:51:24,855
0,305 305,530 530,800 800,1055 1055,1265
What this will allow us
这将使我们能够实现不同类型数据之间的配对转换。例如，我们可以有街景的图像，我们可以有街景的分割图像，我们可以重新构建可以在街景和分割之间直接转换的图像。

1483
00:51:24,855 --> 00:51:27,080
0,270 270,630 630,1025 1375,1875 1875,2225
to achieve is paired translation

1484
00:51:27,670 --> 00:51:30,470
0,400 780,1180 1200,1600 1620,2020 2400,2800
between different types of data.

1485
00:51:30,820 --> 00:51:32,610
0,305 305,515 515,820 1350,1625 1625,1790
{So,,for -} example, we can

1486
00:51:32,610 --> 00:51:35,310
0,290 310,710 1270,1670 1690,2090 2350,2700
have images of a street

1487
00:51:35,310 --> 00:51:36,795
0,350 490,795 795,975 975,1200 1200,1485
view and we can have

1488
00:51:36,795 --> 00:51:38,715
0,335 445,765 765,975 975,1650 1650,1920
images of the segmentation of

1489
00:51:38,715 --> 00:51:40,560
0,275 295,615 615,935 1405,1695 1695,1845
that street view and we

1490
00:51:40,560 --> 00:51:42,075
0,165 165,465 465,825 825,1155 1155,1515
can build again that can

1491
00:51:42,075 --> 00:51:44,790
0,395 775,1170 1170,1565 1885,2285 2395,2715
directly translate between the street

1492
00:51:44,790 --> 00:51:47,080
0,320 520,840 840,1035 1035,1670
view and the segmentation.|
|

1493
00:51:47,300 --> 00:51:48,300
0,245 245,365 365,530 530,710 710,1000
{Let's -} make this more
让我们通过考虑一些特定的例子来使这一点更加具体。

1494
00:51:48,350 --> 00:51:50,250
0,320 320,635 635,1025 1025,1420 1500,1900
concrete by considering some particular

1495
00:51:50,360 --> 00:51:51,640
0,400
examples.|
|

1496
00:51:51,680 --> 00:51:53,485
0,400 750,1010 1010,1145 1145,1415 1415,1805
So what I just described
所以我刚才描述的就是从一个分割标签变成一个街头场景。

1497
00:51:53,485 --> 00:51:55,735
0,395 685,1050 1050,1350 1350,1590 1590,2250
was going from a segmentation

1498
00:51:55,735 --> 00:51:57,300
0,395 475,735 735,930 930,1215 1215,1565
label to a street scene.|
|

1499
00:51:58,040 --> 00:51:59,680
0,275 275,530 530,910 990,1370 1370,1640
We can also translate between
我们还可以将卫星视图航空卫星图像转换为相当于该航空卫星图像的路线图，或将建筑物图像的特定注释或标签转换为该建筑物的实际视觉实现和视觉立面。

1500
00:51:59,680 --> 00:52:02,395
0,210 210,780 780,1130 1690,2265 2265,2715
a satellite view aerial satellite

1501
00:52:02,395 --> 00:52:04,350
0,305 745,1065 1065,1320 1320,1605 1605,1955
image to what is the

1502
00:52:04,610 --> 00:52:07,645
0,700 780,1180 1350,1670 1670,1990 2490,3035
roadmap equivalent of that aerial

1503
00:52:07,645 --> 00:52:11,065
0,450 450,755 1525,1925 2305,2705 3025,3420
satellite image or a particular

1504
00:52:11,065 --> 00:52:13,375
0,695 745,1050 1050,1545 1545,1895 2065,2310
annotation or labels of the

1505
00:52:13,375 --> 00:52:14,680
0,165 165,345 345,465 465,725 1015,1305
image of a building to

1506
00:52:14,680 --> 00:52:18,070
0,290 370,770 1330,1730 2470,3135 3135,3390
the actual visual realization and

1507
00:52:18,070 --> 00:52:19,470
0,270 270,750 750,930 930,1095 1095,1400
visual facade of that building.|
|

1508
00:52:20,280 --> 00:52:21,860
0,260 260,520 600,1000 1020,1325 1325,1580
We can translate between different
我们可以从白天到晚上在不同的照明条件之间进行转换。

1509
00:52:21,860 --> 00:52:23,980
0,350 490,890 1240,1620 1620,1860 1860,2120
lighting conditions day to night.|
|

1510
00:52:25,010 --> 00:52:26,310
0,290 290,470 470,740 740,1010 1010,1300
Black and white to color
从黑白到彩色轮廓再到彩色照片。

1511
00:52:26,690 --> 00:52:30,510
0,700 930,1250 1250,1490 1490,1810 3420,3820
outlines to a colored photo.|
|

1512
00:52:31,440 --> 00:52:33,170
0,305 305,545 545,880 1350,1595 1595,1730
All these cases, and I
所有这些案例，特别是我认为，对我来说最有趣和最有影响力的是街景和鸟瞰之间的转换。这是用来考虑，例如，如果你有来自谷歌地图的数据，你如何在地图的街景和航拍图像之间移动。

1513
00:52:33,170 --> 00:52:34,625
0,240 240,570 570,950 970,1275 1275,1455
think in particular the, the

1514
00:52:34,625 --> 00:52:36,755
0,275 655,915 915,1175 1465,1995 1995,2130
most interesting and impactful to

1515
00:52:36,755 --> 00:52:38,980
0,245 535,825 825,1065 1065,1415 1825,2225
me is this translation between

1516
00:52:39,150 --> 00:52:40,660
0,335 335,590 590,800 800,1220 1220,1510
street view and aerial view.

1517
00:52:41,010 --> 00:52:43,760
0,320 320,590 590,940 2010,2390 2390,2750
{And,this -} is used to

1518
00:52:43,760 --> 00:52:44,900
0,315 315,555 555,795 795,990 990,1140
consider, for example, if you

1519
00:52:44,900 --> 00:52:47,140
0,225 225,555 555,950 1540,1890 1890,2240
have data from Google maps,

1520
00:52:47,400 --> 00:52:48,790
0,305 305,500 500,680 680,970 990,1390
how you can go between

1521
00:52:48,990 --> 00:52:50,750
0,400 630,950 950,1270 1350,1625 1625,1760
a street view of the

1522
00:52:50,750 --> 00:52:52,745
0,260 700,990 990,1155 1155,1635 1635,1995
map to the aerial image

1523
00:52:52,745 --> 00:52:53,800
0,255 255,545
of that.|
|

1524
00:52:55,250 --> 00:52:58,780
0,400 1290,1690 2640,3095 3095,3275 3275,3530
Finally, again extending the same
最后，再次将同一翻译概念在一个域之间扩展到另一个域，另一个想法是完全不成对的翻译，这使用了一种称为青色的特定GAN体系结构。

1525
00:52:58,780 --> 00:53:01,405
0,350 430,735 735,1040 2050,2370 2370,2625
concept of translation between one

1526
00:53:01,405 --> 00:53:04,075
0,335 355,645 645,935 1795,2195 2305,2670
domain to another, another idea

1527
00:53:04,075 --> 00:53:06,150
0,255 255,450 450,755 835,1235 1345,2075
is that of completely unpaired

1528
00:53:06,560 --> 00:53:08,920
0,400 1170,1445 1445,1640 1640,1960 2010,2360
translation, and this uses a

1529
00:53:08,920 --> 00:53:11,610
0,350 490,840 840,1160 1540,1935 1935,2690
particular Gan architecture called cyan.|
|

1530
00:53:12,650 --> 00:53:13,855
0,320 320,530 530,740 740,1010 1010,1205
So in this video that
在我在这里展示的这段视频中，该模型将一个域中的一组图像作为输入。它并不一定要在另一个目标域中有对应的图像，但它被训练成试图在该目标域中生成大致对应于源域的示例，将源的样式转移到目标上，反之亦然。所以这个例子展示了从马域到斑马域的图像转换。

1531
00:53:13,855 --> 00:53:15,540
0,210 210,465 465,845 1105,1395 1395,1685
I'm showing here, the model

1532
00:53:15,890 --> 00:53:17,305
0,335 335,670 720,1010 1010,1190 1190,1415
takes as input a bunch

1533
00:53:17,305 --> 00:53:19,050
0,335 445,845 925,1215 1215,1425 1425,1745
of images in one domain.

1534
00:53:19,700 --> 00:53:21,550
0,395 395,665 665,1120 1170,1565 1565,1850
{And,it -} doesn't necessarily have

1535
00:53:21,550 --> 00:53:22,840
0,135 135,240 240,390 390,1020 1020,1290
to have a corresponding image

1536
00:53:22,840 --> 00:53:25,180
0,300 300,615 615,975 975,1340 2080,2340
in another target domain, but

1537
00:53:25,180 --> 00:53:26,425
0,105 105,315 315,675 675,990 990,1245
it is trained to try

1538
00:53:26,425 --> 00:53:28,750
0,255 255,575 1345,1745 1855,2130 2130,2325
to generate examples in that

1539
00:53:28,750 --> 00:53:31,800
0,285 285,650 1270,1670 1990,2390 2650,3050
target domain that roughly correspond

1540
00:53:32,270 --> 00:53:34,990
0,400 540,830 830,1070 1070,1420 2070,2720
to the source domain, transferring

1541
00:53:34,990 --> 00:53:36,330
0,225 225,530 580,870 870,1050 1050,1340
the style of the source

1542
00:53:36,350 --> 00:53:38,455
0,380 380,635 635,910 1320,1720 1770,2105
onto the target and vice

1543
00:53:38,455 --> 00:53:41,050
0,575 1165,1565 1585,1935 1935,2280 2280,2595
{versa.,So -} this example is

1544
00:53:41,050 --> 00:53:43,470
0,320 850,1250 1300,1700 1840,2130 2130,2420
showing the translation of images

1545
00:53:43,970 --> 00:53:46,660
0,395 395,755 755,1120 1710,2110 2220,2690
in horse domain to zebra

1546
00:53:46,660 --> 00:53:47,540
0,290
domain.|
|

1547
00:53:48,020 --> 00:53:49,500
0,290 290,580 660,965 965,1175 1175,1480
The concept here is this
这里的概念是循环依赖，对吗？你有两个GAN通过这个循环损耗连接在一起，在一个域和另一个域之间转换。

1548
00:53:49,640 --> 00:53:52,255
0,635 635,1450 1560,1960 2070,2375 2375,2615
cyclic dependency, right? You have

1549
00:53:52,255 --> 00:53:53,640
0,240 240,570 570,825 825,1035 1035,1385
two gans that are connected

1550
00:53:53,660 --> 00:53:56,760
0,400 660,1040 1040,1420 2220,2765 2765,3100
together VIA this cyclic loss,

1551
00:53:57,470 --> 00:53:59,665
0,590 590,905 905,1205 1205,1570 1860,2195
transforming between one domain and

1552
00:53:59,665 --> 00:54:00,740
0,335
another.|
|

1553
00:54:01,340 --> 00:54:02,695
0,305 305,560 560,845 845,1100 1100,1355
And really like all the
就像我们到目前为止在这节课上看到的所有例子一样，直觉是分布变换的想法。通常情况下，在g的作用下，你会在循环中从噪声转向某个目标。再一次，你试图从一些源分布，一些数据流形X到目标分布，另一个数据流形Y。

1554
00:54:02,695 --> 00:54:04,600
0,335 445,720 720,1005 1005,1295 1615,1905
examples that we've seen so

1555
00:54:04,600 --> 00:54:06,100
0,180 180,330 330,495 495,800 1240,1500
far in this lecture, the

1556
00:54:06,100 --> 00:54:07,810
0,525 525,825 825,1125 1125,1425 1425,1710
intuition is this idea of

1557
00:54:07,810 --> 00:54:11,080
0,380 520,920 2140,2540 2590,2940 2940,3270
distribution transformation. {Normally,with -} g,

1558
00:54:11,080 --> 00:54:12,565
0,330 330,540 540,810 810,1130 1210,1485
you're going from noise to

1559
00:54:12,565 --> 00:54:14,545
0,210 210,545 1255,1530 1530,1695 1695,1980
some target with the {cycle.,Again,

1560
00:54:14,545 --> 00:54:16,015
0,395 475,855 855,1065 1065,1260 1260,1470
-} you're trying to go

1561
00:54:16,015 --> 00:54:19,165
0,285 285,635 955,1355 1915,2315 2815,3150
from some source distribution, some

1562
00:54:19,165 --> 00:54:21,055
0,270 270,810 810,1145 1495,1740 1740,1890
data manifold X to a

1563
00:54:21,055 --> 00:54:24,145
0,305 1135,1535 1855,2220 2220,2505 2505,3090
target distribution, another data manifold

1564
00:54:24,145 --> 00:54:25,080
0,395
y.|
|

1565
00:54:25,420 --> 00:54:26,870
0,275 275,425 425,650 650,1000 1050,1450
And this is really, really
这真的，真的不仅很酷，而且在思考我们如何灵活地转换这些不同的发行版时也很强大。

1566
00:54:27,220 --> 00:54:28,935
0,290 290,560 560,940 990,1390 1410,1715
not only cool, but also

1567
00:54:28,935 --> 00:54:31,020
0,305 355,755 835,1230 1230,1625 1765,2085
powerful in thinking about how

1568
00:54:31,020 --> 00:54:33,030
0,195 195,470 610,1010 1330,1710 1710,2010
we can translate across these

1569
00:54:33,030 --> 00:54:35,840
0,320 400,920 1210,1970
different distributions flexibly.|
|

1570
00:54:35,850 --> 00:54:38,105
0,275 275,455 455,760 1080,1480 1890,2255
And in fact, this allows
事实上，这让我们不仅可以对图像进行转换，还可以对语音和音频进行转换。

1571
00:54:38,105 --> 00:54:40,040
0,270 270,465 465,755 775,1535 1645,1935
us to do transformations not

1572
00:54:40,040 --> 00:54:41,810
0,240 240,465 465,740 1240,1545 1545,1770
only to images, but to

1573
00:54:41,810 --> 00:54:43,390
0,315 315,615 615,920 970,1275 1275,1580
speech and audio as well.|
|

1574
00:54:44,210 --> 00:54:45,400
0,365 365,605 605,770 770,965 965,1190
So in the case of
所以在语音和音频的情况下，事实证明你可以利用声波。代表。

1575
00:54:45,400 --> 00:54:47,155
0,210 210,390 390,680 1210,1530 1530,1755
speech and audio, turns out

1576
00:54:47,155 --> 00:54:48,205
0,165 165,300 300,495 495,765 765,1050
that you can take sound

1577
00:54:48,205 --> 00:54:50,760
0,335 805,1205
waves. Represent.|
|

1578
00:54:50,980 --> 00:54:53,000
0,800 800,950 950,1115 1115,1685 1685,2020
Compactly in a spectrogram image
然后使用循环g将语音从一个域中的一个人的声音转换到另一个域中的另一个人的声音，对吗？这是我们定义的两个独立的数据分布。

1579
00:54:53,500 --> 00:54:54,950
0,335 335,575 575,770 770,1055 1055,1450
and use a cycle g

1580
00:54:55,180 --> 00:54:58,280
0,275 275,550 1170,1570 1890,2290 2700,3100
to then translate and transform

1581
00:54:58,570 --> 00:55:00,500
0,400 600,920 920,1175 1175,1670 1670,1930
speech from one person's voice

1582
00:55:00,850 --> 00:55:02,700
0,320 320,560 560,880 1230,1550 1550,1850
in one domain to another

1583
00:55:02,700 --> 00:55:04,790
0,525 525,800 1150,1470 1470,1740 1740,2090
person's voice in another domain,

1584
00:55:05,080 --> 00:55:06,740
0,380 380,620 620,815 815,1150 1260,1660
right? These are two independent

1585
00:55:06,940 --> 00:55:08,780
0,400 480,970 990,1265 1265,1490 1490,1840
data distributions that we define.|
|

1586
00:55:09,700 --> 00:55:10,935
0,335 335,605 605,785 785,1025 1025,1235
Maybe you're getting a sense
也许你已经明白我在暗示什么了。也许不是，但事实上，这正是我们开发模型来合成奥巴马声音背后的音频的方式，我们在昨天的入门课中看到了这一点。我们所做的是训练一个周期g来获取亚历山大声音中的数据，并将其转换为奥巴马声音中的数据。

1587
00:55:10,935 --> 00:55:11,925
0,165 165,285 285,495 495,795 795,990
of where I'm hinting at.

1588
00:55:11,925 --> 00:55:13,665
0,270 270,605 1015,1275 1275,1455 1455,1740
{Maybe,not, -} but in fact,

1589
00:55:13,665 --> 00:55:15,950
0,255 255,545 565,965 1135,1535 1885,2285
this was exactly how we

1590
00:55:16,090 --> 00:55:18,900
0,400 510,785 785,1060 1560,1960 2100,2810
developed the model to synthesize

1591
00:55:18,900 --> 00:55:21,140
0,225 225,500 760,1160 1450,1965 1965,2240
the audio behind Obama's voice

1592
00:55:21,430 --> 00:55:23,310
0,290 290,485 485,790 960,1250 1250,1880
that we saw in yesterday's

1593
00:55:23,310 --> 00:55:25,875
0,660 660,950 1900,2175 2175,2340 2340,2565
introductory {lecture.,What -} we did

1594
00:55:25,875 --> 00:55:27,405
0,255 255,510 510,825 825,1170 1170,1530
was we trained a cycle

1595
00:55:27,405 --> 00:55:30,050
0,395 805,1205 1225,1620 1620,2015 2245,2645
g to take data in

1596
00:55:30,070 --> 00:55:32,870
0,560 560,820 1410,1810 2160,2480 2480,2800
Alexander's voice and transform it

1597
00:55:33,220 --> 00:55:35,070
0,365 365,730 870,1130 1130,1265 1265,1850
into data in the manifold

1598
00:55:35,070 --> 00:55:36,920
0,350 490,1050 1050,1310
of Obama's voice.|
|

1599
00:55:37,520 --> 00:55:39,190
0,365 365,605 605,770 770,1370 1370,1670
So we can visualize how
所以我们可以直观地看到亚历山大的声音和奥巴马的声音的频谱图波形是什么样子的，这是完全使用循环方法合成的。

1600
00:55:39,190 --> 00:55:41,650
0,255 255,900 900,1550 1900,2220 2220,2460
that spectrogram waveform looks like

1601
00:55:41,650 --> 00:55:44,080
0,320 340,885 885,1130 1270,1670 1900,2430
for Alexander's voice versus Obama's

1602
00:55:44,080 --> 00:55:46,110
0,260 370,630 630,890 910,1290 1290,2030
voice that was completely synthesized

1603
00:55:46,310 --> 00:55:54,620
0,395 395,790 930,1330 1950,2350
using this cycle approach.|
|

1604
00:55:56,330 --> 00:55:56,680
0,320
The.|
这个。|

1605
00:55:59,130 --> 00:56:02,500
0,400 750,1150 1230,1630
Here at MIT.|
在麻省理工学院。|

1606
00:56:03,260 --> 00:56:06,840
0,400 1680,2165 2165,2410 2550,2950 3180,3580
I replayed it, okay, but
我重放了一遍，好的，但基本上我们所做的就是亚历山大说了昨天播放的那个短语，我们又有了火车周期，模型，然后我们可以把它部署到那个精确的音频上，把它从亚历山大的声音领域转换到奥巴马的声音领域。

1607
00:56:06,860 --> 00:56:08,370
0,400 480,755 755,920 920,1160 1160,1510
basically what we did was

1608
00:56:08,540 --> 00:56:10,950
0,400 480,785 785,1090 1110,1510 2010,2410
Alexander spoke that exact phrase

1609
00:56:11,030 --> 00:56:13,735
0,335 335,620 620,970 1170,1570 2400,2705
that was played yesterday, and

1610
00:56:13,735 --> 00:56:14,875
0,210 210,420 420,615 615,840 840,1140
we had the train cycle

1611
00:56:14,875 --> 00:56:16,600
0,285 285,605 1075,1365 1365,1500 1500,1725
again, model, and we can

1612
00:56:16,600 --> 00:56:17,755
0,255 255,405 405,600 600,855 855,1155
deploy it then on that

1613
00:56:17,755 --> 00:56:19,980
0,315 315,665 1075,1475 1585,1905 1905,2225
exact audio to transform it

1614
00:56:20,240 --> 00:56:22,315
0,400 480,770 770,1060 1110,1510 1560,2075
from the domain of Alexander's

1615
00:56:22,315 --> 00:56:23,970
0,245 385,785 895,1200 1200,1380 1380,1655
voice to Obama s voice.|
|

1616
00:56:24,600 --> 00:56:27,090
0,510 510,890 1120,1650 1650,2000 2230,2490
Generating the synthetic audio that
生成为该视频剪辑播放的合成音频。

1617
00:56:27,090 --> 00:56:28,140
0,180 180,420 420,600 600,765 765,1050
was played for that video

1618
00:56:28,140 --> 00:56:29,400
0,410
clip.|
|

1619
00:56:32,260 --> 00:56:35,325
0,380 380,710 710,1060 1200,1990 2700,3065
Okay, before I accidentally played
好吧，在我不小心再次上场之前，我。

1620
00:56:35,325 --> 00:56:36,860
0,315 315,665
again, I.|
|

1621
00:56:36,860 --> 00:56:38,170
0,225 225,560 580,885 885,1050 1050,1310
Jump now to the summary
现在跳到摘要幻灯片。今天，在这堂课中，我们学习了深入的生成模型，主要讨论了潜变量模型，自动编码器，变分自动编码器，我们的目标是学习数据的低维潜在编码，以及生成对抗网络，在那里，我们有这些相互竞争的生成器和鉴别器组件，试图合成合成例子。

1622
00:56:38,190 --> 00:56:40,370
0,400 1050,1445 1445,1805 1805,2030 2030,2180
slide. {So,today -} in this

1623
00:56:40,370 --> 00:56:42,560
0,290 310,675 675,920 1450,1755 1755,2190
lecture we've learned deep generative

1624
00:56:42,560 --> 00:56:44,960
0,260 340,740 1330,1710 1710,2085 2085,2400
models specifically talking mostly about

1625
00:56:44,960 --> 00:56:47,950
0,390 390,680 880,1280 1990,2325 2325,2990
latent variable models, auto encoders,

1626
00:56:47,970 --> 00:56:50,120
0,580 600,935 935,1570 1710,1970 1970,2150
variational auto encoders where our

1627
00:56:50,120 --> 00:56:51,410
0,270 270,540 540,735 735,960 960,1290
goal is to learn this

1628
00:56:51,410 --> 00:56:53,630
0,285 285,1010 1090,1590 1590,2100 2100,2220
low dimensional latent encoding of

1629
00:56:53,630 --> 00:56:55,690
0,135 135,410 1150,1470 1470,1725 1725,2060
the data, as well as

1630
00:56:56,070 --> 00:56:58,370
0,590 590,1235 1235,1510 1860,2135 2135,2300
generative adversarial networks where we

1631
00:56:58,370 --> 00:57:00,550
0,210 210,530 550,1100 1240,1845 1845,2180
have these competing generator and

1632
00:57:00,720 --> 00:57:03,065
0,790 840,1240 1650,1910 1910,2090 2090,2345
discriminator components that are trying

1633
00:57:03,065 --> 00:57:06,420
0,225 225,815 1555,2105 2305,2705
to synthesize synthetic examples.|
|

1634
00:57:07,510 --> 00:57:09,020
0,365 365,605 605,875 875,1145 1145,1510
We've talked about these core
我们已经讨论了这些核心的基本生成方法，但事实证明，就像我在讲座开始时提到的那样，特别是在过去的一年里，我们看到了生成建模的真正、真正的巨大进步，其中许多并不是来自这两种方法，我们描述的两种基本方法。

1635
00:57:09,130 --> 00:57:12,375
0,610 660,1220 1220,1510 2370,2770 2940,3245
foundational generative methods, but it

1636
00:57:12,375 --> 00:57:13,695
0,210 210,480 480,720 720,855 855,1320
turns out, as I alluded

1637
00:57:13,695 --> 00:57:15,060
0,305 505,795 795,990 990,1200 1200,1365
to in the beginning of

1638
00:57:15,060 --> 00:57:17,265
0,120 120,380 1210,1610 1690,1980 1980,2205
the lecture, that in this

1639
00:57:17,265 --> 00:57:19,125
0,315 315,585 585,840 840,1205 1435,1860
past year in particular, we've

1640
00:57:19,125 --> 00:57:21,675
0,305 535,935 985,1350 1350,1715 1975,2550
seen truly, truly tremendous advances

1641
00:57:21,675 --> 00:57:23,940
0,195 195,600 600,1145 1705,2040 2040,2265
in generative modeling, many of

1642
00:57:23,940 --> 00:57:25,970
0,290 580,915 915,1245 1245,1635 1635,2030
which have not been from

1643
00:57:26,080 --> 00:57:27,930
0,365 365,620 620,910 1320,1640 1640,1850
those two methods, those two

1644
00:57:27,930 --> 00:57:29,780
0,525 525,915 915,1185 1185,1455 1455,1850
foundational methods that we described.|
|

1645
00:57:30,500 --> 00:57:32,070
0,290 290,545 545,815 815,1120 1170,1570
But rather a new approach
而是一种名为扩散模型的新方法。

1646
00:57:32,210 --> 00:57:34,480
0,400 450,965 965,1570
called diffusion modeling.|
|

1647
00:57:34,700 --> 00:57:37,495
0,545 545,910 990,1355 1355,1720 2520,2795
Diffusion models are driving are
扩散模型是推动生成性人工智能巨大进步的驱动工具，特别是在过去的一年里。

1648
00:57:37,495 --> 00:57:39,660
0,165 165,455 565,965 1075,1475 1765,2165
the driving tools behind the

1649
00:57:39,680 --> 00:57:42,360
0,400 720,1420 1470,1775 1775,2315 2315,2680
tremendous advances in generative AI

1650
00:57:42,500 --> 00:57:43,525
0,275 275,515 515,680 680,845 845,1025
that we've seen in this

1651
00:57:43,525 --> 00:57:45,780
0,270 270,630 630,975 975,1325
past year in particular.|
|

1652
00:57:46,130 --> 00:57:48,820
0,400 960,1390 1800,2150 2150,2390 2390,2690
These gans, they're learning these
这些GAN，他们正在学习这些转换，这些编码，但他们在很大程度上被限制在生成类似于他们以前见过的数据空间的例子。

1653
00:57:48,820 --> 00:57:51,265
0,680 700,975 975,1640 1930,2205 2205,2445
transformations, these encodings, but they're

1654
00:57:51,265 --> 00:57:55,230
0,275 835,1475 1525,1925 2815,3455 3565,3965
largely restricted to generating examples

1655
00:57:55,460 --> 00:57:57,840
0,380 380,760 930,1330 1560,1960 1980,2380
that fall similar to the

1656
00:57:58,040 --> 00:57:59,485
0,395 395,785 785,1055 1055,1280 1280,1445
data space that they've seen

1657
00:57:59,485 --> 00:58:00,660
0,305
before.|
|

1658
00:58:01,270 --> 00:58:03,320
0,560 560,940 1200,1505 1505,1730 1730,2050
Diffusion models have this ability
扩散模型现在有这种能力，可以幻觉、想象和想象全新的物体和实例，作为人类，我们可能没有见过，甚至没有考虑到训练数据没有覆盖的设计空间的部分。

1659
00:58:03,340 --> 00:58:05,490
0,275 275,550 750,1505 1505,1715 1715,2150
to now hallucinate and envision

1660
00:58:05,490 --> 00:58:08,235
0,285 285,620 1330,1730 1780,2180 2410,2745
and imagine completely new objects

1661
00:58:08,235 --> 00:58:10,320
0,225 225,965 1105,1485 1485,1815 1815,2085
and instances, which we as

1662
00:58:10,320 --> 00:58:12,020
0,320 670,975 975,1170 1170,1380 1380,1700
humans may not have seen

1663
00:58:12,250 --> 00:58:14,370
0,290 290,530 530,800 800,1120 1800,2120
or even thought about parts

1664
00:58:14,370 --> 00:58:16,020
0,180 180,375 375,710 730,1130 1360,1650
of the design space that

1665
00:58:16,020 --> 00:58:17,640
0,210 210,530 700,1100 1120,1425 1425,1620
are not covered by the

1666
00:58:17,640 --> 00:58:18,900
0,255 255,620
training data.|
|

1667
00:58:18,940 --> 00:58:20,490
0,395 395,725 725,1060 1080,1370 1370,1550
So an example here is
这里的一个例子是人工智能生成的艺术，如果你愿意的话，它是艺术，它是由扩散模型创建的，我认为这不仅是为了达到目的。

1668
00:58:20,490 --> 00:58:22,640
0,270 270,585 585,920 1030,1430 1750,2150
this AI generated art, which

1669
00:58:22,810 --> 00:58:25,275
0,335 335,545 545,680 680,940 2190,2465
art, if you will, which

1670
00:58:25,275 --> 00:58:26,790
0,225 225,555 555,915 915,1170 1170,1515
was created by a diffusion

1671
00:58:26,790 --> 00:58:28,920
0,350 910,1155 1155,1275 1275,1550 1840,2130
model, and I think not

1672
00:58:28,920 --> 00:58:30,530
0,255 255,510 510,800 940,1275 1275,1610
only does this get at.|
|

1673
00:58:31,240 --> 00:58:32,790
0,210 210,345 345,620 670,1070 1150,1550
Some of the limits and
这些强大模型的一些限制和功能，但也有关于创建新实例意味着什么的问题。这些模型的极限和界限是什么，它们如何，我们如何看待它们在人类能力和人类智力方面的进步？

1674
00:58:33,080 --> 00:58:35,010
0,400 720,995 995,1190 1190,1510 1530,1930
capabilities of these powerful models,

1675
00:58:35,390 --> 00:58:37,750
0,395 395,725 725,1060 1230,1630 2070,2360
but also questions about what

1676
00:58:37,750 --> 00:58:39,570
0,150 150,285 285,560 640,1040 1420,1820
does it mean to create

1677
00:58:39,950 --> 00:58:42,040
0,365 365,1150 1410,1730 1730,1925 1925,2090
new instances? What are the

1678
00:58:42,040 --> 00:58:43,795
0,290 670,960 960,1280 1330,1590 1590,1755
limits and bounds of these

1679
00:58:43,795 --> 00:58:45,540
0,305 685,1020 1020,1290 1290,1485 1485,1745
models, and how do they,

1680
00:58:45,920 --> 00:58:46,930
0,290 290,425 425,560 560,770 770,1010
how can we think about

1681
00:58:46,930 --> 00:58:48,805
0,270 270,825 825,1130 1150,1550 1600,1875
their advances with respect to

1682
00:58:48,805 --> 00:58:51,120
0,255 255,635 1255,1560 1560,1865 1915,2315
human capabilities and human intelligence?|
|

1683
00:58:52,540 --> 00:58:53,840
0,290 290,455 455,755 755,1040 1040,1300
And so I'm, I'm really
因此，我非常兴奋，在周四的第七讲深度学习的新领域，我们将深入研究扩散模型，讨论它们的基本原理，不仅讨论图像的应用，也讨论其他领域，我们看到这些模型真正开始取得变革性的进展，因为它们确实处于当今生成性人工智能的前沿和新前沿。

1684
00:58:53,890 --> 00:58:55,425
0,350 350,575 575,785 785,1120 1230,1535
excited that on thursday in

1685
00:58:55,425 --> 00:58:57,780
0,305 325,725 1015,1415 1555,1875 1875,2355
lecture seven on new frontiers

1686
00:58:57,780 --> 00:58:59,280
0,195 195,375 375,680 1030,1365 1365,1500
in deep learning, we're going

1687
00:58:59,280 --> 00:59:00,210
0,165 165,300 300,435 435,645 645,930
to take a really deep

1688
00:59:00,210 --> 00:59:02,445
0,345 345,675 675,1125 1125,1490 1900,2235
dive into diffusion models, talk

1689
00:59:02,445 --> 00:59:04,665
0,240 240,435 435,1205 1585,1920 1920,2220
about their fundamentals, talk about

1690
00:59:04,665 --> 00:59:06,410
0,285 285,605 865,1260 1260,1500 1500,1745
not only applications to images,

1691
00:59:06,820 --> 00:59:08,240
0,290 290,560 560,860 860,1100 1100,1420
but other fields as well,

1692
00:59:08,470 --> 00:59:09,795
0,290 290,500 500,815 815,1055 1055,1325
in which we're seeing these

1693
00:59:09,795 --> 00:59:11,360
0,335 385,785 835,1140 1140,1305 1305,1565
models really start to make

1694
00:59:11,530 --> 00:59:14,295
0,670 870,1540 2040,2375 2375,2555 2555,2765
transformative advances because they are

1695
00:59:14,295 --> 00:59:15,450
0,285 285,495 495,645 645,870 870,1155
indeed at the very cutting

1696
00:59:15,450 --> 00:59:17,010
0,320 550,855 855,1080 1080,1350 1350,1560
edge and very much the

1697
00:59:17,010 --> 00:59:19,020
0,165 165,690 690,930 930,1490 1660,2010
new frontier of generative AI

1698
00:59:19,020 --> 00:59:19,840
0,350
today.|
|

1699
00:59:20,470 --> 00:59:22,070
0,260 260,520 690,1040 1040,1295 1295,1600
All right. {So,with -} that.|
好的。因此，有了这些。|

1700
00:59:23,180 --> 00:59:26,140
0,440 440,665 665,1000 2250,2650 2670,2960
Tease and and hopefully set
梳理并希望为周四的第七讲做好准备，结束并提醒你们，我们现在大约有一个小时的开放办公时间。该是您在软件实验室工作的时候了。请到我们这里来，提出任何你可能有的问题，以及将在这里的助教们。

1701
00:59:26,140 --> 00:59:27,780
0,165 165,440 550,900 900,1245 1245,1640
the stage for lecture seven

1702
00:59:27,860 --> 00:59:32,250
0,335 335,670 1440,1840 3600,4070 4070,4390
on thursday and conclude and

1703
00:59:32,330 --> 00:59:33,640
0,335 335,530 530,725 725,1040 1040,1310
remind you all that we

1704
00:59:33,640 --> 00:59:35,520
0,210 210,530 1120,1425 1425,1605 1605,1880
have now about an hour

1705
00:59:35,630 --> 00:59:37,440
0,400 660,1010 1010,1220 1220,1445 1445,1810
for open office hour. {Time,for

1706
00:59:37,700 --> 00:59:38,755
0,260 260,425 425,605 605,815 815,1055
-} you to work on

1707
00:59:38,755 --> 00:59:40,510
0,210 210,510 510,1085 1225,1560 1560,1755
your software {labs.,Come -} to

1708
00:59:40,510 --> 00:59:42,400
0,260 790,1110 1110,1350 1350,1650 1650,1890
us, ask any questions you

1709
00:59:42,400 --> 00:59:44,335
0,165 165,470 1330,1635 1635,1800 1800,1935
may have as well as

1710
00:59:44,335 --> 00:59:45,450
0,180 180,450 450,675 675,825 825,1115
the tas who will be

1711
00:59:45,560 --> 00:59:46,960
0,365 365,650 650,970
here as well.|
|

1712
00:59:47,030 --> 00:59:52,057
0,290 290,455 455,590 590,850
Thank you so much.|
非常感谢。|
