1
00:00:09,220 --> 00:00:11,025
0,260 260,520 810,1210 1380,1670 1670,1805
Good afternoon everyone. {Thank,you -}

2
00:00:11,025 --> 00:00:12,135
0,135 135,300 300,555 555,885 885,1110
all for joining {today.,My -}

3
00:00:12,135 --> 00:00:14,565
0,150 150,420 420,960 960,1475 2065,2430
name is alexandra amini and

4
00:00:14,565 --> 00:00:15,285
0,285 285,375 375,480 480,570 570,720
I'll be one of your

5
00:00:15,285 --> 00:00:17,430
0,305 385,995 1195,1500 1500,1805 1855,2145
course organizers this year along

6
00:00:17,430 --> 00:00:19,830
0,270 270,830 1210,1610 1630,1965 1965,2400
with ava and together we're

7
00:00:19,830 --> 00:00:21,750
0,380 460,860 1030,1430 1480,1770 1770,1920
super excited to introduce you

8
00:00:21,750 --> 00:00:23,955
0,240 240,620 1090,1490 1780,2040 2040,2205
all to introduction to {deep,learning.,Now,

9
00:00:23,955 --> 00:00:26,145
0,305 415,815 1105,1505 1765,2025 2025,2190
- -} MIT to deep

10
00:00:26,145 --> 00:00:27,750
0,305 445,780 780,1020 1020,1275 1275,1605
learning is a really, really

11
00:00:27,750 --> 00:00:29,930
0,380 640,1040 1060,1365 1365,1665 1665,2180
fun, exciting and fast paced

12
00:00:30,430 --> 00:00:31,890
0,400 450,725 725,950 950,1250 1250,1460
program here at MIT and

13
00:00:31,890 --> 00:00:32,820
0,105 105,255 255,480 480,720 720,930
let me start by just

14
00:00:32,820 --> 00:00:33,660
0,195 195,330 330,480 480,705 705,840
first of all, giving you

15
00:00:33,660 --> 00:00:34,460
0,90 90,240 240,375 375,510 510,800
a little bit of background

16
00:00:34,900 --> 00:00:36,675
0,400 930,1190 1190,1340 1340,1550 1550,1775
into what we do and

17
00:00:36,675 --> 00:00:37,485
0,180 180,345 345,465 465,615 615,810
what you're going to learn

18
00:00:37,485 --> 00:00:39,560
0,255 255,465 465,755 1225,1625 1675,2075
{about,this -} year. So this

19
00:00:39,820 --> 00:00:41,160
0,400 540,800 800,995 995,1190 1190,1340
week of entry to deep

20
00:00:41,160 --> 00:00:42,165
0,285 285,570 570,675 675,810 810,1005
learning, we're going to cover

21
00:00:42,165 --> 00:00:43,425
0,315 315,570 570,795 795,1065 1065,1260
{a,ton -} of material. In

22
00:00:43,425 --> 00:00:45,060
0,275 385,690 690,995 1195,1515 1515,1635
just one week you'll learn

23
00:00:45,060 --> 00:00:46,560
0,135 135,590 670,990 990,1230 1230,1500
the foundations of this really,

24
00:00:46,560 --> 00:00:48,915
0,350 610,1010 1150,1550 1570,1970 1990,2355
really fascinating and exciting field

25
00:00:48,915 --> 00:00:50,550
0,360 360,660 660,930 930,1275 1275,1635
of deep learning and artificial

26
00:00:50,550 --> 00:00:53,355
0,380 1330,1730 1930,2250 2250,2520 2520,2805
intelligence and more importantly, you're

27
00:00:53,355 --> 00:00:54,440
0,135 135,270 270,420 420,705 705,1085
going to get hands on.|

28
00:00:55,190 --> 00:00:57,115
0,400 660,935 935,1610 1610,1775 1775,1925
Experience actually reinforcing what you

29
00:00:57,115 --> 00:00:58,990
0,275 325,660 660,855 855,1355 1555,1875
learn in the lectures as

30
00:00:58,990 --> 00:01:00,100
0,195 195,360 360,570 570,810 810,1110
part of hands on software

31
00:01:00,100 --> 00:01:01,560
0,560
labs.|

32
00:01:01,570 --> 00:01:02,750
0,305 305,515 515,680 680,860 860,1180
Now, over the past decade,

33
00:01:03,100 --> 00:01:04,290
0,275 275,455 455,665 665,935 935,1190
AI and deep learning have

34
00:01:04,290 --> 00:01:06,225
0,240 240,590 760,1095 1095,1335 1335,1935
really had a huge resurgence

35
00:01:06,225 --> 00:01:08,715
0,240 240,545 655,1055 1195,1985 2185,2490
and had incredible successes and

36
00:01:08,715 --> 00:01:09,735
0,165 165,285 285,420 420,695 715,1020
a lot of problems that

37
00:01:09,735 --> 00:01:11,085
0,270 270,555 555,750 750,1020 1020,1350
even just a decade ago

38
00:01:11,085 --> 00:01:12,045
0,225 225,420 420,600 600,765 765,960
we thought were not really

39
00:01:12,045 --> 00:01:14,010
0,305 595,1295 1375,1650 1650,1785 1785,1965
even solvable in the near

40
00:01:14,010 --> 00:01:15,420
0,320 460,735 735,945 945,1245 1245,1410
future. {Now,we're -} solving with

41
00:01:15,420 --> 00:01:17,540
0,180 180,470 850,1245 1245,1640 1720,2120
deep learning with incredible {ease.,Now,

42
00:01:18,220 --> 00:01:19,620
0,380 380,695 695,965 965,1160 1160,1400
-} this past year in

43
00:01:19,620 --> 00:01:21,765
0,380 460,780 780,1290 1290,1905 1905,2145
{particular,of -} 2022 has been

44
00:01:21,765 --> 00:01:23,295
0,240 240,570 570,965 1045,1335 1335,1530
an incredible year for deep

45
00:01:23,295 --> 00:01:25,035
0,285 285,665 1195,1440 1440,1605 1605,1740
learning progress, and I'd like

46
00:01:25,035 --> 00:01:25,935
0,165 165,300 300,510 510,720 720,900
to say that actually this

47
00:01:25,935 --> 00:01:27,520
0,240 240,435 435,645 645,995
past year in particular.|

48
00:01:27,520 --> 00:01:28,770
0,285 285,480 480,630 630,870 870,1250
Has been the year of

49
00:01:28,850 --> 00:01:30,625
0,545 545,755 755,1060 1080,1475 1475,1775
generative deep learning, using deep

50
00:01:30,625 --> 00:01:32,365
0,300 300,600 600,905 1165,1500 1500,1740
learning to generate brand new

51
00:01:32,365 --> 00:01:33,820
0,305 415,720 720,1025 1045,1305 1305,1455
types of data that I've

52
00:01:33,820 --> 00:01:35,280
0,150 150,375 375,585 585,890 1060,1460
never been seen before and

53
00:01:35,450 --> 00:01:37,885
0,400 960,1535 1535,1790 1790,2080 2160,2435
never existed in reality. {In,fact,

54
00:01:37,885 --> 00:01:39,070
0,275 415,690 690,810 810,960 960,1185
-} I want to start

55
00:01:39,070 --> 00:01:40,255
0,240 240,495 495,765 765,975 975,1185
this class by actually showing

56
00:01:40,255 --> 00:01:41,245
0,225 225,405 405,585 585,780 780,990
you how we started this

57
00:01:41,245 --> 00:01:43,260
0,210 210,465 465,780 780,1145 1615,2015
class several years ago, which

58
00:01:43,460 --> 00:01:44,695
0,305 305,530 530,755 755,965 965,1235
was by playing this video

59
00:01:44,695 --> 00:01:45,415
0,210 210,390 390,495 495,600 600,720
that I'll play in a

60
00:01:45,415 --> 00:01:47,860
0,275 775,1095 1095,1415 1525,1925 2065,2445
{second.,Now -} this video actually

61
00:01:47,860 --> 00:01:49,870
0,380 520,795 795,1440 1440,1730 1750,2010
was an introductory video for

62
00:01:49,870 --> 00:01:51,900
0,135 135,410 430,705 705,980 1630,2030
{the,class. -} It was and

63
00:01:51,920 --> 00:01:53,905
0,260 260,470 470,1270 1290,1685 1685,1985
kind of exemplifies this idea

64
00:01:53,905 --> 00:01:55,200
0,150 150,345 345,600 600,995
that I'm talking about.|

65
00:01:55,210 --> 00:01:56,145
0,230 230,320 320,440 440,665 665,935
So let me just stop

66
00:01:56,145 --> 00:01:57,090
0,195 195,375 375,555 555,720 720,945
there and play this video,

67
00:01:57,090 --> 00:01:58,280
0,255 255,405 405,650
first of all.|

68
00:02:01,050 --> 00:02:03,515
0,395 395,790 1110,1445 1445,1780 2100,2465
Hi, everybody, and welcome to

69
00:02:03,515 --> 00:02:05,810
0,365 805,1155 1155,1505 1705,2025 2025,2295
MIT. {Six,s -} one, nine,

70
00:02:05,810 --> 00:02:06,700
0,350
one.|

71
00:02:06,740 --> 00:02:09,520
0,380 380,760 900,1835 1835,2200 2430,2780
The official introductory course on

72
00:02:09,520 --> 00:02:11,970
0,300 300,650 940,1320 1320,1700 2050,2450
deep learning talk here at

73
00:02:12,020 --> 00:02:13,300
0,400
MIT.|

74
00:02:13,950 --> 00:02:16,925
0,350 350,700 960,1310 1310,2230 2580,2975
E one is revolutionizing so

75
00:02:16,925 --> 00:02:18,300
0,390 390,785
many fears.|

76
00:02:18,300 --> 00:02:21,050
0,210 210,800 1180,1455 1455,1730 2350,2750
From robotics to medicine and

77
00:02:21,370 --> 00:02:23,080
0,365 365,730 780,1180
everything in between.|

78
00:02:23,760 --> 00:02:26,075
0,455 455,760 870,1145 1145,1985 1985,2315
You'll learn the fundamentals of

79
00:02:26,075 --> 00:02:27,965
0,285 285,635 955,1260 1260,1530 1530,1890
this field and how you

80
00:02:27,965 --> 00:02:29,860
0,330 330,665 985,1335 1335,1590 1590,1895
can build some of these

81
00:02:30,240 --> 00:02:32,420
0,400 990,1540
incredible algorithms.|

82
00:02:32,850 --> 00:02:35,560
0,320 320,640 1170,1570 1800,2200 2310,2710
In fact, this entire speech

83
00:02:35,880 --> 00:02:38,350
0,380 380,760 1230,1610 1610,1990 2070,2470
and video are not real.|

84
00:02:39,080 --> 00:02:40,940
0,120 120,345 345,680 1030,1430 1510,1860
And were created using deep

85
00:02:40,940 --> 00:02:44,560
0,350 820,1220 1540,1940 2260,2660
learning and artificial intelligence.|

86
00:02:45,310 --> 00:02:47,100
0,275 275,425 425,650 650,1000 1290,1790
And in this class, you'll

87
00:02:47,100 --> 00:02:48,520
0,320 340,740
learn how.|

88
00:02:48,880 --> 00:02:49,830
0,210 210,375 375,525 525,660 660,950
It has been an honor

89
00:02:50,090 --> 00:02:51,240
0,305 305,485 485,650 650,845 845,1150
to speak with you today,

90
00:02:51,800 --> 00:02:54,580
0,275 275,455 455,760
and I hope.|

91
00:02:56,770 --> 00:02:58,140
0,400 600,860 860,1010 1010,1160 1160,1370
So in case you couldn't

92
00:02:58,140 --> 00:03:00,570
0,260 550,950 1150,1550 1750,2115 2115,2430
tell this video and its

93
00:03:00,570 --> 00:03:02,535
0,270 270,590 700,1100 1540,1800 1800,1965
entire audio was actually not

94
00:03:02,535 --> 00:03:04,220
0,225 225,420 420,695 715,1395 1395,1685
real. {It,was -} synthetically generated

95
00:03:04,360 --> 00:03:06,120
0,260 260,395 395,560 560,850 1380,1760
by a deep learning {algorithm.,And

96
00:03:06,120 --> 00:03:07,620
0,135 135,300 300,590 760,1155 1155,1500
-} when we introduced this

97
00:03:07,620 --> 00:03:09,090
0,350 460,735 735,885 885,1125 1125,1470
class a few years ago,

98
00:03:09,090 --> 00:03:10,410
0,270 270,510 510,765 765,1020 1020,1320
this video was created several

99
00:03:10,410 --> 00:03:12,570
0,300 300,650 1000,1320 1320,1640 1810,2160
{years,ago. -} But even several

100
00:03:12,570 --> 00:03:14,685
0,315 315,680 1330,1590 1590,1830 1830,2115
years ago when we introduced

101
00:03:14,685 --> 00:03:15,555
0,240 240,480 480,630 630,735 735,870
this and put it on

102
00:03:15,555 --> 00:03:17,505
0,275 595,900 900,1140 1140,1665 1665,1950
Youtube went {somewhat,viral, -} right.

103
00:03:17,505 --> 00:03:18,660
0,240 240,495 495,720 720,915 915,1155
People {really,loved -} this video.

104
00:03:18,660 --> 00:03:20,340
0,330 330,645 645,1290 1290,1470 1470,1680
They were intrigued by how

105
00:03:20,340 --> 00:03:22,190
0,320 670,945 945,1220 1270,1560 1560,1850
real the video and audio

106
00:03:22,600 --> 00:03:25,430
0,380 380,695 695,1030 2070,2450 2450,2830
felt and looked entirely generated

107
00:03:25,510 --> 00:03:27,120
0,400 420,815 815,1250 1250,1415 1415,1610
by {an,algorithm -} by a

108
00:03:27,120 --> 00:03:29,160
0,350 850,1215 1215,1485 1485,1695 1695,2040
computer. And people were shocked

109
00:03:29,160 --> 00:03:30,030
0,120 120,285 285,540 540,750 750,870
with the power and the

110
00:03:30,030 --> 00:03:31,665
0,645 645,930 930,1185 1185,1410 1410,1635
{realism,of -} these types of

111
00:03:31,665 --> 00:03:32,985
0,335 355,720 720,960 960,1110 1110,1320
approaches. And this was a

112
00:03:32,985 --> 00:03:34,260
0,240 240,510 510,875
few years ago.|

113
00:03:34,260 --> 00:03:35,780
0,240 240,465 465,800 970,1245 1245,1520
Now, fast forward to today

114
00:03:36,190 --> 00:03:37,305
0,275 275,470 470,680 680,890 890,1115
and the state of deep

115
00:03:37,305 --> 00:03:39,900
0,305 325,725 1045,1350 1350,1655
learning today we have.|

116
00:03:40,090 --> 00:03:42,530
0,400 870,1220 1220,1460 1460,1750 1800,2440
Have seen deep learning accelerating

117
00:03:42,580 --> 00:03:44,205
0,260 260,410 410,700 990,1385 1385,1625
at a rate faster than

118
00:03:44,205 --> 00:03:46,320
0,165 165,330 330,665 685,1085 1825,2115
we've ever seen before. {In,fact,

119
00:03:46,320 --> 00:03:47,340
0,270 270,525 525,660 660,825 825,1020
-} we can use deep

120
00:03:47,340 --> 00:03:49,140
0,240 240,510 510,720 720,1010 1480,1800
learning now to generate not

121
00:03:49,140 --> 00:03:51,780
0,320 640,1040 1330,1635 1635,1940 2320,2640
just images of faces, but

122
00:03:51,780 --> 00:03:54,075
0,320 340,705 705,1250 1270,1940 2050,2295
generate full synthetic environments where

123
00:03:54,075 --> 00:03:55,550
0,120 120,300 300,600 600,1185 1185,1475
we can train autonomous vehicles

124
00:03:55,900 --> 00:03:57,765
0,335 335,545 545,1030 1230,1595 1595,1865
entirely in simulation and deploy

125
00:03:57,765 --> 00:03:59,270
0,180 180,375 375,630 630,965 1105,1505
them on full scale vehicles

126
00:03:59,350 --> 00:04:01,040
0,275 275,395 395,530 530,820 930,1690
in the real world {seamlessly.,The

127
00:04:01,240 --> 00:04:02,490
0,260 260,520 600,905 905,1100 1100,1250
-} videos here you see

128
00:04:02,490 --> 00:04:03,570
0,255 255,540 540,705 705,855 855,1080
are actually from a data

129
00:04:03,570 --> 00:04:05,210
0,300 300,840 840,1095 1095,1380 1380,1640
driven simulator from neural networks

130
00:04:05,290 --> 00:04:07,605
0,400 1230,1550 1550,1955 1955,2105 2105,2315
generated called vista that we

131
00:04:07,605 --> 00:04:09,020
0,210 210,435 435,750 750,1065 1065,1415
actually built here at MIT

132
00:04:09,550 --> 00:04:10,995
0,260 260,485 485,740 740,1030 1200,1445
and have open source to

133
00:04:10,995 --> 00:04:11,760
0,105 105,330 330,540 540,645 645,765
{the,public. -} So all of

134
00:04:11,760 --> 00:04:13,245
0,180 180,450 450,720 720,1040 1210,1485
you can actually train and

135
00:04:13,245 --> 00:04:14,505
0,150 150,300 300,510 510,690 690,1260
build the future of autonomy

136
00:04:14,505 --> 00:04:16,500
0,240 240,450 450,755 805,1205 1705,1995
and {self,driving -} cars. And

137
00:04:16,500 --> 00:04:17,550
0,180 180,375 375,540 540,750 750,1050
of course it goes far

138
00:04:17,550 --> 00:04:18,615
0,285 285,510 510,675 675,855 855,1065
{beyond,this -} as well. Deep

139
00:04:18,615 --> 00:04:19,905
0,305 535,795 795,915 915,1095 1095,1290
learning can be used to

140
00:04:19,905 --> 00:04:22,710
0,275 355,755 1105,1505 1945,2345 2515,2805
generate content directly from how

141
00:04:22,710 --> 00:04:23,780
0,225 225,465 465,645 645,795 795,1070
we speak and the language

142
00:04:24,010 --> 00:04:25,310
0,275 275,440 440,860 860,1055 1055,1300
that we convey to it

143
00:04:25,390 --> 00:04:26,930
0,365 365,830 830,1025 1025,1220 1220,1540
from prompts that we say.|

144
00:04:27,500 --> 00:04:28,880
0,270 270,510 510,750 750,1040 1060,1380
Deep learning can reason about

145
00:04:28,880 --> 00:04:30,425
0,225 225,660 660,950 1000,1275 1275,1545
the prompts in natural language

146
00:04:30,425 --> 00:04:32,345
0,375 375,755 835,1140 1140,1445 1645,1920
and English, for example, and

147
00:04:32,345 --> 00:04:34,010
0,180 180,485 625,1025 1075,1440 1440,1665
then guide and control what

148
00:04:34,010 --> 00:04:36,515
0,165 165,470 1300,1700 1810,2190 2190,2505
is generated according to what

149
00:04:36,515 --> 00:04:38,020
0,285 285,905
we specify.|

150
00:04:38,090 --> 00:04:39,850
0,350 350,560 560,910 990,1390 1440,1760
We've seen examples of where

151
00:04:39,850 --> 00:04:41,430
0,195 195,375 375,680 970,1275 1275,1580
we can generate, for example,

152
00:04:42,050 --> 00:04:43,810
0,365 365,730 870,1235 1235,1490 1490,1760
things that again have never

153
00:04:43,810 --> 00:04:44,995
0,435 435,570 570,840 840,1080 1080,1185
existed. {In,reality. {-},We -} can

154
00:04:44,995 --> 00:04:46,000
0,165 165,330 330,525 525,765 765,1005
ask a neural network to

155
00:04:46,000 --> 00:04:47,370
0,195 195,530 550,900 900,1110 1110,1370
generate a photo of an

156
00:04:47,420 --> 00:04:50,160
0,820 840,1240 1260,1535 1535,1810 2340,2740
astronaut riding a horse and

157
00:04:50,270 --> 00:04:52,530
0,380 380,680 680,935 935,1270 1410,2260
it actually can imagine hallucinate

158
00:04:52,700 --> 00:04:54,010
0,305 305,530 530,800 800,1055 1055,1310
what this might look like,

159
00:04:54,010 --> 00:04:55,075
0,270 270,435 435,570 570,810 810,1065
even though of course this

160
00:04:55,075 --> 00:04:57,100
0,305 835,1215 1215,1530 1530,1770 1770,2025
photo, not only this photo

161
00:04:57,100 --> 00:04:58,360
0,270 270,570 570,825 825,1050 1050,1260
has never occurred before, but

162
00:04:58,360 --> 00:04:59,220
0,90 90,210 210,315 315,525 525,860
I don't think any photo

163
00:04:59,240 --> 00:05:00,565
0,260 260,365 365,860 860,1115 1115,1325
of an astronaut riding a

164
00:05:00,565 --> 00:05:01,780
0,240 240,480 480,735 735,990 990,1215
horse has ever {occurred,before. -}

165
00:05:01,780 --> 00:05:02,710
0,195 195,360 360,495 495,690 690,930
So there's not really even

166
00:05:02,710 --> 00:05:04,105
0,300 300,650 850,1110 1110,1245 1245,1395
training data that you could

167
00:05:04,105 --> 00:05:05,050
0,150 150,330 330,510 510,705 705,945
go off {in,this -} case.

168
00:05:05,050 --> 00:05:06,790
0,320 460,780 780,1080 1080,1410 1410,1740
And my personal favorite is

169
00:05:06,790 --> 00:05:07,660
0,270 270,450 450,615 615,750 750,870
actually how we can not

170
00:05:07,660 --> 00:05:09,550
0,210 210,525 525,890 1420,1710 1710,1890
only build software that can

171
00:05:09,550 --> 00:05:11,320
0,285 285,680 730,1020 1020,1310 1480,1770
generate images and videos, but

172
00:05:11,320 --> 00:05:12,810
0,225 225,560 700,990 990,1185 1185,1490
build software that can {generate,software

173
00:05:13,250 --> 00:05:15,240
0,400 750,1070 1070,1390 1410,1700 1700,1990
-} as well. We can

174
00:05:15,290 --> 00:05:16,645
0,260 260,500 500,875 875,1115 1115,1355
also have algorithms that can

175
00:05:16,645 --> 00:05:18,625
0,240 240,575 655,1265 1435,1740 1740,1980
take language prompts, for example

176
00:05:18,625 --> 00:05:20,200
0,210 210,420 420,645 645,935 1225,1575
a prompt like this write

177
00:05:20,200 --> 00:05:21,870
0,315 315,585 585,1185 1185,1395 1395,1670
code in tensorflow to generate

178
00:05:22,160 --> 00:05:23,200
0,290 290,470 470,665 665,830 830,1040
or {to,train -} a neural

179
00:05:23,200 --> 00:05:24,730
0,260 670,945 945,1095 1095,1320 1320,1530
network. And not only will

180
00:05:24,730 --> 00:05:26,370
0,260 280,585 585,780 780,1070 1240,1640
it write the code and

181
00:05:26,420 --> 00:05:28,150
0,400 690,1010 1010,1280 1280,1505 1505,1730
create that neural network, but

182
00:05:28,150 --> 00:05:29,820
0,105 105,350 610,975 975,1305 1305,1670
it will have the ability

183
00:05:29,990 --> 00:05:31,615
0,290 290,580 660,1060 1170,1445 1445,1625
to reason about the code

184
00:05:31,615 --> 00:05:32,620
0,165 165,360 360,555 555,795 795,1005
that it's generated and walk

185
00:05:32,620 --> 00:05:33,960
0,255 255,555 555,810 810,1020 1020,1340
you through step by step

186
00:05:34,010 --> 00:05:35,430
0,410 410,635 635,905 905,1160 1160,1420
explaining the process and procedure

187
00:05:35,930 --> 00:05:36,775
0,290 290,425 425,560 560,710 710,845
all the way from the

188
00:05:36,775 --> 00:05:37,660
0,195 195,435 435,615 615,750 750,885
ground up to you so

189
00:05:37,660 --> 00:05:38,460
0,150 150,330 330,555 555,750
that you can actually.|

190
00:05:38,460 --> 00:05:39,675
0,290 520,810 810,945 945,1050 1050,1215
Learn how to do this

191
00:05:39,675 --> 00:05:41,420
0,305 475,795 795,1115
process as well.|

192
00:05:41,520 --> 00:05:42,600
0,400
Note.|

193
00:05:42,720 --> 00:05:43,865
0,275 275,550 570,830 830,950 950,1145
I think some of these

194
00:05:43,865 --> 00:05:45,560
0,335 355,660 660,885 885,1205 1345,1695
examples really just highlight how

195
00:05:45,560 --> 00:05:47,000
0,350 370,675 675,980 1000,1260 1260,1440
far deep learning and these

196
00:05:47,000 --> 00:05:48,425
0,300 300,600 600,920 1030,1290 1290,1425
methods have come in the

197
00:05:48,425 --> 00:05:49,535
0,210 210,465 465,705 705,915 915,1110
past six years since we

198
00:05:49,535 --> 00:05:50,600
0,255 255,525 525,750 750,915 915,1065
started this course. {And,you -}

199
00:05:50,600 --> 00:05:52,235
0,165 165,375 375,710 1210,1485 1485,1635
saw that example just a

200
00:05:52,235 --> 00:05:53,570
0,150 150,390 390,755 895,1185 1185,1335
few years ago from that

201
00:05:53,570 --> 00:05:55,205
0,555 555,860 880,1185 1185,1395 1395,1635
introductory video, but now we're

202
00:05:55,205 --> 00:05:56,830
0,195 195,480 480,845 865,1365 1365,1625
seeing such incredible {advances.,And -}

203
00:05:56,850 --> 00:05:58,145
0,290 290,545 545,890 890,1145 1145,1295
the most amazing part of

204
00:05:58,145 --> 00:05:59,200
0,180 180,390 390,555 555,735 735,1055
this course, in my opinion,

205
00:05:59,550 --> 00:06:01,250
0,400 450,740 740,1030 1110,1460 1460,1700
is actually that within this

206
00:06:01,250 --> 00:06:02,470
0,195 195,435 435,675 675,870 870,1220
one week we're going to

207
00:06:02,580 --> 00:06:03,770
0,290 290,500 500,785 785,1040 1040,1190
take you through from the

208
00:06:03,770 --> 00:06:05,110
0,195 195,495 495,810 810,1050 1050,1340
ground up, starting from today,

209
00:06:05,370 --> 00:06:07,325
0,305 305,485 485,665 665,1180 1560,1955
all of that foundational building

210
00:06:07,325 --> 00:06:08,675
0,395 565,840 840,1020 1020,1200 1200,1350
blocks that will allow you

211
00:06:08,675 --> 00:06:10,445
0,270 270,665 1105,1395 1395,1590 1590,1770
to understand and make all

212
00:06:10,445 --> 00:06:12,430
0,150 150,425 805,1185 1185,1695 1695,1985
of this amazing advances possible.|

213
00:06:13,730 --> 00:06:15,100
0,365 365,605 605,800 800,1100 1100,1370
So with that, hopefully now

214
00:06:15,100 --> 00:06:17,200
0,180 180,410 760,1155 1155,1550 1810,2100
you're all super excited about

215
00:06:17,200 --> 00:06:18,265
0,195 195,405 405,630 630,840 840,1065
what this class will teach.

216
00:06:18,265 --> 00:06:19,920
0,330 330,600 600,765 765,1055 1255,1655
{And,I,want - -} to basically

217
00:06:19,970 --> 00:06:21,220
0,320 320,575 575,830 830,1055 1055,1250
now just start by taking

218
00:06:21,220 --> 00:06:23,220
0,165 165,330 330,620 1060,1395 1395,2000
a step back and introducing

219
00:06:23,270 --> 00:06:24,955
0,275 275,425 425,695 695,1520 1520,1685
some of these terminologies that

220
00:06:24,955 --> 00:06:25,750
0,195 195,300 300,390 390,525 525,795
I've kind of been throwing

221
00:06:25,750 --> 00:06:27,295
0,380 430,735 735,1035 1035,1335 1335,1545
around so far with deep

222
00:06:27,295 --> 00:06:29,050
0,305 625,990 990,1355 1375,1635 1635,1755
learning, artificial {intelligence.,What -} do

223
00:06:29,050 --> 00:06:30,600
0,150 150,435 435,720 720,1010
these things actually mean?|

224
00:06:30,800 --> 00:06:31,840
0,380 380,665 665,815 815,920 920,1040
So first of all, I

225
00:06:31,840 --> 00:06:33,520
0,120 120,330 330,680 730,1130 1420,1680
want to maybe just take

226
00:06:33,520 --> 00:06:35,300
0,150 150,440 610,1010
a second to.|

227
00:06:35,430 --> 00:06:36,400
0,275 275,380 380,515 515,695 695,970
Speak a little bit about

228
00:06:36,420 --> 00:06:38,470
0,400 420,680 680,920 920,1300 1650,2050
intelligence and what intelligence means

229
00:06:38,730 --> 00:06:39,875
0,245 245,425 425,710 710,980 980,1145
at its core. {So,to -}

230
00:06:39,875 --> 00:06:42,260
0,245 745,1145 1345,1710 1710,2055 2055,2385
me, intelligence is simply the

231
00:06:42,260 --> 00:06:44,765
0,350 430,750 750,1070 1300,1700 2200,2505
ability to process information such

232
00:06:44,765 --> 00:06:45,560
0,180 180,315 315,450 450,615 615,795
that we can use it

233
00:06:45,560 --> 00:06:47,260
0,255 255,615 615,915 915,1220 1300,1700
to inform some future decision

234
00:06:47,640 --> 00:06:49,120
0,290 290,580 720,995 995,1175 1175,1480
or action that we take.|

235
00:06:49,810 --> 00:06:51,405
0,400 420,695 695,875 875,1180 1200,1595
Now, the field of artificial

236
00:06:51,405 --> 00:06:53,430
0,395 445,795 795,1145 1225,1625 1645,2025
intelligence is simply the ability

237
00:06:53,430 --> 00:06:55,250
0,195 195,300 300,450 450,710 1330,1820
for us to build algorithms,

238
00:06:55,480 --> 00:06:57,090
0,400 510,1025 1025,1295 1295,1430 1430,1610
artificial algorithms that can do

239
00:06:57,090 --> 00:06:59,475
0,270 270,620 820,1220 1390,1790 2050,2385
exactly this process information to

240
00:06:59,475 --> 00:07:01,940
0,285 285,540 540,845 985,1385 2065,2465
inform some future decision. {Now,,machine

241
00:07:01,960 --> 00:07:03,195
0,275 275,550 570,845 845,1040 1040,1235
-} learning is simply a

242
00:07:03,195 --> 00:07:04,700
0,315 315,525 525,780 780,960 960,1505
subset of AI, which focuses

243
00:07:04,900 --> 00:07:06,380
0,400 510,845 845,1055 1055,1205 1205,1480
specifically on how we can

244
00:07:06,490 --> 00:07:09,870
0,350 350,700 870,1270 1740,2140 3060,3380
build a machine to teach

245
00:07:09,870 --> 00:07:10,965
0,240 240,525 525,780 780,945 945,1095
a machine how to do

246
00:07:10,965 --> 00:07:12,920
0,275 355,720 720,1035 1035,1385 1555,1955
this from some experiences or

247
00:07:13,030 --> 00:07:15,405
0,400 480,815 815,1150 1740,2105 2105,2375
data, for {example.,Now -} deep

248
00:07:15,405 --> 00:07:16,800
0,255 255,525 525,795 795,1110 1110,1395
learning goes one step beyond

249
00:07:16,800 --> 00:07:18,105
0,320 400,675 675,810 810,945 945,1305
this and is a subset

250
00:07:18,105 --> 00:07:19,640
0,225 225,450 450,690 690,960 960,1535
of machine learning which focuses

251
00:07:19,720 --> 00:07:20,775
0,530 530,635 635,755 755,875 875,1055
explicitly on what are called

252
00:07:20,775 --> 00:07:21,900
0,270 270,545 565,840 840,990 990,1125
neural networks and how we

253
00:07:21,900 --> 00:07:23,325
0,105 105,240 240,510 510,770 1120,1425
can build neural networks that

254
00:07:23,325 --> 00:07:24,690
0,285 285,630 630,990 990,1245 1245,1365
can extract features in {the,data.

255
00:07:24,690 --> 00:07:25,935
0,255 255,510 510,675 675,980 1000,1245
-} These are basically what

256
00:07:25,935 --> 00:07:26,580
0,90 90,210 210,345 345,450 450,645
you can think of as

257
00:07:26,580 --> 00:07:28,230
0,350 730,1050 1050,1245 1245,1455 1455,1650
patterns that occur within the

258
00:07:28,230 --> 00:07:29,205
0,260 340,600 600,720 720,840 840,975
data so that it can

259
00:07:29,205 --> 00:07:30,440
0,210 210,480 480,705 705,915 915,1235
learn to complete these tasks

260
00:07:30,760 --> 00:07:31,680
0,290 290,580
as well.|

261
00:07:32,700 --> 00:07:34,450
0,400 570,995 995,1250 1250,1475 1475,1750
Now that's exactly what this

262
00:07:34,620 --> 00:07:35,960
0,320 320,530 530,755 755,1025 1025,1340
class is really all about.

263
00:07:35,960 --> 00:07:37,265
0,225 225,420 420,770 850,1185 1185,1305
{At,its -} core, we're going

264
00:07:37,265 --> 00:07:37,940
0,135 135,240 240,375 375,555 555,675
to try and teach you

265
00:07:37,940 --> 00:07:38,770
0,90 90,195 195,285 285,390 390,830
and give you the foundational

266
00:07:38,940 --> 00:07:40,490
0,400 630,950 950,1175 1175,1370 1370,1550
understanding and how we can

267
00:07:40,490 --> 00:07:43,385
0,290 670,1065 1065,1460 1570,1970 2620,2895
build and teach computers to

268
00:07:43,385 --> 00:07:45,140
0,240 240,605 955,1275 1275,1530 1530,1755
learn tasks, many different types

269
00:07:45,140 --> 00:07:47,135
0,210 210,530 910,1305 1305,1695 1695,1995
of tasks directly from raw

270
00:07:47,135 --> 00:07:48,530
0,305 565,825 825,1050 1050,1200 1200,1395
{data.,And -} that's really what

271
00:07:48,530 --> 00:07:49,685
0,180 180,420 420,780 780,990 990,1155
this class boils down to

272
00:07:49,685 --> 00:07:51,640
0,245 415,720 720,1025 1225,1590 1590,1955
at its most {simple,form. -}

273
00:07:52,140 --> 00:07:53,240
0,260 260,470 470,695 695,890 890,1100
And we'll provide a very

274
00:07:53,240 --> 00:07:55,085
0,285 285,620 880,1140 1140,1400 1540,1845
solid foundation for you both

275
00:07:55,085 --> 00:07:56,780
0,195 195,375 375,665 715,1115 1405,1695
on the technical side through

276
00:07:56,780 --> 00:07:58,490
0,150 150,650 1030,1305 1305,1455 1455,1710
the lectures, which will happen

277
00:07:58,490 --> 00:07:59,915
0,270 270,450 450,740 910,1230 1230,1425
in two parts {throughout,the -}

278
00:07:59,915 --> 00:08:00,950
0,240 240,480 480,660 660,885 885,1035
class. The first lecture, in

279
00:08:00,950 --> 00:08:02,105
0,105 105,315 315,645 645,945 945,1155
the second lecture, each one

280
00:08:02,105 --> 00:08:03,460
0,210 210,375 375,585 585,935
about one hour long.|

281
00:08:03,500 --> 00:08:05,005
0,380 380,695 695,905 905,1175 1175,1505
Followed by a software lab

282
00:08:05,005 --> 00:08:06,265
0,225 225,420 420,725 745,1080 1080,1260
which will immediately follow the

283
00:08:06,265 --> 00:08:07,675
0,485 685,960 960,1125 1125,1290 1290,1410
lectures, which will try to

284
00:08:07,675 --> 00:08:08,605
0,435 435,600 600,705 705,810 810,930
reinforce a lot of what

285
00:08:08,605 --> 00:08:10,495
0,150 150,330 330,635 925,1325 1615,1890
we cover in the in

286
00:08:10,495 --> 00:08:11,725
0,240 240,605 625,930 930,1095 1095,1230
the technical part of the

287
00:08:11,725 --> 00:08:13,810
0,275 775,1175 1495,1740 1740,1905 1905,2085
class and you know, give

288
00:08:13,810 --> 00:08:15,280
0,120 120,270 270,510 510,860 940,1470
you hands on experience implementing

289
00:08:15,280 --> 00:08:17,220
0,255 255,650 1210,1455 1455,1620 1620,1940
those ideas. {So,this -} program

290
00:08:17,570 --> 00:08:19,150
0,380 380,740 740,1040 1040,1310 1310,1580
is split between these two

291
00:08:19,150 --> 00:08:20,710
0,320 430,720 720,990 990,1425 1425,1560
pieces, the technical lectures and

292
00:08:20,710 --> 00:08:22,195
0,135 135,390 390,980 1150,1380 1380,1485
the software {labs.,We -} have

293
00:08:22,195 --> 00:08:23,395
0,225 225,510 510,765 765,990 990,1200
several new updates this year

294
00:08:23,395 --> 00:08:25,375
0,210 210,515 895,1295 1345,1725 1725,1980
in specific, especially in many

295
00:08:25,375 --> 00:08:27,295
0,150 150,300 300,525 525,1085 1645,1920
of the {later,lectures. -} The

296
00:08:27,295 --> 00:08:28,480
0,180 180,435 435,675 675,945 945,1185
first lecture will cover the

297
00:08:28,480 --> 00:08:29,935
0,405 405,645 645,825 825,1110 1110,1455
foundations of deep learning, which

298
00:08:29,935 --> 00:08:30,775
0,255 255,435 435,555 555,660 660,840
is going to be right

299
00:08:30,775 --> 00:08:31,860
0,305
now.|

300
00:08:32,090 --> 00:08:33,715
0,290 290,545 545,950 950,1385 1385,1625
And finally, we'll conclude the

301
00:08:33,715 --> 00:08:35,370
0,335 355,660 660,945 945,1290 1290,1655
course with some very exciting

302
00:08:35,390 --> 00:08:37,500
0,350 350,910 930,1205 1205,1445 1445,2110
guest lectures from both academia

303
00:08:37,520 --> 00:08:38,880
0,400 570,860 860,980 980,1085 1085,1360
and industry who are really

304
00:08:39,110 --> 00:08:41,140
0,400 420,725 725,1030 1080,1480 1710,2030
leading and driving forward the

305
00:08:41,140 --> 00:08:42,550
0,315 315,675 675,990 990,1230 1230,1410
state of AI and deep

306
00:08:42,550 --> 00:08:44,110
0,290 580,840 840,975 975,1250 1300,1560
learning. {And,of -} course, we

307
00:08:44,110 --> 00:08:45,895
0,225 225,590 790,1110 1110,1560 1560,1785
have many awesome prizes that

308
00:08:45,895 --> 00:08:47,815
0,240 240,545 1375,1650 1650,1785 1785,1920
go with all of the

309
00:08:47,815 --> 00:08:49,560
0,255 255,810 810,1095 1095,1380 1380,1745
software labs and the project

310
00:08:49,730 --> 00:08:50,695
0,365 365,590 590,695 695,815 815,965
competition at the end of

311
00:08:50,695 --> 00:08:52,435
0,150 150,425 535,935 985,1380 1380,1740
the {course.,So -} maybe quickly

312
00:08:52,435 --> 00:08:53,515
0,225 225,390 390,585 585,810 810,1080
to go through these {each,day.,Like

313
00:08:53,515 --> 00:08:54,670
0,335 385,660 660,825 825,990 990,1155
- -} I said, we'll

314
00:08:54,670 --> 00:08:56,110
0,105 105,380 490,855 855,1275 1275,1440
have dedicated software labs that

315
00:08:56,110 --> 00:08:58,280
0,240 240,480 480,615 615,1100
couple with the lectures.|

316
00:08:58,290 --> 00:08:59,770
0,365 365,650 650,860 860,1115 1115,1480
Starting today, with lab one,

317
00:08:59,970 --> 00:09:01,580
0,520 690,980 980,1205 1205,1400 1400,1610
you'll actually build a neural

318
00:09:01,580 --> 00:09:02,870
0,240 240,585 585,840 840,1050 1050,1290
network. {Keeping,with -} the theme

319
00:09:02,870 --> 00:09:04,280
0,225 225,735 735,1020 1020,1260 1260,1410
of generative AI, you'll build

320
00:09:04,280 --> 00:09:05,270
0,135 135,330 330,555 555,795 795,990
a neural network that can

321
00:09:05,270 --> 00:09:06,770
0,320 640,1020 1020,1230 1230,1335 1335,1500
learn, listen to a lot

322
00:09:06,770 --> 00:09:08,360
0,290 310,710 880,1230 1230,1440 1440,1590
of music, and actually learn

323
00:09:08,360 --> 00:09:09,515
0,150 150,285 285,560 610,930 930,1155
how to generate brand new

324
00:09:09,515 --> 00:09:11,090
0,305 445,735 735,960 960,1395 1395,1575
songs in that genre of

325
00:09:11,090 --> 00:09:12,160
0,320
music.|

326
00:09:12,710 --> 00:09:13,945
0,275 275,395 395,640 810,1085 1085,1235
At the end, at the

327
00:09:13,945 --> 00:09:14,980
0,210 210,465 465,660 660,825 825,1035
next level of the class

328
00:09:14,980 --> 00:09:16,225
0,270 270,615 615,915 915,1065 1065,1245
on friday, we'll host a

329
00:09:16,225 --> 00:09:17,815
0,255 255,635 715,1080 1080,1335 1335,1590
project pitch competition where either

330
00:09:17,815 --> 00:09:20,095
0,365 595,1170 1170,1565 1825,2130 2130,2280
you individually or as part

331
00:09:20,095 --> 00:09:21,600
0,90 90,240 240,545 805,1155 1155,1505
of a group can participate

332
00:09:21,800 --> 00:09:23,875
0,400 420,820 960,1340 1340,1720 1800,2075
and present an idea, a

333
00:09:23,875 --> 00:09:26,350
0,270 270,570 570,875 895,1295 2125,2475
novel deep learning idea to

334
00:09:26,350 --> 00:09:27,670
0,225 225,360 360,620 1000,1230 1230,1320
all of us. {It,will -}

335
00:09:27,670 --> 00:09:29,035
0,120 120,380 430,720 720,1010 1030,1365
be roughly three minutes in

336
00:09:29,035 --> 00:09:31,410
0,335 745,1145 1555,1845 1845,2055 2055,2375
length and we will focus

337
00:09:31,850 --> 00:09:32,875
0,275 275,455 455,695 695,890 890,1025
not as much because this

338
00:09:32,875 --> 00:09:34,290
0,135 135,395 415,750 750,1050 1050,1415
is a one week {program.,We

339
00:09:34,460 --> 00:09:35,185
0,230 230,320 320,470 470,620 620,725
-} are not going to

340
00:09:35,185 --> 00:09:36,100
0,225 225,450 450,570 570,720 720,915
focus so much on the

341
00:09:36,100 --> 00:09:37,480
0,320 400,675 675,855 855,1125 1125,1380
results of your pitch, but

342
00:09:37,480 --> 00:09:38,935
0,270 270,585 585,920 970,1245 1245,1455
rather the innovation and the

343
00:09:38,935 --> 00:09:40,375
0,195 195,300 300,405 405,995 1135,1440
idea and the novelty of

344
00:09:40,375 --> 00:09:41,460
0,180 180,390 390,540 540,705 705,1085
what you're trying to propose.|

345
00:09:42,080 --> 00:09:43,315
0,275 275,545 545,815 815,1010 1010,1235
The prices here are are

346
00:09:43,315 --> 00:09:45,130
0,210 210,515 775,1110 1110,1445 1465,1815
quite significant already where first

347
00:09:45,130 --> 00:09:45,940
0,255 255,435 435,585 585,705 705,810
prize is going to get

348
00:09:45,940 --> 00:09:46,960
0,120 120,585 585,765 765,900 900,1020
an Nvidia G P U,

349
00:09:46,960 --> 00:09:48,355
0,150 150,300 300,555 555,920 1060,1395
which is really a key

350
00:09:48,355 --> 00:09:49,770
0,225 225,420 420,725 895,1155 1155,1415
piece of hardware that is

351
00:09:50,030 --> 00:09:51,400
0,635 635,800 800,920 920,1100 1100,1370
instrumental. {If,you -} want to

352
00:09:51,400 --> 00:09:52,645
0,300 300,615 615,840 840,990 990,1245
actually build a deep learning

353
00:09:52,645 --> 00:09:54,130
0,360 360,675 675,930 930,1200 1200,1485
project and train these neural

354
00:09:54,130 --> 00:09:55,030
0,225 225,465 465,600 600,720 720,900
networks which can be very

355
00:09:55,030 --> 00:09:55,990
0,270 270,555 555,750 750,855 855,960
large and require a lot

356
00:09:55,990 --> 00:09:57,670
0,120 120,530 880,1185 1185,1545 1545,1680
of compute, these prizes will

357
00:09:57,670 --> 00:09:58,600
0,120 120,240 240,360 360,735 735,930
give you the compute to

358
00:09:58,600 --> 00:10:00,445
0,165 165,470 910,1200 1200,1490 1570,1845
do {so.,And -} finally this

359
00:10:00,445 --> 00:10:01,570
0,165 165,360 360,510 510,915 915,1125
year we'll be awarding a

360
00:10:01,570 --> 00:10:03,010
0,240 240,590 670,945 945,1275 1275,1440
grand prize for labs two

361
00:10:03,010 --> 00:10:04,780
0,150 150,420 420,800 1270,1560 1560,1770
and three combined, which will

362
00:10:04,780 --> 00:10:06,180
0,240 240,495 495,825 825,1110 1110,1400
occur on tuesday and wednesday

363
00:10:06,320 --> 00:10:07,825
0,380 380,760 870,1130 1130,1310 1310,1505
focused on what I believe

364
00:10:07,825 --> 00:10:08,860
0,210 210,420 420,795 795,930 930,1035
is actually solving some of

365
00:10:08,860 --> 00:10:10,975
0,135 135,410 430,830 1180,1580 1840,2115
the most exciting problems in

366
00:10:10,975 --> 00:10:12,085
0,180 180,405 405,615 615,810 810,1110
this field of deep learning

367
00:10:12,085 --> 00:10:13,600
0,285 285,575 685,1085 1105,1380 1380,1515
and how specifically how we

368
00:10:13,600 --> 00:10:15,865
0,135 135,405 405,800 1390,1790 1990,2265
can build models that can

369
00:10:15,865 --> 00:10:17,950
0,275 445,845 1105,1380 1380,1655 1765,2085
be robust, not only accurate,

370
00:10:17,950 --> 00:10:19,495
0,285 285,615 615,870 870,1395 1395,1545
but robust and trustworthy and

371
00:10:19,495 --> 00:10:21,400
0,305 655,930 930,1200 1200,1620 1620,1905
safe when they're deployed as

372
00:10:21,400 --> 00:10:22,660
0,210 210,480 480,885 885,1065 1065,1260
well and you'll actually get

373
00:10:22,660 --> 00:10:24,175
0,350 430,830 850,1140 1140,1320 1320,1515
experience developing those types of

374
00:10:24,175 --> 00:10:26,130
0,305 865,1140 1140,1350 1350,1620 1620,1955
solutions that can actually advance

375
00:10:26,180 --> 00:10:27,025
0,290 290,440 440,575 575,680 680,845
the state of the art

376
00:10:27,025 --> 00:10:28,460
0,335 355,755
and AI.|

377
00:10:28,600 --> 00:10:29,625
0,290 290,455 455,590 590,755 755,1025
Now, all of these labs

378
00:10:29,625 --> 00:10:31,340
0,90 90,210 210,485 715,1080 1080,1715
that I mentioned and competitions

379
00:10:31,420 --> 00:10:33,140
0,400 660,995 995,1265 1265,1460 1460,1720
here are going to be

380
00:10:33,880 --> 00:10:35,430
0,305 305,515 515,800 800,1180 1230,1550
due on thursday night at

381
00:10:35,430 --> 00:10:36,765
0,285 285,660 660,885 885,1125 1125,1335
eleven PM, right before the

382
00:10:36,765 --> 00:10:38,390
0,225 225,420 420,555 555,845 1225,1625
last day of class. {And,we'll

383
00:10:38,410 --> 00:10:39,420
0,305 305,380 380,575 575,800 800,1010
-} be helping you all

384
00:10:39,420 --> 00:10:40,680
0,240 240,405 405,585 585,915 915,1260
along the {way.,This, -} this

385
00:10:40,680 --> 00:10:42,675
0,350 550,825 825,1100 1330,1680 1680,1995
prize or this competition in

386
00:10:42,675 --> 00:10:45,195
0,365 775,1175 1405,1710 1710,2015 2065,2520
particular has very {significant,prizes.,So -

387
00:10:45,195 --> 00:10:45,930
0,105 105,240 240,435 435,600 600,735
-} I encourage all of

388
00:10:45,930 --> 00:10:47,130
0,240 240,465 465,675 675,975 975,1200
you to really enter this

389
00:10:47,130 --> 00:10:48,825
0,225 225,560 580,900 900,1220 1390,1695
prize and try to try

390
00:10:48,825 --> 00:10:49,660
0,305
to.|

391
00:10:49,660 --> 00:10:50,515
0,120 120,240 240,435 435,630 630,855
Get a chance to win

392
00:10:50,515 --> 00:10:51,720
0,225 225,485
the prize.|

393
00:10:52,290 --> 00:10:53,315
0,260 260,395 395,650 650,890 890,1025
And of course, like I

394
00:10:53,315 --> 00:10:53,975
0,180 180,405 405,495 495,585 585,660
said, we're going to be

395
00:10:53,975 --> 00:10:54,905
0,180 180,390 390,570 570,780 780,930
helping you all along the

396
00:10:54,905 --> 00:10:56,440
0,210 210,420 420,615 615,965 1135,1535
way. {We,are -} many available

397
00:10:56,640 --> 00:10:57,980
0,395 395,695 695,905 905,1145 1145,1340
resources throughout this class to

398
00:10:57,980 --> 00:11:00,200
0,165 165,470 730,1065 1065,1400 1900,2220
help you achieve {this.,Please -}

399
00:11:00,200 --> 00:11:01,505
0,225 225,405 405,975 975,1200 1200,1305
post a piazza if you

400
00:11:01,505 --> 00:11:02,825
0,90 90,270 270,605 835,1125 1125,1320
have any questions and of

401
00:11:02,825 --> 00:11:04,565
0,305 625,990 990,1305 1305,1530 1530,1740
course this program has an

402
00:11:04,565 --> 00:11:06,200
0,335 355,755 1075,1350 1350,1500 1500,1635
incredible team that you can

403
00:11:06,200 --> 00:11:06,995
0,135 135,330 330,495 495,600 600,795
reach out to at any

404
00:11:06,995 --> 00:11:08,075
0,335 445,720 720,870 870,990 990,1080
point in case you have

405
00:11:08,075 --> 00:11:09,740
0,180 180,515 865,1155 1155,1425 1425,1665
any issues or questions on

406
00:11:09,740 --> 00:11:12,040
0,180 180,500 1120,1520 1540,1830 1830,2300
{the,materials. -} Myself and ava

407
00:11:12,060 --> 00:11:12,950
0,260 260,395 395,575 575,740 740,890
will be your two main

408
00:11:12,950 --> 00:11:14,360
0,500 700,960 960,1080 1080,1245 1245,1410
lectures for the first part

409
00:11:14,360 --> 00:11:15,935
0,135 135,285 285,560 1000,1365 1365,1575
of the class will also

410
00:11:15,935 --> 00:11:16,910
0,165 165,450 450,690 690,840 840,975
be hearing like I said

411
00:11:16,910 --> 00:11:17,900
0,150 150,375 375,660 660,870 870,990
in the later part of

412
00:11:17,900 --> 00:11:18,860
0,120 120,315 315,540 540,735 735,960
the class from some guest

413
00:11:18,860 --> 00:11:20,690
0,530 790,1065 1065,1320 1320,1635 1635,1830
lectures who will share some

414
00:11:20,690 --> 00:11:22,100
0,260 340,645 645,950 1030,1290 1290,1410
really cutting edge state of

415
00:11:22,100 --> 00:11:23,090
0,90 90,240 240,555 555,825 825,990
the art development {and,deep -}

416
00:11:23,090 --> 00:11:24,680
0,290 730,1065 1065,1275 1275,1440 1440,1590
learning. And of course I

417
00:11:24,680 --> 00:11:25,360
0,120 120,225 225,315 315,420 420,680
want to give a huge

418
00:11:25,530 --> 00:11:26,510
0,290 290,455 455,620 620,830 830,980
shout out and thanks to

419
00:11:26,510 --> 00:11:27,920
0,90 90,195 195,360 360,890 1150,1410
all of our sponsors who

420
00:11:27,920 --> 00:11:29,140
0,195 195,480 480,735 735,930 930,1220
without their support this program

421
00:11:29,280 --> 00:11:31,330
0,320 320,425 425,575 575,850 1650,2050
wouldn't have been possible for

422
00:11:31,410 --> 00:11:32,915
0,305 305,590 590,905 905,1235 1235,1505
{yet,again -} another year. So

423
00:11:32,915 --> 00:11:33,820
0,165 165,300 300,545
thank you all.|

424
00:11:35,010 --> 00:11:36,370
0,365 365,590 590,830 830,1085 1085,1360
Okay, so now with that,

425
00:11:36,570 --> 00:11:37,880
0,425 425,665 665,905 905,1145 1145,1310
let's really dive into the

426
00:11:37,880 --> 00:11:39,260
0,165 165,450 450,720 720,975 975,1380
really fun stuff of today's

427
00:11:39,260 --> 00:11:41,135
0,260 310,600 600,890 1360,1620 1620,1875
lecture, which is, you know,

428
00:11:41,135 --> 00:11:42,650
0,390 390,645 645,905 955,1305 1305,1515
the, the technical part. {And,I,think

429
00:11:42,650 --> 00:11:43,475
0,150 150,360 360,540 540,660 660,825
- -} I want to

430
00:11:43,475 --> 00:11:44,800
0,225 225,465 465,705 705,975 975,1325
start this part by asking

431
00:11:46,080 --> 00:11:47,105
0,275 275,425 425,620 620,815 815,1025
all of you and having

432
00:11:47,105 --> 00:11:48,980
0,375 375,665 955,1215 1215,1475 1525,1875
yourselves ask, you know, having

433
00:11:48,980 --> 00:11:50,450
0,225 225,420 420,930 930,1200 1200,1470
you ask yourselves this question

434
00:11:50,450 --> 00:11:52,415
0,380 640,870 870,1100 1180,1580 1660,1965
of, you know, why are

435
00:11:52,415 --> 00:11:53,300
0,165 165,285 285,420 420,645 645,885
all of you here? First

436
00:11:53,300 --> 00:11:53,915
0,120 120,240 240,405 405,510 510,615
of all, why do you

437
00:11:53,915 --> 00:11:55,520
0,180 180,375 375,570 570,875 1345,1605
care about this topic in

438
00:11:55,520 --> 00:11:57,260
0,120 120,315 315,650
the first place?|

439
00:11:57,400 --> 00:11:58,965
0,290 290,580 840,1085 1085,1295 1295,1565
I think to answer this

440
00:11:58,965 --> 00:11:59,850
0,300 300,540 540,645 645,765 765,885
question we have to take

441
00:11:59,850 --> 00:12:00,915
0,150 150,330 330,585 585,840 840,1065
a step back and think

442
00:12:00,915 --> 00:12:02,265
0,335 535,780 780,945 945,1110 1110,1350
about, you know, the history

443
00:12:02,265 --> 00:12:03,945
0,375 375,645 645,935 1075,1395 1395,1680
of machine learning and what

444
00:12:03,945 --> 00:12:05,325
0,255 255,545 625,945 945,1170 1170,1380
machine learning is and what

445
00:12:05,325 --> 00:12:06,630
0,195 195,465 465,845 925,1170 1170,1305
deep learning brings to the

446
00:12:06,630 --> 00:12:08,040
0,290 430,735 735,915 915,1155 1155,1410
table on top of machine

447
00:12:08,040 --> 00:12:10,250
0,290 880,1230 1230,1580 1630,1920 1920,2210
learning. {Now,traditional -} machine learning

448
00:12:10,300 --> 00:12:12,195
0,470 470,850 990,1390 1500,1760 1760,1895
algorithms typically define what are

449
00:12:12,195 --> 00:12:13,970
0,240 240,605 1045,1320 1320,1485 1485,1775
called these set of features

450
00:12:14,140 --> 00:12:15,000
0,260 260,380 380,560 560,740 740,860
in the {data.,You -} can

451
00:12:15,000 --> 00:12:15,825
0,120 120,255 255,405 405,570 570,825
think of these as certain

452
00:12:15,825 --> 00:12:17,445
0,365 385,660 660,795 795,1055 1375,1620
patterns in the data and

453
00:12:17,445 --> 00:12:18,705
0,120 120,390 390,705 705,990 990,1260
then usually these features are

454
00:12:18,705 --> 00:12:20,025
0,195 195,660 660,900 900,1125 1125,1320
{hand,engineered. -} So probably a

455
00:12:20,025 --> 00:12:21,120
0,275 295,570 570,720 720,915 915,1095
human will come into the

456
00:12:21,120 --> 00:12:22,140
0,195 195,480 480,735 735,900 900,1020
data set and with a

457
00:12:22,140 --> 00:12:23,550
0,120 120,240 240,500 700,1080 1080,1410
lot of domain knowledge and

458
00:12:23,550 --> 00:12:25,755
0,350 760,1140 1140,1425 1425,1650 1650,2205
experience can try to uncover

459
00:12:25,755 --> 00:12:27,110
0,195 195,390 390,695 745,1050 1050,1355
what these {features,might -} be.

460
00:12:27,190 --> 00:12:28,260
0,260 260,425 425,695 695,920 920,1070
Now the key idea of

461
00:12:28,260 --> 00:12:29,295
0,195 195,495 495,750 750,870 870,1035
deep learning, and this is

462
00:12:29,295 --> 00:12:30,800
0,270 270,635 745,990 990,1170 1170,1505
really central to this class,

463
00:12:31,150 --> 00:12:32,295
0,290 290,575 575,830 830,965 965,1145
is that instead of having

464
00:12:32,295 --> 00:12:33,920
0,210 210,515 775,1095 1095,1320 1320,1625
a human define these features,

465
00:12:34,060 --> 00:12:34,940
0,245 245,350 350,470 470,605 605,880
what if we could have?|

466
00:12:35,360 --> 00:12:36,860
0,210 210,530 970,1245 1245,1380 1380,1500
A machine look at all

467
00:12:36,860 --> 00:12:38,240
0,135 135,315 315,620 700,1100 1120,1380
of this data and actually

468
00:12:38,240 --> 00:12:39,790
0,150 150,405 405,705 705,900 900,1550
try to extract and uncover

469
00:12:39,810 --> 00:12:41,020
0,290 290,470 470,650 650,875 875,1210
what are the core patterns

470
00:12:41,100 --> 00:12:42,215
0,260 260,395 395,670 750,995 995,1115
in the data so that

471
00:12:42,215 --> 00:12:43,685
0,165 165,455 505,840 840,1175 1195,1470
it can use those when

472
00:12:43,685 --> 00:12:44,765
0,165 165,375 375,585 585,855 855,1080
it sees new data to

473
00:12:44,765 --> 00:12:46,325
0,135 135,375 375,725 1105,1380 1380,1560
make some decisions. {So,for -}

474
00:12:46,325 --> 00:12:47,500
0,255 255,465 465,585 585,810 810,1175
example, if we wanted to

475
00:12:47,580 --> 00:12:48,970
0,320 320,640 780,1040 1040,1145 1145,1390
detect faces in an image,

476
00:12:49,500 --> 00:12:51,110
0,275 275,470 470,770 770,1030 1230,1610
a deep neural network algorithm

477
00:12:51,110 --> 00:12:52,610
0,290 310,630 630,950 1120,1380 1380,1500
might actually learn that in

478
00:12:52,610 --> 00:12:53,650
0,165 165,390 390,600 600,765 765,1040
order to detect a face

479
00:12:53,700 --> 00:12:54,890
0,290 290,485 485,710 710,950 950,1190
it first has to detect

480
00:12:54,890 --> 00:12:56,435
0,255 255,590 670,1200 1200,1425 1425,1545
things like edges in the

481
00:12:56,435 --> 00:12:58,295
0,245 325,690 690,930 930,1385 1585,1860
image lines and {edges.,And -}

482
00:12:58,295 --> 00:12:59,225
0,120 120,225 225,525 525,735 735,930
when you combine those lines

483
00:12:59,225 --> 00:13:00,490
0,180 180,540 540,705 705,915 915,1265
and edges you can actually

484
00:13:00,510 --> 00:13:02,740
0,400 600,1220 1220,1445 1445,1720 1830,2230
create compositions of features like

485
00:13:02,820 --> 00:13:04,940
0,530 530,815 815,1390 1620,1940 1940,2120
corners and curves, which when

486
00:13:04,940 --> 00:13:06,335
0,135 135,410 610,885 885,1050 1050,1395
you create, when you combine

487
00:13:06,335 --> 00:13:07,430
0,270 270,465 465,645 645,885 885,1095
those you can create more

488
00:13:07,430 --> 00:13:08,750
0,195 195,435 435,770 820,1095 1095,1320
high level features, for example

489
00:13:08,750 --> 00:13:10,240
0,330 330,600 600,1065 1065,1215 1215,1490
eyes and noses {and,ears. -}

490
00:13:10,710 --> 00:13:11,720
0,275 275,440 440,665 665,875 875,1010
And then those are the

491
00:13:11,720 --> 00:13:13,325
0,260 310,710 910,1215 1215,1395 1395,1605
features that allow you to

492
00:13:13,325 --> 00:13:14,810
0,335 595,945 945,1170 1170,1320 1320,1485
ultimately detect what you care

493
00:13:14,810 --> 00:13:15,830
0,210 210,570 570,705 705,870 870,1020
about, detecting which {is,the -}

494
00:13:15,830 --> 00:13:16,685
0,240 240,480 480,600 600,705 705,855
face. But all of these

495
00:13:16,685 --> 00:13:17,720
0,210 210,480 480,690 690,810 810,1035
come from what are called

496
00:13:17,720 --> 00:13:19,390
0,195 195,315 315,465 465,1160 1270,1670
kind of a hierarchical learning

497
00:13:19,440 --> 00:13:20,960
0,305 305,610 990,1250 1250,1355 1355,1520
of features and you can

498
00:13:20,960 --> 00:13:22,040
0,210 210,375 375,570 570,855 855,1080
actually see {some,examples -} of

499
00:13:22,040 --> 00:13:23,530
0,260 580,840 840,975 975,1170 1170,1490
these. These are real features

500
00:13:23,610 --> 00:13:24,710
0,335 335,530 530,635 635,875 875,1100
learned by a neural network

501
00:13:24,710 --> 00:13:26,480
0,225 225,390 390,660 660,980 1270,1770
and how they're combined defines

502
00:13:26,480 --> 00:13:28,400
0,270 270,810 810,1050 1050,1430
this progression of information.|

503
00:13:29,360 --> 00:13:30,295
0,245 245,365 365,590 590,800 800,935
But in fact, what I

504
00:13:30,295 --> 00:13:32,185
0,270 270,615 615,965 985,1385 1585,1890
just described, this underlying and

505
00:13:32,185 --> 00:13:34,150
0,305 475,870 870,1265 1315,1650 1650,1965
fundamental building block of neural

506
00:13:34,150 --> 00:13:36,000
0,260 280,555 555,720 720,1010 1450,1850
networks and deep learning have

507
00:13:36,020 --> 00:13:38,310
0,335 335,770 770,995 995,1330 1890,2290
actually existed for decades now.

508
00:13:38,540 --> 00:13:39,670
0,305 305,470 470,620 620,875 875,1130
{Why,are -} we studying all

509
00:13:39,670 --> 00:13:40,840
0,150 150,405 405,720 720,945 945,1170
of this now and today

510
00:13:40,840 --> 00:13:42,100
0,225 225,450 450,770 820,1095 1095,1260
in this class with all

511
00:13:42,100 --> 00:13:44,065
0,195 195,500 700,1635 1635,1785 1785,1965
this great enthusiasm to learn

512
00:13:44,065 --> 00:13:45,600
0,335 355,755 835,1110 1110,1260 1260,1535
this, right? Well, for one,

513
00:13:46,010 --> 00:13:47,425
0,260 260,380 380,620 620,1000 1020,1415
there have been several key

514
00:13:47,425 --> 00:13:48,940
0,615 615,870 870,1065 1065,1320 1320,1515
advances that have occurred in

515
00:13:48,940 --> 00:13:51,010
0,120 120,380 550,950 1510,1845 1845,2070
the past {decade.,Number -} one

516
00:13:51,010 --> 00:13:52,630
0,180 180,390 390,710 820,1220 1330,1620
is that data is so

517
00:13:52,630 --> 00:13:54,145
0,195 195,420 420,1125 1125,1365 1365,1515
much more pervasive than it

518
00:13:54,145 --> 00:13:55,750
0,180 180,405 405,690 690,1055 1345,1605
has ever been before in

519
00:13:55,750 --> 00:13:57,745
0,150 150,710 1090,1395 1395,1695 1695,1995
{our,lifetimes. -} These models are

520
00:13:57,745 --> 00:13:59,800
0,285 285,570 570,810 810,1145 1675,2055
hungry for {more -} data,

521
00:13:59,800 --> 00:14:01,300
0,210 210,300 300,560 730,1130 1240,1500
and we're living in the

522
00:14:01,300 --> 00:14:03,260
0,260 490,825 825,1095 1095,1430
age of big data.|

523
00:14:03,260 --> 00:14:04,445
0,240 240,450 450,675 675,960 960,1185
More data is available to

524
00:14:04,445 --> 00:14:05,480
0,120 120,330 330,525 525,720 720,1035
these models than ever before

525
00:14:05,480 --> 00:14:06,920
0,330 330,660 660,1095 1095,1260 1260,1440
and they thrive off of

526
00:14:06,920 --> 00:14:09,845
0,290 640,1230 1230,1550 1630,2150 2530,2925
that. {Secondly,,these -} algorithms are

527
00:14:09,845 --> 00:14:12,155
0,825 825,1740 1740,1980 1980,2175 2175,2310
massively {parallelizable.,They -} require a

528
00:14:12,155 --> 00:14:13,970
0,135 135,285 285,725 865,1265 1375,1815
lot of compute and we're

529
00:14:13,970 --> 00:14:15,220
0,210 210,345 345,480 480,740 850,1250
also at a unique time

530
00:14:15,450 --> 00:14:17,030
0,290 290,580 780,1130 1130,1370 1370,1580
in history where we have

531
00:14:17,030 --> 00:14:18,730
0,225 225,530 760,1080 1080,1350 1350,1700
the ability to train these

532
00:14:18,900 --> 00:14:21,065
0,380 380,725 725,1090 1110,1630 1860,2165
extremely large scale algorithms and

533
00:14:21,065 --> 00:14:22,340
0,305 355,615 615,795 795,1170 1170,1275
techniques that have existed for

534
00:14:22,340 --> 00:14:23,180
0,75 75,225 225,450 450,675 675,840
a very {long,time. -} But

535
00:14:23,180 --> 00:14:23,990
0,120 120,225 225,390 390,615 615,810
we can now train them

536
00:14:23,990 --> 00:14:25,505
0,180 180,315 315,405 405,650 1030,1515
due to the hardware advances

537
00:14:25,505 --> 00:14:26,705
0,135 135,255 255,375 375,635 895,1200
that {have,been -} made. And

538
00:14:26,705 --> 00:14:28,010
0,305 355,645 645,840 840,1035 1035,1305
finally, due to open source

539
00:14:28,010 --> 00:14:30,310
0,530 580,900 900,1220 1420,1820 1900,2300
toolbox and software platforms like

540
00:14:30,570 --> 00:14:32,165
0,680 680,890 890,1175 1175,1460 1460,1595
tensorflow for example, which all

541
00:14:32,165 --> 00:14:32,840
0,105 105,240 240,390 390,540 540,675
of you will get a

542
00:14:32,840 --> 00:14:34,085
0,90 90,255 255,510 510,830 970,1245
lot of experience on in

543
00:14:34,085 --> 00:14:36,340
0,195 195,515 1165,1565 1615,1935 1935,2255
this class, training and building

544
00:14:36,450 --> 00:14:38,000
0,320 320,640 810,1115 1115,1310 1310,1550
the code for these neural

545
00:14:38,000 --> 00:14:39,130
0,240 240,525 525,735 735,885 885,1130
networks {has,never -} been easier.

546
00:14:39,480 --> 00:14:40,505
0,275 275,425 425,575 575,740 740,1025
So that from the software

547
00:14:40,505 --> 00:14:41,470
0,270 270,390 390,495 495,660 660,965
point of view as well,

548
00:14:41,520 --> 00:14:42,905
0,230 230,320 320,515 515,830 830,1385
there have been incredible advances

549
00:14:42,905 --> 00:14:44,390
0,225 225,435 435,725 1045,1305 1305,1485
to open source, you know,

550
00:14:44,390 --> 00:14:46,955
0,320 580,980 1150,1940 2140,2415 2415,2565
the underlying fundamentals of what

551
00:14:46,955 --> 00:14:47,960
0,195 195,315 315,450 450,695
you're going to learn.|

552
00:14:48,530 --> 00:14:49,675
0,365 365,590 590,740 740,935 935,1145
So let me start now

553
00:14:49,675 --> 00:14:51,325
0,210 210,515 745,1110 1110,1425 1425,1650
with just building up from

554
00:14:51,325 --> 00:14:52,440
0,150 150,345 345,615 615,840 840,1115
the ground up the fundamental

555
00:14:52,640 --> 00:14:54,775
0,400 420,820 1020,1370 1370,1720 1770,2135
building block of every single

556
00:14:54,775 --> 00:14:55,960
0,330 330,600 600,870 870,1065 1065,1185
neural network that you're going

557
00:14:55,960 --> 00:14:56,905
0,135 135,285 285,465 465,645 645,945
to learn in this class.

558
00:14:56,905 --> 00:14:57,930
0,255 255,495 495,645 645,780 780,1025
{And,that's -} going to be

559
00:14:58,130 --> 00:14:59,900
0,290 290,470 470,740 740,1270
just a single neuron.|

560
00:15:00,060 --> 00:15:01,570
0,400 480,740 740,950 950,1160 1160,1510
And in neural network language,

561
00:15:01,740 --> 00:15:03,125
0,290 290,515 515,890 890,1100 1100,1385
a single neuron is called

562
00:15:03,125 --> 00:15:04,640
0,225 225,845
a perceptron.|

563
00:15:05,230 --> 00:15:06,600
0,380 380,605 605,710 710,830 830,1370
So what is a perceptron?

564
00:15:06,600 --> 00:15:09,015
0,225 225,920 1210,1610 2020,2280 2280,2415
A perceptron is, like I

565
00:15:09,015 --> 00:15:10,725
0,225 225,465 465,705 705,1205 1435,1710
said, a single neuron. {And,it's

566
00:15:10,725 --> 00:15:12,465
0,395 505,905 1105,1440 1440,1575 1575,1740
-} actually, I'm going to

567
00:15:12,465 --> 00:15:13,850
0,135 135,420 420,720 720,1035 1035,1385
say it's very, very simple

568
00:15:13,900 --> 00:15:14,640
0,320 320,470 470,560 560,665 665,740
{idea.,So,I - -} want to

569
00:15:14,640 --> 00:15:15,510
0,120 120,255 255,495 495,750 750,870
make sure that everyone in

570
00:15:15,510 --> 00:15:16,995
0,195 195,530 580,990 990,1275 1275,1485
the audience understands exactly what

571
00:15:16,995 --> 00:15:18,225
0,90 90,510 510,785 805,1080 1080,1230
a perceptron is and how

572
00:15:18,225 --> 00:15:20,505
0,150 150,425 1465,1815 1815,2115 2115,2280
{it,works. -} So let's start

573
00:15:20,505 --> 00:15:22,110
0,210 210,495 495,900 900,1005 1005,1605
by first defining a perceptron

574
00:15:22,110 --> 00:15:23,820
0,350 400,800 850,1245 1245,1530 1530,1710
as taking as input a

575
00:15:23,820 --> 00:15:25,470
0,180 180,470 640,1080 1080,1395 1395,1650
set of inputs, right? So

576
00:15:25,470 --> 00:15:26,340
0,165 165,300 300,465 465,675 675,870
on the left hand side,

577
00:15:26,340 --> 00:15:28,065
0,150 150,315 315,620 790,1110 1110,1725
you can see this perceptron

578
00:15:28,065 --> 00:15:30,735
0,335 865,1265 1345,1745 1765,2280 2280,2670
takes m different inputs, one

579
00:15:30,735 --> 00:15:31,700
0,240 240,485
to m.|

580
00:15:31,770 --> 00:15:33,010
0,275 275,395 395,545 545,755 755,1240
These are the blue circles.

581
00:15:33,330 --> 00:15:34,475
0,215 215,290 290,650 650,875 875,1145
We're denoting these inputs as

582
00:15:34,475 --> 00:15:35,980
0,135 135,360 360,695
X S.|

583
00:15:36,520 --> 00:15:38,595
0,305 305,500 500,790 870,1270 1800,2075
Each of these numbers, each

584
00:15:38,595 --> 00:15:40,155
0,150 150,425 445,995 1105,1380 1380,1560
of these inputs, is then

585
00:15:40,155 --> 00:15:42,290
0,665 745,1020 1020,1200 1200,1845 1845,2135
multiplied by a corresponding weight,

586
00:15:42,340 --> 00:15:43,490
0,290 290,440 440,575 575,800 800,1150
which we can call w,

587
00:15:43,750 --> 00:15:44,970
0,350 350,665 665,935 935,1100 1100,1220
so X one will be

588
00:15:44,970 --> 00:15:47,180
0,390 390,615 615,885 885,1250 1810,2210
multiplied by w one, and

589
00:15:47,320 --> 00:15:48,675
0,350 350,610 690,995 995,1190 1190,1355
we'll add the result of

590
00:15:48,675 --> 00:15:50,390
0,135 135,255 255,405 405,995 1315,1715
all of these multiplications together.|

591
00:15:51,130 --> 00:15:52,740
0,400 570,875 875,1085 1085,1325 1325,1610
Now, we take that single

592
00:15:52,740 --> 00:15:54,555
0,350 400,765 765,1020 1020,1310 1510,1815
number after the addition, and

593
00:15:54,555 --> 00:15:55,845
0,305 385,690 690,885 885,1065 1065,1290
we pass it through this

594
00:15:55,845 --> 00:15:57,075
0,660 660,885 885,990 990,1095 1095,1230
nonlinear, what we call a

595
00:15:57,075 --> 00:15:59,235
0,570 570,1020 1020,1415 1735,1995 1995,2160
nonlinear activation function, and that

596
00:15:59,235 --> 00:16:00,585
0,345 345,555 555,845 925,1200 1200,1350
produces our final output of

597
00:16:00,585 --> 00:16:01,725
0,135 135,515 535,840 840,1005 1005,1140
the perception, which we can

598
00:16:01,725 --> 00:16:02,780
0,120 120,255 255,545
call it y.|

599
00:16:03,390 --> 00:16:04,680
0,400
Now.|

600
00:16:05,160 --> 00:16:06,460
0,275 275,485 485,680 680,920 920,1300
This is actually not entirely

601
00:16:06,660 --> 00:16:08,525
0,305 305,500 500,790 1260,1640 1640,1865
accurate of the picture of

602
00:16:08,525 --> 00:16:09,800
0,120 120,630 630,840 840,1050 1050,1275
a perceptron. There's one step

603
00:16:09,800 --> 00:16:10,610
0,135 135,285 285,480 480,615 615,810
that I forgot to mention

604
00:16:10,610 --> 00:16:12,880
0,350 520,780 780,975 975,1310 1870,2270
here. {So,in -} addition to

605
00:16:13,320 --> 00:16:15,005
0,730 750,1025 1025,1160 1160,1385 1385,1685
multiplying all of these inputs

606
00:16:15,005 --> 00:16:16,205
0,105 105,240 240,690 690,930 930,1200
with their corresponding weights, we're

607
00:16:16,205 --> 00:16:17,150
0,225 225,420 420,615 615,765 765,945
also now going to add

608
00:16:17,150 --> 00:16:18,340
0,255 255,360 360,495 495,870 870,1190
what's called a bias term

609
00:16:18,510 --> 00:16:20,680
0,320 320,680 680,1000 1500,1835 1835,2170
here denoted as this w

610
00:16:20,700 --> 00:16:21,995
0,400 600,860 860,980 980,1115 1115,1295
zero, which is just a

611
00:16:21,995 --> 00:16:23,165
0,405 405,695 715,960 960,1050 1050,1170
scalar {weight.,And -} you can

612
00:16:23,165 --> 00:16:24,220
0,135 135,240 240,375 375,660 660,1055
think of it coming with

613
00:16:24,240 --> 00:16:25,570
0,400 420,680 680,830 830,1025 1025,1330
an input of just one,

614
00:16:25,860 --> 00:16:26,930
0,245 245,470 470,620 620,815 815,1070
so that's going to allow

615
00:16:26,930 --> 00:16:28,870
0,210 210,470 760,1020 1020,1280 1540,1940
the network to basically shift

616
00:16:29,190 --> 00:16:32,555
0,350 350,935 935,1310 1310,1660 3120,3365
its nonlinear activation function, you

617
00:16:32,555 --> 00:16:34,570
0,195 195,450 450,1085 1435,1725 1725,2015
know, non linearly as it

618
00:16:34,890 --> 00:16:36,300
0,305 305,605 605,1090
sees its inputs.|

619
00:16:36,780 --> 00:16:37,670
0,290 290,440 440,545 545,695 695,890
Now on the right hand

620
00:16:37,670 --> 00:16:38,450
0,195 195,345 345,480 480,630 630,780
side, you can see this

621
00:16:38,450 --> 00:16:42,005
0,590 1120,1790 2020,2690 2920,3300 3300,3555
diagram mathematically formulated right as

622
00:16:42,005 --> 00:16:43,130
0,150 150,420 420,780 780,1005 1005,1125
a single equation. {We,can -}

623
00:16:43,130 --> 00:16:45,280
0,180 180,630 630,870 870,1400 1750,2150
now rewrite this linear this

624
00:16:45,750 --> 00:16:47,990
0,400 420,725 725,1010 1010,1600 1920,2240
equation with linear algebra terms

625
00:16:47,990 --> 00:16:49,450
0,210 210,645 645,870 870,1110 1110,1460
of vectorctors and dot products,

626
00:16:49,500 --> 00:16:50,705
0,305 305,485 485,665 665,950 950,1205
right? So for example, we

627
00:16:50,705 --> 00:16:52,685
0,240 240,605 655,1050 1050,1445 1495,1980
can define our entire inputs

628
00:16:52,685 --> 00:16:54,370
0,285 285,635 865,1125 1125,1335 1335,1685
X one to X M

629
00:16:54,780 --> 00:16:57,380
0,400 810,1145 1145,1460 1460,1810
as large vector X.|

630
00:16:57,380 --> 00:16:58,670
0,270 270,465 465,705 705,1005 1005,1290
Right that large vector X

631
00:16:58,670 --> 00:17:01,190
0,210 210,375 375,1010 1690,2090 2230,2520
can be multiplied by or

632
00:17:01,190 --> 00:17:02,330
0,180 180,345 345,615 615,900 900,1140
take a dot, excuse me,

633
00:17:02,330 --> 00:17:04,930
0,465 465,1010 1150,1550 1780,2085 2085,2600
matrix multiplied with our weights

634
00:17:05,190 --> 00:17:07,430
0,400 780,1180 1230,1565 1565,1960 1980,2240
w again, another vector of

635
00:17:07,430 --> 00:17:09,050
0,150 150,480 480,780 780,1160 1360,1620
our weights w one to

636
00:17:09,050 --> 00:17:10,220
0,225 225,590
w N.|

637
00:17:10,320 --> 00:17:12,560
0,335 335,545 545,740 740,1060 1950,2240
Taking their dot product not

638
00:17:12,560 --> 00:17:14,075
0,290 490,1050 1050,1215 1215,1335 1335,1515
only multiplies them, but it

639
00:17:14,075 --> 00:17:15,740
0,195 195,635 655,975 975,1290 1290,1665
also adds the resulting terms

640
00:17:15,740 --> 00:17:18,005
0,380 820,1155 1155,1350 1350,1790 1990,2265
together, adding a bias, like

641
00:17:18,005 --> 00:17:19,490
0,165 165,390 390,725 925,1245 1245,1485
we said before, and applying

642
00:17:19,490 --> 00:17:20,960
0,210 210,1100
this nonlinearity.|

643
00:17:22,710 --> 00:17:23,830
0,365 365,590 590,725 725,860 860,1120
Now you might be wondering,

644
00:17:23,910 --> 00:17:25,145
0,260 260,410 410,665 665,920 920,1235
what is this non linear

645
00:17:25,145 --> 00:17:26,120
0,225 225,495 495,660 660,855 855,975
function? I've mentioned it a

646
00:17:26,120 --> 00:17:27,815
0,150 150,440 490,890 1300,1560 1560,1695
few times already. {Well,,I,said -

647
00:17:27,815 --> 00:17:29,050
0,270 270,525 525,750 750,975 975,1235
-} it is a function,

648
00:17:29,310 --> 00:17:31,355
0,305 305,730 960,1360 1470,1775 1775,2045
right? That's that we pass

649
00:17:31,355 --> 00:17:32,720
0,365 625,960 960,1065 1065,1170 1170,1365
the outputs of the neural

650
00:17:32,720 --> 00:17:34,565
0,210 210,555 555,950 970,1370 1570,1845
network through before we return

651
00:17:34,565 --> 00:17:35,675
0,275 325,585 585,795 795,990 990,1110
it, you know, to the

652
00:17:35,675 --> 00:17:37,400
0,210 210,665 895,1185 1185,1455 1455,1725
next neuron in the, in

653
00:17:37,400 --> 00:17:39,050
0,270 270,650 880,1155 1155,1350 1350,1650
the {pipeline.,So -} one common

654
00:17:39,050 --> 00:17:40,580
0,380 430,705 705,840 840,1290 1290,1530
example of a nonlinear function

655
00:17:40,580 --> 00:17:42,230
0,300 300,560 670,1070 1180,1470 1470,1650
that's very popular in deep

656
00:17:42,230 --> 00:17:43,205
0,210 210,405 405,675 675,810 810,975
neural networks, it's called the

657
00:17:43,205 --> 00:17:44,705
0,345 345,635 985,1230 1230,1350 1350,1500
{sigmoid,function. -} You can think

658
00:17:44,705 --> 00:17:45,395
0,135 135,270 270,435 435,585 585,690
of this as kind of

659
00:17:45,395 --> 00:17:46,700
0,210 210,555 555,935 955,1200 1200,1305
a continuous version of a

660
00:17:46,700 --> 00:17:48,395
0,405 405,710 790,1190 1240,1515 1515,1695
threshold function, right? It goes

661
00:17:48,395 --> 00:17:49,840
0,210 210,495 495,735 735,995
from zero to one.|

662
00:17:49,950 --> 00:17:51,350
0,275 275,620 620,970 990,1265 1265,1400
And it's having, it can

663
00:17:51,350 --> 00:17:52,280
0,120 120,315 315,525 525,720 720,930
take us, input any real

664
00:17:52,280 --> 00:17:53,750
0,290 370,690 690,975 975,1230 1230,1470
number on the real number

665
00:17:53,750 --> 00:17:54,780
0,350
line.|

666
00:17:54,780 --> 00:17:55,515
0,225 225,345 345,480 480,600 600,735
And you can see an

667
00:17:55,515 --> 00:17:56,805
0,255 255,450 450,600 600,1095 1095,1290
example of it illustrated on

668
00:17:56,805 --> 00:17:58,275
0,120 120,300 300,540 540,845 1165,1470
the bottom right hand. {Now,,in

669
00:17:58,275 --> 00:17:59,240
0,180 180,375 375,540 540,675 675,965
-} fact, there are many

670
00:17:59,350 --> 00:18:01,460
0,350 350,590 590,1180 1320,1745 1745,2110
types of nonlinear activation functions

671
00:18:01,540 --> 00:18:02,955
0,275 275,470 470,790 960,1250 1250,1415
that are popular in deep

672
00:18:02,955 --> 00:18:03,945
0,240 240,435 435,705 705,870 870,990
neural networks and here are

673
00:18:03,945 --> 00:18:05,505
0,165 165,405 405,755 1135,1395 1395,1560
some common {ones.,And -} throughout

674
00:18:05,505 --> 00:18:07,065
0,210 210,515 745,1155 1155,1365 1365,1560
this presentation you'll actually see

675
00:18:07,065 --> 00:18:08,610
0,305 415,810 810,1065 1065,1260 1260,1545
some examples of these code

676
00:18:08,610 --> 00:18:09,675
0,375 375,540 540,660 660,855 855,1065
snippets on the bottom of

677
00:18:09,675 --> 00:18:10,965
0,180 180,435 435,785 835,1170 1170,1290
the slides where we'll try

678
00:18:10,965 --> 00:18:12,660
0,240 240,605 655,975 975,1295 1435,1695
and actually tie in some

679
00:18:12,660 --> 00:18:13,560
0,180 180,375 375,555 555,720 720,900
of what you're learning in

680
00:18:13,560 --> 00:18:15,540
0,105 105,560 880,1280 1390,1710 1710,1980
the lectures to actual software

681
00:18:15,540 --> 00:18:16,410
0,195 195,315 315,450 450,615 615,870
and how you can implement

682
00:18:16,410 --> 00:18:18,195
0,285 285,620 1270,1545 1545,1665 1665,1785
these pieces, which will help

683
00:18:18,195 --> 00:18:18,780
0,105 105,180 180,315 315,450 450,585
you a lot for your

684
00:18:18,780 --> 00:18:20,625
0,255 255,740 760,1320 1320,1590 1590,1845
software {labs,explicitly. -} So the

685
00:18:20,625 --> 00:18:21,915
0,405 405,780 780,990 990,1125 1125,1290
sigmoid activation on the left

686
00:18:21,915 --> 00:18:23,430
0,195 195,390 390,690 690,1085 1195,1515
is very popular since it's

687
00:18:23,430 --> 00:18:24,705
0,90 90,315 315,680 700,1125 1125,1275
a function that outputs, you

688
00:18:24,705 --> 00:18:25,800
0,210 210,480 480,675 675,855 855,1095
know, between zero and one,

689
00:18:25,800 --> 00:18:26,925
0,330 330,645 645,840 840,960 960,1125
so especially when you want

690
00:18:26,925 --> 00:18:28,580
0,165 165,300 300,480 480,1025 1135,1655
to deal with probabilities distributions,

691
00:18:28,600 --> 00:18:30,090
0,290 290,530 530,755 755,1030 1140,1490
for example, this is very

692
00:18:30,090 --> 00:18:32,205
0,350 400,690 690,1160 1450,1830 1830,2115
important because probabilities live between

693
00:18:32,205 --> 00:18:33,800
0,180 180,330 330,605
zero and one.|

694
00:18:33,800 --> 00:18:34,970
0,165 165,420 420,690 690,930 930,1170
In modern deep neural networks

695
00:18:34,970 --> 00:18:36,425
0,270 270,435 435,765 765,1040 1180,1455
though, the relu function, which

696
00:18:36,425 --> 00:18:36,995
0,120 120,255 255,375 375,450 450,570
you can see on the

697
00:18:36,995 --> 00:18:38,270
0,195 195,435 435,750 750,1080 1080,1275
far right hand, is a

698
00:18:38,270 --> 00:18:39,920
0,180 180,500 550,990 990,1340 1390,1650
very popular activation function. {Because,it's

699
00:18:39,920 --> 00:18:41,090
0,225 225,390 390,600 600,900 900,1170
-} piece wise, linear, it's

700
00:18:41,090 --> 00:18:43,100
0,350 430,780 780,990 990,1400 1690,2010
extremely efficient to compute, especially

701
00:18:43,100 --> 00:18:45,110
0,255 255,540 540,770 880,1550 1720,2010
when computing its {derivatives.,Its -}

702
00:18:45,110 --> 00:18:47,630
0,465 465,770 1360,2030 2110,2385 2385,2520
derivatives are constants, except for

703
00:18:47,630 --> 00:18:49,360
0,260 310,600 600,960 960,1310
one, non linear yet.|

704
00:18:49,600 --> 00:18:50,660
0,400
Zero.|

705
00:18:51,600 --> 00:18:53,060
0,395 395,650 650,910 990,1265 1265,1460
Now I hope actually all

706
00:18:53,060 --> 00:18:54,095
0,180 180,345 345,555 555,765 765,1035
of you are probably asking

707
00:18:54,095 --> 00:18:55,205
0,240 240,450 450,675 675,840 840,1110
this question to yourself of

708
00:18:55,205 --> 00:18:56,120
0,315 315,465 465,540 540,705 705,915
why do we even need

709
00:18:56,120 --> 00:18:57,620
0,180 180,675 675,1035 1035,1305 1305,1500
this nonlinear activation function? It

710
00:18:57,620 --> 00:18:58,400
0,165 165,375 375,555 555,675 675,780
seems like it kind of

711
00:18:58,400 --> 00:18:59,740
0,165 165,660 660,870 870,1050 1050,1340
just complicates this whole picture

712
00:18:59,850 --> 00:19:00,860
0,290 290,440 440,635 635,815 815,1010
when we didn't really need

713
00:19:00,860 --> 00:19:01,720
0,105 105,210 210,330 330,525 525,860
it in the first place.

714
00:19:02,250 --> 00:19:03,335
0,320 320,500 500,650 650,815 815,1085
{And,I,want - -} to just

715
00:19:03,335 --> 00:19:04,670
0,270 270,390 390,635 655,915 915,1335
spend a moment on answering

716
00:19:04,670 --> 00:19:06,065
0,285 285,680 760,1065 1065,1260 1260,1395
this because the point of

717
00:19:06,065 --> 00:19:07,720
0,105 105,600 600,990 990,1305 1305,1655
a nonlinear activation function is

718
00:19:08,070 --> 00:19:09,545
0,290 290,580 660,980 980,1235 1235,1475
of course number one is

719
00:19:09,545 --> 00:19:11,600
0,300 300,695 805,1755 1755,1920 1920,2055
to introduce nonlinearities to our

720
00:19:11,600 --> 00:19:13,100
0,290 640,960 960,1140 1140,1290 1290,1500
data, right? If we think

721
00:19:13,100 --> 00:19:15,380
0,285 285,585 585,920 1750,2055 2055,2280
about our data, almost all

722
00:19:15,380 --> 00:19:16,445
0,270 270,495 495,645 645,825 825,1065
data that we care about,

723
00:19:16,445 --> 00:19:17,570
0,255 255,465 465,675 675,900 900,1125
all real world data is

724
00:19:17,570 --> 00:19:19,730
0,270 270,980 1510,1800 1800,1950 1950,2160
highly {nonlinear.,Now -} this is

725
00:19:19,730 --> 00:19:21,395
0,350 400,800 1150,1410 1410,1530 1530,1665
important because if we want

726
00:19:21,395 --> 00:19:21,995
0,120 120,195 195,300 300,450 450,600
to be able to deal

727
00:19:21,995 --> 00:19:23,120
0,135 135,285 285,465 465,615 615,1125
with those types of datasets,

728
00:19:23,120 --> 00:19:24,260
0,225 225,405 405,705 705,960 960,1140
we need models that are

729
00:19:24,260 --> 00:19:25,445
0,195 195,690 690,900 900,1035 1035,1185
also nonlinear so they can

730
00:19:25,445 --> 00:19:26,645
0,240 240,510 510,705 705,900 900,1200
capture those same types {of,patterns.

731
00:19:26,645 --> 00:19:28,130
0,395 595,870 870,1080 1080,1305 1305,1485
-} So imagine I told

732
00:19:28,130 --> 00:19:29,135
0,150 150,270 270,530 550,825 825,1005
{you,to -} separate. For example,

733
00:19:29,135 --> 00:19:29,990
0,165 165,285 285,405 405,555 555,855
I gave you this dataas

734
00:19:29,990 --> 00:19:31,415
0,260 490,810 810,1035 1035,1230 1230,1425
set, red points from green

735
00:19:31,415 --> 00:19:32,045
0,195 195,330 330,405 405,510 510,630
points, and I ask you

736
00:19:32,045 --> 00:19:33,020
0,120 120,225 225,330 330,605 655,975
to try and separate those

737
00:19:33,020 --> 00:19:34,720
0,285 285,615 615,885 885,1190 1300,1700
two {types,of -} data points.

738
00:19:35,340 --> 00:19:36,470
0,400 450,710 710,845 845,995 995,1130
Now you might think that

739
00:19:36,470 --> 00:19:37,325
0,120 120,240 240,450 450,705 705,855
this is easy, but if

740
00:19:37,325 --> 00:19:38,390
0,120 120,240 240,485 595,870 870,1065
I could, only if I

741
00:19:38,390 --> 00:19:39,215
0,225 225,405 405,540 540,675 675,825
told you that you could

742
00:19:39,215 --> 00:19:40,570
0,240 240,525 525,735 735,990 990,1355
only use a {single,line -}

743
00:19:40,740 --> 00:19:41,795
0,245 245,395 395,590 590,845 845,1055
to do so. Well, now

744
00:19:41,795 --> 00:19:42,700
0,135 135,300 300,420 420,585 585,905
{it,becomes -} a very complicated

745
00:19:42,750 --> 00:19:44,180
0,350 350,575 575,850 990,1250 1250,1430
problem. In fact, you can't

746
00:19:44,180 --> 00:19:45,650
0,165 165,405 405,690 690,1070 1210,1470
really solve it effectively with

747
00:19:45,650 --> 00:19:46,840
0,135 135,375 375,740
a single line.|

748
00:19:47,510 --> 00:19:48,720
0,290 290,455 455,680 680,920 920,1210
And in fact, if you

749
00:19:49,130 --> 00:19:51,265
0,335 335,830 830,1190 1190,1540 1860,2135
introduce nonlinear activation functions to

750
00:19:51,265 --> 00:19:53,250
0,225 225,575 835,1275 1275,1590 1590,1985
your solution, that's exactly what

751
00:19:53,300 --> 00:19:54,760
0,350 350,635 635,970 990,1235 1235,1460
allows you to, you know,

752
00:19:54,760 --> 00:19:55,980
0,285 285,480 480,675 675,900 900,1220
deal with these types of

753
00:19:56,000 --> 00:19:58,525
0,400 660,1340 1340,1700 1700,2050 2220,2525
problems. {Nonlinear,activation -} functions allow

754
00:19:58,525 --> 00:19:59,755
0,180 180,315 315,480 480,690 690,1230
you to deal with nonlinear

755
00:19:59,755 --> 00:20:01,080
0,255 255,495 495,785
types of data.|

756
00:20:01,470 --> 00:20:02,825
0,290 290,455 455,725 725,965 965,1355
Now, and that's what exactly

757
00:20:02,825 --> 00:20:04,750
0,345 345,630 630,875 1135,1530 1530,1925
makes neural networks so powerful

758
00:20:04,920 --> 00:20:06,380
0,320 320,560 560,880
at their core.|

759
00:20:06,690 --> 00:20:07,850
0,275 275,590 590,815 815,965 965,1160
So let's understand this maybe

760
00:20:07,850 --> 00:20:09,080
0,180 180,345 345,570 570,870 870,1230
with a very simple example,

761
00:20:09,080 --> 00:20:10,460
0,375 375,645 645,825 825,1245 1245,1380
walking through this diagram of

762
00:20:10,460 --> 00:20:11,830
0,135 135,690 690,885 885,1065 1065,1370
a perceptron one more time.

763
00:20:12,540 --> 00:20:13,790
0,400 420,710 710,860 860,995 995,1250
{Imagine,I,give - -} you this

764
00:20:13,790 --> 00:20:15,515
0,345 345,675 675,950 1000,1335 1335,1725
trained neural network with {weights.,Now,

765
00:20:15,515 --> 00:20:16,850
0,240 240,495 495,765 765,1035 1035,1335
-} not w one w

766
00:20:16,850 --> 00:20:17,900
0,330 330,585 585,705 705,885 885,1050
two, I'm going to actually

767
00:20:17,900 --> 00:20:19,205
0,135 135,360 360,710 790,1065 1065,1305
give you numbers at {these,locations.

768
00:20:19,205 --> 00:20:20,980
0,365 565,825 825,975 975,1230 1230,1775
-} So the trained weights

769
00:20:21,210 --> 00:20:23,500
0,400 1110,1510 1560,1835 1835,2000 2000,2290
w zero will be one

770
00:20:23,610 --> 00:20:24,590
0,305 305,575 575,800 800,890 890,980
and w will be a

771
00:20:24,590 --> 00:20:26,050
0,255 255,615 615,945 945,1170 1170,1460
vector of three and negative

772
00:20:26,070 --> 00:20:26,940
0,400
two.|

773
00:20:27,210 --> 00:20:28,480
0,275 275,440 440,665 665,890 890,1270
So, this neural network has

774
00:20:28,560 --> 00:20:29,870
0,350 350,800 800,1040 1040,1175 1175,1310
two inputs like we said

775
00:20:29,870 --> 00:20:30,815
0,165 165,315 315,510 510,735 735,945
before, it has input X

776
00:20:30,815 --> 00:20:32,120
0,335 415,675 675,885 885,1095 1095,1305
one, it has input X

777
00:20:32,120 --> 00:20:33,485
0,350 730,990 990,1110 1110,1245 1245,1365
two if we want to

778
00:20:33,485 --> 00:20:34,360
0,105 105,315 315,510 510,615 615,875
get the output of it.

779
00:20:34,800 --> 00:20:36,200
0,275 275,545 545,935 935,1205 1205,1400
{This,is -} also the main

780
00:20:36,200 --> 00:20:36,845
0,165 165,270 270,405 405,540 540,645
thing I want all of

781
00:20:36,845 --> 00:20:37,595
0,105 105,225 225,390 390,585 585,750
you to take away from

782
00:20:37,595 --> 00:20:38,795
0,165 165,420 420,720 720,930 930,1200
this lecture today is that

783
00:20:38,795 --> 00:20:39,710
0,255 255,405 405,630 630,810 810,915
to get the output of

784
00:20:39,710 --> 00:20:40,610
0,120 120,570 570,660 660,765 765,900
a perceptron and there are

785
00:20:40,610 --> 00:20:41,450
0,180 180,390 390,570 570,705 705,840
three steps we need to

786
00:20:41,450 --> 00:20:43,490
0,260 430,720 720,930 930,1250 1660,2040
take from this stage, we

787
00:20:43,490 --> 00:20:45,590
0,380 640,1110 1110,1275 1275,1860 1860,2100
first compute the multiplication of

788
00:20:45,590 --> 00:20:46,750
0,225 225,525 525,630 630,765 765,1160
our inputs with our weights.|

789
00:20:48,570 --> 00:20:51,220
0,400 750,1040 1040,1505 1505,1810 2250,2650
Sorry. {Yeah,,multiply -} them together,

790
00:20:51,390 --> 00:20:53,495
0,395 395,740 740,1090 1500,1850 1850,2105
add their result and computer

791
00:20:53,495 --> 00:20:55,420
0,810 810,1095 1095,1305 1305,1590 1590,1925
nonlinearity it's these three steps

792
00:20:55,770 --> 00:20:58,220
0,400 690,1090 1230,1505 1505,1780 1890,2450
that define the forward propagation

793
00:20:58,220 --> 00:21:00,130
0,300 300,680 820,1125 1125,1290 1290,1910
of information through a perceptron.|

794
00:21:00,980 --> 00:21:01,945
0,245 245,545 545,740 740,845 845,965
So let's take a look

795
00:21:01,945 --> 00:21:03,505
0,275 565,825 825,1020 1020,1290 1290,1560
at how that exactly works,

796
00:21:03,505 --> 00:21:04,750
0,255 255,525 525,750 750,990 990,1245
right. {So,if -} we plug

797
00:21:04,750 --> 00:21:06,550
0,165 165,330 330,620 1000,1400 1540,1800
in these numbers to to

798
00:21:06,550 --> 00:21:08,440
0,210 210,590 1360,1620 1620,1755 1755,1890
those equations, we can see

799
00:21:08,440 --> 00:21:10,510
0,260 1000,1400 1420,1740 1740,1905 1905,2070
that everything inside of our

800
00:21:10,510 --> 00:21:11,845
0,210 210,840 840,1035 1035,1185 1185,1335
non linearity here, the non

801
00:21:11,845 --> 00:21:13,675
0,495 495,675 675,1025 1255,1590 1590,1830
linearity is g right that

802
00:21:13,675 --> 00:21:15,130
0,305 385,785 955,1245 1245,1380 1380,1455
function g which could be

803
00:21:15,130 --> 00:21:16,495
0,105 105,560 730,1020 1020,1170 1170,1365
a {sigmoid.,We -} saw a

804
00:21:16,495 --> 00:21:19,800
0,330 330,725 1435,1835 2245,2645 2905,3305
{previous,slide. -} That component inside

805
00:21:19,820 --> 00:21:22,420
0,400 900,1865 1865,2165 2165,2405 2405,2600
our nonlinearity is in fact

806
00:21:22,420 --> 00:21:23,740
0,240 240,450 450,600 600,1080 1080,1320
just a {two,dimensional -} line.

807
00:21:23,740 --> 00:21:24,850
0,210 210,390 390,660 660,990 990,1110
It has two inputs and

808
00:21:24,850 --> 00:21:26,010
0,120 120,330 330,585 585,825 825,1160
if we consider the space

809
00:21:26,300 --> 00:21:27,390
0,290 290,470 470,650 650,815 815,1090
of all of the possible

810
00:21:27,560 --> 00:21:29,110
0,320 320,455 455,730 930,1310 1310,1550
inputs that this neural network

811
00:21:29,110 --> 00:21:30,880
0,315 315,650 970,1245 1245,1485 1485,1770
could see, we can actually

812
00:21:30,880 --> 00:21:32,455
0,225 225,530 700,1035 1035,1305 1305,1575
plot this on a decision

813
00:21:32,455 --> 00:21:33,715
0,450 450,645 645,825 825,1005 1005,1260
boundary, right, we can plot

814
00:21:33,715 --> 00:21:35,480
0,285 285,480 480,975 975,1265
this two dimensional line.|

815
00:21:35,480 --> 00:21:38,675
0,350 760,1080 1080,1400 2470,2805 2805,3195
As as a decision boundary,

816
00:21:38,675 --> 00:21:40,235
0,105 105,270 270,575 655,1290 1290,1560
as a plane separating these

817
00:21:40,235 --> 00:21:41,710
0,270 270,635 655,915 915,1125 1125,1475
two components of our space.

818
00:21:42,390 --> 00:21:44,300
0,290 290,580 1020,1325 1325,1630 1650,1910
{In,fact, -} not only is

819
00:21:44,300 --> 00:21:45,320
0,105 105,225 225,450 450,750 750,1020
it a single plane, there's

820
00:21:45,320 --> 00:21:47,120
0,150 150,870 870,1220 1330,1620 1620,1800
a directionality component depending on

821
00:21:47,120 --> 00:21:48,095
0,225 225,450 450,600 600,750 750,975
which side of the plane

822
00:21:48,095 --> 00:21:49,385
0,225 225,405 405,555 555,815 1015,1290
that we live {on.,If -}

823
00:21:49,385 --> 00:21:50,570
0,165 165,285 285,480 480,845 895,1185
we see an input, for

824
00:21:50,570 --> 00:21:52,450
0,255 255,600 600,975 975,1370 1480,1880
example here, negative one, two,

825
00:21:52,830 --> 00:21:54,110
0,400 420,725 725,965 965,1145 1145,1280
we actually know that it

826
00:21:54,110 --> 00:21:55,385
0,225 225,560 640,960 960,1140 1140,1275
lives on one side of

827
00:21:55,385 --> 00:21:56,645
0,150 150,425 715,975 975,1095 1095,1260
the plane and it will

828
00:21:56,645 --> 00:21:57,910
0,225 225,435 435,690 690,960 960,1265
have a certain type {of,output.

829
00:21:57,990 --> 00:21:59,165
0,245 245,350 350,515 515,800 800,1175
-} In this case, that

830
00:21:59,165 --> 00:22:00,250
0,285 285,465 465,660 660,825 825,1085
output is going to be

831
00:22:00,690 --> 00:22:02,720
0,400 720,1085 1085,1450 1560,1835 1835,2030
positive, right, because in this

832
00:22:02,720 --> 00:22:03,910
0,300 300,540 540,690 690,885 885,1190
case, when we plug those

833
00:22:04,230 --> 00:22:06,050
0,335 335,560 560,800 800,1150 1470,1820
components into our equation, we'll

834
00:22:06,050 --> 00:22:07,430
0,135 135,300 300,590 760,1110 1110,1380
get a positive number that

835
00:22:07,430 --> 00:22:08,920
0,320 400,750 750,1100
passes through the.|

836
00:22:09,490 --> 00:22:10,950
0,635 635,950 950,1160 1160,1295 1295,1460
Nonlinear component, and that gets

837
00:22:10,950 --> 00:22:13,005
0,495 495,860 970,1275 1275,1580 1780,2055
propagated through as well. {Of,course,

838
00:22:13,005 --> 00:22:13,575
0,150 150,285 285,420 420,480 480,570
-} if you're on the

839
00:22:13,575 --> 00:22:14,630
0,195 195,435 435,600 600,765 765,1055
other side of the space,

840
00:22:14,980 --> 00:22:16,455
0,305 305,425 425,560 560,820 1080,1475
you're going to have the

841
00:22:16,455 --> 00:22:18,000
0,375 375,755 775,1110 1110,1335 1335,1545
opposite result, right? And that

842
00:22:18,000 --> 00:22:19,530
0,645 645,900 900,1155 1155,1335 1335,1530
thresholding function is going to

843
00:22:19,530 --> 00:22:21,600
0,290 760,1160 1240,1530 1530,1770 1770,2070
essentially live at this decision

844
00:22:21,600 --> 00:22:22,845
0,480 480,690 690,885 885,1050 1050,1245
{boundary.,So -} depending on which

845
00:22:22,845 --> 00:22:23,565
0,165 165,270 270,390 390,555 555,720
side of the space you

846
00:22:23,565 --> 00:22:25,650
0,135 135,395 595,885 885,1625 1705,2085
live on, that thresholding function,

847
00:22:25,650 --> 00:22:27,540
0,300 300,735 735,1040 1360,1665 1665,1890
that sigmoid function, is going

848
00:22:27,540 --> 00:22:29,270
0,165 165,410 640,1040 1120,1425 1425,1730
to then control how you

849
00:22:29,530 --> 00:22:30,450
0,320 320,485 485,635 635,800 800,920
move to one side or

850
00:22:30,450 --> 00:22:31,160
0,90 90,320
the other.|

851
00:22:32,510 --> 00:22:34,645
0,400 660,935 935,1210 1350,1750 1800,2135
Now in this particular example,

852
00:22:34,645 --> 00:22:36,265
0,195 195,330 330,600 600,995 1345,1620
this is very convenient because

853
00:22:36,265 --> 00:22:37,615
0,135 135,395 445,750 750,1215 1215,1350
we can actually visualize, and

854
00:22:37,615 --> 00:22:38,700
0,90 90,240 240,450 450,720 720,1085
I can draw this exact

855
00:22:38,720 --> 00:22:40,660
0,380 380,725 725,995 995,1300 1650,1940
full space for you on

856
00:22:40,660 --> 00:22:41,635
0,225 225,465 465,705 705,810 810,975
this slide. It's only a

857
00:22:41,635 --> 00:22:42,685
0,135 135,555 555,780 780,900 900,1050
two dimensional space, so it's

858
00:22:42,685 --> 00:22:43,660
0,150 150,450 450,660 660,795 795,975
very easy for us to

859
00:22:43,660 --> 00:22:45,595
0,495 495,675 675,810 810,1100 1570,1935
visualize. {But,of -} course, for

860
00:22:45,595 --> 00:22:47,050
0,315 315,630 630,990 990,1275 1275,1455
almost all problems that we

861
00:22:47,050 --> 00:22:48,930
0,240 240,590 820,1140 1140,1460 1480,1880
care about, our data points

862
00:22:49,010 --> 00:22:49,900
0,290 290,470 470,650 650,770 770,890
are not going to be

863
00:22:49,900 --> 00:22:51,265
0,150 150,770 850,1095 1095,1200 1200,1365
two {dimensional.,If -} you think

864
00:22:51,265 --> 00:22:53,425
0,150 150,270 270,545 1195,1500 1500,2160
about an image, the dimensionality

865
00:22:53,425 --> 00:22:54,190
0,60 60,195 195,405 405,615 615,765
of an image is going

866
00:22:54,190 --> 00:22:54,790
0,90 90,165 165,270 270,420 420,600
to be the number of

867
00:22:54,790 --> 00:22:56,215
0,480 480,735 735,870 870,1130 1150,1425
pixels that you have in

868
00:22:56,215 --> 00:22:57,250
0,135 135,395 535,780 780,900 900,1035
{the,image. -} So these are

869
00:22:57,250 --> 00:22:58,440
0,150 150,300 300,465 465,770 790,1190
going to be thousands of

870
00:22:58,700 --> 00:23:00,520
0,490 630,1010 1010,1265 1265,1595 1595,1820
dimensions, millions of dimensions or

871
00:23:00,520 --> 00:23:01,420
0,195 195,530
even more.|

872
00:23:01,580 --> 00:23:03,115
0,290 290,580 750,1100 1100,1340 1340,1535
And then drawing these types

873
00:23:03,115 --> 00:23:04,900
0,305 805,1230 1230,1500 1500,1650 1650,1785
of plots like you see

874
00:23:04,900 --> 00:23:06,240
0,120 120,300 300,540 540,765 765,1340
here is simply not feasible.

875
00:23:06,560 --> 00:23:07,465
0,245 245,350 350,590 590,770 770,905
{So,we -} can't always do

876
00:23:07,465 --> 00:23:08,335
0,150 150,315 315,510 510,720 720,870
this, but hopefully this gives

877
00:23:08,335 --> 00:23:10,410
0,165 165,455 475,1145 1255,1655 1675,2075
you some intuition to understand,

878
00:23:10,640 --> 00:23:11,515
0,275 275,395 395,530 530,695 695,875
kind of as we build

879
00:23:11,515 --> 00:23:13,680
0,305 415,815 1075,1475 1495,1830 1830,2165
up into more complex models.|

880
00:23:14,930 --> 00:23:15,655
0,260 260,410 410,545 545,635 635,725
So now that we have

881
00:23:15,655 --> 00:23:17,005
0,180 180,375 375,555 555,735 735,1350
an idea of the perceptron,

882
00:23:17,005 --> 00:23:18,145
0,360 360,555 555,780 780,945 945,1140
let's see how we can

883
00:23:18,145 --> 00:23:19,525
0,225 225,435 435,690 690,975 975,1380
actually take this single neuron

884
00:23:19,525 --> 00:23:20,455
0,210 210,420 420,615 615,795 795,930
and start to build it

885
00:23:20,455 --> 00:23:21,720
0,165 165,450 450,765 765,990 990,1265
up into something more complicated,

886
00:23:21,860 --> 00:23:23,050
0,260 260,410 410,665 665,920 920,1190
a full neural network, and

887
00:23:23,050 --> 00:23:24,150
0,180 180,330 330,555 555,810 810,1100
build a model from that.|

888
00:23:25,060 --> 00:23:26,660
0,335 335,635 635,1085 1085,1295 1295,1600
So let's revisit again this

889
00:23:26,740 --> 00:23:28,460
0,350 350,815 815,965 965,1100 1100,1720
previous diagram of the perceptron.

890
00:23:28,840 --> 00:23:30,540
0,400 540,920 920,1175 1175,1310 1310,1700
{If,again, -} just to reiterate

891
00:23:30,540 --> 00:23:32,370
0,195 195,495 495,890 970,1370 1510,1830
one more time, this core

892
00:23:32,370 --> 00:23:33,390
0,195 195,435 435,750 750,930 930,1020
piece of information that I

893
00:23:33,390 --> 00:23:34,245
0,135 135,270 270,390 390,570 570,855
want all of you to

894
00:23:34,245 --> 00:23:35,480
0,285 285,540 540,750 750,930 930,1235
take away from this class

895
00:23:35,950 --> 00:23:37,515
0,365 365,635 635,815 815,1340 1340,1565
is how a perceptron works

896
00:23:37,515 --> 00:23:39,410
0,210 210,375 375,665 745,1415 1495,1895
and how it propagates information

897
00:23:39,700 --> 00:23:41,190
0,245 245,470 470,850 1110,1355 1355,1490
to its {decision.,There -} are

898
00:23:41,190 --> 00:23:42,450
0,225 225,540 540,855 855,1095 1095,1260
{three,steps. -} First is the

899
00:23:42,450 --> 00:23:44,220
0,180 180,500 1090,1410 1410,1620 1620,1770
dot product, second is the

900
00:23:44,220 --> 00:23:45,660
0,440 670,960 960,1140 1140,1305 1305,1440
bias, and third {is,the -}

901
00:23:45,660 --> 00:23:47,700
0,830 1030,1290 1290,1455 1455,1710 1710,2040
nonlinearity. And you keep repeating

902
00:23:47,700 --> 00:23:48,780
0,150 150,435 435,675 675,840 840,1080
this process for every single

903
00:23:48,780 --> 00:23:50,330
0,645 645,885 885,1050 1050,1290 1290,1550
perceptron in your neural network.|

904
00:23:51,450 --> 00:23:52,775
0,335 335,710 710,830 830,1190 1190,1325
Let's simplify the diagram a

905
00:23:52,775 --> 00:23:54,035
0,150 150,455 595,915 915,1065 1065,1260
little bit. I'll get rid

906
00:23:54,035 --> 00:23:56,030
0,195 195,495 495,1055 1555,1845 1845,1995
of the weights and you

907
00:23:56,030 --> 00:23:57,110
0,165 165,345 345,510 510,765 765,1080
can assume that every line

908
00:23:57,110 --> 00:23:58,505
0,330 330,600 600,885 885,1185 1185,1395
here now basically has an

909
00:23:58,505 --> 00:24:00,280
0,305 445,810 810,1245 1245,1500 1500,1775
associated weight scaler that's associated

910
00:24:00,420 --> 00:24:02,290
0,290 290,580 810,1145 1145,1475 1475,1870
with it. {Every,line -} also

911
00:24:02,790 --> 00:24:04,550
0,605 605,755 755,1030 1110,1460 1460,1760
corresponds to the input that's

912
00:24:04,550 --> 00:24:05,540
0,180 180,480 480,720 720,855 855,990
coming in, it has a

913
00:24:05,540 --> 00:24:06,755
0,165 165,480 480,675 675,975 975,1215
weight that's coming in also

914
00:24:06,755 --> 00:24:07,920
0,150 150,455
at the.|

915
00:24:08,200 --> 00:24:09,795
0,320 320,485 485,725 725,1120 1230,1595
On the line itself. {And,I've

916
00:24:09,795 --> 00:24:11,310
0,405 405,660 660,930 930,1125 1125,1515
-} also removed the bias

917
00:24:11,310 --> 00:24:12,765
0,285 285,495 495,660 660,840 840,1455
just for sake of simplicity,

918
00:24:12,765 --> 00:24:14,595
0,150 150,345 345,495 495,785 1555,1830
but it's still {there.,So -}

919
00:24:14,595 --> 00:24:16,670
0,240 240,555 555,905 1285,1680 1680,2075
now the result is that

920
00:24:16,960 --> 00:24:18,540
0,400 480,845 845,1175 1175,1340 1340,1580
z, which let's call that

921
00:24:18,540 --> 00:24:19,665
0,330 330,600 600,735 735,885 885,1125
the result of our dot

922
00:24:19,665 --> 00:24:21,705
0,335 475,780 780,945 945,1355 1705,2040
product plus the bias is

923
00:24:21,705 --> 00:24:22,995
0,300 300,585 585,930 930,1125 1125,1290
going and that's what we

924
00:24:22,995 --> 00:24:24,290
0,180 180,405 405,585 585,1005 1005,1295
pass into our {nonlinear,function. -}

925
00:24:24,880 --> 00:24:26,745
0,365 365,730 900,1235 1235,1520 1520,1865
That piece is going to

926
00:24:26,745 --> 00:24:28,610
0,330 330,665 805,1065 1065,1325 1375,1865
be applied to {that,activation -}

927
00:24:28,660 --> 00:24:30,165
0,400 480,755 755,890 890,1150 1230,1505
function. Now the final output

928
00:24:30,165 --> 00:24:31,935
0,275 745,1065 1065,1335 1335,1605 1605,1770
here is simply going to

929
00:24:31,935 --> 00:24:34,140
0,245 595,995 1615,1890 1890,2010 2010,2205
be g, which is our

930
00:24:34,140 --> 00:24:35,560
0,375 375,740
activation function.|

931
00:24:35,560 --> 00:24:37,675
0,350 460,860 1540,1800 1800,1950 1950,2115
Of Z Z is going

932
00:24:37,675 --> 00:24:38,620
0,105 105,270 270,585 585,825 825,945
to be basically what you

933
00:24:38,620 --> 00:24:39,445
0,135 135,255 255,375 375,555 555,825
can think of the state

934
00:24:39,445 --> 00:24:40,435
0,225 225,375 375,645 645,840 840,990
of this neuron. It's the

935
00:24:40,435 --> 00:24:42,055
0,195 195,360 360,635 985,1305 1305,1620
result of that dot product

936
00:24:42,055 --> 00:24:43,540
0,300 300,785
plus bias.|

937
00:24:44,060 --> 00:24:44,950
0,290 290,440 440,545 545,680 680,890
Now, if we want to

938
00:24:44,950 --> 00:24:46,570
0,320 340,630 630,840 840,1160 1330,1620
define and build up a

939
00:24:46,570 --> 00:24:48,450
0,285 285,860 1060,1350 1350,1605 1605,1880
multi layered output neural network,

940
00:24:49,010 --> 00:24:50,140
0,245 245,335 335,485 485,755 755,1130
if we want two outputs

941
00:24:50,140 --> 00:24:51,160
0,165 165,300 300,570 570,825 825,1020
to this function, for example,

942
00:24:51,160 --> 00:24:52,410
0,225 225,285 285,435 435,740 850,1250
it's a very simple procedure.

943
00:24:52,490 --> 00:24:53,680
0,275 275,425 425,620 620,905 905,1190
{We,just -} have now two

944
00:24:53,680 --> 00:24:55,930
0,470 490,795 795,1410 1410,1695 1695,2250
neurons, two perceptons, each perceptron

945
00:24:55,930 --> 00:24:57,610
0,210 210,420 420,630 630,980 1390,1680
will control the output for

946
00:24:57,610 --> 00:25:00,370
0,255 255,620 730,1130 1300,1700 2500,2760
its associated piece, right? So

947
00:25:00,370 --> 00:25:01,300
0,120 120,210 210,315 315,525 525,930
now we have two {outputs.,Each

948
00:25:01,300 --> 00:25:02,200
0,300 300,495 495,615 615,705 705,900
-} one is the normal

949
00:25:02,200 --> 00:25:03,670
0,630 630,855 855,1050 1050,1275 1275,1470
perceptron it takes all of

950
00:25:03,670 --> 00:25:04,930
0,210 210,615 615,855 855,1035 1035,1260
the inputs so they both

951
00:25:04,930 --> 00:25:06,160
0,195 195,345 345,615 615,1005 1005,1230
take the {same,inputs. -} But

952
00:25:06,160 --> 00:25:08,520
0,650 910,1215 1215,1395 1395,1575 1575,2360
amazingly now with this mathematical

953
00:25:08,540 --> 00:25:10,450
0,400 870,1145 1145,1420 1470,1760 1760,1910
understanding we can start to

954
00:25:10,450 --> 00:25:11,970
0,260 280,630 630,930 930,1260 1260,1520
build our first neural network

955
00:25:12,140 --> 00:25:14,395
0,400 420,755 755,1090 1350,1750 2010,2255
{entirely,from -} scratch. So what

956
00:25:14,395 --> 00:25:15,235
0,105 105,255 255,420 420,630 630,840
does that look like? So

957
00:25:15,235 --> 00:25:16,615
0,150 150,345 345,585 585,870 870,1380
we can start by firstly

958
00:25:16,615 --> 00:25:18,865
0,605 865,1260 1260,1605 1605,1955 1975,2250
{initializing,these -} two components. The

959
00:25:18,865 --> 00:25:19,945
0,255 255,570 570,765 765,900 900,1080
first component that we saw

960
00:25:19,945 --> 00:25:22,060
0,225 225,390 390,555 555,1205 1825,2115
was the weight matrix, excuse

961
00:25:22,060 --> 00:25:23,260
0,195 195,345 345,510 510,860 880,1200
me, the weight vector. It's

962
00:25:23,260 --> 00:25:24,595
0,120 120,405 405,660 660,1080 1080,1335
a vector of weights in

963
00:25:24,595 --> 00:25:26,120
0,180 180,485
this case.|

964
00:25:26,400 --> 00:25:27,875
0,290 290,455 455,730 750,1130 1130,1475
And the second component is

965
00:25:27,875 --> 00:25:30,170
0,365 685,1085 1195,1635 1635,1985 2035,2295
the, the bias vector that

966
00:25:30,170 --> 00:25:31,310
0,150 150,285 285,435 435,945 945,1140
we're going to multiply with

967
00:25:31,310 --> 00:25:32,270
0,120 120,300 300,585 585,825 825,960
the dot product of all

968
00:25:32,270 --> 00:25:33,440
0,105 105,300 300,740 760,1020 1020,1170
of our inputs by our

969
00:25:33,440 --> 00:25:35,720
0,440 760,1155 1155,1550
weights, right? So.|

970
00:25:35,970 --> 00:25:37,445
0,260 260,485 485,830 830,1145 1145,1475
The only remaining step now

971
00:25:37,445 --> 00:25:39,310
0,345 345,705 705,990 990,1305 1305,1865
after we've defined these parameters

972
00:25:39,390 --> 00:25:41,135
0,275 275,470 470,790 1140,1505 1505,1745
of our layer is to

973
00:25:41,135 --> 00:25:42,995
0,270 270,665 775,1035 1035,1295 1555,1860
now define, you know, how

974
00:25:42,995 --> 00:25:44,630
0,210 210,450 450,960 960,1275 1275,1635
this forward propagation of information

975
00:25:44,630 --> 00:25:46,030
0,330 330,525 525,780 780,1035 1035,1400
works. {And,that's -} exactly those

976
00:25:46,140 --> 00:25:47,555
0,305 305,590 590,950 950,1190 1190,1415
three main components that I've

977
00:25:47,555 --> 00:25:49,540
0,245 865,1365 1365,1500 1500,1650 1650,1985
been stressing to you so

978
00:25:49,680 --> 00:25:50,735
0,260 260,410 410,620 620,845 845,1055
we can create this call

979
00:25:50,735 --> 00:25:52,160
0,305 355,615 615,795 795,1095 1095,1425
function to do exactly that,

980
00:25:52,160 --> 00:25:53,690
0,285 285,555 555,780 780,1020 1020,1530
to define this forward propagation

981
00:25:53,690 --> 00:25:55,745
0,315 315,710 1480,1740 1740,1875 1875,2055
of {information.,And -} the story

982
00:25:55,745 --> 00:25:56,885
0,165 165,420 420,750 750,960 960,1140
here is exactly the same

983
00:25:56,885 --> 00:25:57,950
0,195 195,405 405,555 555,795 795,1065
as we've been seeing it,

984
00:25:57,950 --> 00:25:59,980
0,350 370,885 885,1305 1305,1560 1560,2030
right? Matrix, multiply our inputs

985
00:26:00,120 --> 00:26:01,620
0,275 275,440 440,880
with our weights.|

986
00:26:02,550 --> 00:26:04,260
0,290 290,440 440,850
Add a bias.|

987
00:26:04,530 --> 00:26:06,530
0,275 275,550 630,980 980,1205 1205,2000
And then apply a nonlinearity

988
00:26:06,530 --> 00:26:08,270
0,300 300,555 555,765 765,1070 1450,1740
and return the result and

989
00:26:08,270 --> 00:26:09,800
0,285 285,680 760,1095 1095,1335 1335,1530
that literally this code will

990
00:26:09,800 --> 00:26:11,240
0,285 285,570 570,795 795,1130 1150,1440
run. {This,will -} define a

991
00:26:11,240 --> 00:26:12,970
0,290 520,825 825,1110 1110,1470 1470,1730
full, a full neural network

992
00:26:13,020 --> 00:26:15,080
0,400 750,1010 1010,1160 1160,1450 1710,2060
layer that you can then

993
00:26:15,080 --> 00:26:17,630
0,350 460,765 765,1070 1960,2310 2310,2550
take like {this.,And -} of

994
00:26:17,630 --> 00:26:18,770
0,285 285,540 540,915 915,1050 1050,1140
course, actually, luckily for all

995
00:26:18,770 --> 00:26:19,745
0,120 120,380 400,675 675,810 810,975
of you, all of that

996
00:26:19,745 --> 00:26:21,040
0,240 240,450 450,750 750,960 960,1295
code, which wasn't much code

997
00:26:21,090 --> 00:26:22,670
0,380 380,640 660,1130 1130,1340 1340,1580
that's been abstracted away by

998
00:26:22,670 --> 00:26:24,140
0,210 210,615 615,810 810,1320 1320,1470
these libraries, like tensorflow, you

999
00:26:24,140 --> 00:26:25,560
0,135 135,410 430,830
can simply call.|

1000
00:26:25,560 --> 00:26:27,105
0,290 340,645 645,945 945,1245 1245,1545
Functions like this, which will

1001
00:26:27,105 --> 00:26:28,860
0,360 360,570 570,750 750,1380 1380,1755
actually, you know, replicate exactly

1002
00:26:28,860 --> 00:26:30,870
0,285 285,480 480,675 675,980 1720,2010
that piece of code. {So,you

1003
00:26:30,870 --> 00:26:31,940
0,135 135,300 300,420 420,690 690,1070
-} don't need to necessarily

1004
00:26:31,960 --> 00:26:32,850
0,305 305,470 470,590 590,740 740,890
copy all of that code

1005
00:26:32,850 --> 00:26:34,815
0,165 165,470 1360,1620 1620,1785 1785,1965
{down.,You -} can just call

1006
00:26:34,815 --> 00:26:35,660
0,275
it.|

1007
00:26:36,250 --> 00:26:38,745
0,400 540,845 845,1150 1620,2020 2250,2495
And with that understanding, you

1008
00:26:38,745 --> 00:26:39,825
0,210 210,510 510,735 735,915 915,1080
know, we just saw how

1009
00:26:39,825 --> 00:26:40,710
0,135 135,270 270,465 465,660 660,885
you could build a single

1010
00:26:40,710 --> 00:26:42,290
0,315 315,525 525,675 675,980 1180,1580
layer. {But,of -} course, now

1011
00:26:42,670 --> 00:26:43,740
0,275 275,485 485,710 710,905 905,1070
you can actually start to

1012
00:26:43,740 --> 00:26:44,685
0,165 165,360 360,540 540,705 705,945
think about how you can

1013
00:26:44,685 --> 00:26:46,730
0,365 535,855 855,1355 1435,1740 1740,2045
stack these layers as {well.,So

1014
00:26:47,080 --> 00:26:48,660
0,400 750,1055 1055,1220 1220,1370 1370,1580
-} since we now have

1015
00:26:48,660 --> 00:26:51,050
0,320 370,770 1390,1790 1870,2130 2130,2390
this transformation essentially from our

1016
00:26:51,070 --> 00:26:53,295
0,520 840,1160 1160,1400 1400,1720 1920,2225
inputs to a hidden output,

1017
00:26:53,295 --> 00:26:54,045
0,165 165,315 315,465 465,600 600,750
you can think of this

1018
00:26:54,045 --> 00:26:56,330
0,180 180,485 1255,1655 1675,1980 1980,2285
as basically how we can.|

1019
00:26:57,350 --> 00:26:59,800
0,400 750,1085 1085,1385 1385,1750 1920,2450
Define some way of transforming

1020
00:26:59,800 --> 00:27:02,260
0,290 340,890 1240,1640 1810,2190 2190,2460
those inputs right into some

1021
00:27:02,260 --> 00:27:04,900
0,165 165,720 720,1040 1450,1850 2290,2640
new dimensional space, right, perhaps

1022
00:27:04,900 --> 00:27:06,250
0,350 400,720 720,900 900,1125 1125,1350
closer to the value that

1023
00:27:06,250 --> 00:27:07,420
0,120 120,300 300,480 480,740 790,1170
we want to predict. {And,that

1024
00:27:07,420 --> 00:27:08,935
0,360 360,740 910,1185 1185,1365 1365,1515
-} transformation is going to

1025
00:27:08,935 --> 00:27:10,795
0,150 150,455 895,1295 1435,1680 1680,1860
be eventually learned to know

1026
00:27:10,795 --> 00:27:12,190
0,285 285,600 600,840 840,1080 1080,1395
how to transform those inputs

1027
00:27:12,190 --> 00:27:13,705
0,195 195,420 420,830 850,1305 1305,1515
into our desired {outputs.,And -}

1028
00:27:13,705 --> 00:27:14,490
0,135 135,240 240,345 345,480 480,785
we'll get to {that,later. -}

1029
00:27:14,720 --> 00:27:16,150
0,290 290,440 440,700 1020,1280 1280,1430
But for now, the piece

1030
00:27:16,150 --> 00:27:16,930
0,135 135,255 255,435 435,600 600,780
that I want to really

1031
00:27:16,930 --> 00:27:18,085
0,285 285,555 555,795 795,1005 1005,1155
focus on is if we

1032
00:27:18,085 --> 00:27:19,360
0,180 180,390 390,690 690,1005 1005,1275
have these more complex neural

1033
00:27:19,360 --> 00:27:20,860
0,260 670,930 930,1095 1095,1290 1290,1500
networks, I want to really

1034
00:27:20,860 --> 00:27:22,000
0,450 450,660 660,840 840,975 975,1140
distill down that this is

1035
00:27:22,000 --> 00:27:23,230
0,255 255,620 640,945 945,1095 1095,1230
nothing more complex than what

1036
00:27:23,230 --> 00:27:24,340
0,300 300,540 540,795 795,960 960,1110
we've already seen if we

1037
00:27:24,340 --> 00:27:25,980
0,240 240,570 570,870 870,1140 1140,1640
focus on just one neuron

1038
00:27:26,270 --> 00:27:27,980
0,320 320,620 620,1330
in this diagram.|

1039
00:27:28,480 --> 00:27:30,270
0,400 750,1055 1055,1235 1235,1490 1490,1790
Take here, for example, z

1040
00:27:30,270 --> 00:27:31,935
0,320 820,1140 1140,1320 1320,1470 1470,1665
two z two is this

1041
00:27:31,935 --> 00:27:33,180
0,270 270,480 480,945 945,1095 1095,1245
neuron that's highlighted in the

1042
00:27:33,180 --> 00:27:35,235
0,165 165,470 1300,1665 1665,1845 1845,2055
middle layer. It's just the

1043
00:27:35,235 --> 00:27:36,495
0,210 210,750 750,900 900,1110 1110,1260
same perceptron that we've been

1044
00:27:36,495 --> 00:27:38,010
0,285 285,555 555,845 985,1275 1275,1515
seeing so far in this

1045
00:27:38,010 --> 00:27:39,705
0,350 460,705 705,950 970,1370 1420,1695
class. {It,was -} its output

1046
00:27:39,705 --> 00:27:40,935
0,135 135,600 600,855 855,1050 1050,1230
is obtained by taking a

1047
00:27:40,935 --> 00:27:42,600
0,195 195,515 775,1095 1095,1260 1260,1665
dot product, adding a bias,

1048
00:27:42,600 --> 00:27:44,295
0,240 240,420 420,660 660,870 870,1695
and then applying that nonlinearity

1049
00:27:44,295 --> 00:27:45,380
0,195 195,315 315,420 420,630 630,1085
between all of its inputs.|

1050
00:27:46,280 --> 00:27:47,005
0,260 260,380 380,500 500,605 605,725
If we look at a

1051
00:27:47,005 --> 00:27:48,400
0,255 255,705 705,870 870,1110 1110,1395
different node, for example z

1052
00:27:48,400 --> 00:27:49,105
0,210 210,360 360,480 480,585 585,705
three, which is the one

1053
00:27:49,105 --> 00:27:50,440
0,165 165,390 390,725 835,1185 1185,1335
right below it, it's the

1054
00:27:50,440 --> 00:27:51,595
0,195 195,405 405,645 645,930 930,1155
exact same story. {Again,,it -}

1055
00:27:51,595 --> 00:27:52,705
0,165 165,375 375,555 555,780 780,1110
sees all the same inputs,

1056
00:27:52,705 --> 00:27:53,470
0,120 120,240 240,390 390,525 525,765
but it has a different

1057
00:27:53,470 --> 00:27:55,090
0,240 240,360 360,555 555,1220 1360,1620
set of weight matrix that

1058
00:27:55,090 --> 00:27:56,380
0,165 165,315 315,585 585,950 1030,1290
it's going to apply to

1059
00:27:56,380 --> 00:27:57,310
0,255 255,615 615,720 720,840 840,930
those {inputs.,So -} we'll have

1060
00:27:57,310 --> 00:27:58,840
0,120 120,380 430,830 1060,1335 1335,1530
a different output, but the

1061
00:27:58,840 --> 00:28:00,625
0,690 690,990 990,1275 1275,1560 1560,1785
mathematical equations are exactly the

1062
00:28:00,625 --> 00:28:01,440
0,275
same.|

1063
00:28:01,540 --> 00:28:02,385
0,275 275,425 425,575 575,695 695,845
So from now on, I'm

1064
00:28:02,385 --> 00:28:03,180
0,120 120,285 285,510 510,690 690,795
just going to kind of

1065
00:28:03,180 --> 00:28:04,370
0,420 420,570 570,705 705,885 885,1190
simplify all of these lines

1066
00:28:04,390 --> 00:28:06,450
0,380 380,1145 1145,1430 1430,1775 1775,2060
and diagrams just to show

1067
00:28:06,450 --> 00:28:07,610
0,210 210,660 660,810 810,915 915,1160
these icons in the middle,

1068
00:28:07,660 --> 00:28:08,985
0,305 305,610 720,1010 1010,1160 1160,1325
just to demonstrate that these

1069
00:28:08,985 --> 00:28:10,095
0,305 565,795 795,900 900,1035 1035,1110
means everything is going to

1070
00:28:10,095 --> 00:28:11,280
0,90 90,330 330,630 630,945 945,1185
be fully connected to everything

1071
00:28:11,280 --> 00:28:13,170
0,195 195,450 450,660 660,950 1120,1890
and defined by those mathematical

1072
00:28:13,170 --> 00:28:14,390
0,320 340,615 615,825 825,945 945,1220
equations that we've been covering.

1073
00:28:14,920 --> 00:28:16,590
0,260 260,485 485,635 635,910 1020,1670
{But,there's -} no extra complexity

1074
00:28:16,590 --> 00:28:18,060
0,255 255,480 480,800 1090,1350 1350,1470
in these models from what

1075
00:28:18,060 --> 00:28:19,240
0,225 225,435 435,770
you've already seen.|

1076
00:28:19,580 --> 00:28:20,440
0,275 275,410 410,515 515,650 650,860
Now, if you want to

1077
00:28:20,440 --> 00:28:22,650
0,320 430,750 750,990 990,1310 1810,2210
stack these types of solutions

1078
00:28:22,970 --> 00:28:23,935
0,305 305,485 485,620 620,755 755,965
on top of each other,

1079
00:28:23,935 --> 00:28:25,000
0,240 240,600 600,765 765,930 930,1065
these layers on top of

1080
00:28:25,000 --> 00:28:26,365
0,135 135,410 820,1095 1095,1230 1230,1365
each other, you can not

1081
00:28:26,365 --> 00:28:27,505
0,240 240,510 510,705 705,915 915,1140
only define one layer very

1082
00:28:27,505 --> 00:28:28,915
0,305 445,705 705,840 840,1110 1110,1410
easily, but you can actually

1083
00:28:28,915 --> 00:28:30,370
0,305 325,585 585,705 705,900 900,1455
create what are called sequential

1084
00:28:30,370 --> 00:28:32,290
0,260 310,615 615,1080 1080,1340 1660,1920
models. {These,sequential -} models, you

1085
00:28:32,290 --> 00:28:33,460
0,180 180,435 435,675 675,885 885,1170
can define one layer after

1086
00:28:33,460 --> 00:28:35,580
0,380 640,900 900,1160 1300,1700 1720,2120
another and they define basically

1087
00:28:35,600 --> 00:28:37,270
0,290 290,515 515,1010 1010,1310 1310,1670
the forward propagation of information,

1088
00:28:37,270 --> 00:28:38,920
0,300 300,620 910,1185 1185,1335 1335,1650
not just from the neuron

1089
00:28:38,920 --> 00:28:40,120
0,290 430,705 705,885 885,1065 1065,1200
level, but now from the

1090
00:28:40,120 --> 00:28:41,725
0,210 210,540 540,900 900,1280 1330,1605
layer {level.,Every -} layer will

1091
00:28:41,725 --> 00:28:43,150
0,180 180,465 465,845 1075,1320 1320,1425
be fully connected to the

1092
00:28:43,150 --> 00:28:44,695
0,195 195,530 670,945 945,1185 1185,1545
next layer, and the inputs

1093
00:28:44,695 --> 00:28:46,210
0,165 165,450 450,765 765,1085 1255,1515
of the secondary layer will

1094
00:28:46,210 --> 00:28:47,440
0,135 135,315 315,495 495,765 765,1230
be all of the outputs

1095
00:28:47,440 --> 00:28:48,740
0,240 240,405 405,630 630,980
of the prior layer.|

1096
00:28:50,030 --> 00:28:50,815
0,260 260,380 380,530 530,680 680,785
Now, of course, if you

1097
00:28:50,815 --> 00:28:51,655
0,105 105,225 225,375 375,555 555,840
want to create a very

1098
00:28:51,655 --> 00:28:53,275
0,315 315,615 615,875 1105,1425 1425,1620
deep neural network, all the

1099
00:28:53,275 --> 00:28:54,430
0,180 180,435 435,645 645,930 930,1155
deep neural network is, is

1100
00:28:54,430 --> 00:28:55,675
0,165 165,405 405,690 690,1095 1095,1245
we just keep stacking these

1101
00:28:55,675 --> 00:28:56,530
0,285 285,435 435,600 600,735 735,855
layers on top of each

1102
00:28:56,530 --> 00:28:58,015
0,260 520,855 855,1050 1050,1305 1305,1485
other. There's nothing else to

1103
00:28:58,015 --> 00:28:59,155
0,150 150,435 435,765 765,945 945,1140
this story that's really as

1104
00:28:59,155 --> 00:29:00,660
0,195 195,390 390,525 525,785 1105,1505
simple as it is once.

1105
00:29:01,670 --> 00:29:02,730
0,260 260,410 410,680 680,800 800,1060
{So,these -} layers are basically

1106
00:29:02,840 --> 00:29:03,790
0,290 290,440 440,590 590,755 755,950
all they are is just

1107
00:29:03,790 --> 00:29:05,155
0,360 360,510 510,645 645,920 1060,1365
layers where the final output

1108
00:29:05,155 --> 00:29:06,480
0,285 285,815
is computed.|

1109
00:29:06,640 --> 00:29:08,550
0,400 720,1040 1040,1310 1310,1640 1640,1910
Right, by going deeper and

1110
00:29:08,550 --> 00:29:10,200
0,270 270,570 570,795 795,1410 1410,1650
deeper into this progression of

1111
00:29:10,200 --> 00:29:11,700
0,270 270,800 970,1230 1230,1350 1350,1500
different layers, and you just

1112
00:29:11,700 --> 00:29:12,810
0,180 180,555 555,765 765,975 975,1110
keep stacking them until you

1113
00:29:12,810 --> 00:29:13,760
0,150 150,285 285,375 375,585 585,950
get to the last layer,

1114
00:29:14,080 --> 00:29:15,135
0,275 275,425 425,635 635,830 830,1055
which is your output layer.

1115
00:29:15,135 --> 00:29:16,320
0,285 285,390 390,600 600,975 975,1185
It's your final prediction that

1116
00:29:16,320 --> 00:29:17,480
0,120 120,270 270,495 495,830
you want to output.|

1117
00:29:18,450 --> 00:29:19,370
0,335 335,515 515,650 650,800 800,920
Right, we can create a

1118
00:29:19,370 --> 00:29:20,150
0,135 135,330 330,525 525,705 705,780
deep neural network to do

1119
00:29:20,150 --> 00:29:21,610
0,120 120,285 285,510 510,840 840,1460
all of this by stacking

1120
00:29:21,750 --> 00:29:23,120
0,290 290,560 560,725 725,1025 1025,1370
these layers and creating these

1121
00:29:23,120 --> 00:29:24,785
0,315 315,1005 1005,1245 1245,1500 1500,1665
more hierarchical models like we

1122
00:29:24,785 --> 00:29:25,880
0,240 240,510 510,765 765,960 960,1095
saw very early in the

1123
00:29:25,880 --> 00:29:27,905
0,225 225,560 790,1365 1365,1700 1720,2025
beginning of today's lecture, one

1124
00:29:27,905 --> 00:29:29,050
0,165 165,300 300,575 625,885 885,1145
where the final output is

1125
00:29:29,190 --> 00:29:30,635
0,395 395,755 755,1030 1050,1295 1295,1445
really computed by, you know,

1126
00:29:30,635 --> 00:29:31,780
0,210 210,450 450,705 705,885 885,1145
just going deeper and deeper

1127
00:29:32,040 --> 00:29:33,640
0,365 365,635 635,940
into this system.|

1128
00:29:34,910 --> 00:29:36,415
0,305 305,590 590,1010 1010,1235 1235,1505
Okay, so that's awesome. {So,we've

1129
00:29:36,415 --> 00:29:37,855
0,345 345,555 555,905 925,1230 1230,1440
-} now seen how we

1130
00:29:37,855 --> 00:29:39,430
0,305 475,840 840,1125 1125,1335 1335,1575
can go from a single

1131
00:29:39,430 --> 00:29:41,305
0,470 640,870 870,990 990,1280 1600,1875
neuron to a layer to

1132
00:29:41,305 --> 00:29:41,950
0,165 165,285 285,420 420,555 555,645
all the way to a

1133
00:29:41,950 --> 00:29:43,480
0,150 150,435 435,710 970,1305 1305,1530
deep neural network building off

1134
00:29:43,480 --> 00:29:46,420
0,165 165,440 820,1400 1450,1850
of these foundational principles.|

1135
00:29:46,430 --> 00:29:47,305
0,350 350,470 470,575 575,695 695,875
Let's take a look at

1136
00:29:47,305 --> 00:29:49,200
0,270 270,600 600,825 825,1085 1495,1895
how exactly we can use

1137
00:29:49,460 --> 00:29:52,345
0,400 1110,1355 1355,1600 1950,2350 2610,2885
these, you know, principles that

1138
00:29:52,345 --> 00:29:53,920
0,210 210,455 475,875 1105,1380 1380,1575
we've just discussed to solve

1139
00:29:53,920 --> 00:29:55,435
0,180 180,390 390,675 675,1010 1270,1515
a very real problem that

1140
00:29:55,435 --> 00:29:56,125
0,120 120,270 270,405 405,525 525,690
I think all of you

1141
00:29:56,125 --> 00:29:57,630
0,195 195,435 435,785 835,1170 1170,1505
are probably very concerned about

1142
00:29:58,670 --> 00:30:00,310
0,305 305,575 575,830 830,1120 1380,1640
this morning when you, when

1143
00:30:00,310 --> 00:30:01,525
0,260 310,600 600,840 840,1050 1050,1215
you woke up. {So,that -}

1144
00:30:01,525 --> 00:30:03,550
0,270 270,635 1045,1445 1465,1755 1755,2025
problem is how we can

1145
00:30:03,550 --> 00:30:04,765
0,270 270,420 420,630 630,890 970,1215
build a neural network to

1146
00:30:04,765 --> 00:30:05,935
0,195 195,435 435,705 705,945 945,1170
answer this question, which is

1147
00:30:05,935 --> 00:30:07,345
0,365 415,810 810,1065 1065,1215 1215,1410
I, how will I pass

1148
00:30:07,345 --> 00:30:08,410
0,210 210,450 450,720 720,915 915,1065
this class, and if I

1149
00:30:08,410 --> 00:30:10,500
0,285 285,540 540,675 675,950
will, will I not?|

1150
00:30:10,500 --> 00:30:12,170
0,380 640,885 885,1095 1095,1365 1365,1670
So to answer this question,

1151
00:30:12,220 --> 00:30:12,915
0,335 335,395 395,470 470,575 575,695
let's see if we can

1152
00:30:12,915 --> 00:30:13,785
0,120 120,225 225,435 435,645 645,870
train a neural network to

1153
00:30:13,785 --> 00:30:16,240
0,275 445,765 765,1085
solve this problem.|

1154
00:30:16,240 --> 00:30:17,305
0,330 330,525 525,630 630,795 795,1065
So to do this, let's

1155
00:30:17,305 --> 00:30:18,310
0,150 150,270 270,405 405,660 660,1005
start with a very simple

1156
00:30:18,310 --> 00:30:20,575
0,360 360,620 1450,1845 1845,2055 2055,2265
neural network. We'll train this

1157
00:30:20,575 --> 00:30:21,955
0,285 285,585 585,840 840,1185 1185,1380
model with two inputs, just

1158
00:30:21,955 --> 00:30:23,125
0,225 225,510 510,765 765,1020 1020,1170
two inputs. {One,input -} is

1159
00:30:23,125 --> 00:30:23,740
0,150 150,255 255,345 345,465 465,615
going to be the number

1160
00:30:23,740 --> 00:30:25,500
0,180 180,710 940,1215 1215,1425 1425,1760
of lectures that you attend

1161
00:30:25,580 --> 00:30:26,530
0,305 305,470 470,635 635,800 800,950
over the course of this

1162
00:30:26,530 --> 00:30:28,020
0,195 195,500 820,1080 1080,1215 1215,1490
one {week.,And -} the second

1163
00:30:28,160 --> 00:30:29,190
0,275 275,440 440,635 635,785 785,1030
input is going to be

1164
00:30:29,450 --> 00:30:30,745
0,275 275,440 440,730 870,1130 1130,1295
how many hours that you

1165
00:30:30,745 --> 00:30:31,765
0,195 195,345 345,480 480,690 690,1020
spend on your final project

1166
00:30:31,765 --> 00:30:33,500
0,255 255,515 685,1085
or your competition.|

1167
00:30:34,480 --> 00:30:35,925
0,335 335,670 930,1190 1190,1340 1340,1445
OK, so what we're going

1168
00:30:35,925 --> 00:30:37,080
0,120 120,210 210,345 345,875 895,1155
to do is firstly go

1169
00:30:37,080 --> 00:30:37,890
0,165 165,375 375,570 570,705 705,810
out and collect a lot

1170
00:30:37,890 --> 00:30:38,640
0,120 120,330 330,540 540,645 645,750
of data from all of

1171
00:30:38,640 --> 00:30:39,615
0,135 135,345 345,585 585,765 765,975
the past years that we've

1172
00:30:39,615 --> 00:30:40,950
0,150 150,390 390,725 985,1230 1230,1335
taught this course. {And,we -}

1173
00:30:40,950 --> 00:30:41,760
0,165 165,375 375,540 540,660 660,810
can plot all of this

1174
00:30:41,760 --> 00:30:43,020
0,290 370,630 630,810 810,975 975,1260
data because it's only two

1175
00:30:43,020 --> 00:30:44,040
0,270 270,540 540,735 735,855 855,1020
input {space.,We -} can plot

1176
00:30:44,040 --> 00:30:44,805
0,165 165,360 360,525 525,645 645,765
this data on a two

1177
00:30:44,805 --> 00:30:47,300
0,605 745,1145 1285,1685 1975,2235 2235,2495
dimensional {feature,space. -} We can

1178
00:30:47,560 --> 00:30:48,435
0,275 275,410 410,530 530,695 695,875
actually look at all of

1179
00:30:48,435 --> 00:30:49,755
0,165 165,455 475,840 840,1125 1125,1320
the students before you that

1180
00:30:49,755 --> 00:30:51,285
0,275 445,845 865,1125 1125,1320 1320,1530
have passed the class and

1181
00:30:51,285 --> 00:30:52,860
0,195 195,375 375,615 615,995 1285,1575
failed the class and see

1182
00:30:52,860 --> 00:30:54,255
0,210 210,435 435,740 850,1155 1155,1395
where they lived in this

1183
00:30:54,255 --> 00:30:55,350
0,335 445,705 705,825 825,975 975,1095
space for the amount of

1184
00:30:55,350 --> 00:30:56,295
0,165 165,345 345,585 585,780 780,945
hours that they've spent, the

1185
00:30:56,295 --> 00:30:57,300
0,135 135,285 285,600 600,750 750,1005
number of lectures {that,they've -}

1186
00:30:57,300 --> 00:30:59,025
0,225 225,465 465,645 645,920 1180,1725
{attended.,And -} so on. Greenpoint

1187
00:30:59,025 --> 00:30:59,910
0,165 165,270 270,510 510,735 735,885
are the people who have

1188
00:30:59,910 --> 00:31:01,065
0,300 300,570 570,735 735,960 960,1155
passed read or those who

1189
00:31:01,065 --> 00:31:02,060
0,120 120,395
have failed.|

1190
00:31:02,180 --> 00:31:04,615
0,400 1110,1415 1415,1700 1700,1960 2040,2435
Now, and here's you right,

1191
00:31:04,615 --> 00:31:05,785
0,360 360,525 525,810 810,1035 1035,1170
you're right here. {Four,or -}

1192
00:31:05,785 --> 00:31:07,200
0,225 225,465 465,675 675,1095 1095,1415
five is your coordinate {space.,You

1193
00:31:07,910 --> 00:31:09,355
0,305 305,610 690,1025 1025,1265 1265,1445
-} fall right there and

1194
00:31:09,355 --> 00:31:10,750
0,255 255,495 495,750 750,1140 1140,1395
you've attended four lectures. You've

1195
00:31:10,750 --> 00:31:11,770
0,195 195,405 405,660 660,885 885,1020
spent five hours on your

1196
00:31:11,770 --> 00:31:13,360
0,195 195,530 1060,1320 1320,1455 1455,1590
{final,project. -} We want to

1197
00:31:13,360 --> 00:31:14,290
0,135 135,270 270,450 450,705 705,930
build a neural network to

1198
00:31:14,290 --> 00:31:15,745
0,180 180,390 390,645 645,1040 1150,1455
answer the question of will

1199
00:31:15,745 --> 00:31:16,765
0,210 210,435 435,615 615,810 810,1020
you pass the class or

1200
00:31:16,765 --> 00:31:17,640
0,135 135,270 270,435 435,600 600,875
will you fail the class?

1201
00:31:18,410 --> 00:31:19,600
0,365 365,725 725,845 845,1010 1010,1190
So {let's,do -} it. {We,have

1202
00:31:19,600 --> 00:31:20,845
0,225 225,525 525,900 900,1080 1080,1245
-} two inputs. One is

1203
00:31:20,845 --> 00:31:22,330
0,300 300,570 570,750 750,1055 1225,1485
four, one is {five,,these -}

1204
00:31:22,330 --> 00:31:23,230
0,135 135,285 285,540 540,765 765,900
are two numbers. We can

1205
00:31:23,230 --> 00:31:24,445
0,180 180,390 390,705 705,975 975,1215
feed them through a neural

1206
00:31:24,445 --> 00:31:25,720
0,255 255,510 510,720 720,965 985,1275
network that we've just seen

1207
00:31:25,720 --> 00:31:26,760
0,180 180,345 345,495 495,705 705,1040
how we can build that.|

1208
00:31:27,540 --> 00:31:28,290
0,105 105,240 240,420 420,570 570,750
And we feed that into

1209
00:31:28,290 --> 00:31:29,690
0,225 225,480 480,870 870,1140 1140,1400
a single layered neural network,

1210
00:31:30,430 --> 00:31:31,590
0,305 305,515 515,770 770,980 980,1160
three hidden units in this

1211
00:31:31,590 --> 00:31:32,460
0,270 270,480 480,600 600,735 735,870
example. {But,we -} could make

1212
00:31:32,460 --> 00:31:33,480
0,195 195,510 510,735 735,840 840,1020
it larger if we wanted

1213
00:31:33,480 --> 00:31:34,695
0,165 165,300 300,555 555,1020 1020,1215
to be more expressive and

1214
00:31:34,695 --> 00:31:36,140
0,165 165,455
more powerful.|

1215
00:31:36,150 --> 00:31:37,070
0,275 275,440 440,605 605,770 770,920
And we see here that

1216
00:31:37,070 --> 00:31:38,120
0,135 135,510 510,690 690,825 825,1050
the probability of you passing

1217
00:31:38,120 --> 00:31:39,520
0,270 270,510 510,795 795,1080 1080,1400
those classes is point one.|

1218
00:31:40,240 --> 00:31:42,595
0,255 255,920 1360,1760 1900,2190 2190,2355
Pretty abysmal. {So,why -} would

1219
00:31:42,595 --> 00:31:43,710
0,150 150,270 270,390 390,665 715,1115
this be the case, right?

1220
00:31:43,790 --> 00:31:44,760
0,260 260,380 380,515 515,680 680,970
What did we do wrong?

1221
00:31:44,960 --> 00:31:45,960
0,245 245,350 350,545 545,635 635,1000
Because I don't think it's

1222
00:31:45,980 --> 00:31:47,515
0,380 380,760 900,1175 1175,1355 1355,1535
{correct.,Right -} when we looked

1223
00:31:47,515 --> 00:31:49,165
0,165 165,375 375,695 1255,1515 1515,1650
at the space, it looked

1224
00:31:49,165 --> 00:31:50,110
0,255 255,495 495,615 615,720 720,945
like actually you were a

1225
00:31:50,110 --> 00:31:51,325
0,285 285,585 585,855 855,1035 1035,1215
good candidate to pass {the,class.

1226
00:31:51,325 --> 00:31:52,270
0,240 240,465 465,615 615,795 795,945
-} But why is the

1227
00:31:52,270 --> 00:31:54,040
0,195 195,435 435,830 1330,1605 1605,1770
neural network saying that {there's,only

1228
00:31:54,040 --> 00:31:55,420
0,105 105,270 270,675 675,1230 1230,1380
-} a 10% likelihood that

1229
00:31:55,420 --> 00:31:56,770
0,135 135,285 285,560 910,1200 1200,1350
you should pass? Does anyone

1230
00:31:56,770 --> 00:31:57,940
0,105 105,285 285,620
have any ideas?|

1231
00:32:02,680 --> 00:32:05,520
0,400 1260,1625 1625,1990
Exactly, exactly so.|

1232
00:32:05,960 --> 00:32:07,350
0,290 290,515 515,710 710,1025 1025,1390
This neural network is just

1233
00:32:07,760 --> 00:32:08,730
0,275 275,395 395,500 500,665 665,970
like it was just born

1234
00:32:08,840 --> 00:32:10,105
0,320 320,500 500,665 665,935 935,1265
right. {It,has -} no information

1235
00:32:10,105 --> 00:32:12,175
0,240 240,515 895,1290 1290,1685 1735,2070
about the world or this

1236
00:32:12,175 --> 00:32:13,315
0,255 255,435 435,690 690,885 885,1140
{class.,It -} doesn't know what

1237
00:32:13,315 --> 00:32:14,880
0,210 210,360 360,635 685,1085 1165,1565
four and five mean, or

1238
00:32:15,290 --> 00:32:16,450
0,305 305,470 470,680 680,920 920,1160
what the notion of passing

1239
00:32:16,450 --> 00:32:19,360
0,210 210,540 540,830 1360,1760
or failing means. Right?|

1240
00:32:19,750 --> 00:32:20,940
0,350 350,605 605,800 800,1010 1010,1190
Exactly right. {This,neural -} network

1241
00:32:20,940 --> 00:32:22,110
0,225 225,420 420,645 645,945 945,1170
has not been {trained.,You -}

1242
00:32:22,110 --> 00:32:22,845
0,135 135,285 285,420 420,570 570,735
can think of it kind

1243
00:32:22,845 --> 00:32:24,195
0,180 180,450 450,675 675,935 1075,1350
of as {a,baby. -} It

1244
00:32:24,195 --> 00:32:25,875
0,330 330,635 895,1215 1215,1455 1455,1680
hasn't {learned,anything -} yet. So

1245
00:32:25,875 --> 00:32:28,050
0,240 240,540 540,1175 1375,1775 1885,2175
our job firstly {is,to -}

1246
00:32:28,050 --> 00:32:29,025
0,180 180,360 360,585 585,810 810,975
train it. And part of

1247
00:32:29,025 --> 00:32:30,225
0,275 355,645 645,825 825,1005 1005,1200
that understanding is we first

1248
00:32:30,225 --> 00:32:31,200
0,165 165,330 330,570 570,765 765,975
need to tell the neural

1249
00:32:31,200 --> 00:32:32,630
0,260 280,540 540,660 660,920 1030,1430
{network,when -} it makes mistakes.

1250
00:32:33,550 --> 00:32:34,995
0,260 260,790 900,1160 1160,1280 1280,1445
So mathematically we should now

1251
00:32:34,995 --> 00:32:36,075
0,240 240,540 540,810 810,975 975,1080
think about how we can

1252
00:32:36,075 --> 00:32:37,340
0,195 195,450 450,735 735,990 990,1265
answer this question, which is

1253
00:32:38,350 --> 00:32:39,480
0,290 290,455 455,665 665,890 890,1130
did my neural network make

1254
00:32:39,480 --> 00:32:40,680
0,225 225,590 640,915 915,1065 1065,1200
a mistake? And if it

1255
00:32:40,680 --> 00:32:41,955
0,120 120,315 315,650 820,1125 1125,1275
made a mistake, how can

1256
00:32:41,955 --> 00:32:42,705
0,135 135,285 285,435 435,615 615,750
I tell it how big

1257
00:32:42,705 --> 00:32:43,500
0,90 90,270 270,480 480,615 615,795
of a mistake it was

1258
00:32:43,500 --> 00:32:44,340
0,180 180,315 315,465 465,660 660,840
so that the next time

1259
00:32:44,340 --> 00:32:46,130
0,165 165,470 880,1200 1200,1455 1455,1790
it sees this data point,

1260
00:32:46,480 --> 00:32:48,060
0,260 260,395 395,560 560,850 1020,1580
can it do better minimize

1261
00:32:48,060 --> 00:32:49,240
0,345 345,740
that mistake.|

1262
00:32:49,490 --> 00:32:51,240
0,400 660,935 935,1160 1160,1385 1385,1750
So in neural network language,

1263
00:32:51,710 --> 00:32:53,490
0,400 480,860 860,1130 1130,1400 1400,1780
those mistakes are called losses

1264
00:32:54,380 --> 00:32:55,600
0,305 305,610 630,875 875,1010 1010,1220
and specifically you want to

1265
00:32:55,600 --> 00:32:56,635
0,225 225,480 480,630 630,795 795,1035
define what's called a loss

1266
00:32:56,635 --> 00:32:58,195
0,365 805,1080 1080,1230 1230,1395 1395,1560
function, which is going to

1267
00:32:58,195 --> 00:33:00,360
0,180 180,485 505,905 1405,1725 1725,2165
take as input your prediction

1268
00:33:01,040 --> 00:33:02,800
0,395 395,665 665,845 845,1240 1440,1760
and the true prediction right

1269
00:33:02,800 --> 00:33:03,850
0,210 210,420 420,660 660,855 855,1050
and how far away your

1270
00:33:03,850 --> 00:33:05,605
0,375 375,740 1120,1410 1410,1590 1590,1755
prediction is from the true

1271
00:33:05,605 --> 00:33:06,880
0,365 445,765 765,960 960,1125 1125,1275
prediction tells you how big

1272
00:33:06,880 --> 00:33:08,340
0,105 105,225 225,500 880,1170 1170,1460
of a loss there is.|

1273
00:33:08,980 --> 00:33:10,800
0,225 225,450 450,645 645,950
Right. {So,,for -} example.|

1274
00:33:11,190 --> 00:33:12,425
0,365 365,560 560,785 785,995 995,1235
Let's say we want to

1275
00:33:12,425 --> 00:33:14,585
0,305 745,1145 1345,1695 1695,1935 1935,2160
build a neural network to

1276
00:33:14,585 --> 00:33:17,080
0,240 240,845 1255,1655
do classification of.|

1277
00:33:17,120 --> 00:33:18,355
0,290 290,575 575,815 815,965 965,1235
Or, sorry, actually, even before

1278
00:33:18,355 --> 00:33:19,450
0,285 285,465 465,585 585,795 795,1095
that, I want to maybe

1279
00:33:19,450 --> 00:33:21,090
0,240 240,405 405,630 630,1335 1335,1640
give you some terminology. {So,there

1280
00:33:21,200 --> 00:33:22,660
0,245 245,490 510,905 905,1220 1220,1460
-} are multiple different ways

1281
00:33:22,660 --> 00:33:23,760
0,210 210,405 405,585 585,780 780,1100
of saying the same thing

1282
00:33:24,230 --> 00:33:25,840
0,400 630,980 980,1220 1220,1460 1460,1610
in neural networks and deep

1283
00:33:25,840 --> 00:33:27,415
0,290 310,710 940,1185 1185,1320 1320,1575
{learning.,So -} what I just

1284
00:33:27,415 --> 00:33:28,530
0,255 255,420 420,555 555,765 765,1115
described as a loss function

1285
00:33:28,640 --> 00:33:30,160
0,395 395,695 695,1000 1110,1400 1400,1520
is also commonly referred to

1286
00:33:30,160 --> 00:33:32,140
0,90 90,255 255,525 525,890 1270,1980
as an objective function, empirical

1287
00:33:32,140 --> 00:33:33,580
0,285 285,585 585,825 825,1160 1180,1440
risk, a {cost,function. -} These

1288
00:33:33,580 --> 00:33:35,065
0,135 135,410 700,1050 1050,1275 1275,1485
are all exactly the same

1289
00:33:35,065 --> 00:33:36,265
0,335 415,720 720,870 870,1020 1020,1200
thing. They're all a way

1290
00:33:36,265 --> 00:33:37,315
0,165 165,300 300,570 570,870 870,1050
for us to train the

1291
00:33:37,315 --> 00:33:38,410
0,195 195,435 435,705 705,915 915,1095
neural network, to teach the

1292
00:33:38,410 --> 00:33:39,780
0,210 210,470 730,990 990,1110 1110,1370
neural network when {it,makes -}

1293
00:33:39,890 --> 00:33:42,355
0,400 1050,1450 1680,1955 1955,2150 2150,2465
mistakes. And what we really

1294
00:33:42,355 --> 00:33:43,680
0,330 330,555 555,705 705,945 945,1325
ultimately want to do is

1295
00:33:43,760 --> 00:33:44,845
0,335 335,560 560,740 740,875 875,1085
over the course of an

1296
00:33:44,845 --> 00:33:46,645
0,330 330,660 660,1025 1315,1605 1605,1800
entire data set, not just

1297
00:33:46,645 --> 00:33:48,115
0,225 225,495 495,795 795,1125 1125,1470
one data point of mistakes

1298
00:33:48,115 --> 00:33:48,850
0,225 225,345 345,435 435,540 540,735
we want to say over

1299
00:33:48,850 --> 00:33:50,710
0,255 255,525 525,810 810,1160 1600,1860
the entire data set, we

1300
00:33:50,710 --> 00:33:51,985
0,150 150,315 315,830 850,1125 1125,1275
want to minimize all of

1301
00:33:51,985 --> 00:33:53,755
0,210 210,525 525,825 825,1145 1465,1770
the mistakes on average that

1302
00:33:53,755 --> 00:33:55,800
0,210 210,465 465,725 985,1385
this neural network makes.|

1303
00:33:56,730 --> 00:33:57,455
0,260 260,380 380,500 500,620 620,725
So if we look at

1304
00:33:57,455 --> 00:33:58,460
0,105 105,360 360,630 630,795 795,1005
the problem, like I said,

1305
00:33:58,460 --> 00:34:00,365
0,225 225,630 630,1190 1480,1740 1740,1905
of binary classification, will I

1306
00:34:00,365 --> 00:34:01,445
0,225 225,435 435,705 705,945 945,1080
pass this class or will

1307
00:34:01,445 --> 00:34:02,375
0,150 150,345 345,585 585,690 690,930
I not? There's a yes

1308
00:34:02,375 --> 00:34:03,680
0,285 285,570 570,885 885,1110 1110,1305
or no answer. {That,means -}

1309
00:34:03,680 --> 00:34:05,740
0,405 405,920
binary classification.|

1310
00:34:05,740 --> 00:34:07,210
0,320 610,885 885,1020 1020,1185 1185,1470
Now we can use what's

1311
00:34:07,210 --> 00:34:08,800
0,260 340,615 615,810 810,1130 1240,1590
called a loss function of

1312
00:34:08,800 --> 00:34:10,795
0,240 240,765 765,1035 1035,1605 1605,1995
the softmax cross entropy loss.

1313
00:34:10,795 --> 00:34:11,740
0,330 330,525 525,660 660,765 765,945
{And,for -} those of you

1314
00:34:11,740 --> 00:34:13,135
0,195 195,420 420,660 660,1035 1035,1395
who aren't familiar, this notion

1315
00:34:13,135 --> 00:34:14,860
0,365 415,705 705,1140 1140,1365 1365,1725
of cross entropy is actually

1316
00:34:14,860 --> 00:34:16,555
0,380 460,735 735,930 930,1250 1360,1695
developed here at MIT by

1317
00:34:16,555 --> 00:34:21,850
0,365 2215,2505 2505,2795 3085,3485 4825,5295
{Shan.,Excuse -} me, yes, claude

1318
00:34:21,850 --> 00:34:25,195
0,530 1030,1290 1290,1550 2380,3050 3100,3345
shannon, who {is,visionary. -} He

1319
00:34:25,195 --> 00:34:27,190
0,90 90,225 225,785 835,1235 1645,1995
did his masters here over

1320
00:34:27,190 --> 00:34:28,465
0,285 285,570 570,825 825,1065 1065,1275
{fifty,years -} ago. He introduced

1321
00:34:28,465 --> 00:34:29,935
0,180 180,465 465,795 795,1035 1035,1470
this notion of cross entropy,

1322
00:34:29,935 --> 00:34:31,700
0,120 120,300 300,605
and that was.|

1323
00:34:31,700 --> 00:34:33,620
0,165 165,410 430,1035 1035,1400 1630,1920
You know, pivotal and the

1324
00:34:33,620 --> 00:34:35,090
0,285 285,510 510,675 675,1010 1180,1470
ability for us to train

1325
00:34:35,090 --> 00:34:36,130
0,180 180,375 375,555 555,780 780,1040
these types of neural networks

1326
00:34:36,420 --> 00:34:37,720
0,320 320,590 590,860 860,1040 1040,1300
even now into the future.|

1327
00:34:38,560 --> 00:34:40,945
0,380 670,1050 1050,1185 1185,1460 2110,2385
So let's start by instead

1328
00:34:40,945 --> 00:34:43,270
0,275 415,1115 1255,1590 1590,2100 2100,2325
of predicting a binary cross

1329
00:34:43,270 --> 00:34:44,530
0,540 540,855 855,1035 1035,1140 1140,1260
entropy output. {What,if -} we

1330
00:34:44,530 --> 00:34:46,615
0,195 195,420 420,710 1210,1610 1690,2085
wanted to predict a final

1331
00:34:46,615 --> 00:34:48,685
0,395 565,870 870,1095 1095,1415 1705,2070
grade of your class score,

1332
00:34:48,685 --> 00:34:49,750
0,255 255,465 465,735 735,870 870,1065
for example, that's no longer

1333
00:34:49,750 --> 00:34:51,025
0,195 195,675 675,945 945,1110 1110,1275
a binary output, yes or

1334
00:34:51,025 --> 00:34:52,645
0,275 415,855 855,1050 1050,1275 1275,1620
no? It's actually a continuous

1335
00:34:52,645 --> 00:34:54,085
0,365 505,780 780,1005 1005,1155 1155,1440
variable, right? It's the grade,

1336
00:34:54,085 --> 00:34:55,410
0,315 315,390 390,480 480,570 570,1325
let's say out {of,100} {points.,What

1337
00:34:55,790 --> 00:34:57,115
0,260 260,455 455,785 785,1130 1130,1325
-} is the value of

1338
00:34:57,115 --> 00:34:58,410
0,180 180,405 405,570 570,845 895,1295
your score in the class

1339
00:34:58,520 --> 00:35:00,020
0,400
project?|

1340
00:35:00,020 --> 00:35:00,920
0,240 240,390 390,555 555,705 705,900
For this type of loss,

1341
00:35:00,920 --> 00:35:01,745
0,180 180,300 300,465 465,705 705,825
we can use what's called

1342
00:35:01,745 --> 00:35:02,960
0,150 150,375 375,660 660,915 915,1215
a mean squared error loss.

1343
00:35:02,960 --> 00:35:03,710
0,195 195,315 315,435 435,555 555,750
{You,can -} think of this

1344
00:35:03,710 --> 00:35:05,495
0,270 270,510 510,690 690,1395 1395,1785
literally as just subtracting your

1345
00:35:05,495 --> 00:35:07,175
0,555 555,845 925,1245 1245,1470 1470,1680
predicted grade from the true

1346
00:35:07,175 --> 00:35:09,460
0,305 655,1055 1135,1695 1695,1980 1980,2285
grade and minimizing that distance

1347
00:35:09,840 --> 00:35:10,700
0,400
apart.|

1348
00:35:12,450 --> 00:35:13,775
0,365 365,605 605,770 770,1025 1025,1325
So I think now we're

1349
00:35:13,775 --> 00:35:14,915
0,245 355,600 600,795 795,1005 1005,1140
ready to really put all

1350
00:35:14,915 --> 00:35:16,900
0,135 135,390 390,785 835,1235 1585,1985
of this information together and

1351
00:35:17,220 --> 00:35:19,010
0,485 485,725 725,1060 1110,1475 1475,1790
tackle this problem of training

1352
00:35:19,010 --> 00:35:21,620
0,210 210,450 450,710 1750,2150 2290,2610
a neural network to not

1353
00:35:21,620 --> 00:35:23,560
0,320 850,1250
just identify.|

1354
00:35:23,630 --> 00:35:25,585
0,400 720,1340 1340,1475 1475,1685 1685,1955
How erroneous it is, how

1355
00:35:25,585 --> 00:35:27,235
0,240 240,510 510,795 795,1145 1375,1650
large its loss is, but

1356
00:35:27,235 --> 00:35:29,155
0,195 195,515 625,1235 1315,1635 1635,1920
more importantly, minimize that loss

1357
00:35:29,155 --> 00:35:30,220
0,225 225,360 360,585 585,840 840,1065
as a function of seeing

1358
00:35:30,220 --> 00:35:31,375
0,195 195,360 360,585 585,855 855,1155
all of this training data

1359
00:35:31,375 --> 00:35:33,080
0,210 210,405 405,905
that it observes.|

1360
00:35:33,690 --> 00:35:34,670
0,350 350,575 575,725 725,860 860,980
So we know that we

1361
00:35:34,670 --> 00:35:35,720
0,135 135,300 300,540 540,795 795,1050
want to find this neural

1362
00:35:35,720 --> 00:35:37,115
0,260 430,720 720,870 870,1080 1080,1395
network like we mentioned before

1363
00:35:37,115 --> 00:35:40,150
0,315 315,875 1345,1650 1650,2495 2635,3035
that minimizes this empirical risk

1364
00:35:40,200 --> 00:35:42,350
0,275 275,410 410,995 995,1270 1500,2150
or this empirical loss averaged

1365
00:35:42,350 --> 00:35:44,020
0,300 300,600 600,975 975,1320 1320,1670
across our entire data set.

1366
00:35:44,400 --> 00:35:45,335
0,290 290,470 470,650 650,800 800,935
{Now,this -} means that we

1367
00:35:45,335 --> 00:35:47,680
0,195 195,510 510,905 1135,1805 1945,2345
want to find mathematically these

1368
00:35:48,090 --> 00:35:50,975
0,790 810,1210 1320,1670 1670,2230 2580,2885
w's right, that minimize j

1369
00:35:50,975 --> 00:35:52,175
0,180 180,455 475,735 735,855 855,1200
of W J of w,

1370
00:35:52,175 --> 00:35:53,735
0,135 135,345 345,665 835,1365 1365,1560
our loss function averaged over

1371
00:35:53,735 --> 00:35:55,040
0,225 225,480 480,750 750,1035 1035,1305
our entire data {set.,And -}

1372
00:35:55,040 --> 00:35:56,360
0,320 460,735 735,900 900,1125 1125,1320
w is {our,weight. -} So

1373
00:35:56,360 --> 00:35:57,125
0,105 105,225 225,375 375,555 555,765
we want to find the

1374
00:35:57,125 --> 00:35:59,150
0,180 180,360 360,845 1315,1715 1735,2025
set of weights that on

1375
00:35:59,150 --> 00:36:00,455
0,290 550,840 840,1020 1020,1170 1170,1305
average is going to give

1376
00:36:00,455 --> 00:36:02,340
0,225 225,495 495,815
us the minimum.|

1377
00:36:02,650 --> 00:36:05,220
0,560 560,940 1080,1400 1400,1720
Smallest loss as possible.|

1378
00:36:05,620 --> 00:36:07,035
0,365 365,665 665,890 890,1160 1160,1415
Now, remember that w here

1379
00:36:07,035 --> 00:36:09,075
0,165 165,455 715,1115 1225,1625 1705,2040
is just a list. {Basically,,it's

1380
00:36:09,075 --> 00:36:10,020
0,255 255,360 360,540 540,750 750,945
-} just a group of

1381
00:36:10,020 --> 00:36:10,815
0,195 195,375 375,495 495,705 705,795
all of the weights in

1382
00:36:10,815 --> 00:36:11,760
0,120 120,345 345,570 570,795 795,945
our neural {network.,You -} may

1383
00:36:11,760 --> 00:36:14,115
0,290 460,860 1180,1580 1810,2235 2235,2355
have hundreds of weights and

1384
00:36:14,115 --> 00:36:15,180
0,105 105,285 285,570 570,825 825,1065
a very, very small neural

1385
00:36:15,180 --> 00:36:16,665
0,260 280,675 675,960 960,1305 1305,1485
network, or in today's neural

1386
00:36:16,665 --> 00:36:17,670
0,180 180,360 360,465 465,630 630,1005
networks, you may have billions

1387
00:36:17,670 --> 00:36:19,440
0,195 195,680 940,1245 1245,1605 1605,1770
or trillions {of,weights. -} And

1388
00:36:19,440 --> 00:36:20,505
0,105 105,255 255,435 435,710 790,1065
you want to find what

1389
00:36:20,505 --> 00:36:21,405
0,150 150,300 300,510 510,705 705,900
is the value of every

1390
00:36:21,405 --> 00:36:22,610
0,255 255,435 435,555 555,720 720,1205
single one of these weights

1391
00:36:22,780 --> 00:36:23,880
0,335 335,455 455,650 650,905 905,1100
that's going to result in

1392
00:36:23,880 --> 00:36:25,340
0,195 195,555 555,860 880,1170 1170,1460
the smallest loss as possible.|

1393
00:36:26,540 --> 00:36:27,610
0,400 450,710 710,815 815,935 935,1070
Now, how can you do

1394
00:36:27,610 --> 00:36:29,095
0,260 370,750 750,1035 1035,1245 1245,1485
this? Remember that our loss

1395
00:36:29,095 --> 00:36:30,990
0,335 415,705 705,870 870,1145 1495,1895
function j of w is

1396
00:36:31,130 --> 00:36:32,725
0,380 380,620 620,880 1080,1385 1385,1595
just a function of our

1397
00:36:32,725 --> 00:36:34,950
0,455 565,855 855,1005 1005,1265 1525,2225
weights. {So,for -} any instantiation

1398
00:36:34,970 --> 00:36:36,205
0,245 245,410 410,860 860,1085 1085,1235
of our weights we can

1399
00:36:36,205 --> 00:36:38,460
0,330 330,525 525,990 990,1265 1855,2255
compute a scalar value of,

1400
00:36:38,720 --> 00:36:40,930
0,245 245,470 470,850 1260,1535 1535,2210
you know how how erroneous

1401
00:36:40,930 --> 00:36:42,390
0,255 255,435 435,735 735,1010 1060,1460
would our neural network be

1402
00:36:42,650 --> 00:36:44,215
0,320 320,640 690,1265 1265,1445 1445,1565
for this instantiation of our

1403
00:36:44,215 --> 00:36:46,495
0,425 865,1265 1675,2055 2055,2160 2160,2280
{weights.,So -} let's try and

1404
00:36:46,495 --> 00:36:47,590
0,450 450,645 645,840 840,990 990,1095
visualize, for example, in a

1405
00:36:47,590 --> 00:36:49,690
0,210 210,560 760,1160 1660,1920 1920,2100
very simple example of a

1406
00:36:49,690 --> 00:36:50,875
0,180 180,720 720,945 945,1095 1095,1185
two dimensional space where we

1407
00:36:50,875 --> 00:36:52,590
0,135 135,390 390,630 630,1055 1315,1715
have only two weights, extremely

1408
00:36:53,240 --> 00:36:55,060
0,400 570,935 935,1175 1175,1490 1490,1820
simple neural network here, very

1409
00:36:55,060 --> 00:36:56,520
0,380 430,720 720,915 915,1185 1185,1460
small, two weight neural network,

1410
00:36:56,840 --> 00:36:57,810
0,260 260,380 380,515 515,680 680,970
and we want to find

1411
00:36:58,280 --> 00:37:00,025
0,320 320,635 635,995 995,1340 1340,1745
what are the optimal weights

1412
00:37:00,025 --> 00:37:01,330
0,210 210,360 360,615 615,945 945,1305
that would train this neural

1413
00:37:01,330 --> 00:37:02,080
0,260
network.|

1414
00:37:02,080 --> 00:37:03,820
0,210 210,405 405,740 940,1340 1450,1740
We can plot basically the

1415
00:37:03,820 --> 00:37:06,430
0,290 1210,1515 1515,2175 2175,2400 2400,2610
loss, how erroneous the neural

1416
00:37:06,430 --> 00:37:08,010
0,210 210,525 525,810 810,1130 1180,1580
network is for every single

1417
00:37:08,540 --> 00:37:10,795
0,730 810,1160 1160,1490 1490,1775 1775,2255
instantiation of these two weights,

1418
00:37:10,795 --> 00:37:11,905
0,395 415,675 675,780 780,900 900,1110
right? This is a huge

1419
00:37:11,905 --> 00:37:13,230
0,240 240,465 465,555 555,1005 1005,1325
space. It's an infinite space,

1420
00:37:13,490 --> 00:37:14,875
0,335 335,605 605,815 815,1070 1070,1385
but still we can try

1421
00:37:14,875 --> 00:37:15,985
0,335 475,735 735,855 855,975 975,1110
to, we can have a

1422
00:37:15,985 --> 00:37:17,725
0,270 270,585 585,1205 1255,1530 1530,1740
function that evaluates at every

1423
00:37:17,725 --> 00:37:19,500
0,240 240,405 405,585 585,905
point in this space.|

1424
00:37:19,570 --> 00:37:20,940
0,350 350,560 560,815 815,1145 1145,1370
Now, what we ultimately want

1425
00:37:20,940 --> 00:37:22,350
0,150 150,300 300,590 910,1230 1230,1410
to do is, again, we

1426
00:37:22,350 --> 00:37:25,050
0,150 150,315 315,590 1360,1760 2380,2700
want to find which set

1427
00:37:25,050 --> 00:37:26,655
0,240 240,945 945,1245 1245,1410 1410,1605
of ws will give us

1428
00:37:26,655 --> 00:37:29,565
0,285 285,845 1585,1985 2005,2405 2635,2910
the smallest loss possible. {That,means

1429
00:37:29,565 --> 00:37:31,200
0,275 295,675 675,975 975,1295 1315,1635
-} basically the lowest point

1430
00:37:31,200 --> 00:37:32,685
0,210 210,500 880,1185 1185,1350 1350,1485
on this landscape that you

1431
00:37:32,685 --> 00:37:33,900
0,165 165,345 345,635
can see here.|

1432
00:37:33,910 --> 00:37:35,420
0,350 350,620 620,800 800,1235 1235,1510
Where is the ws that

1433
00:37:35,530 --> 00:37:36,650
0,290 290,485 485,650 650,815 815,1120
bring us to that lowest

1434
00:37:36,670 --> 00:37:37,740
0,400
point?|

1435
00:37:39,000 --> 00:37:39,740
0,245 245,350 350,455 455,575 575,740
The way that we do

1436
00:37:39,740 --> 00:37:41,560
0,285 285,680 910,1260 1260,1515 1515,1820
this is actually just by

1437
00:37:41,730 --> 00:37:43,070
0,560 560,815 815,995 995,1100 1100,1340
firstly starting at a random

1438
00:37:43,070 --> 00:37:44,150
0,315 315,495 495,645 645,885 885,1080
place. {We,have -} no idea

1439
00:37:44,150 --> 00:37:45,080
0,135 135,285 285,480 480,735 735,930
where to {start.,So -} pick

1440
00:37:45,080 --> 00:37:46,235
0,135 135,405 405,765 765,1005 1005,1155
a random place to start

1441
00:37:46,235 --> 00:37:47,795
0,150 150,360 360,695 1075,1335 1335,1560
in this space and let's

1442
00:37:47,795 --> 00:37:49,570
0,180 180,485 685,1020 1020,1355 1375,1775
{start,there. -} At this location,

1443
00:37:49,710 --> 00:37:51,230
0,470 470,790 810,1100 1100,1310 1310,1520
let's evaluate {our,neural -} network.

1444
00:37:51,230 --> 00:37:52,900
0,225 225,360 360,800 1120,1395 1395,1670
We can compute the loss

1445
00:37:52,920 --> 00:37:55,160
0,320 320,560 560,880 1020,1420 1980,2240
at this specific location and

1446
00:37:55,160 --> 00:37:55,910
0,150 150,315 315,450 450,600 600,750
on top of that we

1447
00:37:55,910 --> 00:37:58,160
0,210 210,450 450,920 1750,2070 2070,2250
can actually compute how {the,loss

1448
00:37:58,160 --> 00:37:59,510
0,240 240,555 555,890 940,1215 1215,1350
-} is changing. We can

1449
00:37:59,510 --> 00:38:00,785
0,285 285,450 450,915 915,1155 1155,1275
compute the gradient of the

1450
00:38:00,785 --> 00:38:02,320
0,225 225,495 495,785 835,1185 1185,1535
loss because our loss {function,is

1451
00:38:02,790 --> 00:38:05,045
0,380 380,755 755,1150 1410,1810 2010,2255
-} a continuous function. So

1452
00:38:05,045 --> 00:38:06,370
0,105 105,300 300,495 495,780 780,1325
we can actually compute derivatives

1453
00:38:06,540 --> 00:38:08,150
0,260 260,410 410,700 1050,1385 1385,1610
of our function across the

1454
00:38:08,150 --> 00:38:09,220
0,210 210,405 405,680
space of our.|

1455
00:38:09,220 --> 00:38:11,035
0,530 760,1020 1020,1170 1170,1575 1575,1815
Weights and the gradient tells

1456
00:38:11,035 --> 00:38:13,480
0,330 330,660 660,995 1255,1655 2155,2445
us the direction of the

1457
00:38:13,480 --> 00:38:15,040
0,290 340,740 910,1185 1185,1350 1350,1560
highest point. {So,from -} where

1458
00:38:15,040 --> 00:38:16,495
0,240 240,560 640,915 915,1260 1260,1455
we stand, the gradient tells

1459
00:38:16,495 --> 00:38:17,580
0,225 225,435 435,615 615,795 795,1085
us where we should go

1460
00:38:18,050 --> 00:38:20,160
0,380 380,740 740,1025 1025,1330
to increase our loss.|

1461
00:38:20,380 --> 00:38:21,300
0,290 290,455 455,620 620,770 770,920
Now, of course, we don't

1462
00:38:21,300 --> 00:38:22,125
0,105 105,285 285,480 480,645 645,825
want to increase our loss,

1463
00:38:22,125 --> 00:38:23,055
0,165 165,315 315,555 555,780 780,930
we want to decrease our

1464
00:38:23,055 --> 00:38:24,930
0,180 180,485 565,965 1045,1565 1585,1875
loss so we negate our

1465
00:38:24,930 --> 00:38:25,875
0,390 390,525 525,645 645,780 780,945
gradient and we take a

1466
00:38:25,875 --> 00:38:27,440
0,305 355,615 615,875 925,1245 1245,1565
step in the opposite direction

1467
00:38:27,550 --> 00:38:28,650
0,290 290,455 455,755 755,920 920,1100
of the gradient. {That,brings -}

1468
00:38:28,650 --> 00:38:30,300
0,210 210,465 465,720 720,1040 1390,1650
us one step closer to

1469
00:38:30,300 --> 00:38:31,790
0,120 120,380 490,795 795,1095 1095,1490
the bottom of the {landscape.,And

1470
00:38:32,230 --> 00:38:33,510
0,380 380,635 635,785 785,995 995,1280
-} we just keep repeating

1471
00:38:33,510 --> 00:38:35,520
0,135 135,410 1300,1620 1620,1785 1785,2010
this process over and {over,again.

1472
00:38:35,520 --> 00:38:36,990
0,375 375,705 705,1040 1060,1305 1305,1470
-} We evaluate the neural

1473
00:38:36,990 --> 00:38:38,240
0,180 180,390 390,585 585,870 870,1250
network at this new location,

1474
00:38:38,350 --> 00:38:40,110
0,440 440,650 650,1180 1230,1550 1550,1760
compute its gradient, and step

1475
00:38:40,110 --> 00:38:41,280
0,150 150,315 315,540 540,860 880,1170
in {that,new -} direction. We

1476
00:38:41,280 --> 00:38:43,890
0,290 400,1160 1270,1670 1780,2180 2230,2610
keep traversing this landscape until

1477
00:38:43,890 --> 00:38:45,140
0,345 345,750 750,900 900,1005 1005,1250
we converge to the minimum.|

1478
00:38:47,260 --> 00:38:48,770
0,260 260,440 440,695 695,1175 1175,1510
We can really summarize this

1479
00:38:48,790 --> 00:38:50,535
0,395 395,545 545,820 870,1235 1235,1745
algorithm which is known formally

1480
00:38:50,535 --> 00:38:52,530
0,270 270,810 810,1295 1375,1710 1710,1995
as gradient descent, right. {So,gradient

1481
00:38:52,530 --> 00:38:54,465
0,390 390,690 690,1010 1480,1770 1770,1935
-} descent simply can be

1482
00:38:54,465 --> 00:38:55,980
0,240 240,510 510,815 835,1110 1110,1515
written like {this.,We -} initialize

1483
00:38:55,980 --> 00:38:57,810
0,150 150,255 255,390 390,830 1540,1830
all of {our,weights. -} This

1484
00:38:57,810 --> 00:38:59,340
0,165 165,435 435,830 850,1350 1350,1530
can be two weights like

1485
00:38:59,340 --> 00:38:59,925
0,120 120,210 210,270 270,360 360,585
you saw in {the,previous -}

1486
00:38:59,925 --> 00:39:01,305
0,315 315,510 510,615 615,875 895,1380
example. It can be billions

1487
00:39:01,305 --> 00:39:03,075
0,180 180,605 655,930 930,1205 1465,1770
of weights like {in,real -}

1488
00:39:03,075 --> 00:39:05,270
0,270 270,545 1225,1500 1500,1875 1875,2195
neural networks. We compute this

1489
00:39:05,440 --> 00:39:08,120
0,610 630,1030 1470,1745 1745,2105 2105,2680
gradient of the partial derivative

1490
00:39:08,860 --> 00:39:10,245
0,290 290,515 515,815 815,1130 1130,1385
of our loss with respect

1491
00:39:10,245 --> 00:39:11,070
0,135 135,225 225,555 555,720 720,825
to the weights and then

1492
00:39:11,070 --> 00:39:11,955
0,105 105,315 315,540 540,675 675,885
we can update our weights

1493
00:39:11,955 --> 00:39:13,875
0,105 105,365 445,810 810,1175 1555,1920
in the opposite direction of

1494
00:39:13,875 --> 00:39:15,160
0,345 345,905
this gradient.|

1495
00:39:15,810 --> 00:39:17,450
0,400 690,1055 1055,1280 1280,1445 1445,1640
So essentially we just take

1496
00:39:17,450 --> 00:39:19,460
0,240 240,540 540,890 1300,1665 1665,2010
this small amount, small step,

1497
00:39:19,460 --> 00:39:20,380
0,240 240,375 375,540 540,675 675,920
you can think of it,

1498
00:39:20,490 --> 00:39:22,000
0,350 350,620 620,830 830,1175 1175,1510
which here is denoted as

1499
00:39:22,260 --> 00:39:23,340
0,550
ada.|

1500
00:39:23,350 --> 00:39:25,050
0,350 350,700 810,1210 1230,1490 1490,1700
And we refer to this

1501
00:39:25,050 --> 00:39:27,230
0,345 345,740 1240,1515 1515,1785 1785,2180
small step. {This,is -} commonly

1502
00:39:27,430 --> 00:39:28,800
0,335 335,560 560,845 845,1220 1220,1370
referred to as what's known

1503
00:39:28,800 --> 00:39:30,225
0,210 210,420 420,705 705,1100 1120,1425
as the learning rate. It's

1504
00:39:30,225 --> 00:39:31,050
0,120 120,315 315,510 510,675 675,825
like how much we want

1505
00:39:31,050 --> 00:39:32,805
0,180 180,470 790,1110 1110,1530 1530,1755
to trust that gradient and

1506
00:39:32,805 --> 00:39:33,855
0,225 225,390 390,555 555,810 810,1050
step in the direction of

1507
00:39:33,855 --> 00:39:35,130
0,195 195,630 630,900 900,1065 1065,1275
that gradient. We'll talk more

1508
00:39:35,130 --> 00:39:36,540
0,180 180,360 360,650
about this later.|

1509
00:39:36,540 --> 00:39:37,760
0,350 370,660 660,795 795,930 930,1220
But just to give you

1510
00:39:37,810 --> 00:39:39,620
0,400 480,785 785,1025 1025,1360 1410,1810
some sense of code, this

1511
00:39:40,000 --> 00:39:41,780
0,350 350,515 515,770 770,1025 1025,1780
algorithm is very well translatable

1512
00:39:41,800 --> 00:39:43,020
0,260 260,455 455,740 740,995 995,1220
to real code as well.

1513
00:39:43,020 --> 00:39:44,025
0,180 180,405 405,690 690,885 885,1005
{For,every -} line on the

1514
00:39:44,025 --> 00:39:44,790
0,270 270,420 420,540 540,660 660,765
pseudo code you can see

1515
00:39:44,790 --> 00:39:45,765
0,90 90,195 195,440 610,855 855,975
on the left, you can

1516
00:39:45,765 --> 00:39:47,520
0,195 195,1055 1075,1410 1410,1620 1620,1755
see corresponding real code on

1517
00:39:47,520 --> 00:39:48,570
0,120 120,315 315,510 510,660 660,1050
the right that is runable

1518
00:39:48,570 --> 00:39:50,085
0,195 195,530 550,1200 1200,1395 1395,1515
and directly implementable by all

1519
00:39:50,085 --> 00:39:51,020
0,120 120,240 240,345 345,480 480,935
of you in your labs.|

1520
00:39:51,860 --> 00:39:52,505
0,120 120,240 240,435 435,540 540,645
But now let's take a

1521
00:39:52,505 --> 00:39:54,155
0,245 265,665 835,1140 1140,1380 1380,1650
look specifically at this term

1522
00:39:54,155 --> 00:39:55,445
0,300 300,540 540,720 720,900 900,1290
here. {This,is -} the {gradient.,We

1523
00:39:55,445 --> 00:39:56,915
0,285 285,615 615,900 900,1200 1200,1470
-} touch very briefly on

1524
00:39:56,915 --> 00:39:58,150
0,240 240,435 435,555 555,815 835,1235
this and the {visual,example. -}

1525
00:39:58,560 --> 00:40:00,035
0,395 395,785 785,1010 1010,1175 1175,1475
This explains, like I said,

1526
00:40:00,035 --> 00:40:01,780
0,395 505,810 810,1095 1095,1410 1410,1745
how the loss is changing

1527
00:40:01,830 --> 00:40:02,945
0,275 275,440 440,695 695,965 965,1115
as a function {of,the -}

1528
00:40:02,945 --> 00:40:04,570
0,425 535,855 855,1080 1080,1245 1245,1625
weights. So as the weights

1529
00:40:04,650 --> 00:40:06,280
0,365 365,730 930,1205 1205,1355 1355,1630
move around, will my loss

1530
00:40:06,300 --> 00:40:07,445
0,275 275,530 530,830 830,995 995,1145
increase or decrease? And that

1531
00:40:07,445 --> 00:40:08,350
0,180 180,345 345,480 480,645 645,905
will tell the neural network

1532
00:40:08,610 --> 00:40:09,850
0,260 260,380 380,560 560,860 860,1240
if it needs to move

1533
00:40:09,870 --> 00:40:10,790
0,260 260,500 500,605 605,710 710,920
the weights in a certain

1534
00:40:10,790 --> 00:40:12,240
0,315 315,540 540,800
direction or not.|

1535
00:40:12,490 --> 00:40:13,470
0,245 245,350 350,560 560,785 785,980
But I never actually told

1536
00:40:13,470 --> 00:40:14,895
0,320 430,720 720,855 855,1140 1140,1425
you how to compute this,

1537
00:40:14,895 --> 00:40:16,125
0,395 535,780 780,900 900,1035 1035,1230
right? And I think that's

1538
00:40:16,125 --> 00:40:17,430
0,210 210,570 570,870 870,1125 1125,1305
an extremely important part because

1539
00:40:17,430 --> 00:40:18,210
0,120 120,225 225,405 405,525 525,780
if you don't know that,

1540
00:40:18,210 --> 00:40:20,295
0,255 255,480 480,980 1660,1935 1935,2085
then you can't, well, you

1541
00:40:20,295 --> 00:40:21,290
0,225 225,390 390,555 555,735 735,995
can't train your neural network.

1542
00:40:21,490 --> 00:40:22,730
0,260 260,380 380,560 560,860 860,1240
{This,is -} a critical part

1543
00:40:23,050 --> 00:40:24,795
0,400 570,905 905,1220 1220,1475 1475,1745
of training neural networks and

1544
00:40:24,795 --> 00:40:26,430
0,195 195,515 595,975 975,1395 1395,1635
that process of computing this

1545
00:40:26,430 --> 00:40:28,485
0,350 460,780 780,1200 1200,1490 1750,2055
line, this gradient line is

1546
00:40:28,485 --> 00:40:30,405
0,210 210,515 655,990 990,1535 1675,1920
known as back {propagation.,So -}

1547
00:40:30,405 --> 00:40:31,460
0,225 225,345 345,480 480,705 705,1055
let's do a very quick.|

1548
00:40:32,440 --> 00:40:34,275
0,640 690,950 950,1130 1130,1610 1610,1835
Intro to back propagation and

1549
00:40:34,275 --> 00:40:35,780
0,150 150,315 315,605
how it works.|

1550
00:40:36,190 --> 00:40:37,335
0,275 275,470 470,770 770,935 935,1145
So again, let's start with

1551
00:40:37,335 --> 00:40:39,030
0,270 270,750 750,1005 1005,1265 1345,1695
the simplest neural network in

1552
00:40:39,030 --> 00:40:40,610
0,350 460,765 765,1020 1020,1230 1230,1580
existence. {This,neural -} network has

1553
00:40:40,810 --> 00:40:42,360
0,365 365,710 710,1085 1085,1385 1385,1550
one input, one output, and

1554
00:40:42,360 --> 00:40:43,905
0,180 180,405 405,830 1150,1410 1410,1545
only one {neuron.,This -} is

1555
00:40:43,905 --> 00:40:44,930
0,165 165,360 360,555 555,735 735,1025
as simple as {it,gets. -}

1556
00:40:45,580 --> 00:40:46,665
0,260 260,410 410,560 560,905 905,1085
We want to compute the

1557
00:40:46,665 --> 00:40:48,530
0,450 450,675 675,855 855,1175 1465,1865
gradient of our loss with

1558
00:40:48,610 --> 00:40:49,980
0,335 335,560 560,770 770,1090 1110,1370
respect {to,our -} weight. In

1559
00:40:49,980 --> 00:40:50,910
0,150 150,330 330,585 585,825 825,930
this case, let's compute it

1560
00:40:50,910 --> 00:40:52,340
0,210 210,480 480,720 720,1035 1035,1430
with respect to w two,

1561
00:40:52,360 --> 00:40:53,940
0,290 290,545 545,910
the second weight.|

1562
00:40:54,070 --> 00:40:56,025
0,400 660,980 980,1550 1550,1790 1790,1955
So this derivative is going

1563
00:40:56,025 --> 00:40:57,105
0,150 150,285 285,495 495,780 780,1080
to tell us how much

1564
00:40:57,105 --> 00:40:59,520
0,330 330,675 675,1055 1615,2015 2095,2415
a small change in this

1565
00:40:59,520 --> 00:41:01,050
0,315 315,675 675,1005 1005,1260 1260,1530
weight will affect our loss,

1566
00:41:01,050 --> 00:41:02,280
0,375 375,630 630,795 795,1020 1020,1230
if if a small change,

1567
00:41:02,280 --> 00:41:03,075
0,150 150,300 300,480 480,645 645,795
if we change our weight

1568
00:41:03,075 --> 00:41:03,915
0,165 165,360 360,525 525,645 645,840
a little bit in one

1569
00:41:03,915 --> 00:41:05,430
0,335 505,885 885,1155 1155,1320 1320,1515
direction, will increase our loss

1570
00:41:05,430 --> 00:41:07,360
0,285 285,540 540,705 705,980
or decrease our loss.|

1571
00:41:07,530 --> 00:41:08,750
0,350 350,545 545,830 830,1040 1040,1220
So to compute that, we

1572
00:41:08,750 --> 00:41:09,860
0,120 120,255 255,420 420,615 615,1110
can write out this derivative.

1573
00:41:09,860 --> 00:41:11,585
0,195 195,420 420,770 850,1250 1390,1725
{We,can -} start with applying

1574
00:41:11,585 --> 00:41:14,015
0,225 225,450 450,785 1225,1895 2125,2430
the chain rule backwards from

1575
00:41:14,015 --> 00:41:15,905
0,165 165,345 345,665 1105,1500 1500,1890
the loss function through the

1576
00:41:15,905 --> 00:41:17,780
0,395 685,1085 1345,1620 1620,1755 1755,1875
{output.,Specifically, -} what we can

1577
00:41:17,780 --> 00:41:18,545
0,120 120,255 255,390 390,570 570,765
do is we can actually

1578
00:41:18,545 --> 00:41:21,130
0,165 165,815 925,1230 1230,1895 2185,2585
just decompose this derivative into

1579
00:41:21,330 --> 00:41:23,090
0,400 480,880 900,1160 1160,1415 1415,1760
{two,components. -} The first component

1580
00:41:23,090 --> 00:41:24,410
0,255 255,435 435,975 975,1170 1170,1320
is the derivative of our

1581
00:41:24,410 --> 00:41:25,655
0,285 285,630 630,870 870,1005 1005,1245
loss with respect to our

1582
00:41:25,655 --> 00:41:27,530
0,395 595,1215 1215,1380 1380,1500 1500,1875
output multiplied by the derivative

1583
00:41:27,530 --> 00:41:29,060
0,150 150,410 520,885 885,1230 1230,1530
{of,our -} output. With respect

1584
00:41:29,060 --> 00:41:30,260
0,210 210,495 495,840 840,1065 1065,1200
to w two, right, this

1585
00:41:30,260 --> 00:41:32,940
0,150 150,315 315,590 1030,1430
is just a standard.|

1586
00:41:33,730 --> 00:41:35,775
0,640 750,1100 1100,1445 1445,1760 1760,2045
Instantiation of the chain rule

1587
00:41:35,775 --> 00:41:37,410
0,270 270,510 510,845 955,1500 1500,1635
with this original derivative that

1588
00:41:37,410 --> 00:41:38,085
0,120 120,255 255,405 405,525 525,675
we had on the left

1589
00:41:38,085 --> 00:41:39,520
0,210 210,515
hand side.|

1590
00:41:39,770 --> 00:41:40,735
0,380 380,560 560,710 710,845 845,965
Let's suppose we wanted to

1591
00:41:40,735 --> 00:41:42,130
0,315 315,465 465,900 900,1215 1215,1395
compute the gradients of the

1592
00:41:42,130 --> 00:41:43,405
0,210 210,510 510,765 765,1035 1035,1275
weight before that, which in

1593
00:41:43,405 --> 00:41:44,560
0,165 165,375 375,660 660,915 915,1155
this case are not w

1594
00:41:44,560 --> 00:41:46,195
0,350 370,675 675,980 1210,1485 1485,1635
one but w excuse me

1595
00:41:46,195 --> 00:41:47,580
0,180 180,465 465,825 825,1095 1095,1385
not w two but w

1596
00:41:47,600 --> 00:41:49,735
0,400 1230,1625 1625,1880 1880,2015 2015,2135
one well all we do

1597
00:41:49,735 --> 00:41:50,740
0,180 180,420 420,675 675,885 885,1005
is replace w two with

1598
00:41:50,740 --> 00:41:52,225
0,225 225,590 760,1035 1035,1245 1245,1485
w one and that chain

1599
00:41:52,225 --> 00:41:53,800
0,255 255,540 540,875 1015,1335 1335,1575
rule still holds right. {That,same

1600
00:41:53,800 --> 00:41:55,410
0,270 270,620 640,1040 1060,1335 1335,1610
-} equation holds but now

1601
00:41:55,640 --> 00:41:56,935
0,260 260,425 425,730 840,1130 1130,1295
you can see on the

1602
00:41:56,935 --> 00:41:58,450
0,270 270,615 615,870 870,1175 1195,1515
red component that last component

1603
00:41:58,450 --> 00:41:59,545
0,180 180,315 315,480 480,770 850,1095
of the chain rule we

1604
00:41:59,545 --> 00:42:01,615
0,150 150,455 565,945 945,1275 1275,2070
have to once again recursively

1605
00:42:01,615 --> 00:42:02,650
0,240 240,420 420,615 615,825 825,1035
apply one more chain rule

1606
00:42:02,650 --> 00:42:05,050
0,180 180,450 450,740 910,1310 1750,2400
because that's again another derivative

1607
00:42:05,050 --> 00:42:06,310
0,270 270,465 465,690 690,915 915,1260
that we can't directly {evaluate.,We

1608
00:42:06,310 --> 00:42:07,555
0,240 240,480 480,780 780,990 990,1245
-} can expand that once

1609
00:42:07,555 --> 00:42:09,355
0,360 360,690 690,1025 1045,1590 1590,1800
more with another instantiation of

1610
00:42:09,355 --> 00:42:10,705
0,135 135,315 315,635 835,1125 1125,1350
the chain rule and now

1611
00:42:10,705 --> 00:42:12,595
0,210 210,345 345,600 600,995 1615,1890
all of these components we

1612
00:42:12,595 --> 00:42:14,640
0,275 385,765 765,1305 1305,1500 1500,2045
can directly propagate these gradients

1613
00:42:14,810 --> 00:42:16,705
0,400 420,695 695,890 890,1210 1590,1895
through the hidden units right

1614
00:42:16,705 --> 00:42:17,875
0,165 165,330 330,600 600,870 870,1170
in our neural network all

1615
00:42:17,875 --> 00:42:19,105
0,150 150,285 285,575 865,1110 1110,1230
the way back to the

1616
00:42:19,105 --> 00:42:20,530
0,210 210,845 895,1155 1155,1275 1275,1425
weight that're interested in in

1617
00:42:20,530 --> 00:42:21,850
0,225 225,560 640,945 945,1140 1140,1320
this {example,,right. -} So we

1618
00:42:21,850 --> 00:42:23,200
0,255 255,615 615,750 750,1125 1125,1350
first computed the derivative with

1619
00:42:23,200 --> 00:42:24,680
0,225 225,360 360,585 585,950
respect to w two.|

1620
00:42:24,680 --> 00:42:25,715
0,210 210,315 315,420 420,615 615,1035
Then we can back propagate

1621
00:42:25,715 --> 00:42:26,770
0,120 120,225 225,405 405,690 690,1055
that and use that information.

1622
00:42:27,180 --> 00:42:28,265
0,230 230,335 335,575 575,830 830,1085
{Also,with -} w one, that's

1623
00:42:28,265 --> 00:42:29,030
0,120 120,270 270,465 465,645 645,765
why we really call it

1624
00:42:29,030 --> 00:42:30,520
0,180 180,720 720,990 990,1185 1185,1490
back propagation, because this process

1625
00:42:30,630 --> 00:42:32,105
0,320 320,515 515,740 740,1090 1200,1475
occurs from the output all

1626
00:42:32,105 --> 00:42:32,870
0,120 120,240 240,435 435,600 600,765
the way back to the

1627
00:42:32,870 --> 00:42:33,600
0,320
input.|

1628
00:42:34,650 --> 00:42:36,070
0,335 335,620 620,905 905,1130 1130,1420
Now, we repeat this process

1629
00:42:36,150 --> 00:42:38,930
0,400 1110,1430 1430,1700 1700,2050 2460,2780
essentially many, many times over

1630
00:42:38,930 --> 00:42:40,010
0,180 180,345 345,555 555,810 810,1080
the course of training by

1631
00:42:40,010 --> 00:42:41,825
0,540 540,810 810,1335 1335,1635 1635,1815
propagating these gradients over and

1632
00:42:41,825 --> 00:42:43,000
0,240 240,570 570,780 780,915 915,1175
over again through the network,

1633
00:42:43,290 --> 00:42:44,285
0,320 320,485 485,620 620,770 770,995
all the way from the

1634
00:42:44,285 --> 00:42:45,710
0,360 360,630 630,855 855,1200 1200,1425
output to the inputs to

1635
00:42:45,710 --> 00:42:47,170
0,270 270,480 480,740 760,1110 1110,1460
determine for every single weight

1636
00:42:47,250 --> 00:42:49,420
0,545 545,740 740,1060 1560,1865 1865,2170
answering this question, which is

1637
00:42:49,620 --> 00:42:50,870
0,335 335,605 605,800 800,980 980,1250
how much does a small

1638
00:42:50,870 --> 00:42:52,720
0,315 315,540 540,720 720,1220 1450,1850
change in these weights affect

1639
00:42:52,770 --> 00:42:54,080
0,305 305,545 545,860 860,1100 1100,1310
our loss function if it

1640
00:42:54,080 --> 00:42:55,265
0,285 285,480 480,690 690,1035 1035,1185
increases it or decreases, and

1641
00:42:55,265 --> 00:42:56,270
0,255 255,465 465,585 585,750 750,1005
how we can use that

1642
00:42:56,270 --> 00:42:58,160
0,350 580,980 1000,1260 1260,1520 1540,1890
to improve the loss ultimately,

1643
00:42:58,160 --> 00:42:59,440
0,225 225,450 450,600 600,900 900,1280
because that's our final goal.|

1644
00:42:59,800 --> 00:43:01,300
0,150 150,360 360,680
In this class.|

1645
00:43:02,710 --> 00:43:04,100
0,320 320,620 620,725 725,875 875,1390
So that's the back propagation

1646
00:43:04,150 --> 00:43:06,060
0,500 500,1030 1080,1460 1460,1640 1640,1910
algorithm. That's, that's the core

1647
00:43:06,060 --> 00:43:08,370
0,270 270,590 790,1155 1155,1430 1990,2310
of training neural networks. {In,theory.

1648
00:43:08,370 --> 00:43:10,050
0,320 520,870 870,1050 1050,1305 1305,1680
-} It's very simple. It's,

1649
00:43:10,050 --> 00:43:12,050
0,470 550,915 915,1170 1170,1395 1395,2000
it's really just an instantiation

1650
00:43:12,160 --> 00:43:14,080
0,400 480,755 755,965 965,1300
of the chain rule.|

1651
00:43:14,380 --> 00:43:16,070
0,400 720,1100 1100,1235 1235,1400 1400,1690
But let's touch on some

1652
00:43:16,090 --> 00:43:17,820
0,440 440,680 680,1025 1025,1385 1385,1730
insights that make training neural

1653
00:43:17,820 --> 00:43:20,085
0,260 670,1005 1005,1305 1305,1670 1960,2265
networks actually extremely complicated in

1654
00:43:20,085 --> 00:43:21,780
0,305 385,750 750,1035 1035,1335 1335,1695
practice, even though the algorithm

1655
00:43:21,780 --> 00:43:24,105
0,150 150,360 360,890 1060,1460 1930,2325
of back propagation is simple

1656
00:43:24,105 --> 00:43:25,640
0,395 475,720 720,915 915,1200 1200,1535
and, you know, many decades

1657
00:43:25,840 --> 00:43:28,905
0,400 1080,1400 1400,1720 1800,2200 2460,3065
old. {In,practice -} though optimization

1658
00:43:28,905 --> 00:43:30,705
0,300 300,570 570,845 1225,1545 1545,1800
of neural networks looks something

1659
00:43:30,705 --> 00:43:31,680
0,225 225,375 375,495 495,675 675,975
like {this.,It -} looks nothing

1660
00:43:31,680 --> 00:43:32,730
0,285 285,480 480,705 705,900 900,1050
like that picture that I

1661
00:43:32,730 --> 00:43:34,140
0,195 195,360 360,620 1060,1305 1305,1410
showed {you,before. -} There are

1662
00:43:34,140 --> 00:43:35,265
0,180 180,360 360,465 465,600 600,1125
ways that we can visualize

1663
00:43:35,265 --> 00:43:36,890
0,285 285,635 715,1035 1035,1350 1350,1625
very large, deep neural networks,

1664
00:43:37,300 --> 00:43:38,580
0,400 480,740 740,890 890,1070 1070,1280
and you can think of

1665
00:43:38,580 --> 00:43:39,860
0,320 370,645 645,795 795,975 975,1280
the landscape of these models

1666
00:43:40,240 --> 00:43:41,565
0,365 365,650 650,905 905,1130 1130,1325
looking like {something,like -} this.

1667
00:43:41,565 --> 00:43:42,645
0,165 165,285 285,405 405,870 870,1080
This is an illustration from

1668
00:43:42,645 --> 00:43:43,620
0,120 120,330 330,555 555,720 720,975
a paper that came out

1669
00:43:43,620 --> 00:43:45,180
0,330 330,630 630,980 1150,1410 1410,1560
several years ago where they

1670
00:43:45,180 --> 00:43:46,610
0,180 180,390 390,600 600,1110 1110,1430
tried to actually visualize the

1671
00:43:46,630 --> 00:43:47,895
0,260 260,440 440,725 725,1025 1025,1265
landscape of very, {very,deep -}

1672
00:43:47,895 --> 00:43:49,710
0,285 285,545 1195,1455 1455,1680 1680,1815
neural networks. And that's what

1673
00:43:49,710 --> 00:43:50,925
0,290 310,690 690,930 930,1065 1065,1215
this landscape actually looks like.

1674
00:43:50,925 --> 00:43:51,660
0,210 210,315 315,480 480,600 600,735
That's what you're trying to

1675
00:43:51,660 --> 00:43:52,410
0,150 150,300 300,450 450,615 615,750
deal with and find {the,minimum

1676
00:43:52,410 --> 00:43:53,610
0,210 210,435 435,645 645,960 960,1200
-} in this space. And

1677
00:43:53,610 --> 00:43:55,010
0,105 105,225 225,500 820,1110 1110,1400
you can imagine the challenges

1678
00:43:55,030 --> 00:43:57,780
0,290 290,470 470,650 650,940
that come with that.|

1679
00:43:57,790 --> 00:43:59,280
0,275 275,455 455,635 635,910 1110,1490
To cover the challenges, let's

1680
00:43:59,280 --> 00:44:00,860
0,225 225,465 465,750 750,1130 1180,1580
first think of and recall

1681
00:44:01,030 --> 00:44:03,180
0,400 450,800 800,1150 1290,1690 1860,2150
that update equation defined in

1682
00:44:03,180 --> 00:44:05,655
0,360 360,770 940,1275 1275,1610 2200,2475
gradient descent, right. {So,I,didn't -

1683
00:44:05,655 --> 00:44:06,600
0,210 210,375 375,540 540,705 705,945
-} talk too much about

1684
00:44:06,600 --> 00:44:08,250
0,240 240,765 765,1160 1210,1485 1485,1650
this parameter a, but now

1685
00:44:08,250 --> 00:44:09,150
0,255 255,390 390,570 570,750 750,900
let's spend a bit of

1686
00:44:09,150 --> 00:44:10,635
0,290 400,765 765,1020 1020,1260 1260,1485
time thinking about {this.,This -}

1687
00:44:10,635 --> 00:44:11,790
0,195 195,435 435,630 630,855 855,1155
is called the {learning,rate. -}

1688
00:44:11,790 --> 00:44:13,260
0,240 240,405 405,570 570,860 1150,1470
Like {we,saw -} before. It

1689
00:44:13,260 --> 00:44:15,075
0,405 405,740 850,1250 1420,1695 1695,1815
determines basically how big of

1690
00:44:15,075 --> 00:44:16,380
0,195 195,545 715,990 990,1140 1140,1305
a step we need to

1691
00:44:16,380 --> 00:44:17,790
0,290 520,795 795,960 960,1200 1200,1410
take in the direction of

1692
00:44:17,790 --> 00:44:19,005
0,150 150,525 525,675 675,930 930,1215
our gradient and every single

1693
00:44:19,005 --> 00:44:20,920
0,390 390,600 600,795 795,1325
iteration of back propagation.|

1694
00:44:21,230 --> 00:44:23,515
0,305 305,610 870,1270 1470,1870 2010,2285
In practice, even setting the

1695
00:44:23,515 --> 00:44:24,550
0,225 225,540 540,750 750,855 855,1035
learning rate can be very

1696
00:44:24,550 --> 00:44:25,870
0,320 340,615 615,890 910,1170 1170,1320
challenging. {You,as -} you as

1697
00:44:25,870 --> 00:44:26,920
0,240 240,630 630,735 735,855 855,1050
the designer of the neural

1698
00:44:26,920 --> 00:44:28,200
0,255 255,540 540,735 735,960 960,1280
network have to set this

1699
00:44:28,220 --> 00:44:30,070
0,395 395,695 695,950 950,1300 1560,1850
value, this learning rate and

1700
00:44:30,070 --> 00:44:30,715
0,135 135,195 195,285 285,450 450,645
how do you pick this

1701
00:44:30,715 --> 00:44:32,050
0,300 300,600 600,885 885,1140 1140,1335
value, right? So that can

1702
00:44:32,050 --> 00:44:32,935
0,165 165,285 285,540 540,780 780,885
actually be quite {difficult.,It -}

1703
00:44:32,935 --> 00:44:36,055
0,195 195,545 1255,1655 1765,2165 2725,3120
has really large consequences when

1704
00:44:36,055 --> 00:44:37,255
0,330 330,525 525,720 720,960 960,1200
building a {neural,network. -} So,

1705
00:44:37,255 --> 00:44:38,460
0,165 165,455
for example.|

1706
00:44:38,530 --> 00:44:40,305
0,305 305,610 960,1340 1340,1580 1580,1775
If we set the learning

1707
00:44:40,305 --> 00:44:41,980
0,335 385,690 690,995
rate too low.|

1708
00:44:41,980 --> 00:44:43,990
0,290 340,740 850,1230 1230,1610 1630,2010
Then we learn very slowly.

1709
00:44:43,990 --> 00:44:44,980
0,225 225,450 450,615 615,795 795,990
{So,let's -} assume we start

1710
00:44:44,980 --> 00:44:45,880
0,150 150,270 270,435 435,660 660,900
on the right hand side

1711
00:44:45,880 --> 00:44:47,335
0,320 400,675 675,885 885,1155 1155,1455
{here.,At -} that initial guess,

1712
00:44:47,335 --> 00:44:48,355
0,240 240,435 435,690 690,885 885,1020
if our learning rate is

1713
00:44:48,355 --> 00:44:50,020
0,180 180,465 465,845 1135,1425 1425,1665
not large enough, not only

1714
00:44:50,020 --> 00:44:51,850
0,195 195,440 700,1110 1110,1430 1450,1830
do we converge slowly, we

1715
00:44:51,850 --> 00:44:53,185
0,300 300,585 585,825 825,1215 1215,1335
actually don't even converge to

1716
00:44:53,185 --> 00:44:54,850
0,120 120,395 565,965 1255,1530 1530,1665
the global minimum because we

1717
00:44:54,850 --> 00:44:55,555
0,105 105,195 195,375 375,585 585,705
kind of get stuck in

1718
00:44:55,555 --> 00:44:57,180
0,105 105,345 345,725
a local minimum.|

1719
00:44:57,740 --> 00:44:58,780
0,335 335,515 515,635 635,845 845,1040
Now, what if we set

1720
00:44:58,780 --> 00:44:59,880
0,120 120,300 300,585 585,825 825,1100
our learning rate too high,

1721
00:44:59,960 --> 00:45:01,030
0,335 335,545 545,740 740,905 905,1070
right? What can actually happen

1722
00:45:01,030 --> 00:45:02,650
0,210 210,360 360,1095 1095,1425 1425,1620
is we overshoot and we

1723
00:45:02,650 --> 00:45:04,080
0,195 195,435 435,630 630,765 765,1430
can actually start to diverge

1724
00:45:04,160 --> 00:45:06,115
0,290 290,470 470,760 1200,1505 1505,1955
from the solution. {The,gradients -}

1725
00:45:06,115 --> 00:45:07,870
0,240 240,540 540,965 1225,1530 1530,1755
can actually {explode.,Very -} bad

1726
00:45:07,870 --> 00:45:09,115
0,240 240,560 610,885 885,1080 1080,1245
things happen, and then the

1727
00:45:09,115 --> 00:45:10,700
0,225 225,450 450,825 825,1115
neural network doesn't trade.|

1728
00:45:10,700 --> 00:45:11,765
0,225 225,495 495,660 660,825 825,1065
So that's also not good.

1729
00:45:11,765 --> 00:45:13,330
0,210 210,485 655,1065 1065,1260 1260,1565
{In,reality, -} there's a very

1730
00:45:13,680 --> 00:45:15,545
0,350 350,700 1020,1420 1440,1715 1715,1865
happy medium between setting it

1731
00:45:15,545 --> 00:45:16,610
0,195 195,510 510,780 780,915 915,1065
too small, setting it too

1732
00:45:16,610 --> 00:45:18,335
0,290 790,1125 1125,1365 1365,1530 1530,1725
large, where you set it,

1733
00:45:18,335 --> 00:45:19,490
0,270 270,570 570,855 855,1035 1035,1155
just large enough to kind

1734
00:45:19,490 --> 00:45:20,810
0,165 165,840 840,1005 1005,1155 1155,1320
of overshoot some of these

1735
00:45:20,810 --> 00:45:22,400
0,290 400,980
local minima.|

1736
00:45:22,470 --> 00:45:23,470
0,275 275,395 395,560 560,740 740,1000
Put you into a reasonable

1737
00:45:23,520 --> 00:45:24,670
0,275 275,425 425,575 575,800 800,1150
part of the search space

1738
00:45:24,930 --> 00:45:26,180
0,290 290,455 455,605 605,880 900,1250
where then you can actually

1739
00:45:26,180 --> 00:45:27,320
0,360 360,480 480,600 600,860 880,1140
converge on the solutions that

1740
00:45:27,320 --> 00:45:29,530
0,165 165,375 375,630 630,980 1810,2210
you care most about, but.|

1741
00:45:29,860 --> 00:45:30,630
0,260 260,365 365,440 440,560 560,770
Actually, how do you set

1742
00:45:30,630 --> 00:45:32,090
0,320 430,765 765,990 990,1170 1170,1460
these learning rates in practice?

1743
00:45:32,200 --> 00:45:33,315
0,260 260,335 335,455 455,760 810,1115
How do you pick what

1744
00:45:33,315 --> 00:45:34,790
0,255 255,540 540,810 810,1110 1110,1475
is the ideal learning rate?

1745
00:45:35,080 --> 00:45:36,570
0,260 260,520 690,1025 1025,1235 1235,1490
One option, and this is

1746
00:45:36,570 --> 00:45:37,670
0,225 225,330 330,510 510,765 765,1100
actually a very common option

1747
00:45:37,750 --> 00:45:39,140
0,290 290,580 600,920 920,1115 1115,1390
in practice, is to simply

1748
00:45:39,490 --> 00:45:40,320
0,275 275,410 410,515 515,650 650,830
try out a bunch of

1749
00:45:40,320 --> 00:45:41,430
0,225 225,480 480,705 705,885 885,1110
learning rates and see what

1750
00:45:41,430 --> 00:45:42,900
0,300 300,495 495,740 1060,1335 1335,1470
works the best. {So,try -}

1751
00:45:42,900 --> 00:45:43,920
0,210 210,480 480,600 600,765 765,1020
out, let's say, a whole

1752
00:45:43,920 --> 00:45:45,290
0,300 300,480 480,720 720,1020 1020,1370
grid of different learning rates

1753
00:45:45,670 --> 00:45:47,040
0,400 600,860 860,1025 1025,1220 1220,1370
and, you know, train all

1754
00:45:47,040 --> 00:45:48,015
0,120 120,270 270,510 510,735 735,975
of these neural networks, see

1755
00:45:48,015 --> 00:45:48,950
0,150 150,315 315,525 525,690 690,935
which one works the best.|

1756
00:45:49,850 --> 00:45:50,920
0,350 350,575 575,740 740,890 890,1070
But I think we can

1757
00:45:50,920 --> 00:45:52,020
0,210 210,405 405,555 555,720 720,1100
do something a lot smarter,

1758
00:45:52,220 --> 00:45:53,230
0,350 350,560 560,680 680,845 845,1010
right? So what are some

1759
00:45:53,230 --> 00:45:54,580
0,195 195,530 550,945 945,1215 1215,1350
more intelligent ways that we

1760
00:45:54,580 --> 00:45:55,390
0,120 120,240 240,420 420,600 600,810
could do this? Instead of

1761
00:45:55,390 --> 00:45:56,665
0,675 675,870 870,1035 1035,1155 1155,1275
exhaustively trying out a whole

1762
00:45:56,665 --> 00:45:57,990
0,180 180,345 345,605 625,975 975,1325
bunch of different learning rates?

1763
00:45:58,430 --> 00:45:59,905
0,245 245,440 440,790 900,1220 1220,1475
Can we design a learning

1764
00:45:59,905 --> 00:46:01,950
0,335 385,870 870,1170 1170,1380 1380,2045
rate algorithm that actually adapts

1765
00:46:02,120 --> 00:46:03,715
0,290 290,470 470,770 770,1060 1350,1595
to our neural network and

1766
00:46:03,715 --> 00:46:04,975
0,360 360,465 465,695 775,1095 1095,1260
adapts to its landscape so

1767
00:46:04,975 --> 00:46:05,605
0,90 90,225 225,300 300,420 420,630
that it's a bit more

1768
00:46:05,605 --> 00:46:08,010
0,335 565,965 1255,1575 1575,1895 2005,2405
intelligent than that previous idea?|

1769
00:46:09,450 --> 00:46:11,015
0,305 305,515 515,815 815,1190 1190,1565
So this really ultimately means

1770
00:46:11,015 --> 00:46:12,440
0,395
that.|

1771
00:46:12,440 --> 00:46:14,375
0,255 255,525 525,890 1360,1695 1695,1935
The learning rate, the speed

1772
00:46:14,375 --> 00:46:16,325
0,210 210,515 745,1145 1405,1770 1770,1950
at which the algorithm is

1773
00:46:16,325 --> 00:46:17,795
0,525 525,750 750,1170 1170,1320 1320,1470
trusting the gradients that it

1774
00:46:17,795 --> 00:46:19,450
0,305 535,825 825,1005 1005,1275 1275,1655
sees is going to depend

1775
00:46:19,530 --> 00:46:21,365
0,400 540,920 920,1295 1295,1550 1550,1835
on how large the gradient

1776
00:46:21,365 --> 00:46:23,255
0,165 165,315 315,555 555,935 1525,1890
is in that location and

1777
00:46:23,255 --> 00:46:24,605
0,270 270,510 510,780 780,1025 1075,1350
how fast we're learning. {How,many

1778
00:46:24,605 --> 00:46:27,485
0,225 225,540 540,905 2155,2555 2605,2880
-} other options and sorry

1779
00:46:27,485 --> 00:46:29,435
0,165 165,345 345,540 540,845 1585,1950
and many other options that

1780
00:46:29,435 --> 00:46:30,755
0,240 240,420 420,725 775,1110 1110,1320
we might have as part

1781
00:46:30,755 --> 00:46:31,835
0,240 240,510 510,675 675,855 855,1080
of training in neural networks,

1782
00:46:31,835 --> 00:46:32,765
0,255 255,375 375,540 540,660 660,930
right? So it's not only

1783
00:46:32,765 --> 00:46:34,235
0,300 300,570 570,855 855,1085 1195,1470
how quickly we're learning, you

1784
00:46:34,235 --> 00:46:35,495
0,275 475,765 765,915 915,1065 1065,1260
may judge in on many

1785
00:46:35,495 --> 00:46:36,790
0,255 255,605 655,930 930,1050 1050,1295
different factors in the learning

1786
00:46:37,020 --> 00:46:37,840
0,400
landscape.|

1787
00:46:39,210 --> 00:46:40,990
0,275 275,530 530,1030 1110,1445 1445,1780
In fact, we've all been

1788
00:46:41,580 --> 00:46:43,775
0,400 540,940 1020,1510 1710,1985 1985,2195
these different algorithms that I'm

1789
00:46:43,775 --> 00:46:45,260
0,195 195,435 435,675 675,1230 1230,1485
talking about. {These,adaptive -} learning

1790
00:46:45,260 --> 00:46:46,775
0,330 330,795 795,1065 1065,1275 1275,1515
rate algorithms have been very

1791
00:46:46,775 --> 00:46:48,995
0,335 685,1085 1165,1470 1470,1775 1885,2220
widely studied in {practice.,There -}

1792
00:46:48,995 --> 00:46:50,980
0,195 195,455 475,870 870,1475 1585,1985
is a very thriving community

1793
00:46:51,300 --> 00:46:52,390
0,275 275,410 410,560 560,770 770,1090
in the deep learning research

1794
00:46:52,500 --> 00:46:54,430
0,365 365,635 635,1115 1115,1390 1530,1930
community that focuses on developing

1795
00:46:54,750 --> 00:46:57,040
0,290 290,695 695,1040 1040,1540 1890,2290
and designing new algorithms for

1796
00:46:57,390 --> 00:46:59,470
0,335 335,620 620,1150 1380,1730 1730,2080
learning rate adaptation and faster

1797
00:46:59,580 --> 00:47:01,490
0,560 560,890 890,1270 1290,1655 1655,1910
optimization of large neural networks

1798
00:47:01,490 --> 00:47:03,365
0,285 285,590 1270,1545 1545,1710 1710,1875
{like,these. -} And during your

1799
00:47:03,365 --> 00:47:04,900
0,405 405,780 780,1035 1035,1245 1245,1535
labs, you'll actually get the

1800
00:47:05,160 --> 00:47:06,755
0,400 510,845 845,1055 1055,1310 1310,1595
opportunity to not only try

1801
00:47:06,755 --> 00:47:08,060
0,305 595,870 870,1020 1020,1155 1155,1305
out a lot of these

1802
00:47:08,060 --> 00:47:09,605
0,285 285,885 885,1260 1260,1425 1425,1545
different adaptive algorithms, which you

1803
00:47:09,605 --> 00:47:10,780
0,150 150,360 360,665
can see here.|

1804
00:47:10,780 --> 00:47:12,100
0,300 300,585 585,780 780,900 900,1320
But also try to uncover

1805
00:47:12,100 --> 00:47:12,835
0,120 120,300 300,450 450,570 570,735
what are kind of the

1806
00:47:12,835 --> 00:47:14,425
0,240 240,510 510,815 1045,1350 1350,1590
patterns and benefits of one

1807
00:47:14,425 --> 00:47:15,610
0,315 315,525 525,675 675,915 915,1185
versus the other. {And,that's -}

1808
00:47:15,610 --> 00:47:16,500
0,120 120,210 210,315 315,540 540,890
going to be something that

1809
00:47:17,240 --> 00:47:18,630
0,260 260,410 410,920 920,1085 1085,1390
I think you'll find very

1810
00:47:18,650 --> 00:47:19,900
0,575 575,815 815,965 965,1085 1085,1250
insightful as part of your

1811
00:47:19,900 --> 00:47:21,060
0,440
labs.|

1812
00:47:21,330 --> 00:47:23,060
0,400 630,980 980,1280 1280,1550 1550,1730
So another key component of

1813
00:47:23,060 --> 00:47:24,110
0,135 135,375 375,495 495,840 840,1050
your labs that you'll see

1814
00:47:24,110 --> 00:47:24,995
0,195 195,390 390,510 510,690 690,885
is how you can actually

1815
00:47:24,995 --> 00:47:26,090
0,165 165,345 345,495 495,750 750,1095
put all of this information

1816
00:47:26,090 --> 00:47:27,760
0,210 210,375 375,570 570,920 1270,1670
that we've covered today into

1817
00:47:27,900 --> 00:47:29,420
0,400 450,785 785,1070 1070,1310 1310,1520
a single picture that looks

1818
00:47:29,420 --> 00:47:30,815
0,315 315,675 675,930 930,1155 1155,1395
roughly something like this, which

1819
00:47:30,815 --> 00:47:32,720
0,425 775,1080 1080,1385 1465,1740 1740,1905
defines your model at the

1820
00:47:32,720 --> 00:47:33,845
0,255 255,495 495,645 645,840 840,1125
first. {At,the -} top here,

1821
00:47:33,845 --> 00:47:34,670
0,300 300,390 390,525 525,690 690,825
that's where you define your

1822
00:47:34,670 --> 00:47:35,525
0,210 210,405 405,510 510,675 675,855
model, where you talked about

1823
00:47:35,525 --> 00:47:36,380
0,150 150,285 285,435 435,660 660,855
this in the beginning part

1824
00:47:36,380 --> 00:47:38,100
0,120 120,225 225,470
of the lecture.|

1825
00:47:38,100 --> 00:47:39,435
0,165 165,440 700,1005 1005,1185 1185,1335
For every piece in your

1826
00:47:39,435 --> 00:47:40,800
0,275 415,795 795,990 990,1200 1200,1365
model, you're now going to

1827
00:47:40,800 --> 00:47:42,600
0,180 180,435 435,720 720,1070 1150,1800
need to define this optimizer,

1828
00:47:42,600 --> 00:47:43,710
0,240 240,465 465,615 615,870 870,1110
which we've just talked about.

1829
00:47:43,710 --> 00:47:45,585
0,255 255,735 735,900 900,1220 1540,1875
{This,optimizer -} is defined together

1830
00:47:45,585 --> 00:47:47,025
0,195 195,315 315,525 525,875 1075,1440
with a learning rate, right?

1831
00:47:47,025 --> 00:47:48,210
0,300 300,585 585,795 795,945 945,1185
How quickly you want to

1832
00:47:48,210 --> 00:47:50,235
0,440 640,915 915,1190 1330,1730 1780,2025
optimize your lost landscape and

1833
00:47:50,235 --> 00:47:51,720
0,180 180,465 465,990 990,1350 1350,1485
over many loops, you're going

1834
00:47:51,720 --> 00:47:52,680
0,165 165,360 360,615 615,825 825,960
to pass over all of

1835
00:47:52,680 --> 00:47:53,685
0,150 150,420 420,660 660,795 795,1005
the examples in your data

1836
00:47:53,685 --> 00:47:54,720
0,335
set.|

1837
00:47:54,720 --> 00:47:56,955
0,285 285,680 1090,1490 1600,1965 1965,2235
And observe essentially how to

1838
00:47:56,955 --> 00:47:58,065
0,180 180,330 330,600 600,975 975,1110
improve your network, that's the

1839
00:47:58,065 --> 00:47:59,835
0,485 565,855 855,1145 1165,1500 1500,1770
gradient, and then actually improves

1840
00:47:59,835 --> 00:48:00,840
0,105 105,315 315,510 510,705 705,1005
the network in those directions.

1841
00:48:00,840 --> 00:48:01,890
0,255 255,465 465,675 675,855 855,1050
{And,keep -} doing that over

1842
00:48:01,890 --> 00:48:02,960
0,165 165,315 315,465 465,690 690,1070
and over and over again

1843
00:48:03,190 --> 00:48:04,610
0,305 305,610 630,920 920,1160 1160,1420
until eventually your neural network

1844
00:48:04,660 --> 00:48:06,720
0,580 930,1330 1470,1760 1760,1910 1910,2060
converges to some sort of

1845
00:48:06,720 --> 00:48:07,740
0,290
solution.|

1846
00:48:09,730 --> 00:48:10,710
0,260 260,380 380,515 515,710 710,980
So I want to very

1847
00:48:10,710 --> 00:48:12,240
0,350 550,900 900,1125 1125,1275 1275,1530
quickly, briefly, in the remaining

1848
00:48:12,240 --> 00:48:13,725
0,255 255,390 390,525 525,800 1150,1485
time that we have continue

1849
00:48:13,725 --> 00:48:15,320
0,210 210,420 420,735 735,1115 1195,1595
to talk about tips for

1850
00:48:15,340 --> 00:48:16,995
0,400 600,920 920,1175 1175,1370 1370,1655
training these neural networks in

1851
00:48:16,995 --> 00:48:19,005
0,335 805,1125 1125,1445 1465,1785 1785,2010
practice and focus on this

1852
00:48:19,005 --> 00:48:21,795
0,300 300,695 955,1355 1735,2135 2245,2790
very powerful idea of batching

1853
00:48:21,795 --> 00:48:23,895
0,165 165,455 505,905 1735,1980 1980,2100
your data into what are

1854
00:48:23,895 --> 00:48:25,940
0,275 325,645 645,1175 1315,1680 1680,2045
called mini batches of of

1855
00:48:26,110 --> 00:48:28,665
0,380 380,760 840,1160 1160,1480 2310,2555
smaller pieces of data. {To,do

1856
00:48:28,665 --> 00:48:29,955
0,120 120,300 300,630 630,1125 1125,1290
-} this, let's revisit that

1857
00:48:29,955 --> 00:48:31,940
0,345 345,725 895,1350 1350,1635 1635,1985
gradient descent algorithm, {right.,So -}

1858
00:48:32,020 --> 00:48:33,585
0,380 380,680 680,1205 1205,1430 1430,1565
here, this gradient that we

1859
00:48:33,585 --> 00:48:35,570
0,195 195,405 405,695 1045,1445 1585,1985
talked about before is actually

1860
00:48:35,740 --> 00:48:39,080
0,640 660,1690 2040,2440 2670,2930 2930,3340
extraordinarily computationally expensive to compute

1861
00:48:39,160 --> 00:48:40,515
0,350 350,725 725,1070 1070,1205 1205,1355
because it's computed as a

1862
00:48:40,515 --> 00:48:42,630
0,395 985,1385 1465,1770 1770,1965 1965,2115
summation across all of the

1863
00:48:42,630 --> 00:48:44,300
0,260 610,885 885,1065 1065,1320 1320,1670
pieces in your data set.|

1864
00:48:45,270 --> 00:48:46,610
0,245 245,365 365,640 660,1010 1010,1340
And in most real life

1865
00:48:46,610 --> 00:48:48,395
0,285 285,495 495,800 850,1250 1540,1785
or real world problems, you

1866
00:48:48,395 --> 00:48:50,015
0,135 135,420 420,690 690,1005 1005,1620
know, it's simply not feasible

1867
00:48:50,015 --> 00:48:51,530
0,240 240,690 690,945 945,1290 1290,1515
to compute a gradient over

1868
00:48:51,530 --> 00:48:52,910
0,315 315,615 615,885 885,1140 1140,1380
your entire data set. {Data,sets

1869
00:48:52,910 --> 00:48:53,950
0,195 195,345 345,540 540,750 750,1040
-} are just too large

1870
00:48:54,330 --> 00:48:56,870
0,320 320,640 1050,1450 2130,2390 2390,2540
these {days.,So -} you know

1871
00:48:56,870 --> 00:48:58,100
0,150 150,315 315,570 570,990 990,1230
there are some alternatives, right?

1872
00:48:58,100 --> 00:48:59,360
0,180 180,300 300,480 480,950 1000,1260
What are the alternatives? Instead

1873
00:48:59,360 --> 00:49:01,580
0,240 240,770 940,1335 1335,2010 2010,2220
of computing the derivative or

1874
00:49:01,580 --> 00:49:03,665
0,150 150,650 1210,1500 1500,1770 1770,2085
the gradients across your entire

1875
00:49:03,665 --> 00:49:05,750
0,270 270,605 1135,1440 1440,1725 1725,2085
data set, what if you

1876
00:49:05,750 --> 00:49:07,250
0,360 360,690 690,825 825,1230 1230,1500
instead computed the gradient over

1877
00:49:07,250 --> 00:49:09,740
0,350 880,1230 1230,1580 1780,2180 2230,2490
just a single example in

1878
00:49:09,740 --> 00:49:10,835
0,150 150,465 465,660 660,870 870,1095
your dataas set? Just one

1879
00:49:10,835 --> 00:49:12,490
0,335 355,630 630,810 810,1115 1255,1655
example? Well, of course this,

1880
00:49:12,840 --> 00:49:14,705
0,400 840,1160 1160,1340 1340,1490 1490,1865
this estimate of your gradient

1881
00:49:14,705 --> 00:49:15,905
0,150 150,285 285,390 390,635 865,1200
is going to be exactly

1882
00:49:15,905 --> 00:49:16,925
0,180 180,330 330,545 565,825 825,1020
that. It's an estimate, it's

1883
00:49:16,925 --> 00:49:17,920
0,105 105,195 195,285 285,435 435,995
going to be {very,noisy. -}

1884
00:49:18,030 --> 00:49:19,700
0,260 260,500 500,880 1110,1430 1430,1670
It may roughly reflect the

1885
00:49:19,700 --> 00:49:21,050
0,285 285,525 525,735 735,1035 1035,1350
trends of your entire data

1886
00:49:21,050 --> 00:49:22,355
0,350 490,840 840,1065 1065,1230 1230,1305
set, but because it's a

1887
00:49:22,355 --> 00:49:23,720
0,245 415,705 705,840 840,1080 1080,1365
very, it's only one example

1888
00:49:23,720 --> 00:49:24,740
0,225 225,405 405,570 570,765 765,1020
in fact of your entire

1889
00:49:24,740 --> 00:49:26,015
0,255 255,590 730,990 990,1125 1125,1275
data set, it may be

1890
00:49:26,015 --> 00:49:27,040
0,180 180,755
very noisy.|

1891
00:49:29,610 --> 00:49:31,055
0,350 350,635 635,965 965,1250 1250,1445
Well, the advantage of this

1892
00:49:31,055 --> 00:49:32,840
0,225 225,420 420,645 645,1115 1435,1785
though is that it's much

1893
00:49:32,840 --> 00:49:34,790
0,315 315,540 540,950 1030,1430 1630,1950
faster to compute obviously the

1894
00:49:34,790 --> 00:49:36,520
0,510 510,810 810,1020 1020,1310 1330,1730
gradient over a single example

1895
00:49:36,690 --> 00:49:38,015
0,275 275,485 485,680 680,1010 1010,1325
because it's one example. {So,computationally

1896
00:49:38,015 --> 00:49:40,060
0,960 960,1200 1200,1395 1395,1680 1680,2045
-} this has huge advantages,

1897
00:49:40,620 --> 00:49:41,990
0,320 320,515 515,995 995,1235 1235,1370
but the downside is that

1898
00:49:41,990 --> 00:49:43,985
0,255 255,480 480,980 1570,1905 1905,1995
it's extremely stochastic. That's the

1899
00:49:43,985 --> 00:49:45,065
0,165 165,345 345,570 570,915 915,1080
reason why this algorithm is

1900
00:49:45,065 --> 00:49:46,415
0,195 195,465 465,855 855,1125 1125,1350
not called gradient descent. It's

1901
00:49:46,415 --> 00:49:48,340
0,245 265,795 795,1200 1200,1575 1575,1925
called stochastic gradient descent now.|

1902
00:49:49,440 --> 00:49:50,645
0,365 365,695 695,785 785,935 935,1205
Now, what's the middle ground,

1903
00:49:50,645 --> 00:49:51,770
0,330 330,540 540,735 735,1035 1035,1125
right? Instead of computing it

1904
00:49:51,770 --> 00:49:52,910
0,225 225,480 480,645 645,870 870,1140
with respect to one example

1905
00:49:52,910 --> 00:49:54,080
0,165 165,285 285,495 495,830 910,1170
in your data set, what

1906
00:49:54,080 --> 00:49:55,655
0,165 165,470 820,1200 1200,1425 1425,1575
if we computed what's called

1907
00:49:55,655 --> 00:49:57,070
0,195 195,405 405,750 750,1080 1080,1415
a mini batch of examples?

1908
00:49:57,180 --> 00:49:58,660
0,350 350,635 635,905 905,1145 1145,1480
A small batch of examples

1909
00:49:59,070 --> 00:50:00,395
0,400 480,755 755,905 905,1190 1190,1325
that we can compute the

1910
00:50:00,395 --> 00:50:01,910
0,375 375,665 745,1080 1080,1290 1290,1515
gradients over. {And,when -} we

1911
00:50:01,910 --> 00:50:04,700
0,285 285,620 790,1400 1990,2480 2500,2790
take these gradients, they're still

1912
00:50:04,700 --> 00:50:07,445
0,860 1180,1580 1600,1860 1860,2330 2440,2745
computationally efficient to compute because

1913
00:50:07,445 --> 00:50:08,540
0,240 240,360 360,555 555,825 825,1095
it's a mini batch. It's

1914
00:50:08,540 --> 00:50:09,650
0,180 180,390 390,615 615,870 870,1110
not too {large.,Maybe -} we're

1915
00:50:09,650 --> 00:50:10,565
0,150 150,330 330,435 435,630 630,915
talking on the order of

1916
00:50:10,565 --> 00:50:12,590
0,300 300,540 540,815 1285,1665 1665,2025
tens or hundreds of examples

1917
00:50:12,590 --> 00:50:15,280
0,225 225,360 360,920 1450,1850
in our dataset, but.|

1918
00:50:15,620 --> 00:50:17,875
0,350 350,700 1320,1625 1625,1955 1955,2255
More importantly, because we've expanded

1919
00:50:17,875 --> 00:50:19,060
0,210 210,330 330,585 585,960 960,1185
from a single example to

1920
00:50:19,060 --> 00:50:20,785
0,120 120,270 270,525 525,920 1480,1725
maybe a hundred examples, the

1921
00:50:20,785 --> 00:50:23,155
0,810 810,990 990,1325 1765,2130 2130,2370
stochasticity is significantly reduced and

1922
00:50:23,155 --> 00:50:24,385
0,255 255,645 645,735 735,885 885,1230
the accuracy of our gradient

1923
00:50:24,385 --> 00:50:25,960
0,270 270,645 645,1025
is much improved.|

1924
00:50:26,310 --> 00:50:28,370
0,400 600,1000 1200,1565 1565,1790 1790,2060
So normally we're thinking of

1925
00:50:28,370 --> 00:50:29,855
0,270 270,620 700,1005 1005,1215 1215,1485
batch sizes, mini batch sizes,

1926
00:50:29,855 --> 00:50:30,800
0,300 300,495 495,585 585,750 750,945
roughly on the order of

1927
00:50:30,800 --> 00:50:32,720
0,105 105,350 670,1020 1020,1370 1570,1920
a hundred data points, tens

1928
00:50:32,720 --> 00:50:33,995
0,240 240,510 510,750 750,945 945,1275
or hundreds of data points.

1929
00:50:33,995 --> 00:50:35,900
0,255 255,515 655,960 960,1265 1525,1905
{This,is -} much faster obviously

1930
00:50:35,900 --> 00:50:37,220
0,240 240,525 525,690 690,1020 1020,1320
to compute than gradient descent

1931
00:50:37,220 --> 00:50:38,780
0,195 195,375 375,680 700,1100 1300,1560
and much more accurate to

1932
00:50:38,780 --> 00:50:40,805
0,440 670,975 975,1155 1155,1620 1620,2025
compute compared to stochastic gradient

1933
00:50:40,805 --> 00:50:41,950
0,300 300,480 480,630 630,825 825,1145
descent, which is that single

1934
00:50:42,840 --> 00:50:44,420
0,335 335,635 635,1000
single point example.|

1935
00:50:44,830 --> 00:50:46,760
0,335 335,670 870,1190 1190,1430 1430,1930
So this increase in gradient

1936
00:50:46,840 --> 00:50:49,760
0,640 1500,1835 1835,2135 2135,2500 2520,2920
accuracy allows us to essentially

1937
00:50:49,900 --> 00:50:51,255
0,470 470,590 590,710 710,1000 1020,1355
converge to our solution much

1938
00:50:51,255 --> 00:50:52,965
0,465 465,845 1195,1485 1485,1620 1620,1710
quicker than it could have

1939
00:50:52,965 --> 00:50:54,690
0,135 135,390 390,645 645,935 1405,1725
been possible in practice due

1940
00:50:54,690 --> 00:50:56,385
0,225 225,555 555,870 870,1365 1365,1695
to gradient descent limitations. {It,also

1941
00:50:56,385 --> 00:50:57,800
0,195 195,360 360,665 895,1155 1155,1415
-} means that we can

1942
00:50:57,940 --> 00:50:59,355
0,335 335,575 575,830 830,1160 1160,1415
increase our learning rate because

1943
00:50:59,355 --> 00:51:00,315
0,135 135,315 315,570 570,795 795,960
we can trust each of

1944
00:51:00,315 --> 00:51:02,600
0,210 210,780 780,1110 1110,1445 1585,2285
those gradients much more efficiently,

1945
00:51:02,650 --> 00:51:04,380
0,395 395,800 800,1025 1025,1535 1535,1730
right? We're now averaging over

1946
00:51:04,380 --> 00:51:05,760
0,180 180,470 850,1185 1185,1290 1290,1380
a batch. It's going to

1947
00:51:05,760 --> 00:51:06,870
0,90 90,255 255,555 555,885 885,1110
be much more accurate than

1948
00:51:06,870 --> 00:51:08,070
0,150 150,555 555,885 885,1095 1095,1200
the stochastic {version.,So -} we

1949
00:51:08,070 --> 00:51:09,030
0,195 195,405 405,555 555,750 750,960
can increase that learning rate

1950
00:51:09,030 --> 00:51:10,740
0,240 240,465 465,720 720,1100 1390,1710
and actually learn faster as

1951
00:51:10,740 --> 00:51:11,480
0,320
well.|

1952
00:51:12,240 --> 00:51:14,015
0,320 320,575 575,860 860,1210 1440,1775
This allows us to also

1953
00:51:14,015 --> 00:51:16,475
0,645 645,1230 1230,1515 1515,1865 2095,2460
massively parallelize this entire algorithm

1954
00:51:16,475 --> 00:51:17,930
0,150 150,755 775,1080 1080,1245 1245,1455
and computation. {Right,,we -} can

1955
00:51:17,930 --> 00:51:19,610
0,210 210,360 360,855 855,1220 1300,1680
split up batches onto separate

1956
00:51:19,610 --> 00:51:22,150
0,380 1000,1400 1510,1830 1830,2145 2145,2540
workers and achieve even more

1957
00:51:22,260 --> 00:51:24,460
0,400 750,1385 1385,1610 1610,1850 1850,2200
significant speedups of this entire

1958
00:51:24,600 --> 00:51:26,650
0,400 540,905 905,1480 1560,1805 1805,2050
problem using gpus the last

1959
00:51:26,670 --> 00:51:28,700
0,400 870,1130 1130,1385 1385,1730 1730,2030
topic that I very, very

1960
00:51:28,700 --> 00:51:30,410
0,330 330,645 645,975 975,1370 1420,1710
briefly want to cover in

1961
00:51:30,410 --> 00:51:32,420
0,330 330,590 1060,1425 1425,1710 1710,2010
today's lecture is this topic

1962
00:51:32,420 --> 00:51:34,565
0,360 360,1170 1170,1500 1500,1770 1770,2145
of overfitting right when we're

1963
00:51:34,565 --> 00:51:36,640
0,455 565,840 840,1065 1065,1325 1675,2075
optimizing a neural network with.|

1964
00:51:37,320 --> 00:51:39,810
0,435 435,825 825,1220 1720,2115 2115,2490
Stochastic gradient descent, we have

1965
00:51:39,810 --> 00:51:41,325
0,300 300,620 670,975 975,1275 1275,1515
this challenge of what's called

1966
00:51:41,325 --> 00:51:44,085
0,315 315,690 690,1385 2275,2565 2565,2760
over fitting overfitting looks like

1967
00:51:44,085 --> 00:51:45,555
0,305 355,755 805,1110 1110,1305 1305,1470
this roughly right, so on

1968
00:51:45,555 --> 00:51:47,400
0,120 120,285 285,525 525,845
the left hand side.|

1969
00:51:47,440 --> 00:51:48,675
0,275 275,440 440,620 620,910 930,1235
We want to build a

1970
00:51:48,675 --> 00:51:49,800
0,270 270,525 525,810 810,1035 1035,1125
neural network, or let's say,

1971
00:51:49,800 --> 00:51:50,760
0,165 165,435 435,660 660,810 810,960
in general, we want to

1972
00:51:50,760 --> 00:51:52,020
0,150 150,360 360,600 600,900 900,1260
build a machine learning model

1973
00:51:52,020 --> 00:51:54,690
0,270 270,555 555,1190 1360,1760 2320,2670
that can accurately describe some

1974
00:51:54,690 --> 00:51:56,580
0,350 400,660 660,840 840,1160
patterns in our data.|

1975
00:51:56,580 --> 00:51:58,500
0,330 330,710 1150,1530 1530,1770 1770,1920
But remember, ultimately we don't

1976
00:51:58,500 --> 00:51:59,400
0,120 120,330 330,540 540,690 690,900
want to describe the patterns

1977
00:51:59,400 --> 00:52:01,320
0,210 210,435 435,765 765,1130 1300,1920
in our training data. {Ideally,,we

1978
00:52:01,320 --> 00:52:02,325
0,120 120,285 285,540 540,780 780,1005
-} want to define the

1979
00:52:02,325 --> 00:52:03,915
0,335 475,735 735,975 975,1275 1275,1590
patterns in our test {data.,Of

1980
00:52:03,915 --> 00:52:05,220
0,270 270,435 435,585 585,930 930,1305
-} course, we don't observe

1981
00:52:05,220 --> 00:52:06,555
0,300 300,585 585,795 795,1005 1005,1335
test data, we only observe

1982
00:52:06,555 --> 00:52:07,905
0,300 300,600 600,965 1015,1245 1245,1350
{training,data. -} So we have

1983
00:52:07,905 --> 00:52:09,470
0,180 180,435 435,780 780,1260 1260,1565
this challenge of extracting patterns

1984
00:52:09,550 --> 00:52:11,235
0,320 320,560 560,880 1110,1415 1415,1685
from training data and hoping

1985
00:52:11,235 --> 00:52:12,765
0,225 225,375 375,965 1135,1365 1365,1530
that they generalize to {our,test

1986
00:52:12,765 --> 00:52:14,565
0,255 255,575 1135,1440 1440,1605 1605,1800
-} data. So said {in,one

1987
00:52:14,565 --> 00:52:16,170
0,225 225,495 495,875 1195,1455 1455,1605
-} different way. We want

1988
00:52:16,170 --> 00:52:17,100
0,150 150,315 315,570 570,795 795,930
to build models that can

1989
00:52:17,100 --> 00:52:18,750
0,165 165,860 910,1170 1170,1365 1365,1650
learn representations from our training

1990
00:52:18,750 --> 00:52:21,105
0,350 730,1065 1065,1320 1320,1640 1660,2355
data that can still generalize

1991
00:52:21,105 --> 00:52:22,335
0,345 345,585 585,795 795,1020 1020,1230
even when we show them

1992
00:52:22,335 --> 00:52:24,410
0,255 255,525 525,1145 1225,1625 1675,2075
brand new {unseen,pieces -} of

1993
00:52:24,460 --> 00:52:26,430
0,320 320,640 1260,1610 1610,1835 1835,1970
test data. So assume that

1994
00:52:26,430 --> 00:52:27,105
0,105 105,225 225,360 360,510 510,675
you want to build a

1995
00:52:27,105 --> 00:52:29,790
0,275 445,840 840,1235 1705,2105 2395,2685
line that can describe or

1996
00:52:29,790 --> 00:52:30,900
0,180 180,345 345,615 615,885 885,1110
find the patterns in these

1997
00:52:30,900 --> 00:52:31,755
0,270 270,450 450,570 570,720 720,855
points that you can see

1998
00:52:31,755 --> 00:52:33,600
0,105 105,255 255,545 1375,1665 1665,1845
on the slide if you

1999
00:52:33,600 --> 00:52:34,920
0,210 210,420 420,675 675,990 990,1320
have a very simple neural

2000
00:52:34,920 --> 00:52:36,165
0,260 400,675 675,885 885,1095 1095,1245
network, which is just a

2001
00:52:36,165 --> 00:52:38,540
0,225 225,575 895,1185 1185,1475
single line straight line.|

2002
00:52:38,610 --> 00:52:41,530
0,400 510,910 1290,1690 1770,2170 2520,2920
You can describe this data

2003
00:52:41,820 --> 00:52:43,685
0,920 920,1250 1250,1490 1490,1655 1655,1865
suboptimally right, because the data

2004
00:52:43,685 --> 00:52:45,230
0,195 195,360 360,1025 1135,1425 1425,1545
here is nonlinear, you're not

2005
00:52:45,230 --> 00:52:46,490
0,180 180,405 405,795 795,1035 1035,1260
going to accurately capture all

2006
00:52:46,490 --> 00:52:48,250
0,120 120,240 240,830 940,1230 1230,1760
of the nuances and subtleties

2007
00:52:48,510 --> 00:52:50,090
0,290 290,500 500,755 755,1090 1260,1580
in this data set that's

2008
00:52:50,090 --> 00:52:50,870
0,90 90,195 195,345 345,555 555,780
on the left hand side.

2009
00:52:50,870 --> 00:52:51,590
0,180 180,315 315,480 480,615 615,720
{If,you -} move to the

2010
00:52:51,590 --> 00:52:53,180
0,165 165,390 390,710 1060,1380 1380,1590
right hand side, you can

2011
00:52:53,180 --> 00:52:54,070
0,135 135,240 240,390 390,585 585,890
see a much more complicated

2012
00:52:54,330 --> 00:52:55,895
0,400 450,755 755,935 935,1250 1250,1565
model, but here you're actually

2013
00:52:55,895 --> 00:52:58,120
0,365 505,1080 1080,1380 1380,1635 1635,2225
over expressive, you're too expressive,

2014
00:52:58,200 --> 00:52:59,525
0,275 275,515 515,1010 1010,1160 1160,1325
and you're capturing kind of

2015
00:52:59,525 --> 00:53:02,465
0,180 180,810 810,1175 1285,1925 2155,2940
the nuances, the spurious nuances

2016
00:53:02,465 --> 00:53:03,890
0,300 300,525 525,795 795,1110 1110,1425
in your training data that

2017
00:53:03,890 --> 00:53:06,230
0,345 345,740 880,1260 1260,1640 2050,2340
are actually not representative of

2018
00:53:06,230 --> 00:53:07,460
0,195 195,435 435,770
your test data.|

2019
00:53:07,790 --> 00:53:08,860
0,620 620,755 755,875 875,980 980,1070
Ideally you want to end

2020
00:53:08,860 --> 00:53:09,595
0,120 120,270 270,405 405,570 570,735
up with the model in

2021
00:53:09,595 --> 00:53:10,980
0,90 90,335 595,870 870,1065 1065,1385
the middle, which is basically

2022
00:53:11,150 --> 00:53:12,190
0,245 245,425 425,665 665,830 830,1040
the middle ground, right? It's

2023
00:53:12,190 --> 00:53:13,885
0,165 165,435 435,800 1300,1530 1530,1695
not too complex and it's

2024
00:53:13,885 --> 00:53:15,370
0,180 180,515 535,935 955,1275 1275,1485
not too simple. {It,still -}

2025
00:53:15,370 --> 00:53:16,180
0,180 180,345 345,510 510,660 660,810
gives you what you want

2026
00:53:16,180 --> 00:53:18,330
0,225 225,465 465,770 1180,1580 1750,2150
to perform well and even

2027
00:53:18,350 --> 00:53:19,225
0,260 260,380 380,500 500,665 665,875
when you give it brand

2028
00:53:19,225 --> 00:53:20,575
0,180 180,455 685,960 960,1140 1140,1350
new {data.,So -} to address

2029
00:53:20,575 --> 00:53:22,450
0,210 210,515 745,1170 1170,1475 1525,1875
this problem, let's briefly talk

2030
00:53:22,450 --> 00:53:25,870
0,350 580,960 960,1200 1200,2060 2650,3420
about what's {called,regularization. -} Regularization

2031
00:53:25,870 --> 00:53:27,160
0,210 210,345 345,620 790,1110 1110,1290
is a technique that you

2032
00:53:27,160 --> 00:53:28,660
0,260 340,740 790,1035 1035,1215 1215,1500
can introduce to your training

2033
00:53:28,660 --> 00:53:31,290
0,350 880,1170 1170,1730 1960,2295 2295,2630
pipeline to discourage complex models

2034
00:53:31,640 --> 00:53:32,900
0,275 275,470 470,790
from being learned.|

2035
00:53:33,320 --> 00:53:34,830
0,350 350,605 605,860 860,1115 1115,1510
Now, as we've seen before,

2036
00:53:34,910 --> 00:53:36,745
0,290 290,485 485,755 755,1120 1470,1835
this is really critical because

2037
00:53:36,745 --> 00:53:38,605
0,360 360,635 655,1055 1105,1500 1500,1860
neural networks are extremely large

2038
00:53:38,605 --> 00:53:40,470
0,365 475,735 735,975 975,1355 1435,1865
models, they are extremely prone

2039
00:53:40,760 --> 00:53:43,165
0,365 365,680 680,1150 1470,1760 1760,2405
to over fitting, so regularization

2040
00:53:43,165 --> 00:53:44,760
0,195 195,405 405,735 735,975 975,1595
and having techniques for regularization

2041
00:53:45,020 --> 00:53:47,455
0,400 720,1025 1025,1660 1920,2240 2240,2435
has extreme implications towards the

2042
00:53:47,455 --> 00:53:49,120
0,275 595,885 885,1125 1125,1380 1380,1665
success of neural networks and

2043
00:53:49,120 --> 00:53:50,785
0,210 210,435 435,1010 1030,1380 1380,1665
having them generalize beyond training

2044
00:53:50,785 --> 00:53:52,420
0,335 625,915 915,1140 1140,1365 1365,1635
data far into our testing

2045
00:53:52,420 --> 00:53:53,320
0,380
domain.|

2046
00:53:53,810 --> 00:53:55,495
0,260 260,425 425,730 990,1390 1410,1685
The most popular technique for

2047
00:53:55,495 --> 00:53:57,370
0,665 895,1185 1185,1380 1380,1635 1635,1875
regularization in deep learning is

2048
00:53:57,370 --> 00:53:58,960
0,270 270,830 970,1245 1245,1425 1425,1590
called dropout. {And,the -} idea

2049
00:53:58,960 --> 00:54:00,415
0,135 135,510 510,765 765,1130 1180,1455
of dropout is is actually

2050
00:54:00,415 --> 00:54:02,500
0,210 210,545 1225,1560 1560,1950 1950,2085
very simple. Let's revisit it

2051
00:54:02,500 --> 00:54:03,910
0,210 210,465 465,690 690,980 1060,1410
by drawing this picture of

2052
00:54:03,910 --> 00:54:04,915
0,240 240,480 480,675 675,870 870,1005
deep neural networks that we

2053
00:54:04,915 --> 00:54:06,240
0,150 150,390 390,690 690,1080 1080,1325
saw earlier in today's {lecture.,In

2054
00:54:06,890 --> 00:54:08,590
0,320 320,755 755,995 995,1330 1380,1700
{-},dropout. -} During training, we

2055
00:54:08,590 --> 00:54:11,350
0,320 460,1305 1305,1700 1840,2205 2205,2760
essentially randomly select some subset

2056
00:54:11,350 --> 00:54:12,580
0,225 225,405 405,825 825,1065 1065,1230
of the neurons in this

2057
00:54:12,580 --> 00:54:13,760
0,240 240,500
neural network.|

2058
00:54:13,760 --> 00:54:16,010
0,260 490,890 1060,1395 1395,1730 1900,2250
And we try to tune

2059
00:54:16,010 --> 00:54:17,390
0,195 195,470 580,885 885,1095 1095,1380
them out with some random

2060
00:54:17,390 --> 00:54:18,860
0,600 600,855 855,1020 1020,1260 1260,1470
probabilities. {So,,for -} example, we

2061
00:54:18,860 --> 00:54:20,560
0,180 180,500 610,930 930,1410 1410,1700
can select this subset of

2062
00:54:20,700 --> 00:54:22,295
0,320 320,725 725,965 965,1130 1130,1595
of {neurons.,We -} can randomly

2063
00:54:22,295 --> 00:54:23,300
0,195 195,360 360,480 480,600 600,1005
select them with a {probability,of

2064
00:54:23,300 --> 00:54:25,925
0,330 330,980 1630,2030 2080,2355 2355,2625
-} 50% and with that

2065
00:54:25,925 --> 00:54:27,770
0,605 685,975 975,1500 1500,1695 1695,1845
probability, we randomly turn them

2066
00:54:27,770 --> 00:54:29,470
0,260 370,645 645,920 1090,1395 1395,1700
off or on on different

2067
00:54:29,820 --> 00:54:31,820
0,580 630,890 890,1100 1100,1450
iterations of our training.|

2068
00:54:32,170 --> 00:54:35,190
0,400 870,1250 1250,1595 1595,1960 2340,3020
So this is essentially forcing

2069
00:54:35,190 --> 00:54:36,420
0,240 240,465 465,735 735,1005 1005,1230
the neural network to learn.

2070
00:54:36,420 --> 00:54:37,185
0,210 210,360 360,525 525,660 660,765
{You,can -} think of an

2071
00:54:37,185 --> 00:54:39,360
0,815 865,1170 1170,1470 1470,1865 1885,2175
ensemble of different models on

2072
00:54:39,360 --> 00:54:41,010
0,225 225,740 1000,1350 1350,1500 1500,1650
every iteration. It's going to

2073
00:54:41,010 --> 00:54:42,945
0,260 700,1100 1420,1680 1680,1815 1815,1935
be exposed to kind of

2074
00:54:42,945 --> 00:54:44,820
0,105 105,345 345,725 925,1575 1575,1875
a different model internally than

2075
00:54:44,820 --> 00:54:45,510
0,165 165,270 270,405 405,555 555,690
the one it had on

2076
00:54:45,510 --> 00:54:46,365
0,90 90,225 225,615 615,780 780,855
the last {iteration.,So -} it

2077
00:54:46,365 --> 00:54:47,370
0,165 165,345 345,570 570,840 840,1005
has to learn how to

2078
00:54:47,370 --> 00:54:49,970
0,260 670,1065 1065,1730 2020,2310 2310,2600
build internal pathways to process

2079
00:54:50,050 --> 00:54:51,660
0,260 260,520 570,970 1260,1505 1505,1610
the {same,information. -} And it

2080
00:54:51,660 --> 00:54:53,025
0,315 315,540 540,795 795,1125 1125,1365
can't rely on information that

2081
00:54:53,025 --> 00:54:54,680
0,195 195,515 565,885 885,1140 1140,1655
it learned {on,previous -} iterations.

2082
00:54:54,940 --> 00:54:56,355
0,400 480,785 785,1040 1040,1265 1265,1415
So it forces it to

2083
00:54:56,355 --> 00:54:58,065
0,150 150,425 595,995 1165,1455 1455,1710
kind of capture some deeper

2084
00:54:58,065 --> 00:54:59,415
0,330 330,615 615,810 810,1200 1200,1350
meaning within the pathways {of,the

2085
00:54:59,415 --> 00:55:00,855
0,135 135,345 345,605 1015,1290 1290,1440
-} neural network. And this

2086
00:55:00,855 --> 00:55:02,190
0,135 135,315 315,570 570,905 985,1335
can be extremely powerful because

2087
00:55:02,190 --> 00:55:03,480
0,270 270,480 480,705 705,1080 1080,1290
number one, it lowers the

2088
00:55:03,480 --> 00:55:05,240
0,320 790,1095 1095,1260 1260,1485 1485,1760
capacity of the neural network

2089
00:55:05,260 --> 00:55:07,260
0,400 960,1370 1370,1655 1655,1775 1775,2000
significantly. {You're,lowering -} it by

2090
00:55:07,260 --> 00:55:09,110
0,330 330,950 1060,1320 1320,1515 1515,1850
roughly 50% in this example.|

2091
00:55:10,310 --> 00:55:11,320
0,350 350,605 605,770 770,875 875,1010
But also because it makes

2092
00:55:11,320 --> 00:55:12,670
0,165 165,390 390,630 630,920 1060,1350
them easier to train, because

2093
00:55:12,670 --> 00:55:14,440
0,165 165,360 360,680 970,1530 1530,1770
the number of weights that

2094
00:55:14,440 --> 00:55:15,745
0,255 255,765 765,930 930,1095 1095,1305
have gradients in this case

2095
00:55:15,745 --> 00:55:16,915
0,255 255,540 540,780 780,930 930,1170
is also reduced, so it's

2096
00:55:16,915 --> 00:55:17,950
0,165 165,315 315,600 600,870 870,1035
actually much faster to train

2097
00:55:17,950 --> 00:55:19,660
0,290 490,795 795,1100
them as well.|

2098
00:55:20,200 --> 00:55:21,990
0,350 350,560 560,695 695,970 1500,1790
Now like I mentioned, on

2099
00:55:21,990 --> 00:55:24,465
0,225 225,705 705,1100 1360,2090 2170,2475
every iteration we randomly drop

2100
00:55:24,465 --> 00:55:25,710
0,305 385,660 660,885 885,1095 1095,1245
out a different set of

2101
00:55:25,710 --> 00:55:27,135
0,360 360,710 820,1065 1065,1185 1185,1425
neurons, right? And that helps

2102
00:55:27,135 --> 00:55:29,450
0,225 225,435 435,945 945,1265 1915,2315
the data generalize better. {And,the

2103
00:55:29,650 --> 00:55:31,545
0,290 290,560 560,1235 1235,1600 1620,1895
-} second regularization techniques, which

2104
00:55:31,545 --> 00:55:32,600
0,210 210,390 390,495 495,705 705,1055
is actually a very broad

2105
00:55:32,920 --> 00:55:34,905
0,755 755,1120 1200,1535 1535,1745 1745,1985
regularization techniques far beyond neural

2106
00:55:34,905 --> 00:55:37,185
0,275 745,1145 1225,1605 1605,1950 1950,2280
networks is simply called early

2107
00:55:37,185 --> 00:55:40,310
0,365 1105,1505 1765,2055 2055,2345 2725,3125
{stopping.,Now -} we know the

2108
00:55:40,480 --> 00:55:43,230
0,400 690,1090 1350,2210 2210,2495 2495,2750
definition of overfitting is simply

2109
00:55:43,230 --> 00:55:44,685
0,225 225,390 390,680 700,1080 1080,1455
when our model starts to

2110
00:55:44,685 --> 00:55:46,580
0,395 625,1025 1075,1350 1350,1560 1560,1895
represent basically the training data

2111
00:55:46,600 --> 00:55:48,585
0,400 690,1090 1110,1385 1385,1640 1640,1985
more than the testing data.

2112
00:55:48,585 --> 00:55:49,965
0,315 315,495 495,690 690,1200 1200,1380
That's really what overfitting comes

2113
00:55:49,965 --> 00:55:50,990
0,225 225,405 405,510 510,690 690,1025
down to at {its,core. -}

2114
00:55:51,700 --> 00:55:52,920
0,260 260,410 410,620 620,935 935,1220
If we set aside some

2115
00:55:52,920 --> 00:55:54,150
0,180 180,360 360,600 600,950 970,1230
of the training data to

2116
00:55:54,150 --> 00:55:55,620
0,240 240,870 870,1050 1050,1185 1185,1470
use separately that we don't

2117
00:55:55,620 --> 00:55:56,340
0,180 180,345 345,465 465,585 585,720
train on it, we can

2118
00:55:56,340 --> 00:55:57,710
0,260 460,705 705,825 825,1035 1035,1370
use kind of a testing

2119
00:55:58,930 --> 00:56:01,365
0,350 350,700 1230,1775 1775,2120 2120,2435
data set, synthetic {testing,data -}

2120
00:56:01,365 --> 00:56:03,090
0,195 195,345 345,495 495,755 1465,1725
set. In some ways we

2121
00:56:03,090 --> 00:56:04,440
0,165 165,470 610,900 900,1095 1095,1350
can monitor how our network

2122
00:56:04,440 --> 00:56:06,230
0,270 270,585 585,900 900,1125 1125,1790
is learning on this {unseen,portion

2123
00:56:06,370 --> 00:56:07,920
0,400 540,845 845,1130 1130,1385 1385,1550
-} of data. So for

2124
00:56:07,920 --> 00:56:09,255
0,255 255,495 495,770 790,1125 1125,1335
example, we can over the

2125
00:56:09,255 --> 00:56:10,155
0,165 165,345 345,570 570,765 765,900
course of training, we can

2126
00:56:10,155 --> 00:56:11,985
0,275 355,755 985,1335 1335,1635 1635,1830
basically plot the performance of

2127
00:56:11,985 --> 00:56:13,060
0,150 150,455
our network.|

2128
00:56:13,060 --> 00:56:14,155
0,195 195,405 405,585 585,810 810,1095
On both the training set

2129
00:56:14,155 --> 00:56:15,100
0,240 240,405 405,540 540,720 720,945
as well as our held

2130
00:56:15,100 --> 00:56:17,050
0,285 285,570 570,890 1360,1695 1695,1950
out test set and as

2131
00:56:17,050 --> 00:56:18,280
0,180 180,360 360,600 600,920 940,1230
the network is trained, we're

2132
00:56:18,280 --> 00:56:19,195
0,105 105,255 255,390 390,645 645,915
going to see that first

2133
00:56:19,195 --> 00:56:20,290
0,105 105,210 210,375 375,665 745,1095
of all these both decrease.

2134
00:56:20,290 --> 00:56:21,205
0,210 210,420 420,585 585,735 735,915
{But,there's -} going to be

2135
00:56:21,205 --> 00:56:23,305
0,240 240,545 1135,1535 1585,1860 1860,2100
a point where the loss

2136
00:56:23,305 --> 00:56:26,130
0,605 1195,1595 1705,2055 2055,2405 2425,2825
plateaus and starts to {increase.,The

2137
00:56:26,180 --> 00:56:27,505
0,305 305,575 575,875 875,1130 1130,1325
-} training loss will actually

2138
00:56:27,505 --> 00:56:28,420
0,150 150,345 345,570 570,735 735,915
start {to,increase. -} This is

2139
00:56:28,420 --> 00:56:29,485
0,255 255,450 450,660 660,885 885,1065
exactly the point where you

2140
00:56:29,485 --> 00:56:31,000
0,225 225,390 390,935 1075,1350 1350,1515
start to overfit because now

2141
00:56:31,000 --> 00:56:33,730
0,255 255,510 510,890 1120,1520 2410,2730
you're starting to have, sorry,

2142
00:56:33,730 --> 00:56:34,540
0,165 165,270 270,405 405,585 585,810
that was {the,test -} loss.

2143
00:56:34,540 --> 00:56:35,680
0,195 195,375 375,660 660,930 930,1140
The test loss actually starts

2144
00:56:35,680 --> 00:56:36,910
0,255 255,585 585,855 855,1020 1020,1230
to increase because now you're

2145
00:56:36,910 --> 00:56:38,010
0,180 180,330 330,690 690,825 825,1100
starting to overfit {on,your -}

2146
00:56:38,030 --> 00:56:40,620
0,400 540,940 1530,1880 1880,2210 2210,2590
training data. This pattern basically

2147
00:56:40,730 --> 00:56:41,770
0,335 335,515 515,620 620,800 800,1040
continues for the rest of

2148
00:56:41,770 --> 00:56:42,720
0,320
training.|

2149
00:56:42,720 --> 00:56:44,235
0,320 670,960 960,1140 1140,1305 1305,1515
And this is the point

2150
00:56:44,235 --> 00:56:44,910
0,180 180,285 285,435 435,570 570,675
that I want you to

2151
00:56:44,910 --> 00:56:46,710
0,210 210,560 670,990 990,1545 1545,1800
focus on. {This,middlepoint -} is

2152
00:56:46,710 --> 00:56:47,640
0,240 240,435 435,585 585,735 735,930
where we need to stop

2153
00:56:47,640 --> 00:56:49,010
0,315 315,585 585,795 795,1050 1050,1370
{training.,Because -} after this point,

2154
00:56:49,540 --> 00:56:51,150
0,400 510,845 845,1115 1115,1370 1370,1610
assuming that this test set

2155
00:56:51,150 --> 00:56:53,445
0,195 195,345 345,600 600,1400 1960,2295
is a valid representation of

2156
00:56:53,445 --> 00:56:54,960
0,240 240,450 450,675 675,995 1225,1515
the true test set, this

2157
00:56:54,960 --> 00:56:56,040
0,180 180,375 375,645 645,870 870,1080
is the place where the

2158
00:56:56,040 --> 00:56:57,045
0,420 420,510 510,615 615,795 795,1005
accuracy of the model will

2159
00:56:57,045 --> 00:56:58,410
0,195 195,405 405,695 1015,1260 1260,1365
only {get,worse. -} So this

2160
00:56:58,410 --> 00:56:59,145
0,150 150,330 330,480 480,600 600,735
is where we would want

2161
00:56:59,145 --> 00:57:00,045
0,120 120,315 315,540 540,690 690,900
to early stop our model

2162
00:57:00,045 --> 00:57:02,580
0,225 225,875 1105,1440 1440,1775
and regularize the performance.|

2163
00:57:02,970 --> 00:57:03,755
0,245 245,335 335,455 455,590 590,785
And we can see that

2164
00:57:03,755 --> 00:57:05,560
0,315 315,840 840,1145 1165,1485 1485,1805
stopping anytime before this point

2165
00:57:06,060 --> 00:57:07,490
0,400 510,785 785,950 950,1175 1175,1430
is also not good. We're

2166
00:57:07,490 --> 00:57:08,660
0,120 120,330 330,525 525,675 675,1170
going to produce an underfit

2167
00:57:08,660 --> 00:57:09,650
0,290 370,630 630,765 765,885 885,990
model where we could have

2168
00:57:09,650 --> 00:57:10,700
0,120 120,240 240,435 435,765 765,1050
had a better model on

2169
00:57:10,700 --> 00:57:12,215
0,165 165,360 360,680 1090,1350 1350,1515
the test data. {But,it's -}

2170
00:57:12,215 --> 00:57:13,070
0,165 165,360 360,525 525,690 690,855
this trade off, right? You

2171
00:57:13,070 --> 00:57:14,000
0,285 285,465 465,645 645,810 810,930
can't stop too late, and

2172
00:57:14,000 --> 00:57:14,885
0,105 105,360 360,525 525,690 690,885
you can't stop too early

2173
00:57:14,885 --> 00:57:15,960
0,240 240,545
as well.|

2174
00:57:17,340 --> 00:57:18,820
0,350 350,725 725,1010 1010,1175 1175,1480
So I'll conclude this lecture

2175
00:57:19,410 --> 00:57:20,870
0,290 290,455 455,920 920,1220 1220,1460
by just summarizing these three

2176
00:57:20,870 --> 00:57:22,090
0,225 225,510 510,765 765,975 975,1220
key points that we've covered

2177
00:57:22,170 --> 00:57:24,305
0,290 290,680 680,940 1560,1865 1865,2135
in today's lecture so far.

2178
00:57:24,305 --> 00:57:25,625
0,285 285,570 570,795 795,1065 1065,1320
{So,we -} first covered these

2179
00:57:25,625 --> 00:57:27,290
0,305 445,825 825,1155 1155,1395 1395,1665
fundamental building blocks of all

2180
00:57:27,290 --> 00:57:28,595
0,375 375,650 670,945 945,1125 1125,1305
neural networks, which is the

2181
00:57:28,595 --> 00:57:30,605
0,225 225,615 615,795 795,1445 1645,2010
single neuron, the perceptron. We've

2182
00:57:30,605 --> 00:57:32,110
0,195 195,405 405,690 690,1085 1105,1505
built these up into larger

2183
00:57:32,730 --> 00:57:34,550
0,500 500,965 965,1175 1175,1450 1530,1820
neural layers, and then from

2184
00:57:34,550 --> 00:57:35,885
0,270 270,615 615,870 870,1140 1140,1335
their neural networks and deep

2185
00:57:35,885 --> 00:57:37,730
0,270 270,545 1165,1500 1500,1665 1665,1845
neural networks, we've learned how

2186
00:57:37,730 --> 00:57:39,125
0,120 120,330 330,645 645,1010 1060,1395
we can train these, apply

2187
00:57:39,125 --> 00:57:40,595
0,270 270,510 510,765 765,1115 1135,1470
them to data sets back,

2188
00:57:40,595 --> 00:57:42,350
0,465 465,645 645,935 1195,1485 1485,1755
propagate through {them.,And -} we've

2189
00:57:42,350 --> 00:57:44,140
0,165 165,405 405,770 850,1250 1390,1790
seen some trips, tips and

2190
00:57:44,220 --> 00:57:46,270
0,545 545,935 935,1420 1440,1745 1745,2050
tricks for optimizing these systems

2191
00:57:46,470 --> 00:57:47,700
0,290 290,425 425,670
end to end.|

2192
00:57:48,010 --> 00:57:49,095
0,260 260,365 365,530 530,815 815,1085
In the next lecture, we'll

2193
00:57:49,095 --> 00:57:50,730
0,150 150,455 565,1125 1125,1395 1395,1635
hear from ava on deep

2194
00:57:50,730 --> 00:57:52,320
0,315 315,855 855,1160 1180,1455 1455,1590
sequence modeling using r and

2195
00:57:52,320 --> 00:57:55,400
0,260 700,1035 1035,1370 2350,2715 2715,3080
n, and specifically this very

2196
00:57:55,420 --> 00:57:57,300
0,400 660,1010 1010,1265 1265,1535 1535,1880
exciting new type of model

2197
00:57:57,300 --> 00:57:59,520
0,270 270,560 580,1160 1270,1670 1900,2220
called the transformer architecture and

2198
00:57:59,520 --> 00:58:02,355
0,315 315,1100 1600,2000 2080,2445 2445,2835
attention mechanisms. {So,maybe -} let's

2199
00:58:02,355 --> 00:58:03,405
0,270 270,465 465,645 645,855 855,1050
resume the class in about

2200
00:58:03,405 --> 00:58:04,380
0,225 225,465 465,720 720,900 900,975
five minutes after we have

2201
00:58:04,380 --> 00:58:05,445
0,105 105,300 300,495 495,735 735,1065
a chance to swap speakers,

2202
00:58:05,445 --> 00:58:07,095
0,395 865,1140 1140,1305 1305,1470 1470,1650
and thank you so much

2203
00:58:07,095 --> 00:58:07,850
0,135 135,210 210,315 315,465 465,755
for all of your attention.|

