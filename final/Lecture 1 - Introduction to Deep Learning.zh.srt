1
00:00:09,220 --> 00:00:10,430
大家下午好，

2
00:00:10,600 --> 00:00:11,910
感谢大家今天的参与，

3
00:00:11,910 --> 00:00:13,610
我的名字是 Alexander Amini，

4
00:00:14,200 --> 00:00:17,090
我将是你们今年的课程组织者之一，

5
00:00:17,140 --> 00:00:18,260
和 Ava 一起，

6
00:00:18,640 --> 00:00:21,260
我们非常兴奋地

7
00:00:21,310 --> 00:00:24,260
向你们所有人介绍深度学习导论，

8
00:00:24,370 --> 00:00:26,925
MIT 的深度学习导论是

9
00:00:26,925 --> 00:00:31,680
一个非常有趣、令人兴奋和快节奏的项目，

10
00:00:31,680 --> 00:00:34,460
让我首先给你们一点背景，

11
00:00:34,900 --> 00:00:38,240
让你们了解我们今年所做的工作和你们将学到的东西。

12
00:00:38,710 --> 00:00:41,445
所以，本周的深度学习导论，

13
00:00:41,445 --> 00:00:44,420
我们将涉及大量的材料，在短短一周内，

14
00:00:44,620 --> 00:00:48,915
你将学习这个非常吸引人和令人兴奋的领域的基础知识，

15
00:00:48,915 --> 00:00:50,930
关于深度学习和人工智能，

16
00:00:51,880 --> 00:00:55,590
更重要的是，你将亲身体验，

17
00:00:55,850 --> 00:00:58,470
巩固了你在课程中学到的内容，

18
00:00:58,670 --> 00:01:00,660
作为动手软件实验的一部分。

19
00:01:01,570 --> 00:01:02,750
现在，在过去的十年里，

20
00:01:03,100 --> 00:01:06,225
人工智能和深度学习有了巨大的复兴，

21
00:01:06,225 --> 00:01:08,210
取得了令人难以置信的成功，

22
00:01:08,410 --> 00:01:11,085
许多问题，即使在十年前，

23
00:01:11,085 --> 00:01:14,330
我们还认为这些问题在不久的将来甚至无法真正解决，

24
00:01:14,470 --> 00:01:17,540
现在，我们用深度学习很轻松地解决。

25
00:01:18,220 --> 00:01:20,910
在过去的这一年，尤其是 2022 年，

26
00:01:20,910 --> 00:01:23,960
对于深度学习的进步来说，是令人难以置信的一年，

27
00:01:24,490 --> 00:01:25,335
我想说的是，

28
00:01:25,335 --> 00:01:29,910
过去的一年成为生成式深度学习的一年，

29
00:01:29,930 --> 00:01:33,390
使用深度学习来生成全新类型的数据，

30
00:01:33,410 --> 00:01:34,710
我以前从未见过，

31
00:01:34,880 --> 00:01:37,530
在现实中也从未存在。

32
00:01:37,610 --> 00:01:39,565
事实上，我想开始这门课，

33
00:01:39,565 --> 00:01:42,390
通过展示几年前我们是如何开始这门课的，

34
00:01:42,860 --> 00:01:45,690
我马上会播放这段视频，

35
00:01:46,190 --> 00:01:50,280
这段视频实际上是这门课的介绍性视频，

36
00:01:50,300 --> 00:01:54,900
在某种程度上证明了我所说的想法。

37
00:01:55,210 --> 00:01:57,740
所以，让我先停下来，播放这段视频。

38
00:02:01,050 --> 00:02:06,160
大家好，欢迎来到 MIT 6.S191 ，

39
00:02:06,740 --> 00:02:12,420
MIT 关于深度学习的官方入门课程，

40
00:02:13,950 --> 00:02:17,710
深度学习彻底改变了很多领域，

41
00:02:18,300 --> 00:02:22,550
从机器人到医学，以及介于两者之间的一切，

42
00:02:23,760 --> 00:02:26,710
你将学习这个领域的基础知识，

43
00:02:27,030 --> 00:02:31,780
以及如何构建一些令人难以置信的算法，

44
00:02:32,850 --> 00:02:38,350
事实上，这个演讲和视频都不是真的，

45
00:02:39,080 --> 00:02:43,600
是使用深度学习和人工智能创建的，

46
00:02:45,310 --> 00:02:47,840
在这节课上，你将学习如何，

47
00:02:48,880 --> 00:02:51,240
今天很荣幸能和你们交谈，

48
00:02:51,800 --> 00:02:52,560
我希望你们喜欢这门课。

49
00:02:56,770 --> 00:02:58,400
所以，为了防止你不能区分，

50
00:02:58,690 --> 00:03:02,760
这个视频和它的整个音频都不是真的，

51
00:03:02,760 --> 00:03:06,120
它是由深度学习算法综合生成的，

52
00:03:06,120 --> 00:03:09,090
当我们几年前介绍这个课程的时候，

53
00:03:09,090 --> 00:03:11,060
这个视频是几年前制作的，

54
00:03:11,410 --> 00:03:13,250
但即使在几年前，

55
00:03:13,900 --> 00:03:17,505
当我们将其放到 YouTube 上时，像病毒一样传播开来，

56
00:03:17,505 --> 00:03:18,660
人们很喜欢这段视频，

57
00:03:18,660 --> 00:03:23,630
他们对视频和音频的感觉的真实性很感兴趣，

58
00:03:24,670 --> 00:03:27,470
完全是由计算机通过算法生成的，

59
00:03:27,970 --> 00:03:32,000
人们被这些方法的力量和现实主义所震惊，

60
00:03:32,020 --> 00:03:33,860
这是几年前的事了。

61
00:03:34,260 --> 00:03:38,030
现在，快进到今天，深度学习今天的状态，

62
00:03:38,350 --> 00:03:45,290
我们已经看到深度学习以前所未有的速度加速，

63
00:03:46,030 --> 00:03:51,080
事实上，我们现在不仅可以使用深度学习来生成人脸图像，

64
00:03:51,460 --> 00:03:53,720
还可以生成完全合成的环境，

65
00:03:53,830 --> 00:03:56,930
我们可以完全在模拟中训练自动驾驶车辆，

66
00:03:57,130 --> 00:04:01,040
并将它们无缝地部署在真实世界的全尺寸车辆上，

67
00:04:01,240 --> 00:04:04,410
你在这里看到的视频实际上是来自一个数据驱动的模拟器，

68
00:04:04,410 --> 00:04:09,020
来自名为 VISTA 神经网络生成的，是我们在 MIT 建造的，

69
00:04:09,550 --> 00:04:11,325
并向公众开放了源代码，

70
00:04:11,325 --> 00:04:15,710
所以，你们所有人都可以训练和构建自动驾驶汽车的未来。

71
00:04:16,210 --> 00:04:18,405
当然，它也远远超出了这一点，

72
00:04:18,405 --> 00:04:23,175
深度学习可以直接从我们所说的话生成内容，

73
00:04:23,175 --> 00:04:26,930
我们传达给它的语言，我们说的提示，

74
00:04:27,500 --> 00:04:31,870
深度学习可以对自然语言中的提示进行推理，比如英语，

75
00:04:32,070 --> 00:04:37,420
然后根据我们指定的内容，指导和控制生成什么，

76
00:04:38,090 --> 00:04:40,530
我们已经看到了我们可以生成的例子，

77
00:04:40,820 --> 00:04:44,380
例如，在现实中从未有过的东西，

78
00:04:44,380 --> 00:04:49,230
我们可以让神经网络生成一张宇航员骑马的照片，

79
00:04:49,760 --> 00:04:54,010
它可以想象这会是什么样子，

80
00:04:54,010 --> 00:04:55,380
即使这张照片，

81
00:04:55,910 --> 00:04:58,150
不仅这张照片以前从未发生过，

82
00:04:58,150 --> 00:05:01,780
而且我认为以前没有发生过宇航员骑马的照片，

83
00:05:01,780 --> 00:05:05,050
所以，在这种情况下，甚至没有训练数据可供参考，

84
00:05:05,050 --> 00:05:07,060
我个人最喜欢的是，

85
00:05:07,060 --> 00:05:10,860
我们如何不仅可以构建能够生成图像和视频的软件，

86
00:05:11,030 --> 00:05:14,640
而且还可以构建能够生成软件的软件，

87
00:05:14,660 --> 00:05:17,910
我们也可以有能够接受语言提示的算法，

88
00:05:18,080 --> 00:05:19,560
例如，像这样的提示，

89
00:05:19,850 --> 00:05:23,460
在 TensorFlow 中编写代码来生成或训练神经网络，

90
00:05:23,870 --> 00:05:27,925
它不仅会编写代码并创建神经网络，

91
00:05:27,925 --> 00:05:32,170
而且它还会有能力对它生成的代码进行推理，

92
00:05:32,170 --> 00:05:35,430
并一步步地向你解释过程和程序，

93
00:05:35,930 --> 00:05:37,525
从头到尾，

94
00:05:37,525 --> 00:05:40,790
这样你也可以学习如何完成这一过程。

95
00:05:41,520 --> 00:05:44,200
我认为这些例子中的一些

96
00:05:44,220 --> 00:05:47,920
确实突显了深度学习和这些方法，

97
00:05:48,030 --> 00:05:50,285
在我们开始这门课程的过去六年里，

98
00:05:50,285 --> 00:05:54,430
你在几年前的介绍性视频中看到了这个例子，

99
00:05:54,450 --> 00:05:56,570
但现在我们看到了如此令人难以置信的进步，

100
00:05:56,570 --> 00:06:00,290
在我看来，这门课程最令人惊叹的部分是，

101
00:06:00,290 --> 00:06:05,110
在这一周内，我们将从头开始，从今天开始，

102
00:06:05,370 --> 00:06:07,720
所有的基础构件，

103
00:06:07,890 --> 00:06:12,430
将让你理解，并使所有这些令人惊叹的进步成为可能。

104
00:06:13,730 --> 00:06:18,265
有了这些，希望你们现在对这门课所教的内容感到很兴奋，

105
00:06:18,265 --> 00:06:21,840
我想现在从退后一步开始，

106
00:06:22,280 --> 00:06:24,790
介绍一些术语，

107
00:06:24,790 --> 00:06:26,785
到目前为止我抛出的，

108
00:06:26,785 --> 00:06:28,650
深度学习，人工智能，

109
00:06:28,670 --> 00:06:30,060
这些东西是什么意思。

110
00:06:30,800 --> 00:06:34,530
所以首先，我想花点时间，

111
00:06:35,430 --> 00:06:36,820
说一下智能，

112
00:06:36,840 --> 00:06:39,440
以及智能的核心含义。

113
00:06:39,440 --> 00:06:43,960
对我来说，智能是一种处理信息的能力，

114
00:06:44,460 --> 00:06:49,120
这样我们就可以用它来指导我们未来的决定或采取的行动，

115
00:06:49,810 --> 00:06:55,250
现在，人工智能领域就是我们构建算法的能力，

116
00:06:55,480 --> 00:06:57,710
人工算法，可以完成这个，

117
00:06:57,910 --> 00:07:00,860
处理信息，指导未来的决策。

118
00:07:01,540 --> 00:07:03,975
机器学习是人工智能的一个子集，

119
00:07:03,975 --> 00:07:05,745
它特别关注，

120
00:07:05,745 --> 00:07:10,395
我们如何构建一台机器或教一台机器，

121
00:07:10,395 --> 00:07:14,180
如何根据一些经验或数据来做到这一点。

122
00:07:14,770 --> 00:07:17,120
现在，深度学习更进一步，

123
00:07:17,200 --> 00:07:18,795
它是机器学习的一个子集，

124
00:07:18,795 --> 00:07:21,320
它明确地关注所谓的神经网络，

125
00:07:21,340 --> 00:07:22,670
以及我们如何构建神经网络，

126
00:07:23,020 --> 00:07:24,945
可以从数据中提取特征，

127
00:07:24,945 --> 00:07:28,490
这些你可以认为是在数据中出现的模式，

128
00:07:28,570 --> 00:07:31,340
这样它也可以学习完成这些任务。

129
00:07:32,700 --> 00:07:36,730
这就是这门课核心的真正意义所在，

130
00:07:36,810 --> 00:07:37,940
我们将尝试教你，

131
00:07:37,940 --> 00:07:39,340
给你基本的理解，

132
00:07:39,570 --> 00:07:43,990
我们如何建立和教计算机学习任务，

133
00:07:44,340 --> 00:07:45,670
许多不同类型的任务，

134
00:07:46,050 --> 00:07:47,440
直接从原始数据，

135
00:07:47,700 --> 00:07:51,640
这就是这门课最简单的形式。

136
00:07:52,140 --> 00:07:54,640
我们将为你提供一个非常坚实的基础，

137
00:07:54,780 --> 00:07:57,430
通过课程在技术方面，

138
00:07:57,810 --> 00:08:00,155
整个课程分两部分进行，

139
00:08:00,155 --> 00:08:01,595
第一节课，第二节课，

140
00:08:01,595 --> 00:08:03,040
每节课大约一个小时，

141
00:08:03,500 --> 00:08:05,005
随后是软件实验，

142
00:08:05,005 --> 00:08:06,750
紧跟在课程之后，

143
00:08:06,950 --> 00:08:08,110
它将试图加强，

144
00:08:08,110 --> 00:08:12,000
很多我们在课程的技术部分中所涉及的内容，

145
00:08:12,500 --> 00:08:15,930
给你实践这些想法的经验。

146
00:08:16,490 --> 00:08:19,470
所以这个项目分为两部分，

147
00:08:19,580 --> 00:08:21,690
技术课程和软件实验，

148
00:08:21,860 --> 00:08:23,910
我们今年有几个具体的新更新，

149
00:08:24,290 --> 00:08:26,460
特别是在后面的许多课程中。

150
00:08:27,020 --> 00:08:29,590
第一节课将涉及深度学习的基础，

151
00:08:29,590 --> 00:08:31,080
这是现在将要进行的。

152
00:08:32,090 --> 00:08:34,050
最后，我们将结束这门课程，

153
00:08:34,070 --> 00:08:38,380
一些非常令人兴奋来自学术界和产业界的客座演讲，

154
00:08:38,380 --> 00:08:42,840
他们真正引领和推动了人工智能和深度学习的发展。

155
00:08:43,130 --> 00:08:45,670
当然，我们还有很多令人惊叹的奖品，

156
00:08:45,670 --> 00:08:48,625
这些奖品来自所有的软件实验，

157
00:08:48,625 --> 00:08:51,120
和课程结束时的项目竞赛，

158
00:08:51,230 --> 00:08:53,850
所以，也许每天要快速地经历这些，

159
00:08:53,900 --> 00:08:57,210
就像我说的，我们会有专门的软件实验，与课程相结合，

160
00:08:58,290 --> 00:08:59,770
从今天开始，从实验一开始，

161
00:08:59,970 --> 00:09:01,820
你将建立一个神经网络，

162
00:09:01,820 --> 00:09:03,890
遵循生成性人工智能的主题，

163
00:09:03,890 --> 00:09:04,835
你将构建一个神经网络，

164
00:09:04,835 --> 00:09:07,480
它可以学习、听很多音乐，

165
00:09:07,650 --> 00:09:11,410
并学习如何在该音乐流派中生成全新的歌曲。

166
00:09:12,710 --> 00:09:15,595
最后，在课程的下一阶段，在星期五，

167
00:09:15,595 --> 00:09:17,305
我们将举办一场项目推介比赛，

168
00:09:17,305 --> 00:09:20,640
你可以单独或作为小组一员，

169
00:09:20,900 --> 00:09:23,520
参与并向我们所有人展示一个想法，

170
00:09:23,600 --> 00:09:26,970
一个新颖的深度学习想法，

171
00:09:27,350 --> 00:09:29,370
这大约是三分钟的时间，

172
00:09:29,780 --> 00:09:32,545
我们不会关注太多，

173
00:09:32,545 --> 00:09:34,290
因为这是一个一周的计划，

174
00:09:34,460 --> 00:09:37,225
我们不会太关注你推介的结果，

175
00:09:37,225 --> 00:09:41,460
而是（关注）你试图提出的东西的创新、想法和新颖性。

176
00:09:42,080 --> 00:09:44,425
这里的奖品已经相当可观了，

177
00:09:44,425 --> 00:09:46,960
一等奖将获得 NVIDIA GPU ，

178
00:09:46,960 --> 00:09:49,080
这是一个关键的硬件，

179
00:09:49,250 --> 00:09:50,665
这是工具性的，

180
00:09:50,665 --> 00:09:53,005
如果你想建立一个深度学习项目，

181
00:09:53,005 --> 00:09:54,355
并训练这些神经网络，

182
00:09:54,355 --> 00:09:56,520
这些神经网络可能非常大，需要大量的计算，

183
00:09:56,870 --> 00:09:59,070
这些奖品将给你这样做的计算。

184
00:09:59,510 --> 00:10:02,160
最后，今年我们颁发一个大奖，

185
00:10:02,240 --> 00:10:03,810
为第二和第三个实验，

186
00:10:04,280 --> 00:10:06,180
将于周二和周三举行，

187
00:10:06,320 --> 00:10:08,035
关注于，我认为是，

188
00:10:08,035 --> 00:10:12,085
解决深度学习领域中一些最令人兴奋的问题，

189
00:10:12,085 --> 00:10:16,710
以及我们如何具体地建立模型，可以健壮，

190
00:10:16,970 --> 00:10:19,800
不仅准确，而且健壮，值得信赖和安全，

191
00:10:20,150 --> 00:10:21,610
当它们部署时，

192
00:10:21,610 --> 00:10:24,480
你将获得开发这些类型的解决方案的经验，

193
00:10:25,040 --> 00:10:27,780
这些可以促进 AI 中最先进的。

194
00:10:28,600 --> 00:10:31,820
我提到的所有这些实验和竞赛，

195
00:10:32,080 --> 00:10:36,090
将在周四晚上 11 点截止，

196
00:10:36,090 --> 00:10:37,610
也就是最后一天上课前。

197
00:10:37,990 --> 00:10:40,005
我们会一路帮你，

198
00:10:40,005 --> 00:10:45,195
这个奖品，特别是这个比赛有很好的奖品，

199
00:10:45,195 --> 00:10:47,355
所以我鼓励你们获取奖品，

200
00:10:47,355 --> 00:10:51,000
努力有机会赢得奖品。

201
00:10:52,290 --> 00:10:55,115
当然，就像我说的，我们会一直帮助你，

202
00:10:55,115 --> 00:10:59,380
在整个课程中提供了许多可用的资源来帮助你实现这一点，

203
00:10:59,880 --> 00:11:02,110
如果你有任何问题，请发布 Piazza ，

204
00:11:02,340 --> 00:11:05,320
当然，这个项目有一个令人难以置信的团队，

205
00:11:05,640 --> 00:11:07,330
你可以随时联系，

206
00:11:07,440 --> 00:11:10,240
以防你对材料有任何问题或疑问，

207
00:11:10,860 --> 00:11:13,450
我和 Ava 将是你们的两个主要讲课，

208
00:11:13,650 --> 00:11:14,920
在课程的第一部分中，

209
00:11:15,360 --> 00:11:16,385
我们也会听到，

210
00:11:16,385 --> 00:11:18,215
就像我说的，在课程后面部分，

211
00:11:18,215 --> 00:11:19,390
从一些客座讲课中，

212
00:11:19,650 --> 00:11:23,380
他们将分享一些深度学习最先进的发展。

213
00:11:23,820 --> 00:11:25,985
当然，我想要大声说出，

214
00:11:25,985 --> 00:11:27,400
感谢我们所有的赞助商，

215
00:11:27,660 --> 00:11:28,655
如果没有他们的支持，

216
00:11:28,655 --> 00:11:32,645
这个项目在另一年是不可能的，

217
00:11:32,645 --> 00:11:33,460
所以谢谢你们。

218
00:11:35,010 --> 00:11:36,370
好的，那么现在，

219
00:11:36,570 --> 00:11:39,520
让我们深入到今天课程中真正有趣的部分，

220
00:11:39,570 --> 00:11:42,440
也就是技术部分，

221
00:11:42,440 --> 00:11:44,180
我想开始这个部分，

222
00:11:44,180 --> 00:11:47,770
通过问你们，让你们自己问，

223
00:11:48,060 --> 00:11:50,450
你们问自己这个问题，

224
00:11:50,450 --> 00:11:53,540
首先，为什么你们在这里，

225
00:11:53,540 --> 00:11:56,170
你们为什么一开始就关心这个话题。

226
00:11:57,400 --> 00:11:59,265
我认为要回答这个问题，

227
00:11:59,265 --> 00:12:00,435
我们必须退后一步，

228
00:12:00,435 --> 00:12:03,200
想想机器学习的历史，

229
00:12:03,340 --> 00:12:04,890
机器学习是什么，

230
00:12:04,890 --> 00:12:08,330
深度学习在机器学习的基础上带来了什么。

231
00:12:08,920 --> 00:12:11,690
传统的机器学习算法通常定义了

232
00:12:11,800 --> 00:12:14,700
数据中所谓的这些特征集，

233
00:12:14,700 --> 00:12:16,880
你可以将这些视为数据中的特定模式，

234
00:12:17,200 --> 00:12:19,365
通常这些特征都是手工设计的，

235
00:12:19,365 --> 00:12:21,600
所以，可能会有一个人进入数据集，

236
00:12:21,600 --> 00:12:23,900
并拥有大量的领域知识和经验，

237
00:12:24,310 --> 00:12:27,110
可以尝试发现这些特征可能是什么，

238
00:12:27,190 --> 00:12:28,755
现在深度学习的关键思想，

239
00:12:28,755 --> 00:12:31,725
也是这门课的核心是，

240
00:12:31,725 --> 00:12:33,920
不是让人来定义这些特征，

241
00:12:34,060 --> 00:12:37,480
如果我们可以让机器查看所有这些数据，

242
00:12:37,560 --> 00:12:41,770
并尝试提取和发现数据中的核心模式，

243
00:12:41,850 --> 00:12:45,490
以便在看到新数据时可以使用这些模式来做出一些决策。

244
00:12:45,870 --> 00:12:48,970
例如，如果我们想要检测图像中的人脸，

245
00:12:49,500 --> 00:12:52,490
深度神经网络算法可能会了解到，

246
00:12:52,490 --> 00:12:53,650
为了检测人脸，

247
00:12:53,700 --> 00:12:57,820
它首先必须检测图像中的线和边等东西，

248
00:12:58,020 --> 00:12:59,765
当你组合这些线和边时，

249
00:12:59,765 --> 00:13:04,210
你可以创建特征的组合，比如角和曲线，

250
00:13:04,440 --> 00:13:06,605
当你创建，当你组合这些特征时，

251
00:13:06,605 --> 00:13:08,200
你可以创建更高层次的特征，

252
00:13:08,250 --> 00:13:10,240
例如眼睛、鼻子和耳朵，

253
00:13:10,710 --> 00:13:15,020
然后，这些特征让你最终能够检测到你关心的东西，

254
00:13:15,020 --> 00:13:16,070
检测出哪个是人脸，

255
00:13:16,070 --> 00:13:20,050
但所有这些都来自于所谓的功能分层学习，

256
00:13:20,430 --> 00:13:22,300
你可以看到一些这样的例子，

257
00:13:22,620 --> 00:13:24,710
这些都是通过神经网络学习到的真实特征，

258
00:13:24,710 --> 00:13:27,910
它们的组合方式定义了这一信息进程。

259
00:13:29,360 --> 00:13:30,910
但事实上，正如我刚才所描述的，

260
00:13:30,910 --> 00:13:37,350
这种神经网络和深度学习的基础和基础构件已经存在了几十年，

261
00:13:37,910 --> 00:13:41,610
为什么我们现在在这门课中学习这个，

262
00:13:41,660 --> 00:13:44,820
怀着如此巨大的热情学习这些。

263
00:13:44,900 --> 00:13:49,890
首先，在过去的十年里，已经取得了几项关键的进展。

264
00:13:50,450 --> 00:13:56,460
第一，数据比我们生活中的任何时候都更加普遍，

265
00:13:56,840 --> 00:13:59,800
这些模型渴望更多的数据，

266
00:13:59,800 --> 00:14:02,730
而我们生活在大数据时代，

267
00:14:03,260 --> 00:14:05,480
这些模型比以往任何时候都有更多的数据可用，

268
00:14:05,480 --> 00:14:07,210
它们也因此而蓬勃发展。

269
00:14:07,560 --> 00:14:11,585
第二，这些算法是大规模可并行化的，

270
00:14:11,585 --> 00:14:12,880
它们需要大量的计算，

271
00:14:13,020 --> 00:14:16,030
我们也正处于历史上一个独特的时刻，

272
00:14:16,230 --> 00:14:21,370
我们有能力训练这些超大规模算法和技术，

273
00:14:21,420 --> 00:14:23,015
它们存在了很长时间，

274
00:14:23,015 --> 00:14:23,990
但是我们现在可以训练它们了，

275
00:14:23,990 --> 00:14:26,140
由于硬件方面的进步。

276
00:14:26,400 --> 00:14:31,745
最后，由于开源工具箱和软件平台，比如 TensorFlow ，

277
00:14:31,745 --> 00:14:34,600
你们在这门课上都会学到很多经验，

278
00:14:35,250 --> 00:14:39,130
训练和构建这些神经网络的代码从来没有像现在这样容易，

279
00:14:39,480 --> 00:14:41,470
所以，从软件的角度来看，

280
00:14:41,520 --> 00:14:43,630
开源已经有了令人难以置信的进步，

281
00:14:43,950 --> 00:14:47,650
你将学到的基本原理。

282
00:14:48,530 --> 00:14:51,940
现在让我从基础开始构建，

283
00:14:51,940 --> 00:14:55,375
每个神经网络的基本构件，

284
00:14:55,375 --> 00:14:56,905
你们会在课程中学到的，

285
00:14:56,905 --> 00:14:59,400
这只是单个神经元，

286
00:15:00,060 --> 00:15:01,570
在神经网络语言中，

287
00:15:01,740 --> 00:15:03,970
单个神经元被称为感知器。

288
00:15:05,230 --> 00:15:06,600
那么什么是感知器，

289
00:15:06,600 --> 00:15:10,220
就像我说的，感知器是一个单一的神经元，

290
00:15:10,450 --> 00:15:14,220
我要说这是一个非常非常简单的想法，

291
00:15:14,220 --> 00:15:17,780
我想确保观众中的每个人都准确地理解感知器是什么，

292
00:15:17,800 --> 00:15:18,650
以及它是如何工作的。

293
00:15:19,690 --> 00:15:22,110
让我们首先定义感知器，

294
00:15:22,110 --> 00:15:25,215
将一组输入作为输入，

295
00:15:25,215 --> 00:15:26,340
所以在左手边，

296
00:15:26,340 --> 00:15:31,220
你可以看到这个感知器接受 m 个不同的输入， 1 到 m ，

297
00:15:31,770 --> 00:15:33,010
这些是蓝色的圆圈，

298
00:15:33,330 --> 00:15:35,170
我们将这些输入表示为 xs ，

299
00:15:36,520 --> 00:15:42,290
这些数字中的每一个，每个输入，乘以相应的权重，

300
00:15:42,340 --> 00:15:43,490
我们可以称之为 w ，

301
00:15:43,750 --> 00:15:46,220
所以 x1 将乘以 w1 ，

302
00:15:46,780 --> 00:15:50,390
我们将所有这些乘法的结果加在一起，

303
00:15:51,130 --> 00:15:54,050
现在，我们取相加后的单个数字，

304
00:15:54,250 --> 00:15:56,505
我们把它通过这个非线性的，

305
00:15:56,505 --> 00:15:58,490
我们称之为非线性激活函数，

306
00:15:58,810 --> 00:16:01,100
这产生了我们感知器的最终输出，

307
00:16:01,120 --> 00:16:02,270
我们可以称之为 y 。

308
00:16:03,390 --> 00:16:09,155
现在，这实际上并不完全准确地描述感知器的图像，

309
00:16:09,155 --> 00:16:10,960
有一个步骤我忘了在这里提了，

310
00:16:11,130 --> 00:16:15,935
所以除了将所有这些输入与其相应的权重相乘，

311
00:16:15,935 --> 00:16:18,340
我们现在还将添加所谓的偏置项，

312
00:16:18,510 --> 00:16:21,100
这里表示为 w0 ，

313
00:16:21,300 --> 00:16:22,690
这只是一个标量权重，

314
00:16:22,710 --> 00:16:25,570
你可以想象它的输入是 1 ，

315
00:16:25,860 --> 00:16:30,850
所以这将允许网络改变它的非线性激活函数，

316
00:16:32,310 --> 00:16:35,980
当它看到它的输入时，是非线性的。

317
00:16:36,780 --> 00:16:37,865
现在在右手边，

318
00:16:37,865 --> 00:16:41,750
你可以看到这张图是以数学形式表述的，

319
00:16:41,750 --> 00:16:42,785
一个单独的方程，

320
00:16:42,785 --> 00:16:49,805
我们现在可以用向量和点积的线性代数项重写这个线性方程，

321
00:16:49,805 --> 00:16:57,650
例如，我们可以将整个输入 x1 到 xm 定义为大的向量 X ，

322
00:16:57,650 --> 00:17:05,590
大的向量 X 可以乘以或者矩阵乘以我们的权重 W ，

323
00:17:05,970 --> 00:17:09,640
同样， W 是权重的向量 w1 到 wm ，

324
00:17:10,320 --> 00:17:11,380
取它们的点积，

325
00:17:12,270 --> 00:17:16,120
不仅会使它们相乘，而且还会将得到的项相加，

326
00:17:16,560 --> 00:17:18,730
像我们前面说过的那样，增加一个偏置，

327
00:17:18,930 --> 00:17:20,590
并应用这个非线性函数。

328
00:17:22,710 --> 00:17:23,830
现在你可能想知道，

329
00:17:23,910 --> 00:17:25,370
这个非线性函数是什么，

330
00:17:25,370 --> 00:17:27,010
我已经提过好几次了，

331
00:17:27,420 --> 00:17:29,615
我说过这是一个函数，

332
00:17:29,615 --> 00:17:33,275
我们将神经网络的输出传给它，

333
00:17:33,275 --> 00:17:38,050
在返回给管道中的下一个神经元之前，

334
00:17:38,280 --> 00:17:40,580
所以一个非线性函数的常见例子，

335
00:17:40,580 --> 00:17:42,635
在深度神经网络中非常流行的，

336
00:17:42,635 --> 00:17:43,840
叫做 sigmoid 函数，

337
00:17:44,190 --> 00:17:47,890
你可以把它看作是阈值函数的一种连续形式，

338
00:17:47,940 --> 00:17:49,390
它从 0 到 1 ，

339
00:17:49,950 --> 00:17:54,100
它可以让我们在实数线上输入任何实数，

340
00:17:54,780 --> 00:17:57,650
你可以在右下角看到一个例子，

341
00:17:57,970 --> 00:18:01,460
事实上，非线性激活函数有很多种，

342
00:18:01,540 --> 00:18:03,390
在深度神经网络中流行的，

343
00:18:03,390 --> 00:18:04,700
下面是一些常见的，

344
00:18:05,080 --> 00:18:06,020
在整个演示过程中，

345
00:18:06,250 --> 00:18:10,110
你将在幻灯片底部看到这些代码片段的一些示例，

346
00:18:10,110 --> 00:18:14,120
我们将尝试将你在课程中学到的一些内容

347
00:18:14,440 --> 00:18:17,030
与实际软件以及你如何实现这些部分联系起来，

348
00:18:17,680 --> 00:18:20,100
这将对你的软件实验有很大帮助，

349
00:18:20,100 --> 00:18:22,605
所以左边的 sigmoid 激活函数非常流行，

350
00:18:22,605 --> 00:18:25,800
因为它是一个输出在 0 到 1 之间的函数，

351
00:18:25,800 --> 00:18:29,130
所以特别是当你想要处理概率分布时，

352
00:18:29,130 --> 00:18:32,810
这是非常重要的，因为概率在 0 到 1 之间，

353
00:18:33,800 --> 00:18:36,010
在现代深层神经网络中， ReLU 函数，

354
00:18:36,150 --> 00:18:37,745
你可以在最右边看到，

355
00:18:37,745 --> 00:18:39,610
是一个非常流行的激活函数，

356
00:18:39,660 --> 00:18:40,820
因为它是分段的，线性的，

357
00:18:40,820 --> 00:18:42,490
所以它的计算非常高效，

358
00:18:42,780 --> 00:18:44,650
特别是在计算它的导数时，

359
00:18:44,820 --> 00:18:47,140
它的导数是常量，

360
00:18:47,220 --> 00:18:50,000
除了一个是非线性的， 0 。

361
00:18:51,600 --> 00:18:55,205
现在，我希望你们所有人都在问自己这个问题，

362
00:18:55,205 --> 00:18:57,425
为什么我们需要这个非线性激活函数，

363
00:18:57,425 --> 00:18:59,740
它似乎让整个情况变得复杂起来，

364
00:18:59,850 --> 00:19:01,720
当我们一开始并不需要它，

365
00:19:02,250 --> 00:19:04,955
我想花一点时间来回答这个问题，

366
00:19:04,955 --> 00:19:07,720
因为非线性激活函数的要点是，

367
00:19:08,070 --> 00:19:12,560
当然，第一是引入数据的非线性，

368
00:19:12,560 --> 00:19:14,020
如果我们考虑我们的数据，

369
00:19:14,850 --> 00:19:16,445
几乎所有我们关心的数据，

370
00:19:16,445 --> 00:19:18,550
所有现实世界的数据都是高度非线性的，

371
00:19:19,080 --> 00:19:20,080
这一点很重要，

372
00:19:20,130 --> 00:19:23,120
因为如果我们想要处理这些类型的数据集，

373
00:19:23,120 --> 00:19:24,950
我们需要同样是非线性的模型，

374
00:19:24,950 --> 00:19:27,040
这样它们才能捕捉到相同类型的模式，

375
00:19:27,240 --> 00:19:28,660
想象一下我让你们分开，

376
00:19:28,680 --> 00:19:31,610
例如，我给你这个数据集，红点和绿点，

377
00:19:31,610 --> 00:19:34,720
我请你试着区分这两种类型的数据点，

378
00:19:35,340 --> 00:19:36,920
现在你可能认为这很容易，

379
00:19:36,920 --> 00:19:41,330
但如果我告诉你，你只能用一条线来做到这一点，

380
00:19:41,330 --> 00:19:43,100
那么现在它变成了一个非常复杂的问题，

381
00:19:43,100 --> 00:19:46,390
事实上，你不可能用一条线有效地解决这个问题，

382
00:19:47,510 --> 00:19:51,840
事实上，如果你在你的解决方案中引入非线性激活函数，

383
00:19:52,100 --> 00:19:56,400
你就可以来处理这类问题，

384
00:19:56,660 --> 00:20:00,540
非线性激活函数允许你处理非线性类型的数据。

385
00:20:01,470 --> 00:20:05,800
这正是神经网络的核心如此强大的原因。

386
00:20:06,690 --> 00:20:09,080
让我们通过一个非常简单的例子来理解这一点，

387
00:20:09,080 --> 00:20:11,830
再看一下感知器的图示，

388
00:20:12,540 --> 00:20:15,515
想象一下，我给你这个训练有素的神经网络和权重，

389
00:20:15,515 --> 00:20:19,570
现在不是 w1 w2 ，我要给你们这些位置的数字，

390
00:20:19,770 --> 00:20:23,500
所以，训练后的 w0 是 1 ，

391
00:20:23,610 --> 00:20:26,470
W 是向量 3 和 -2 ，

392
00:20:27,210 --> 00:20:29,360
所以，这个神经网络有两个输入，

393
00:20:29,360 --> 00:20:32,470
就像我们之前说的，它有输入 x1 和 x2 ，

394
00:20:32,850 --> 00:20:34,360
如果我们想要得到它的输出，

395
00:20:34,800 --> 00:20:36,365
这也是主要内容，

396
00:20:36,365 --> 00:20:38,315
我希望你们从今天的演讲中学到的，

397
00:20:38,315 --> 00:20:40,280
为了得到感知器的输出，

398
00:20:40,280 --> 00:20:41,710
我们需要采取三个步骤，

399
00:20:41,880 --> 00:20:46,750
在这个阶段，我们首先计算输入和权重的乘法，

400
00:20:48,570 --> 00:20:54,305
抱歉，是的，把它们相乘，结果相加，并计算非线性函数，

401
00:20:54,305 --> 00:21:00,130
这三个步骤定义了信息通过感知器的前向传播。

402
00:21:00,980 --> 00:21:03,760
那么，让我们来看看它到底是如何工作的，

403
00:21:03,760 --> 00:21:07,140
所以如果我们把这些数字放入方程中，

404
00:21:07,910 --> 00:21:11,350
我们可以看到非线性函数里面的所有东西，

405
00:21:11,350 --> 00:21:13,435
这里非线性函数是 g ，

406
00:21:13,435 --> 00:21:15,690
g 函数可能是 sigmoid ，

407
00:21:15,860 --> 00:21:17,220
我们在之前的一张幻灯片看到，

408
00:21:17,930 --> 00:21:23,740
事实上，我们的非线性函数中的那个分量只是一条二维线，

409
00:21:23,740 --> 00:21:24,730
它有两个输入，

410
00:21:24,730 --> 00:21:29,760
如果我们考虑这个神经网络可以看到的所有可能的输入的空间，

411
00:21:30,080 --> 00:21:33,100
我们可以把它画在决策边界上，

412
00:21:33,100 --> 00:21:38,675
我们可以画这条二维线，作为决定边界，

413
00:21:38,675 --> 00:21:41,710
就像分隔我们空间的这两个组成部分的平面，

414
00:21:42,390 --> 00:21:45,050
事实上，它不仅是一个单一的平面，

415
00:21:45,050 --> 00:21:48,910
还有一个方向性成分，取决于我们在平面的哪一边，

416
00:21:49,110 --> 00:21:52,450
如果我们看到一个输入，例如这里的 -1 2 ，

417
00:21:52,830 --> 00:21:55,810
我们知道它位于平面的一侧，

418
00:21:56,100 --> 00:21:58,235
它会有某种类型的输出，

419
00:21:58,235 --> 00:22:01,775
在这种情况下，输出将是正的，

420
00:22:01,775 --> 00:22:05,380
因为在这种情况下，当我们将这些分量放入我们的方程时，

421
00:22:05,700 --> 00:22:10,440
我们会得到一个正数，通过非线性分量，

422
00:22:10,440 --> 00:22:12,530
它也会通过传播，

423
00:22:12,730 --> 00:22:14,630
当然，如果你在空间的另一边，

424
00:22:14,980 --> 00:22:17,565
你会得到相反的结果，

425
00:22:17,565 --> 00:22:22,080
这个阈值函数就在这个决策边界上，

426
00:22:22,080 --> 00:22:23,960
所以，根据你所在空间的哪一边，

427
00:22:24,160 --> 00:22:30,770
阈值函数， sigmoid 函数将控制你移动到一边或另一边。

428
00:22:32,510 --> 00:22:35,640
现在在这个特定的例子中，这是非常方便的，

429
00:22:35,990 --> 00:22:37,480
因为我们可以可视化，

430
00:22:37,480 --> 00:22:41,125
我可以在这张幻灯片上为你们画出这个完整的空间，

431
00:22:41,125 --> 00:22:42,415
它只是一个二维空间，

432
00:22:42,415 --> 00:22:44,155
所以我们很容易可视化，

433
00:22:44,155 --> 00:22:47,640
但是当然，对于我们关心的几乎所有问题，

434
00:22:47,870 --> 00:22:50,670
我们的数据点都不会是二维的，

435
00:22:50,750 --> 00:22:51,810
如果你考虑一幅图像，

436
00:22:52,460 --> 00:22:56,610
图像的维度将是你在图像中拥有的像素数，

437
00:22:56,750 --> 00:23:01,050
所以这些将是数千个维度，数百万个维度，甚至更多，

438
00:23:01,580 --> 00:23:06,240
然后像你在这里看到的那样绘制这些类型的图是根本不可行的，

439
00:23:06,560 --> 00:23:07,615
所以我们不能总是这样做，

440
00:23:07,615 --> 00:23:09,480
但希望这能给你一些直觉，

441
00:23:09,590 --> 00:23:13,680
来理解我们建立更复杂的模型。

442
00:23:14,930 --> 00:23:17,005
现在我们已经对感知器有了一个概念，

443
00:23:17,005 --> 00:23:19,525
让我们看看我们如何真正地利用这个单一的神经元，

444
00:23:19,525 --> 00:23:21,720
并开始将其构建成更复杂的东西，

445
00:23:21,860 --> 00:23:22,780
一个完整的神经网络，

446
00:23:22,780 --> 00:23:24,150
并由此建立一个模型。

447
00:23:25,060 --> 00:23:28,460
所以让我们再来回顾一下之前的感知器示意图，

448
00:23:28,840 --> 00:23:31,430
如果，我再重申一次，

449
00:23:31,510 --> 00:23:33,120
这里的核心信息，

450
00:23:33,120 --> 00:23:36,315
我想让你们所有人从这门课上学到的是，

451
00:23:36,315 --> 00:23:37,725
感知器是如何工作的，

452
00:23:37,725 --> 00:23:40,550
以及它是如何将信息传播到决策中的，

453
00:23:40,810 --> 00:23:41,730
这里有三个步骤，

454
00:23:41,730 --> 00:23:46,490
第一是点积，第二是偏置，第三是非线性，

455
00:23:46,690 --> 00:23:50,330
你对你神经网络中的每一个感知器都重复这个过程。

456
00:23:51,450 --> 00:23:53,230
让我们稍微简化一下图表，

457
00:23:53,370 --> 00:23:55,090
我将去掉权重，

458
00:23:55,590 --> 00:24:01,000
你可以假设这里的每一行都有一个关联的权重标量，

459
00:24:01,230 --> 00:24:05,030
每一行也对应于输入，

460
00:24:05,030 --> 00:24:09,320
它有一个权重输入在行上，

461
00:24:09,430 --> 00:24:12,765
为了简单起见，我也去掉了偏置，

462
00:24:12,765 --> 00:24:13,550
但它仍然存在，

463
00:24:14,320 --> 00:24:17,360
现在结果是 z ，

464
00:24:17,440 --> 00:24:22,005
我们称之为点积加上偏置的结果，

465
00:24:22,005 --> 00:24:24,290
这就是我们传递给非线性函数的，

466
00:24:24,880 --> 00:24:29,060
这一块将应用于激活函数，

467
00:24:29,140 --> 00:24:32,930
现在，这里的最终输出将是 g ，

468
00:24:33,550 --> 00:24:36,420
这是我们的 z 的激活函数，

469
00:24:37,100 --> 00:24:40,090
z 就是你可以认为这个神经元的状态，

470
00:24:40,090 --> 00:24:42,840
它是点积加上偏置的结果。

471
00:24:44,060 --> 00:24:48,450
现在，如果我们想定义和建立一个多层输出神经网络，

472
00:24:49,010 --> 00:24:51,160
比如，如果我们想要这个函数的两个输出，

473
00:24:51,160 --> 00:24:52,410
这是一个非常简单的过程，

474
00:24:52,490 --> 00:24:55,090
我们现在只有两个神经元，两个感知器，

475
00:24:55,090 --> 00:24:59,310
每个感知器将控制其相关部分的输出，

476
00:25:00,110 --> 00:25:01,300
所以现在我们有两个输出，

477
00:25:01,300 --> 00:25:02,830
每一个都是正常的感知器，

478
00:25:02,830 --> 00:25:04,285
它接受所有的输入，

479
00:25:04,285 --> 00:25:05,935
所以它们都接受相同的输入，

480
00:25:05,935 --> 00:25:06,810
但令人惊讶的是，

481
00:25:07,070 --> 00:25:08,940
现在有了这样的数学理解，

482
00:25:09,410 --> 00:25:13,230
我们可以开始完全从头开始建立我们的第一个神经网络。

483
00:25:13,490 --> 00:25:15,025
那么这是什么样子的呢，

484
00:25:15,025 --> 00:25:18,570
我们可以首先初始化这两个组件，

485
00:25:18,590 --> 00:25:22,920
我们看到的第一个分量是权重矩阵，抱歉，是权重向量，

486
00:25:22,940 --> 00:25:25,080
在这种情况下，它是权重的向量，

487
00:25:26,400 --> 00:25:29,860
第二个分量是偏置向量，

488
00:25:29,910 --> 00:25:34,595
我们要乘以所有输入的点积乘以我们的权重，

489
00:25:34,595 --> 00:25:37,445
所以，现在剩下的唯一一步，

490
00:25:37,445 --> 00:25:41,135
在我们定义了层的这些参数之后，

491
00:25:41,135 --> 00:25:44,960
定义信息的前向传播是如何工作的，

492
00:25:44,960 --> 00:25:49,205
这正是我向你们强调的三个主要组成部分，

493
00:25:49,205 --> 00:25:52,160
所以我们可以创建这个 call 函数来实现这一点，

494
00:25:52,160 --> 00:25:54,400
定义信息的正向传播，

495
00:25:55,170 --> 00:25:58,300
这里的故事和我们一直看到的完全一样，

496
00:25:58,320 --> 00:26:01,000
矩阵，将我们的输入乘以我们的权重，

497
00:26:02,550 --> 00:26:03,400
添加偏置，

498
00:26:04,530 --> 00:26:06,530
然后应用非线性函数，

499
00:26:06,530 --> 00:26:07,600
并返回结果，

500
00:26:07,980 --> 00:26:10,085
这段代码就会运行，

501
00:26:10,085 --> 00:26:13,420
这将定义一个完整的神经网络层，

502
00:26:13,770 --> 00:26:16,150
然后你可以这样做，

503
00:26:17,040 --> 00:26:19,150
当然，实际上，对你们所有人来说幸运的是，

504
00:26:19,170 --> 00:26:21,040
所有的代码，不是很多代码，

505
00:26:21,090 --> 00:26:23,990
被这些库抽象出来，比如 TensorFlow ，

506
00:26:23,990 --> 00:26:26,505
你可以简单地像这样调用函数，

507
00:26:26,505 --> 00:26:29,840
它会复制这段代码，

508
00:26:30,580 --> 00:26:33,015
所以，你不必将所有代码都复制下来，

509
00:26:33,015 --> 00:26:35,090
你可以直接调用它。

510
00:26:36,250 --> 00:26:38,270
有了这种理解，

511
00:26:38,500 --> 00:26:41,025
我们刚刚看到了如何建立一个单一的层，

512
00:26:41,025 --> 00:26:46,730
当然，现在你也可以开始考虑如何堆叠这些层了，

513
00:26:47,080 --> 00:26:53,295
既然我们现在有了从输入到隐藏输出的转换，

514
00:26:53,295 --> 00:26:54,530
你可以认为，

515
00:26:55,300 --> 00:26:58,735
我们如何定义一些方法，

516
00:26:58,735 --> 00:27:04,110
将这些输入直接转换到某个新的维度空间，

517
00:27:04,550 --> 00:27:06,990
也许更接近我们想要预测的值，

518
00:27:07,040 --> 00:27:10,230
而这种转换最终将被学习，

519
00:27:10,370 --> 00:27:13,495
知道如何将这些输入转换为我们想要的输出，

520
00:27:13,495 --> 00:27:14,490
我们稍后会讲到这一点，

521
00:27:14,720 --> 00:27:17,725
但现在，我真正想要关注的是，

522
00:27:17,725 --> 00:27:19,620
如果我们有这些更复杂的神经网络，

523
00:27:20,030 --> 00:27:21,700
我想真正地提炼出，

524
00:27:21,700 --> 00:27:24,025
这并不比我们已经看到的更复杂，

525
00:27:24,025 --> 00:27:27,600
如果我们只关注这张图中的一个神经元，

526
00:27:28,480 --> 00:27:30,590
以这里的 z2 为例，

527
00:27:31,090 --> 00:27:33,650
z2 是中间层突出显示的神经元，

528
00:27:34,480 --> 00:27:38,360
这就是我们到目前为止在这门课上看到的感知器，

529
00:27:38,470 --> 00:27:39,840
它的输出是

530
00:27:39,840 --> 00:27:45,380
通过取一个点积，加上一个偏置，然后在所有输入之间应用非线性函数得到的，

531
00:27:46,280 --> 00:27:47,710
如果我们看一个不同的节点，

532
00:27:47,710 --> 00:27:49,830
例如 z3 ，就在它的正下方，

533
00:27:49,940 --> 00:27:51,085
它是完全相同的故事，

534
00:27:51,085 --> 00:27:52,705
同样，它看到的是所有相同的输入，

535
00:27:52,705 --> 00:27:54,690
但它有一组不同的权重矩阵，

536
00:27:54,830 --> 00:27:56,995
将应用于这些输入，

537
00:27:56,995 --> 00:27:58,140
所以我们会有不同的输出，

538
00:27:58,370 --> 00:28:00,900
但数学方程是完全相同的。

539
00:28:01,540 --> 00:28:02,235
所以从现在开始，

540
00:28:02,235 --> 00:28:05,535
我将简化所有这些线条和图表，

541
00:28:05,535 --> 00:28:07,610
只是展示中间的这些图标，

542
00:28:07,660 --> 00:28:09,290
只是为了证明，这些意味着，

543
00:28:09,550 --> 00:28:11,280
一切都将完全与一切联系在一起，

544
00:28:11,280 --> 00:28:14,390
并由我们一直介绍的那些数学公式定义，

545
00:28:14,920 --> 00:28:17,390
但这些模型并没有额外的复杂性，

546
00:28:17,680 --> 00:28:18,830
从你已经看到的情况来看。

547
00:28:19,580 --> 00:28:23,935
现在，如果你想把这些类型的解决方案叠加在一起，

548
00:28:23,935 --> 00:28:25,410
这些层叠加在一起，

549
00:28:25,820 --> 00:28:27,810
你不仅可以非常容易地定义一个层，

550
00:28:27,950 --> 00:28:30,630
而且你可以创建所谓的顺序模型，

551
00:28:30,680 --> 00:28:33,840
这些顺序模型，你可以定义一个又一个层，

552
00:28:34,100 --> 00:28:37,270
它们定义了信息的正向传播，

553
00:28:37,270 --> 00:28:40,660
不仅是从神经元层面，现在是从层层面，

554
00:28:40,660 --> 00:28:43,680
每一层都将完全连接到下一层，

555
00:28:43,820 --> 00:28:48,420
第二层的输入将是上一层的所有输出。

556
00:28:50,030 --> 00:28:52,530
现在，当然，如果你想创建一个非常深入的神经网络，

557
00:28:52,760 --> 00:28:54,205
所有的深层神经网络就是，

558
00:28:54,205 --> 00:28:56,790
我们只是把这些层堆叠在一起，

559
00:28:57,050 --> 00:28:58,450
这个故事没有什么不同，

560
00:28:58,450 --> 00:28:59,940
和之前一样简单，

561
00:29:00,260 --> 00:29:04,150
所以，这些层全部只是层，

562
00:29:04,150 --> 00:29:07,040
最终输出被计算，

563
00:29:07,360 --> 00:29:11,000
通过深入不同层的过程，

564
00:29:11,170 --> 00:29:12,465
你只是不断地堆叠它们，

565
00:29:12,465 --> 00:29:15,135
直到你到达最后一层，也就是你的输出层，

566
00:29:15,135 --> 00:29:18,785
这是你想要输出的最终预测，

567
00:29:18,785 --> 00:29:20,660
我们可以创建一个深度神经网络来完成这一切，

568
00:29:20,660 --> 00:29:24,365
通过堆叠这些层并创建这些更具层次感的模型，

569
00:29:24,365 --> 00:29:27,580
就像我们在今天课程开始时看到的那样，

570
00:29:27,600 --> 00:29:30,220
最终输出被计算是通过，

571
00:29:30,240 --> 00:29:32,980
只是越来越深入地进入这个系统。

572
00:29:34,910 --> 00:29:36,145
好的，这太棒了。

573
00:29:36,145 --> 00:29:40,710
我们现在已经看到了如何从单个神经元到一个层，

574
00:29:41,030 --> 00:29:42,660
再到一个深层神经网络，

575
00:29:42,920 --> 00:29:45,330
建立在这些基本原理之上。

576
00:29:46,430 --> 00:29:51,810
让我们来看看我们如何使用这些原理，

577
00:29:52,070 --> 00:29:53,220
我们刚刚讨论的，

578
00:29:53,450 --> 00:29:54,930
来解决一个非常现实的问题，

579
00:29:55,190 --> 00:30:01,150
我想你们今天早上醒来时可能都非常关注这个问题。

580
00:30:01,150 --> 00:30:02,160
所以问题是，

581
00:30:02,570 --> 00:30:05,470
我们如何建立一个神经网络来回答这个问题，

582
00:30:05,470 --> 00:30:07,795
我将如何通过这门课，

583
00:30:07,795 --> 00:30:09,360
我能不能通过。

584
00:30:10,500 --> 00:30:12,170
所以，为了回答这个问题，

585
00:30:12,220 --> 00:30:14,870
让我们看看是否可以训练一个神经网络来解决这个问题。

586
00:30:16,240 --> 00:30:17,035
要做这个，

587
00:30:17,035 --> 00:30:18,930
让我们从一个非常简单的神经网络开始，

588
00:30:19,760 --> 00:30:21,760
我们将用两个输入训练这个模型，

589
00:30:21,760 --> 00:30:22,465
只有两个输入，

590
00:30:22,465 --> 00:30:27,030
一个输入是你在这一周中参加的讲座的数量，

591
00:30:27,350 --> 00:30:32,850
第二个输入将是你在期末项目或竞争中花费了多少小时。

592
00:30:34,480 --> 00:30:36,270
好的，所以我们要做的是，

593
00:30:36,270 --> 00:30:38,220
首先去收集大量数据，

594
00:30:38,220 --> 00:30:40,340
从过去几年我们教授这门课程中，

595
00:30:40,600 --> 00:30:42,050
我们可以绘制所有这些数据，

596
00:30:42,130 --> 00:30:43,560
因为它只有两个输入空间，

597
00:30:43,560 --> 00:30:46,490
我们可以将这些数据绘制在二维特征空间上，

598
00:30:46,780 --> 00:30:49,560
我们可以看看你们前面所有学生，

599
00:30:49,560 --> 00:30:51,900
通过课程和不及格的，

600
00:30:51,900 --> 00:30:54,590
看看他们在这个空间里在哪里，

601
00:30:54,700 --> 00:30:56,130
他们花了多少个小时，

602
00:30:56,130 --> 00:30:57,525
听了多少节课，

603
00:30:57,525 --> 00:30:58,220
诸若此类，

604
00:30:58,480 --> 00:31:01,460
绿点是通过的人，红点是不及格的，

605
00:31:02,180 --> 00:31:04,615
现在，这是你，

606
00:31:04,615 --> 00:31:05,425
你在这里，

607
00:31:05,425 --> 00:31:07,200
4 5 是你的坐标空间，

608
00:31:07,910 --> 00:31:09,175
你在那里，

609
00:31:09,175 --> 00:31:10,495
你已经听了四节课，

610
00:31:10,495 --> 00:31:12,300
你已经在期末作业上花了五个小时，

611
00:31:12,830 --> 00:31:14,935
我们想建立一个神经网络来回答这样一个问题，

612
00:31:14,935 --> 00:31:17,640
你会通过这门课还是不及格。

613
00:31:18,410 --> 00:31:19,420
那我们开始吧，

614
00:31:19,420 --> 00:31:20,500
我们有两个输入，

615
00:31:20,500 --> 00:31:21,900
一个是 4 ，一个是 5 ，

616
00:31:22,070 --> 00:31:22,870
这是两个数字，

617
00:31:22,870 --> 00:31:24,700
我们可以把它们给神经网络，

618
00:31:24,700 --> 00:31:26,760
我们刚刚看到的如何构建的（神经网络），

619
00:31:27,540 --> 00:31:29,690
我们把它输入到一个单层神经网络中，

620
00:31:30,430 --> 00:31:31,860
在这个例子中是三个隐藏的单元，

621
00:31:31,860 --> 00:31:32,970
但我们可以把它做得更大，

622
00:31:32,970 --> 00:31:35,150
如果我们想要更有表现力和更强大，

623
00:31:36,150 --> 00:31:37,070
我们在这里看到，

624
00:31:37,070 --> 00:31:39,520
你通过这些课程的概率是 0.1 ，

625
00:31:40,240 --> 00:31:41,160
相当糟糕。

626
00:31:41,600 --> 00:31:43,710
那么为什么会出现这种情况，

627
00:31:43,790 --> 00:31:44,760
我们做错了什么，

628
00:31:44,960 --> 00:31:46,740
因为我不认为这是正确的，

629
00:31:46,880 --> 00:31:48,210
当我们看这个空间的时候，

630
00:31:48,770 --> 00:31:51,565
看起来你是通过这门课的很好的候选人，

631
00:31:51,565 --> 00:31:55,980
但是为什么神经网络说你通过考试的可能性只有 10% 呢，

632
00:31:56,330 --> 00:31:57,390
有谁有什么想法吗？

633
00:32:02,680 --> 00:32:04,305
完全正确，

634
00:32:04,305 --> 00:32:09,160
所以，这个神经网络它刚刚诞生，

635
00:32:09,160 --> 00:32:12,430
它没有关于这个世界或这个课程的信息，

636
00:32:12,430 --> 00:32:14,400
它不知道 4 和 5 是什么意思，

637
00:32:14,480 --> 00:32:18,210
也不知道及格或不及格的概念是什么意思。

638
00:32:19,750 --> 00:32:20,355
完全正确，

639
00:32:20,355 --> 00:32:21,885
这个神经网络还没有经过训练，

640
00:32:21,885 --> 00:32:23,780
你可以把它想象成一个婴儿，

641
00:32:23,920 --> 00:32:25,650
它还没有学到任何东西，

642
00:32:25,650 --> 00:32:28,410
所以，我们的首要工作是训练它，

643
00:32:28,410 --> 00:32:29,850
这种理解的一部分是，

644
00:32:29,850 --> 00:32:32,630
当神经网络出错时，我们首先需要告诉它，

645
00:32:33,550 --> 00:32:36,810
所以，从数学上讲，我们现在应该考虑如何回答这个问题，

646
00:32:36,810 --> 00:32:40,070
即我的神经网络是否犯了错误，

647
00:32:40,120 --> 00:32:41,330
如果它犯了一个错误，

648
00:32:41,500 --> 00:32:43,500
我如何告诉它这个错误有多大，

649
00:32:43,500 --> 00:32:46,130
以便它下一次看到这个数据点时，

650
00:32:46,480 --> 00:32:48,800
可以做得更好，将错误降到最低。

651
00:32:49,490 --> 00:32:51,240
所以，在神经网络语言中，

652
00:32:51,710 --> 00:32:53,490
这些错误被称为损失，

653
00:32:54,380 --> 00:32:57,000
具体来说，你想定义所谓的损失函数，

654
00:32:57,440 --> 00:33:02,800
它将把你的预测和真实的预测作为输入，

655
00:33:02,800 --> 00:33:05,970
你的预测与真实预测的距离，

656
00:33:06,050 --> 00:33:09,205
告诉你有多大的损失，

657
00:33:09,205 --> 00:33:09,930
比如，

658
00:33:11,190 --> 00:33:16,240
比如，我们想要建立一个神经网络来进行分类，

659
00:33:17,120 --> 00:33:20,785
抱歉，在此之前，我想给你们一些术语，

660
00:33:20,785 --> 00:33:23,760
有多种不同的方式来表达相同的事情，

661
00:33:24,230 --> 00:33:26,130
在神经网络和深度学习中，

662
00:33:26,150 --> 00:33:28,530
所以我刚才描述的损失函数，

663
00:33:28,640 --> 00:33:33,300
也通常被称为目标函数，经验风险，成本函数，

664
00:33:33,320 --> 00:33:35,400
这些都是完全一样的东西，

665
00:33:35,480 --> 00:33:37,750
它们都是我们训练神经网络的一种方式，

666
00:33:37,750 --> 00:33:40,290
在神经网络出错时教它，

667
00:33:40,940 --> 00:33:43,680
我们最终真正想要做的是，

668
00:33:43,760 --> 00:33:45,870
在整个数据集的过程中，

669
00:33:46,160 --> 00:33:48,115
而不仅是一个错误数据点，

670
00:33:48,115 --> 00:33:50,010
我们想要在整个数据集上，

671
00:33:50,450 --> 00:33:55,140
我们想要将这个神经网络犯下的所有平均错误降到最低。

672
00:33:56,730 --> 00:33:59,650
所以，如果我们看二元分类问题，

673
00:33:59,940 --> 00:34:01,790
我会通过这门课吗，

674
00:34:01,790 --> 00:34:03,260
这里有一个是或不是的答案，

675
00:34:03,260 --> 00:34:04,600
这意味着二元分类，

676
00:34:05,740 --> 00:34:10,795
现在我们可以使用所谓的 softmax 交叉熵损失的损失函数，

677
00:34:10,795 --> 00:34:12,400
对于你们中不熟悉的人来说，

678
00:34:12,400 --> 00:34:16,110
交叉熵的概念是在 MIT 开发的，

679
00:34:16,220 --> 00:34:22,380
由 Claude Shannon ，

680
00:34:22,880 --> 00:34:24,900
他很有远见，

681
00:34:24,950 --> 00:34:28,015
五十多年前，他在这里攻读硕士学位，

682
00:34:28,015 --> 00:34:29,935
他引入了交叉熵的概念，

683
00:34:29,935 --> 00:34:36,130
这就是我们有能力训练这些类型的神经网络的关键，

684
00:34:36,420 --> 00:34:37,720
即使是现在也是如此。

685
00:34:38,560 --> 00:34:40,020
所以，让我们开始，

686
00:34:40,670 --> 00:34:44,125
不是预测二元交叉熵输出，

687
00:34:44,125 --> 00:34:49,150
例如，如果我们想要预测你班级成绩的最终分数，

688
00:34:49,150 --> 00:34:51,300
这不再是一个二元输出，是或否，

689
00:34:51,440 --> 00:34:53,425
它是一个连续变量，

690
00:34:53,425 --> 00:34:55,410
这是分数，满分是 100 分，

691
00:34:55,790 --> 00:34:58,920
你在班级项目中的分数是多少，

692
00:35:00,020 --> 00:35:02,960
对于这种损失，我们可以使用所谓的均方误差损失，

693
00:35:02,960 --> 00:35:04,220
你可以从字面上认为，

694
00:35:04,220 --> 00:35:07,480
这只是从真实分数中减去你的预测分数，

695
00:35:07,830 --> 00:35:10,240
并将两者之间的距离最小化。

696
00:35:12,450 --> 00:35:16,150
所以，我认为现在我们已经准备好将所有这些信息整合在一起，

697
00:35:16,500 --> 00:35:19,720
解决这个训练神经网络，

698
00:35:20,760 --> 00:35:25,315
不仅仅是识别它是多么错误，

699
00:35:25,315 --> 00:35:26,730
它的损失有多大，

700
00:35:26,960 --> 00:35:29,155
但更重要的是，将这种损失降至最低，

701
00:35:29,155 --> 00:35:32,280
通过查看它观察到的所有这些训练数据。

702
00:35:33,690 --> 00:35:35,980
所以我们想要找到这个神经网络，

703
00:35:36,150 --> 00:35:37,115
就像我们之前提到的那样，

704
00:35:37,115 --> 00:35:44,020
它可以最小化整个数据集的经验风险或经验损失的平均值，

705
00:35:44,400 --> 00:35:49,300
这意味着我们想要在数学上找到这些 w ，

706
00:35:49,410 --> 00:35:51,430
最小化 J(W) ，

707
00:35:51,450 --> 00:35:54,770
J(W) 是损失函数平均在整个数据集上，

708
00:35:54,770 --> 00:35:56,165
W 是我们的权重，

709
00:35:56,165 --> 00:35:57,970
所以我们想要找到一组权重，

710
00:35:58,440 --> 00:36:04,370
平均而言，它会给我们最小的损失，

711
00:36:05,620 --> 00:36:08,660
现在，记住 W 在这里只是一个列表，

712
00:36:08,740 --> 00:36:11,385
它只是我们神经网络中所有权重的一组，

713
00:36:11,385 --> 00:36:15,440
你可能有数百个权重和一个非常小的神经网络，

714
00:36:15,460 --> 00:36:19,275
或者在今天的神经网络中，你可能有数十亿或万亿的权重，

715
00:36:19,275 --> 00:36:22,610
你想要找出每一个权重的值是多少，

716
00:36:22,780 --> 00:36:25,340
这将导致尽可能小的损失。

717
00:36:26,540 --> 00:36:27,870
现在，你如何做到这个呢，

718
00:36:27,980 --> 00:36:33,180
记住，我们的损失函数 J(W) 只是我们权重的函数，

719
00:36:33,290 --> 00:36:35,830
所以对于任何权重的实例化，

720
00:36:35,830 --> 00:36:42,390
我们可以计算一个神经网络有多大错误的标量，

721
00:36:42,650 --> 00:36:44,640
对于我们的权重实例化。

722
00:36:45,080 --> 00:36:46,945
让我们试着想象一下，

723
00:36:46,945 --> 00:36:50,635
例如，在一个非常简单的二维空间中，

724
00:36:50,635 --> 00:36:51,930
我们只有两个权重，

725
00:36:52,190 --> 00:36:55,440
这里是非常简单的神经网络，非常小，

726
00:36:55,490 --> 00:36:56,520
两个权重的神经网络，

727
00:36:56,840 --> 00:37:01,590
我们想找出训练这个神经网络的最佳权重是什么，

728
00:37:02,080 --> 00:37:04,110
我们可以画出损失，

729
00:37:05,030 --> 00:37:06,955
神经网络的误差有多大，

730
00:37:06,955 --> 00:37:11,190
对于这两个权重的每个实例化，

731
00:37:11,210 --> 00:37:13,230
这是一个很大的空间，这是一个无限的空间，

732
00:37:13,490 --> 00:37:15,210
但我们仍然可以尝试，

733
00:37:15,350 --> 00:37:16,255
我们可以有一个函数，

734
00:37:16,255 --> 00:37:18,630
在这个空间的每一点求值。

735
00:37:19,570 --> 00:37:21,530
现在，我们最终想要做的是，

736
00:37:21,850 --> 00:37:25,995
再次，我们想要找出哪一组 ws

737
00:37:25,995 --> 00:37:29,060
将给我们带来尽可能最小的损失，

738
00:37:29,290 --> 00:37:33,320
这意味着你可以在这里看到的这个图景上的最低点，

739
00:37:33,910 --> 00:37:37,070
将我们带到最低点的 ws 在哪里，

740
00:37:39,000 --> 00:37:40,420
我们做到这一点的方法是，

741
00:37:40,650 --> 00:37:43,385
从一个随机的地方开始，

742
00:37:43,385 --> 00:37:44,630
我们不知道从哪里开始，

743
00:37:44,630 --> 00:37:46,930
在这个空间中选择一个随机的地方开始，

744
00:37:47,310 --> 00:37:48,280
让我们从那里开始，

745
00:37:48,480 --> 00:37:51,230
在这个位置，让我们评估一下我们的神经网络，

746
00:37:51,230 --> 00:37:54,340
我们可以计算这个特定位置的损失，

747
00:37:54,900 --> 00:37:59,050
在此基础上，我们可以计算损失是如何变化的，

748
00:37:59,100 --> 00:38:01,010
我们可以计算损失的梯度，

749
00:38:01,010 --> 00:38:04,600
因为我们的损失函数是连续函数，

750
00:38:04,800 --> 00:38:07,240
所以我们可以计算函数的导数，

751
00:38:07,590 --> 00:38:09,750
通过空间中的权重，

752
00:38:09,980 --> 00:38:14,220
梯度告诉我们最高点的方向，

753
00:38:14,390 --> 00:38:15,600
所以，从我们所处的位置来看，

754
00:38:15,680 --> 00:38:19,380
梯度告诉我们应该去哪里增加我们的损失，

755
00:38:20,380 --> 00:38:22,125
现在，当然，我们不想增加我们的损失，

756
00:38:22,125 --> 00:38:23,235
我们想减少我们的损失，

757
00:38:23,235 --> 00:38:25,320
所以我们对梯度取反，

758
00:38:25,320 --> 00:38:28,305
我们朝着与梯度相反的方向迈出一步，

759
00:38:28,305 --> 00:38:31,790
这让我们离图景的底部又近了一步，

760
00:38:32,230 --> 00:38:35,895
我们只是一遍又一遍地重复这个过程，

761
00:38:35,895 --> 00:38:38,240
我们在这个新位置评估神经网络，

762
00:38:38,350 --> 00:38:40,970
计算它的梯度，并朝着这个新方向前进，

763
00:38:40,990 --> 00:38:45,140
我们一直在穿过这个图景，直到我们汇聚到最小。

764
00:38:47,260 --> 00:38:49,185
我们可以总结这个算法，

765
00:38:49,185 --> 00:38:52,245
它的正式名称是梯度下降，

766
00:38:52,245 --> 00:38:55,280
所以梯度下降可以简单地写成这样，

767
00:38:55,300 --> 00:38:56,810
我们初始化所有的权重，

768
00:38:57,520 --> 00:38:59,160
这可以两个权重，

769
00:38:59,160 --> 00:39:00,240
如你在上一个例子中看到的，

770
00:39:00,240 --> 00:39:01,910
它可以是数十亿个权重，

771
00:39:01,960 --> 00:39:03,620
就像真正的神经网络，

772
00:39:04,300 --> 00:39:10,800
我们计算损失的偏导数相对于权重的这个梯度，

773
00:39:10,800 --> 00:39:14,780
然后我们可以在这个梯度的相反方向上更新我们的权重，

774
00:39:15,810 --> 00:39:18,340
所以我们只是采取这个很小的量，

775
00:39:18,750 --> 00:39:20,380
很小的一步，你可以想象它，

776
00:39:20,490 --> 00:39:22,810
在这里被表示为 η ，

777
00:39:23,350 --> 00:39:25,790
我们指的是这一小步，

778
00:39:26,290 --> 00:39:29,900
这通常被称为学习率，

779
00:39:29,920 --> 00:39:32,580
这就像我们多么相信这个梯度，

780
00:39:32,580 --> 00:39:34,485
并朝着这个梯度的方向前进，

781
00:39:34,485 --> 00:39:35,780
我们稍后将详细讨论这个问题。

782
00:39:36,540 --> 00:39:39,170
但为了给你一些代码的感觉，

783
00:39:39,220 --> 00:39:43,020
这个算法也可以很好地翻译成真实的代码，

784
00:39:43,020 --> 00:39:45,230
对于你可以在左侧看到的伪代码上的每一行，

785
00:39:45,400 --> 00:39:47,835
你都可以在右侧看到相应的真实代码，

786
00:39:47,835 --> 00:39:48,570
这些代码是可以运行的，

787
00:39:48,570 --> 00:39:51,020
并且可以由你们在实验中直接实现。

788
00:39:51,860 --> 00:39:54,455
但现在让我们来具体看看这个术语，

789
00:39:54,455 --> 00:39:55,445
这就是梯度，

790
00:39:55,445 --> 00:39:58,150
我们在可视的例子中非常简要地谈到这一点，

791
00:39:58,560 --> 00:40:00,035
这就解释了，就像我说的，

792
00:40:00,035 --> 00:40:03,370
损失是如何随着权重的变化而变化的，

793
00:40:03,480 --> 00:40:05,380
所以，随着权重的移动，

794
00:40:05,580 --> 00:40:07,130
我的损失是增加还是减少，

795
00:40:07,130 --> 00:40:08,350
这将告诉神经网络，

796
00:40:08,610 --> 00:40:11,590
它是否需要将权重向某个方向移动。

797
00:40:12,490 --> 00:40:15,290
但我从来没有告诉过你如何计算这个，

798
00:40:15,430 --> 00:40:17,250
我认为这是一个非常重要的部分，

799
00:40:17,250 --> 00:40:18,210
因为如果你不知道这一点，

800
00:40:18,210 --> 00:40:21,290
那么你就不能训练你的神经网络，

801
00:40:21,490 --> 00:40:24,525
这是训练神经网络的关键部分，

802
00:40:24,525 --> 00:40:30,020
计算这条梯度线的过程被称为反向传播。

803
00:40:30,160 --> 00:40:34,050
所以，让我们简要介绍一下反向传播，

804
00:40:34,050 --> 00:40:34,880
以及它是如何工作的，

805
00:40:36,190 --> 00:40:39,380
再次，我们从现存的最简单的神经网络开始，

806
00:40:39,490 --> 00:40:43,190
这个神经网络有一个输入，一个输出，并且只有一个神经元，

807
00:40:43,510 --> 00:40:44,930
这就是最简单的事情，

808
00:40:45,580 --> 00:40:49,700
我们想要计算损失相对于我们权重的梯度，

809
00:40:49,720 --> 00:40:53,270
在这种情况下，让我们相对于 w2 ，第二个权重来计算它，

810
00:40:54,070 --> 00:40:56,520
所以这个导数会告诉我们，

811
00:40:56,520 --> 00:41:01,050
权重的微小变化会对损失有多大影响，

812
00:41:01,050 --> 00:41:04,250
如果一个小的变化，如果我们在一个方向上稍微改变一下权重，

813
00:41:04,420 --> 00:41:06,410
会增加我们的损失或减少我们的损失。

814
00:41:07,530 --> 00:41:09,860
所以要计算它，我们可以写出这个导数，

815
00:41:09,860 --> 00:41:12,370
我们可以从应用链式规则开始，

816
00:41:12,810 --> 00:41:16,300
从损失函数到输出的反向，

817
00:41:16,590 --> 00:41:18,035
具体地说，我们可以做的是，

818
00:41:18,035 --> 00:41:22,210
我们可以把这个导数分解成两个分量，

819
00:41:22,230 --> 00:41:26,050
第一个分量是我们的损失对输出的导数，

820
00:41:26,250 --> 00:41:30,125
乘以我们的输出对 w2 的导数，

821
00:41:30,125 --> 00:41:35,775
这只是一个标准的链式规则的实例化，

822
00:41:35,775 --> 00:41:38,600
用左侧的原始导数。

823
00:41:39,770 --> 00:41:42,340
让我们假设我们想要计算权重的梯度，

824
00:41:42,340 --> 00:41:45,540
在此之前，在这种情况下，不是 w1 ，而是 w ，

825
00:41:45,770 --> 00:41:48,000
抱歉，不是 w2 ，而是 w1 ，

826
00:41:48,830 --> 00:41:51,330
我们所做的就是用 w1 替换 w2 ，

827
00:41:51,500 --> 00:41:53,560
链式法则仍然有效，

828
00:41:53,560 --> 00:41:54,840
同样的方程成立，

829
00:41:54,860 --> 00:41:57,550
但现在你可以在红色部分看到，

830
00:41:57,550 --> 00:41:59,220
链式规则的最后一个部分，

831
00:41:59,300 --> 00:42:02,650
我们必须再次递归地应用另一个链式规则，

832
00:42:02,650 --> 00:42:06,310
因为这又是另一个我们不能直接求值的导数，

833
00:42:06,310 --> 00:42:09,990
我们可以用链式规则的另一个实例化再次扩展，

834
00:42:10,190 --> 00:42:11,700
现在所有这些组件，

835
00:42:12,320 --> 00:42:16,705
都可以通过神经网络中的隐藏单元直接传播这些梯度，

836
00:42:16,705 --> 00:42:17,575
在我们的神经网络中，

837
00:42:17,575 --> 00:42:21,475
一直回到这个例子中感兴趣的权重，

838
00:42:21,475 --> 00:42:24,150
所以我们首先计算对 w2 的导数，

839
00:42:24,680 --> 00:42:26,770
然后我们可以反向传播这些信息，并使用这些信息，

840
00:42:27,180 --> 00:42:28,010
也对于 w1 ，

841
00:42:28,010 --> 00:42:29,750
这就是为什么我们称之为反向传播，

842
00:42:29,750 --> 00:42:33,190
因为这个过程发生从输出一直到输入。

843
00:42:34,650 --> 00:42:38,200
现在，我们在训练过程中重复这个过程很多次，

844
00:42:38,610 --> 00:42:43,000
通过网络一遍又一遍地传播这些梯度，

845
00:42:43,290 --> 00:42:48,310
从输出到输入来确定每个单一权重，回答这个问题，

846
00:42:48,810 --> 00:42:53,630
即这些权重的微小变化对我们的损失函数的影响有多大，

847
00:42:53,630 --> 00:42:55,115
它是增加或减少，

848
00:42:55,115 --> 00:42:58,160
以及我们如何利用它来最终改善损失，

849
00:42:58,160 --> 00:43:00,480
因为这是我们这门课的最终目标。

850
00:43:02,710 --> 00:43:04,650
这就是反向传播算法，

851
00:43:04,650 --> 00:43:07,490
那是训练神经网络的核心，

852
00:43:08,050 --> 00:43:09,675
理论上，这很简单，

853
00:43:09,675 --> 00:43:13,460
这只是链条规则的实例化，

854
00:43:14,380 --> 00:43:16,530
但让我们来谈谈一些见解，

855
00:43:16,530 --> 00:43:20,390
使训练神经网络在实践中变得极其复杂，

856
00:43:20,470 --> 00:43:24,105
尽管反向传播的算法很简单，

857
00:43:24,105 --> 00:43:26,240
而且几十年时间，

858
00:43:26,920 --> 00:43:31,080
然而，在实践中，神经网络的优化看起来就像这样，

859
00:43:31,080 --> 00:43:33,350
它看起来一点也不像我之前给你看的那张图片，

860
00:43:33,790 --> 00:43:36,890
有很多方法可以让我们看到非常大、很深的神经网络，

861
00:43:37,300 --> 00:43:41,565
你可以想象这些模型的图景是这样的，

862
00:43:41,565 --> 00:43:44,600
这是几年前发表的一篇论文中的一个插图，

863
00:43:44,770 --> 00:43:48,440
他们试图可视化非常、非常深的神经网络的图景，

864
00:43:49,090 --> 00:43:50,925
这就是这个图景的真实面貌，

865
00:43:50,925 --> 00:43:53,370
这就是你试图处理的，在这个空间中找到最小的，

866
00:43:53,370 --> 00:43:55,970
你可以想象随之而来的挑战，

867
00:43:57,790 --> 00:43:58,700
为了涵盖这些挑战，

868
00:43:58,900 --> 00:44:04,455
让我们首先考虑并回想一下在梯度下降中定义的更新方程。

869
00:44:04,455 --> 00:44:07,760
所以我没有太多地谈论这个参数 η ，

870
00:44:07,810 --> 00:44:10,410
但是现在让我们花一点时间来思考这个，

871
00:44:10,410 --> 00:44:11,790
这就是所谓的学习率，

872
00:44:11,790 --> 00:44:12,650
就像我们之前看到的那样，

873
00:44:12,940 --> 00:44:18,315
它决定了我们需要在梯度方向上迈出多大的一步，

874
00:44:18,315 --> 00:44:20,330
以及反向传播的每一次迭代，

875
00:44:21,230 --> 00:44:24,870
在实践中，即使是设定学习速度也可能是非常具有挑战性的，

876
00:44:24,890 --> 00:44:27,175
你作为神经网络的设计者，

877
00:44:27,175 --> 00:44:29,520
必须设置这个值，这个学习率，

878
00:44:29,780 --> 00:44:31,315
如何选择这个值，

879
00:44:31,315 --> 00:44:32,830
这可能是相当困难的，

880
00:44:32,830 --> 00:44:37,015
在构建神经网络时，它会产生非常大的影响，

881
00:44:37,015 --> 00:44:41,300
所以，比如，如果我们把学习率设定得太低，

882
00:44:41,980 --> 00:44:43,990
然后我们学得很慢，

883
00:44:43,990 --> 00:44:46,200
我们假设我们从右手边开始，

884
00:44:46,280 --> 00:44:47,335
在最初的猜测中，

885
00:44:47,335 --> 00:44:49,200
如果我们的学习率不够大，

886
00:44:49,490 --> 00:44:51,450
我们不仅收敛得很慢，

887
00:44:51,470 --> 00:44:54,150
我们甚至不会收敛到全局最小值，

888
00:44:54,440 --> 00:44:56,280
因为我们有点陷入局部最小值，

889
00:44:57,740 --> 00:45:00,295
现在，如果我们把我们的学习率设定得太高，

890
00:45:00,295 --> 00:45:01,240
可能发生的情况是，

891
00:45:01,240 --> 00:45:04,920
我们超调了，我们可能开始偏离解决方案，

892
00:45:05,360 --> 00:45:07,080
这些渐变可能会爆炸，

893
00:45:07,340 --> 00:45:08,430
非常糟糕的事情发生，

894
00:45:08,480 --> 00:45:10,230
然后神经网络就不交易了，

895
00:45:10,700 --> 00:45:11,765
所以这也不是什么好事，

896
00:45:11,765 --> 00:45:16,900
实际上，在把它设置得太小，设置得太大，之间有一个非常好的中间选择，

897
00:45:17,400 --> 00:45:21,790
你设置的地方，大到足以超过某些局部极小值之间，

898
00:45:22,470 --> 00:45:24,670
把你放在搜索空间的合理部分，

899
00:45:24,930 --> 00:45:28,300
在那里你可以真正收敛到你最关心的解决方案上。

900
00:45:29,130 --> 00:45:32,090
但是，实际上，你是如何在实践中设定这些学习率的，

901
00:45:32,200 --> 00:45:34,790
你如何选择理想的学习率是多少。

902
00:45:35,080 --> 00:45:38,330
一种选择，实际上这在实践中是非常常见的选择，

903
00:45:38,350 --> 00:45:40,800
就是简单地尝试一系列学习率，

904
00:45:40,800 --> 00:45:42,170
看看哪种效果最好，所以试一试，比方说，一个由不同学习率组成的整个网格，你知道，训练所有这些神经网络，看看哪一个工作得最好。

905
00:45:42,490 --> 00:45:45,290
所以试一试，比方说，一个由不同学习率组成的整个网格，

906
00:45:45,670 --> 00:45:47,775
训练所有这些神经网络，

907
00:45:47,775 --> 00:45:48,950
看看哪一个工作得最好。

908
00:45:49,850 --> 00:45:52,570
但我觉得我们可以做更聪明的事，

909
00:45:52,570 --> 00:45:55,000
那么，我们有哪些更聪明的方法可以做到这一点，

910
00:45:55,000 --> 00:45:57,990
而不是详尽地尝试一大堆不同的学习率，

911
00:45:58,430 --> 00:46:00,775
我们能不能设计一种学习率算法

912
00:46:00,775 --> 00:46:04,810
来适应我们的神经网络，并适应它的图景，

913
00:46:04,810 --> 00:46:08,010
这样它就比以前的想法更智能。

914
00:46:09,450 --> 00:46:11,410
所以，这实际上最终意味着，

915
00:46:12,440 --> 00:46:18,100
学习率，算法信任梯度的速度

916
00:46:18,330 --> 00:46:22,300
将取决于该位置的梯度有多大，

917
00:46:22,890 --> 00:46:24,280
以及我们学习的速度有多快，

918
00:46:24,330 --> 00:46:28,330
有多少其他选择，抱歉，许多其他选择，

919
00:46:29,070 --> 00:46:32,090
作为神经网络训练的一部分，

920
00:46:32,090 --> 00:46:33,850
所以，不仅是我们学习的速度有多快，

921
00:46:33,960 --> 00:46:37,420
你还可以根据学习图景中的许多不同因素进行判断。

922
00:46:39,210 --> 00:46:44,210
事实上，我们都是我所说的这些不同的算法，

923
00:46:44,210 --> 00:46:48,550
这些自适应学习率算法在实践中得到了非常广泛的研究，

924
00:46:48,660 --> 00:46:52,865
在深度学习研究社区中，有一个非常蓬勃发展的社区，

925
00:46:52,865 --> 00:46:58,540
专注于开发和设计学习率自适应新的算法，

926
00:46:58,770 --> 00:47:02,080
更快地优化像这样的大型神经网络，

927
00:47:02,760 --> 00:47:03,770
在你的实验中，

928
00:47:03,770 --> 00:47:09,320
你不仅有机会尝试许多不同的自适应算法，

929
00:47:09,320 --> 00:47:10,270
你可以在这里看到的，

930
00:47:10,780 --> 00:47:15,100
但也试图揭示一种模式和另一种模式的好处，

931
00:47:15,100 --> 00:47:19,225
我认为这将是非常有洞察力的东西，

932
00:47:19,225 --> 00:47:20,340
作为你们实验的一部分。

933
00:47:21,330 --> 00:47:24,305
所以，你们将看到的实验的另一个关键部分是，

934
00:47:24,305 --> 00:47:30,575
如何将我们今天介绍的所有这些信息实际放入一张大致类似于下面的图片中，

935
00:47:30,575 --> 00:47:33,845
这张图片首先定义了你的模型，在这里的顶部，

936
00:47:33,845 --> 00:47:34,880
这是你定义模型的地方，

937
00:47:34,880 --> 00:47:36,850
你在课程的开头部分谈到了这一点。

938
00:47:38,100 --> 00:47:39,710
对于模型中的每一部分，

939
00:47:39,850 --> 00:47:42,600
你现在都需要定义这个优化器，

940
00:47:42,600 --> 00:47:43,710
我们刚刚谈到了这一点，

941
00:47:43,710 --> 00:47:47,025
这个优化器是和学习率一起定义的，

942
00:47:47,025 --> 00:47:49,940
你想要以多快的速度优化你的损失图景，

943
00:47:49,990 --> 00:47:54,020
经过许多循环，你将通过你数据集中的所有例子，

944
00:47:54,720 --> 00:47:58,550
从本质上观察如何改善你的神经网络，这就是梯度，

945
00:47:58,630 --> 00:48:00,840
然后在这些方向上改善网络，

946
00:48:00,840 --> 00:48:02,960
一遍又一遍地这样做，

947
00:48:03,190 --> 00:48:07,010
直到你的神经网络最终收敛到某种解决方案。

948
00:48:09,730 --> 00:48:11,610
所以，我想非常简短地，

949
00:48:11,610 --> 00:48:13,040
在剩下的时间里，

950
00:48:13,390 --> 00:48:17,330
继续谈论训练这些神经网络的实践中的技巧，

951
00:48:17,800 --> 00:48:20,360
并专注于这个非常强大的想法，

952
00:48:20,740 --> 00:48:27,590
将你的数据批量处理成所谓的小批量较小的数据片段。

953
00:48:28,420 --> 00:48:31,590
为此，让我们重温梯度下降算法，

954
00:48:31,590 --> 00:48:35,030
所以在这里，我们之前讨论的这个梯度，

955
00:48:35,170 --> 00:48:39,080
计算起来非常昂贵，

956
00:48:39,160 --> 00:48:44,300
因为它是通过数据集中所有部分的总和来计算的，

957
00:48:45,270 --> 00:48:47,860
在大多数现实生活或现实世界的问题中，

958
00:48:48,150 --> 00:48:52,670
在你的整个数据集上计算梯度是不可行的，

959
00:48:52,670 --> 00:48:54,970
如今的数据集实在太大了。

960
00:48:55,380 --> 00:48:58,100
所以你知道还有其他选择，

961
00:48:58,100 --> 00:48:59,050
还有其他选择吗，

962
00:48:59,100 --> 00:49:04,270
不是计算整个数据集的导数或梯度，

963
00:49:04,800 --> 00:49:10,400
而是只计算数据集中的单个示例的梯度，

964
00:49:10,400 --> 00:49:11,170
就一个例子，

965
00:49:11,190 --> 00:49:16,085
当然，这个，对你的梯度的估计，就是这样，

966
00:49:16,085 --> 00:49:17,920
这是一个估计，它将是非常嘈杂的，

967
00:49:18,030 --> 00:49:21,400
它可能粗略地反映了你整个数据集的趋势，

968
00:49:21,540 --> 00:49:25,330
但因为它是一个非常，它只是你整个数据集的一个例子，

969
00:49:25,470 --> 00:49:26,770
它可能会非常嘈杂。

970
00:49:29,610 --> 00:49:31,700
这样做的好处是，

971
00:49:31,700 --> 00:49:36,520
在一个例子中，计算梯度明显要快得多，

972
00:49:36,690 --> 00:49:37,700
因为这是一个例子，

973
00:49:37,700 --> 00:49:40,060
所以从计算上来说，这有很大的优势，

974
00:49:40,620 --> 00:49:42,970
但缺点是它是非常随机的，

975
00:49:43,560 --> 00:49:46,190
这就是为什么这种算法不叫梯度下降的原因，

976
00:49:46,190 --> 00:49:48,340
这现在被称为随机梯度下降。

977
00:49:49,440 --> 00:49:50,975
现在，中间立场是什么，

978
00:49:50,975 --> 00:49:53,740
与其针对数据集中的一个示例计算它，

979
00:49:53,820 --> 00:49:57,070
不如我们计算所谓的一小批示例，

980
00:49:57,180 --> 00:50:01,060
一小批我们可以计算梯度的例子，

981
00:50:01,140 --> 00:50:03,310
当我们计算这些梯度时，

982
00:50:03,900 --> 00:50:07,030
它们的计算效率仍然很高，

983
00:50:07,140 --> 00:50:09,155
因为它是一个小批量，不是太大，

984
00:50:09,155 --> 00:50:13,510
也许我们谈论的是我们数据集中数十或数百个例子的数量级，

985
00:50:14,040 --> 00:50:16,320
但是，更重要的是，

986
00:50:16,940 --> 00:50:19,980
因为我们已经从一个例子扩展到可能一百个例子，

987
00:50:20,540 --> 00:50:22,915
随机性显著降低，

988
00:50:22,915 --> 00:50:25,410
我们的梯度的精度大大提高。

989
00:50:26,310 --> 00:50:29,855
所以通常我们考虑的是批次大小，小型批次大小，

990
00:50:29,855 --> 00:50:33,995
大约 100 个数据点，数十个或数百个数据点，

991
00:50:33,995 --> 00:50:37,220
这显然比梯度下降的计算速度快得多，

992
00:50:37,220 --> 00:50:41,105
比随机梯度下降的计算精度高得多，

993
00:50:41,105 --> 00:50:43,840
后者是单点的例子。

994
00:50:44,830 --> 00:50:48,975
所以，梯度精度的提高使我们

995
00:50:48,975 --> 00:50:51,720
能够更快地收敛到我们的解，

996
00:50:51,720 --> 00:50:56,055
而不是由于梯度下降的限制而在实践中可能实现的，

997
00:50:56,055 --> 00:50:59,100
这也意味着我们可以提高我们的学习率，

998
00:50:59,100 --> 00:51:03,045
因为我们可以更有效地信任每个梯度，

999
00:51:03,045 --> 00:51:04,850
我们现在是一批的平均数，

1000
00:51:05,230 --> 00:51:07,755
它将比随机版本精确得多，

1001
00:51:07,755 --> 00:51:09,030
所以我们可以提高学习率，

1002
00:51:09,030 --> 00:51:11,060
也可以学得更快。

1003
00:51:12,240 --> 00:51:17,555
这使得我们还可以大规模并行化整个算法和计算，

1004
00:51:17,555 --> 00:51:19,990
我们可以把批次分成不同的工作者，

1005
00:51:20,610 --> 00:51:26,080
并使用 GPU 实现整个问题更显著的加速。

1006
00:51:26,160 --> 00:51:31,835
在今天的演讲中，我非常、非常简短地想要讨论的最后一个话题是，

1007
00:51:31,835 --> 00:51:33,920
这个话题就是过度拟合，

1008
00:51:33,920 --> 00:51:38,540
当我们使用随机梯度下降来优化神经网络的时候，

1009
00:51:39,040 --> 00:51:42,015
我们遇到了这个挑战，所谓的过拟合，

1010
00:51:42,015 --> 00:51:45,195
过拟合看起来大致这样，

1011
00:51:45,195 --> 00:51:46,400
所以在左手边，

1012
00:51:47,440 --> 00:51:49,200
我们想要建立一个神经网络，

1013
00:51:49,200 --> 00:51:52,020
或者说，总的来说，我们想要建立一个机器学习模型，

1014
00:51:52,020 --> 00:51:55,850
它可以准确地描述我们数据中的一些模式，

1015
00:51:56,580 --> 00:52:00,530
但请记住，最终我们不想描述我们的训练数据中的模式，

1016
00:52:00,700 --> 00:52:03,915
理想情况下，我们希望定义测试数据中的模式，

1017
00:52:03,915 --> 00:52:07,155
当然，我们不观察测试数据，我们只观察训练数据，

1018
00:52:07,155 --> 00:52:10,430
所以，我们面临着这个挑战，从训练数据中提取模式，

1019
00:52:10,660 --> 00:52:13,340
并希望它们推广到我们的测试数据。

1020
00:52:13,900 --> 00:52:15,440
所以，换一种不同的说法，

1021
00:52:15,760 --> 00:52:19,100
我们希望构建能够从我们的训练数据中学习表示法的模型，

1022
00:52:19,480 --> 00:52:21,105
这些模型仍然可以泛化，

1023
00:52:21,105 --> 00:52:25,100
即使我们向它们展示全新的、没见过的测试数据片段。

1024
00:52:25,720 --> 00:52:27,380
假设你想构建一条直线，

1025
00:52:27,550 --> 00:52:32,300
它可以描述或找到幻灯片上这些点的模式，

1026
00:52:33,130 --> 00:52:35,180
如果你有一个非常简单的神经网络，

1027
00:52:35,320 --> 00:52:37,640
这只是一条直线。

1028
00:52:38,610 --> 00:52:43,070
你可以以次优的方式描述这个数据，

1029
00:52:43,070 --> 00:52:44,710
因为这里的数据是非线性的，

1030
00:52:44,820 --> 00:52:49,600
你不会准确地捕捉到这个数据集中的所有细微差别和微妙之处，

1031
00:52:49,770 --> 00:52:50,870
这在左手边，

1032
00:52:50,870 --> 00:52:52,300
如果你移到右手边，

1033
00:52:52,650 --> 00:52:54,730
你可以看到一个更复杂的模型，

1034
00:52:54,780 --> 00:52:58,120
但这里你表达得太多了，你太有表现力了，

1035
00:52:58,200 --> 00:53:03,575
你捕捉到了训练数据中的某种细微差别，虚假的细微差别，

1036
00:53:03,575 --> 00:53:07,000
实际上并不能表示你的测试数据。

1037
00:53:07,790 --> 00:53:09,930
理想情况下，你希望模型位于中间，

1038
00:53:10,190 --> 00:53:11,980
这就是中间地带，

1039
00:53:11,980 --> 00:53:14,820
这不是太复杂，也不是太简单，

1040
00:53:14,840 --> 00:53:16,950
它仍然会给你你想要的表现良好的东西，

1041
00:53:17,360 --> 00:53:19,680
即使你给了它全新的数据。

1042
00:53:19,910 --> 00:53:21,090
所以，为了解决这个问题，

1043
00:53:21,320 --> 00:53:24,510
让我们简要地谈谈所谓的正则化，

1044
00:53:25,100 --> 00:53:26,490
正则化是一种技术，

1045
00:53:26,660 --> 00:53:29,010
你可以将其引入到训练管道中，

1046
00:53:29,540 --> 00:53:32,430
以阻止学习复杂的模型。

1047
00:53:33,320 --> 00:53:34,830
现在，正如我们之前看到的，

1048
00:53:34,910 --> 00:53:36,030
这一点非常关键，

1049
00:53:36,380 --> 00:53:38,970
因为神经网络是非常大的模型，

1050
00:53:39,080 --> 00:53:41,910
它们非常容易过拟合，

1051
00:53:42,230 --> 00:53:48,835
所以正则化和拥有正则化技术对神经网络的成功有着极端的影响，

1052
00:53:48,835 --> 00:53:51,120
让它们超越训练数据，

1053
00:53:51,410 --> 00:53:52,800
推广到我们的测试领域。

1054
00:53:53,810 --> 00:53:58,200
深度学习中最流行的正则化技术称为 dropout ，

1055
00:53:58,340 --> 00:54:00,960
dropout 的想法其实很简单，

1056
00:54:01,640 --> 00:54:04,585
让我们通过绘制深度神经网络的图片来重温它，

1057
00:54:04,585 --> 00:54:06,240
我们在今天的讲座前面看到的，

1058
00:54:06,890 --> 00:54:08,220
在 dropout ，在训练期间，

1059
00:54:08,270 --> 00:54:13,080
我们随机选择这个神经网络中的一些神经元子集，

1060
00:54:13,760 --> 00:54:17,990
我们试着用一些随机的概率把他们排除在外，

1061
00:54:17,990 --> 00:54:21,425
例如，我们可以选择这部分神经元，

1062
00:54:21,425 --> 00:54:24,280
我们可以以 50% 的概率随机选择它们，

1063
00:54:24,930 --> 00:54:26,530
使用这个概率，

1064
00:54:26,610 --> 00:54:31,270
我们在训练的不同迭代中随机关闭或打开它们。

1065
00:54:32,170 --> 00:54:36,420
所以，这本质上是迫使神经网络学习，

1066
00:54:36,420 --> 00:54:39,050
你可以想到一个由不同模型组成的整体，

1067
00:54:39,070 --> 00:54:40,100
在每次迭代中，

1068
00:54:40,360 --> 00:54:44,520
它将在迭代不同的模型，

1069
00:54:44,520 --> 00:54:46,125
比起它在上一次迭代中，

1070
00:54:46,125 --> 00:54:51,020
所以，它必须学习如何建立内部路径来处理相同的信息，

1071
00:54:51,310 --> 00:54:54,680
而且它不能依赖从以前的迭代中学到的信息，

1072
00:54:54,940 --> 00:55:00,020
它迫使它在某种程度上捕捉到神经网络路径中的一些更深层次的含义，

1073
00:55:00,430 --> 00:55:01,760
这可能是非常强大的，

1074
00:55:01,840 --> 00:55:05,660
因为第一，它显著降低了神经网络的容量，

1075
00:55:06,220 --> 00:55:09,110
在本例中，您将它降低了大约 50% ，

1076
00:55:10,310 --> 00:55:12,240
这也是因为它使它们更容易训练，

1077
00:55:12,380 --> 00:55:16,525
因为在这种情况下，具有梯度的权重的数量也减少了，

1078
00:55:16,525 --> 00:55:19,050
所以训练它们也要快得多。

1079
00:55:20,200 --> 00:55:21,170
现在就像我提到的，

1080
00:55:21,700 --> 00:55:26,420
在每一次迭代中，我们随机 dropout 一组不同的神经元，

1081
00:55:26,530 --> 00:55:28,400
这有助于更好地概括数据。

1082
00:55:29,050 --> 00:55:31,250
第二种正则化技术，

1083
00:55:31,270 --> 00:55:35,180
是一种远远超出神经网络的非常广泛的正则化技术，

1084
00:55:35,650 --> 00:55:37,550
简单地称为提前停止。

1085
00:55:38,290 --> 00:55:43,230
我们知道过拟合的定义是，

1086
00:55:43,230 --> 00:55:48,585
当我们的模型开始更多地代表训练数据而不是测试数据时，

1087
00:55:48,585 --> 00:55:50,990
这确实是过度适应的核心所在，

1088
00:55:51,700 --> 00:55:55,020
如果我们把一些训练数据放在一边单独使用，

1089
00:55:55,020 --> 00:55:56,085
我们没有在上面训练，

1090
00:55:56,085 --> 00:56:01,560
我们可以使用某种测试数据集，合成测试数据集，

1091
00:56:01,560 --> 00:56:07,500
在某些方面，我们可以监控我们的网络如何学习这部分没看见的数据，

1092
00:56:07,500 --> 00:56:09,825
例如，我们可以在整个训练过程中，

1093
00:56:09,825 --> 00:56:12,440
我们可以绘制出网络的性能，

1094
00:56:13,060 --> 00:56:15,990
在训练集和我们的坚持测试集上，

1095
00:56:16,460 --> 00:56:17,970
随着网络的训练，

1096
00:56:17,990 --> 00:56:20,290
我们将首先看到这两个参数都有所下降，

1097
00:56:20,290 --> 00:56:21,750
但总有一天，

1098
00:56:22,340 --> 00:56:26,130
损失会停滞不前并开始增加，

1099
00:56:26,180 --> 00:56:28,075
训练损失开始增加，

1100
00:56:28,075 --> 00:56:30,420
这正是你开始过拟合的时候，

1101
00:56:30,560 --> 00:56:32,520
因为现在你开始有，

1102
00:56:33,410 --> 00:56:34,540
抱歉，这是测试损失，

1103
00:56:34,540 --> 00:56:36,265
测试损失实际上开始增加，

1104
00:56:36,265 --> 00:56:38,970
因为现在你开始过拟合你的训练数据，

1105
00:56:39,560 --> 00:56:42,090
这种模式会在接下来的训练中持续下去，

1106
00:56:42,720 --> 00:56:45,470
这就是我希望你们关注的一点，

1107
00:56:45,580 --> 00:56:47,955
这个中间点是我们需要停止训练的地方，

1108
00:56:47,955 --> 00:56:49,010
因为在这一点之后，

1109
00:56:49,540 --> 00:56:54,440
假设该测试集是真实测试集的有效表示，

1110
00:56:54,670 --> 00:56:57,740
这是模型的准确性只会变差的地方，

1111
00:56:58,060 --> 00:57:00,045
所以这就是我们想要提前停止我们的模式，

1112
00:57:00,045 --> 00:57:01,820
并使性能正规化的地方。

1113
00:57:02,970 --> 00:57:03,755
我们可以看到，

1114
00:57:03,755 --> 00:57:07,235
在这一点之前的任何时候停止也是不好的，

1115
00:57:07,235 --> 00:57:08,950
我们将生产一个不适合的模型，

1116
00:57:09,030 --> 00:57:11,380
在这个模型中，我们可以根据测试数据得到更好的模型，

1117
00:57:11,790 --> 00:57:12,905
但这是一种权衡，

1118
00:57:12,905 --> 00:57:15,430
你不能停得太晚，也不能停得太早。

1119
00:57:17,340 --> 00:57:21,380
所以我将通过总结这三个要点来结束这节课，

1120
00:57:21,380 --> 00:57:24,305
我们在今天的课上讲到的。

1121
00:57:24,305 --> 00:57:27,940
所以我们首先介绍了所有神经网络的这些基本组成部分，

1122
00:57:27,960 --> 00:57:30,040
这就是单个神经元，感知器，

1123
00:57:30,240 --> 00:57:33,695
我们已经将它们构建成更大的神经层，

1124
00:57:33,695 --> 00:57:36,430
然后从它们的神经网络和深度神经网络中，

1125
00:57:37,050 --> 00:57:38,740
我们学习了如何训练它们，

1126
00:57:38,790 --> 00:57:40,595
将它们应用到数据集，

1127
00:57:40,595 --> 00:57:41,530
通过它们进行传播，

1128
00:57:41,790 --> 00:57:47,140
我们已经看到了一些端到端优化这些系统的技巧。

1129
00:57:48,010 --> 00:57:48,825
在下一节课中，

1130
00:57:48,825 --> 00:57:52,580
我们将听到 Ava 关于使用 RNN 深度序列建模，

1131
00:57:53,020 --> 00:57:57,300
特别是这种非常令人兴奋的新型模型，

1132
00:57:57,300 --> 00:58:00,620
称为转换器体系结构和注意机制，

1133
00:58:01,120 --> 00:58:03,870
所以，大约五分钟后我们再继续上课，

1134
00:58:03,870 --> 00:58:05,445
也许在我们有机会交换演讲者之后，

1135
00:58:05,445 --> 00:58:07,850
非常感谢大家的关注。

