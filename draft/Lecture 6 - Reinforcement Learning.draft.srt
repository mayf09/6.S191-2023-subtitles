1
00:00:09,040 --> 00:00:11,480
0,350 350,605 605,875 875,1240 2040,2440
Hi everyone, welcome back. Today,
大家好，欢迎回来。今天，我认为今天的这两堂课真的很令人兴奋，因为它们开始超越。到目前为止，我们在这门课上讲了很多东西，主要集中在非常静态的数据集上，特别是在今天。在这堂课中，我现在要开始谈论如何学习这个由来已久的领域，我们如何特别地结合两个话题。第一个主题是强化学习。

2
00:00:11,710 --> 00:00:13,455
0,400 420,725 725,1030 1230,1550 1550,1745
I think that these two

3
00:00:13,455 --> 00:00:14,895
0,390 390,630 630,840 840,1080 1080,1440
lectures today are really exciting

4
00:00:14,895 --> 00:00:16,395
0,285 285,575 805,1110 1110,1275 1275,1500
because they start to move

5
00:00:16,395 --> 00:00:17,610
0,365 475,735 735,885 885,1050 1050,1215
beyond. You know, a lot

6
00:00:17,610 --> 00:00:18,675
0,180 180,360 360,645 645,855 855,1065
of what we've talked about

7
00:00:18,675 --> 00:00:19,790
0,135 135,285 285,510 510,780 780,1115
in the class so far,

8
00:00:20,140 --> 00:00:21,270
0,275 275,440 440,725 725,980 980,1130
which is focusing a lot

9
00:00:21,270 --> 00:00:23,085
0,270 270,630 630,1040 1180,1530 1530,1815
on really static data sets

10
00:00:23,085 --> 00:00:24,675
0,300 300,665 685,990 990,1295 1315,1590
and specifically in today. In

11
00:00:24,675 --> 00:00:26,610
0,180 180,480 480,810 810,1145 1615,1935
this lecture right now I'm

12
00:00:26,610 --> 00:00:27,375
0,120 120,270 270,420 420,570 570,765
going to start to talk

13
00:00:27,375 --> 00:00:28,245
0,225 225,375 375,495 495,645 645,870
about how we can learn

14
00:00:28,245 --> 00:00:29,870
0,225 225,405 405,695 745,1145 1225,1625
about this very long standing

15
00:00:30,040 --> 00:00:31,035
0,350 350,560 560,695 695,830 830,995
field of how we can

16
00:00:31,035 --> 00:00:33,570
0,305 475,875 1285,1685 1705,2105 2275,2535
specifically marry two topics. The

17
00:00:33,570 --> 00:00:35,840
0,180 180,500 520,920 1180,1980 1980,2270
first topic being reinforcement learning.|
|

18
00:00:36,480 --> 00:00:37,965
0,290 370,705 705,1095 1095,1260 1260,1485
Which has existed for many,
它已经存在了很多，几十年了，还有很多深度学习的最新进展，作为这门课程的一部分，你们已经开始学习了。

19
00:00:37,965 --> 00:00:39,900
0,270 270,605 1015,1410 1410,1740 1740,1935
many decades, together with a

20
00:00:39,900 --> 00:00:40,850
0,135 135,285 285,420 420,615 615,950
lot of the very recent

21
00:00:40,930 --> 00:00:43,005
0,700 960,1325 1325,1595 1595,1850 1850,2075
advances in deep learning, which

22
00:00:43,005 --> 00:00:44,430
0,255 255,495 495,795 795,1125 1125,1425
you've already started learning about

23
00:00:44,430 --> 00:00:45,620
0,255 255,465 465,615 615,840 840,1190
as part of this course.|
|

24
00:00:46,940 --> 00:00:49,090
0,400 540,940 1080,1480 1620,1910 1910,2150
Now this marriage of these
现在这两个领域的结合对我来说真的很吸引人，特别是因为就像我说的，它背离了整个范例啊，或者说是我们到目前为止在课堂上接触到的整个范例。而这种模式就是我们可以使用一些数据集来构建深度学习模型的方式。但这个数据集通常是固定在我们的世界里的，对吗？我们收集，我们出去收集数据集，我们将其部署在我们的机器学习或深度学习算法上，然后我们可以在一个全新的数据集上进行评估。

25
00:00:49,090 --> 00:00:50,550
0,255 255,560 610,975 975,1200 1200,1460
two fields is actually really

26
00:00:50,720 --> 00:00:52,860
0,380 380,620 620,880 960,1360 1740,2140
fascinating to me, particularly because

27
00:00:53,000 --> 00:00:54,115
0,260 260,410 410,605 605,830 830,1115
like I said, it moves

28
00:00:54,115 --> 00:00:55,680
0,285 285,480 480,630 630,810 810,1565
away from this whole paradigm

29
00:00:55,850 --> 00:00:57,895
0,400 990,1295 1295,1475 1475,1750 1770,2045
of ah, or really this

30
00:00:57,895 --> 00:00:59,040
0,165 165,540 540,660 660,900 900,1145
whole paradigm that we've been

31
00:00:59,090 --> 00:01:00,450
0,380 380,710 710,935 935,1085 1085,1360
exposed to in the class

32
00:01:00,590 --> 00:01:02,160
0,335 335,545 545,680 680,845 845,1570
thus far. And that paradigm

33
00:01:02,270 --> 00:01:03,640
0,380 380,710 710,950 950,1130 1130,1370
is really how we can

34
00:01:03,640 --> 00:01:05,340
0,350 550,855 855,1065 1065,1335 1335,1700
build a deep learning model

35
00:01:06,050 --> 00:01:07,315
0,380 380,650 650,890 890,1130 1130,1265
using some data set. But

36
00:01:07,315 --> 00:01:08,460
0,165 165,435 435,645 645,825 825,1145
that data set is typically

37
00:01:08,660 --> 00:01:10,825
0,400 720,1085 1085,1450 1530,1850 1850,2165
fixed in in our world,

38
00:01:10,825 --> 00:01:11,980
0,300 300,540 540,825 825,1035 1035,1155
right? We collect, we go

39
00:01:11,980 --> 00:01:13,000
0,120 120,255 255,530 550,840 840,1020
out and go collect that

40
00:01:13,000 --> 00:01:14,425
0,240 240,590 760,1095 1095,1305 1305,1425
data set, we deploy it

41
00:01:14,425 --> 00:01:15,355
0,105 105,300 300,525 525,735 735,930
on our machine learning or

42
00:01:15,355 --> 00:01:16,885
0,165 165,455 505,1055 1165,1425 1425,1530
deep learning algorithm and then

43
00:01:16,885 --> 00:01:17,950
0,120 120,300 300,605 685,945 945,1065
we can evaluate on a

44
00:01:17,950 --> 00:01:19,260
0,165 165,345 345,570 570,920
brand new data set.|
|

45
00:01:19,330 --> 00:01:20,450
0,275 275,425 425,575 575,785 785,1120
But that is very different
但这与现实世界中的运作方式有很大不同。在现实世界中，你的深度学习模型实际上与数据一起部署在一起，进入现实，探索，与其环境互动，并在该环境中尝试一大堆不同的行动和不同的事情，以便能够学习如何最好地执行任何可能需要完成的特定任务。通常，我们希望能够在没有明确的人类监督的情况下做到这一点，对吧。这是强化学习的关键动机。你将尝试通过强化来学习，在你的世界中犯错误，然后收集这些错误的数据来学习如何改进。

46
00:01:20,530 --> 00:01:21,830
0,335 335,515 515,680 680,950 950,1300
than the way things work

47
00:01:21,970 --> 00:01:23,055
0,290 290,425 425,575 575,860 860,1085
in the real world. In

48
00:01:23,055 --> 00:01:24,510
0,120 120,300 300,605 895,1200 1200,1455
the real world, you have

49
00:01:24,510 --> 00:01:26,130
0,240 240,435 435,705 705,1070 1240,1620
your deep learning model actually

50
00:01:26,130 --> 00:01:27,860
0,525 525,915 915,1245 1245,1455 1455,1730
deployed together with the data

51
00:01:27,940 --> 00:01:30,740
0,320 320,560 560,880 930,1330 2340,2800
together, out into reality, exploring,

52
00:01:31,120 --> 00:01:33,135
0,620 620,875 875,1175 1175,1570 1710,2015
interacting with its environment and

53
00:01:33,135 --> 00:01:34,080
0,225 225,450 450,615 615,765 765,945
trying out a whole bunch

54
00:01:34,080 --> 00:01:35,595
0,165 165,440 460,860 940,1245 1245,1515
of different actions and different

55
00:01:35,595 --> 00:01:37,260
0,360 360,615 615,810 810,1145 1375,1665
things in that environment in

56
00:01:37,260 --> 00:01:38,265
0,255 255,480 480,585 585,780 780,1005
order to be able to

57
00:01:38,265 --> 00:01:39,765
0,275 475,795 795,960 960,1200 1200,1500
learn how to best perform

58
00:01:39,765 --> 00:01:41,690
0,300 300,695 715,1110 1110,1505 1525,1925
any particular task that may

59
00:01:42,550 --> 00:01:44,060
0,275 275,455 455,760 780,1145 1145,1510
need to accomplish. And typically

60
00:01:44,290 --> 00:01:45,240
0,275 275,470 470,665 665,785 785,950
we want to be able

61
00:01:45,240 --> 00:01:46,845
0,180 180,300 300,480 480,800 1120,1605
to do this without explicit

62
00:01:46,845 --> 00:01:48,540
0,305 445,905 1045,1365 1365,1560 1560,1695
human supervision, right. This is

63
00:01:48,540 --> 00:01:51,030
0,165 165,470 1000,1605 1605,1875 1875,2490
the key motivation of reinforcement

64
00:01:51,030 --> 00:01:51,975
0,210 210,510 510,645 645,825 825,945
learning. You're going to try

65
00:01:51,975 --> 00:01:53,810
0,135 135,405 405,675 675,1455 1455,1835
and learn through reinforcement, making

66
00:01:53,950 --> 00:01:55,620
0,400 570,860 860,1055 1055,1360 1410,1670
mistakes in your world and

67
00:01:55,620 --> 00:01:56,900
0,210 210,615 615,810 810,1005 1005,1280
then collecting data on those

68
00:01:56,920 --> 00:01:58,185
0,400 450,695 695,845 845,1055 1055,1265
mistakes to learn how to

69
00:01:58,185 --> 00:01:58,940
0,305
improve.|
|

70
00:01:59,600 --> 00:02:01,680
0,400 690,1025 1025,1360 1410,1745 1745,2080
Now, this is obviously a
现在，这显然是机器人和自主领域的一个巨大的领域或一个巨大的主题。你可以想到自动驾驶汽车和机器人操作，但最近我们也开始看到深度强化学习令人难以置信的进步，特别是在游戏和策略制定方面。所以一件很酷的事情是，现在你甚至可以想象这是正确的。

71
00:02:02,090 --> 00:02:03,865
0,365 365,730 990,1385 1385,1640 1640,1775
huge field in or a

72
00:02:03,865 --> 00:02:05,065
0,225 225,570 570,825 825,975 975,1200
huge topic in the field

73
00:02:05,065 --> 00:02:06,835
0,335 505,990 990,1170 1170,1635 1635,1770
of robotics and autonomy. You

74
00:02:06,835 --> 00:02:07,765
0,150 150,285 285,450 450,660 660,930
can think of self driving

75
00:02:07,765 --> 00:02:10,045
0,330 330,645 645,885 885,1505 1885,2280
cars and robot manipulation, but

76
00:02:10,045 --> 00:02:11,980
0,395 535,855 855,1175 1405,1740 1740,1935
also very recently we've started

77
00:02:11,980 --> 00:02:13,570
0,330 330,710 730,1260 1260,1410 1410,1590
seeing incredible advances of deep

78
00:02:13,570 --> 00:02:16,500
0,630 630,885 885,1280 2230,2580 2580,2930
reinforcement learning, specifically also on

79
00:02:16,610 --> 00:02:18,085
0,305 305,575 575,905 905,1205 1205,1475
the side of game play

80
00:02:18,085 --> 00:02:19,860
0,300 300,665 685,1085 1135,1455 1455,1775
and strategy making as well.

81
00:02:20,480 --> 00:02:21,970
0,400 600,890 890,1115 1115,1340 1340,1490
So one really cool thing

82
00:02:21,970 --> 00:02:22,705
0,135 135,300 300,480 480,630 630,735
is that now you can

83
00:02:22,705 --> 00:02:25,740
0,240 240,635 925,1245 1245,1565
even imagine right this.|
|

84
00:02:26,160 --> 00:02:29,560
0,400 570,970 1410,1810 2070,2650 3000,3400
This combination of robotics, together
这种机器人技术的结合，加上现在的游戏性，训练机器人在现实世界中与我们对抗，我只会在星际争霸和DeepMind上播放这段非常短的视频。

85
00:02:29,580 --> 00:02:31,490
0,320 320,830 830,1150 1260,1610 1610,1910
with gameplay right now, training

86
00:02:31,490 --> 00:02:32,930
0,450 450,675 675,915 915,1215 1215,1440
robots to play against us

87
00:02:32,930 --> 00:02:34,100
0,150 150,255 255,405 405,710 940,1170
in the real world, and

88
00:02:34,100 --> 00:02:34,985
0,165 165,315 315,495 495,675 675,885
I'll just play this very

89
00:02:34,985 --> 00:02:36,800
0,195 195,435 435,720 720,1325 1525,1815
short video on starcraft and

90
00:02:36,800 --> 00:02:38,460
0,470
deepmind.|
|

91
00:02:41,870 --> 00:02:43,825
0,400 660,1060 1140,1445 1445,1715 1715,1955
Perfect information and to display
完美的信息和实时显示。它还需要长期规划和从千千万万种可能性中选择采取什么行动的能力。我希望五零不会输掉任何一场比赛，但我认为现实的目标应该是四比一对我有利。我认为他看起来更自信了，泰勒之前也很紧张。这一次，房间里的气氛要紧张得多。真的不知道会发生什么。他从五岁起就一直在扮演明星，我没想到人工智能会这么好。他所做的每一件事都是恰当的，都是经过计算的，都做得很好。我以为我学到了一些东西。我会认为自己是个好球员，对吗？但我输掉了每一场比赛。

92
00:02:43,825 --> 00:02:45,835
0,165 165,405 405,755 1285,1680 1680,2010
in real time. It also

93
00:02:45,835 --> 00:02:47,485
0,335 415,750 750,1020 1020,1350 1350,1650
requires long term planning and

94
00:02:47,485 --> 00:02:48,745
0,195 195,465 465,765 765,1035 1035,1260
the ability to choose what

95
00:02:48,745 --> 00:02:49,980
0,255 255,495 495,660 660,900 900,1235
action to take from millions

96
00:02:50,060 --> 00:02:52,795
0,320 320,620 620,890 890,1180 2370,2735
and millions of possibilities. I'm

97
00:02:52,795 --> 00:02:53,965
0,225 225,480 480,630 630,870 870,1170
hoping for a five zero

98
00:02:53,965 --> 00:02:54,880
0,210 210,330 330,465 465,675 675,915
not to lose any games,

99
00:02:54,880 --> 00:02:55,945
0,180 180,285 285,390 390,525 525,1065
but I think the realistic

100
00:02:55,945 --> 00:02:56,845
0,180 180,375 375,555 555,735 735,900
goal would be four and

101
00:02:56,845 --> 00:03:00,160
0,180 180,345 345,495 495,785 3055,3315
one in my favor. I

102
00:03:00,160 --> 00:03:01,110
0,120 120,240 240,375 375,600 600,950
think he looks more confident

103
00:03:01,250 --> 00:03:02,965
0,400 810,1115 1115,1295 1295,1460 1460,1715
and Taylor was quite nervous

104
00:03:02,965 --> 00:03:08,650
0,365 5005,5265 5265,5400 5400,5535 5535,5685
before. The room was much

105
00:03:08,650 --> 00:03:11,695
0,165 165,405 405,630 630,920 2740,3045
more tense this time. Really

106
00:03:11,695 --> 00:03:12,870
0,300 300,480 480,660 660,855 855,1175
didn't know what to expect.

107
00:03:13,370 --> 00:03:15,010
0,440 440,650 650,920 920,1400 1400,1640
He's been playing starra pretty

108
00:03:15,010 --> 00:03:21,445
0,210 210,405 405,660 660,950 6160,6435
much since he five, I

109
00:03:21,445 --> 00:03:23,080
0,390 390,785 1015,1305 1305,1455 1455,1635
wasn't expecting AI to be

110
00:03:23,080 --> 00:03:26,590
0,240 240,560 2950,3225 3225,3375 3375,3510
that good. Everything that he

111
00:03:26,590 --> 00:03:27,810
0,135 135,345 345,680 700,960 960,1220
did was proper, it was

112
00:03:27,950 --> 00:03:29,185
0,590 590,785 785,905 905,1055 1055,1235
calculated and it was done

113
00:03:29,185 --> 00:03:31,320
0,305 1135,1455 1455,1635 1635,1875 1875,2135
well. I thought I'm learning

114
00:03:31,340 --> 00:03:37,855
0,400 5460,5720 5720,5945 5945,6275 6275,6515
something. I would consider myself

115
00:03:37,855 --> 00:03:39,490
0,165 165,390 390,690 690,1055 1345,1635
a good player, right? But

116
00:03:39,490 --> 00:03:40,540
0,165 165,360 360,630 630,885 885,1050
I lost every single one

117
00:03:40,540 --> 00:03:42,280
0,150 150,375 375,710
of my games.|
|

118
00:03:45,740 --> 00:03:47,600
0,350 350,620 620,860 860,1180
Where away had won.|
客队赢了的地方。|

119
00:03:48,690 --> 00:03:50,090
0,400 420,785 785,995 995,1205 1205,1400
So let's take maybe a
所以让我们先开始，退后一步，首先，想想强化学习是如何适用于你在这节课上所接触到的所有不同主题的整个范例。作为一个整体，我认为到目前为止，我们在这门课程中已经涵盖了两种不同类型的学习。到目前为止，我们实际上已经开始关注课程的开始部分，首先是我们所说的监督学习。监督学习就是在这个领域，我们得到x的形式的数据，我们的输入和我们的标签y，我们的目标是学习一个函数或神经网络，它可以学习预测y，给定我们的输入x。

120
00:03:50,090 --> 00:03:51,020
0,210 210,420 420,600 600,735 735,930
start and take a step

121
00:03:51,020 --> 00:03:52,630
0,320 520,855 855,1035 1035,1245 1245,1610
back, first of all, and

122
00:03:52,710 --> 00:03:54,460
0,320 320,530 530,740 740,1445 1445,1750
think about how reinforcement learning

123
00:03:54,930 --> 00:03:56,465
0,320 320,605 605,845 845,1025 1025,1535
fits into this whole paradigm

124
00:03:56,465 --> 00:03:57,170
0,120 120,255 255,375 375,495 495,705
of all of the different

125
00:03:57,170 --> 00:03:58,595
0,350 430,705 705,900 900,1095 1095,1425
topics that you've been exposed

126
00:03:58,595 --> 00:03:59,870
0,210 210,315 315,495 495,815 955,1275
to in this class so

127
00:03:59,870 --> 00:04:01,745
0,270 270,620 1210,1485 1485,1620 1620,1875
far. So as a whole,

128
00:04:01,745 --> 00:04:02,720
0,270 270,435 435,585 585,780 780,975
I think that we've really

129
00:04:02,720 --> 00:04:04,265
0,350 430,720 720,975 975,1290 1290,1545
covered two different types of

130
00:04:04,265 --> 00:04:05,900
0,305 415,690 690,915 915,1265 1375,1635
learning in this course to

131
00:04:05,900 --> 00:04:07,220
0,260 340,630 630,870 870,1110 1110,1320
date. Right up until now

132
00:04:07,220 --> 00:04:08,870
0,300 300,555 555,885 885,1220 1390,1650
we've really started focusing in

133
00:04:08,870 --> 00:04:09,725
0,180 180,450 450,645 645,750 750,855
the beginning part of the

134
00:04:09,725 --> 00:04:11,420
0,435 435,1055 1135,1395 1395,1530 1530,1695
lectures, firstly on what we

135
00:04:11,420 --> 00:04:14,590
0,290 460,870 870,1250 2440,2820 2820,3170
call supervised learning. Supervised learning

136
00:04:14,730 --> 00:04:16,295
0,380 380,635 635,815 815,1120 1260,1565
is in this domain where

137
00:04:16,295 --> 00:04:17,945
0,360 360,665 775,1175 1225,1500 1500,1650
we're given data in the

138
00:04:17,945 --> 00:04:20,050
0,180 180,485 655,1290 1290,1565 1585,2105
form of x's, our inputs

139
00:04:20,220 --> 00:04:22,310
0,260 260,425 425,940 1050,1450 1830,2090
and our labels y, and

140
00:04:22,310 --> 00:04:23,285
0,150 150,360 360,570 570,780 780,975
our goal here is to

141
00:04:23,285 --> 00:04:24,380
0,210 210,420 420,695 715,975 975,1095
learn a function or a

142
00:04:24,380 --> 00:04:25,730
0,240 240,500 730,1005 1005,1155 1155,1350
neural network that can learn

143
00:04:25,730 --> 00:04:27,200
0,180 180,420 420,800 880,1185 1185,1470
to predict y given our

144
00:04:27,200 --> 00:04:28,460
0,345 345,620
inputs X.|
|

145
00:04:28,460 --> 00:04:30,560
0,150 150,315 315,620 1150,1550 1750,2100
So, for example, if you
因此，例如，如果你考虑这个例子，一个苹果观察一系列苹果的图像，我们想要检测，你知道，在未来，如果我们看到一个新的苹果图像，来检测这确实是一个苹果。

146
00:04:30,560 --> 00:04:31,580
0,255 255,495 495,750 750,915 915,1020
consider this example of an

147
00:04:31,580 --> 00:04:33,185
0,260 760,1065 1065,1245 1245,1425 1425,1605
Apple observing a bunch of

148
00:04:33,185 --> 00:04:34,700
0,275 295,570 570,845 1075,1350 1350,1515
images of apples, we want

149
00:04:34,700 --> 00:04:35,780
0,225 225,555 555,810 810,945 945,1080
to detect, you know, in

150
00:04:35,780 --> 00:04:36,500
0,120 120,300 300,480 480,615 615,720
the future if we see

151
00:04:36,500 --> 00:04:37,175
0,90 90,225 225,420 420,585 585,675
a new image of an

152
00:04:37,175 --> 00:04:38,585
0,245 355,690 690,930 930,1185 1185,1410
Apple, to detect that this

153
00:04:38,585 --> 00:04:40,680
0,210 210,435 435,570 570,815
is indeed an Apple.|
|

154
00:04:40,750 --> 00:04:42,320
0,290 290,455 455,730 780,1175 1175,1570
Now, the second class of
现在，我们在昨天的课程中发现的第二种学习方法是无监督学习。在这些算法中，你只能访问数据。没有标签的概念，对吗？这就是我们昨天在这些类型的算法中了解到的。你不是在试图预测一个标签，而是试图揭示一些潜在的结构。我们所称的基本上是这些潜在变量，这些隐藏在你的数据中的特征。例如，在这个苹果的例子中，正确地使用了无监督学习。类似的例子基本上是建立一个模型，该模型可以理解这些图像的特定部分并将其聚集在一起。也许它不需要理解这一定是一个苹果的形象。但它需要明白，你知道，红色苹果的形象是相似的。它与这个黑白的苹果轮廓素描具有相同的潜在特征和相同的语义。

155
00:04:42,700 --> 00:04:44,720
0,400 420,820 930,1205 1205,1600 1620,2020
learning approaches that we've discovered

156
00:04:44,920 --> 00:04:46,815
0,400 600,875 875,1370 1370,1610 1610,1895
yesterday in yesterday's lecture was

157
00:04:46,815 --> 00:04:48,530
0,210 210,435 435,1155 1155,1395 1395,1715
that of unsupervised learning. And

158
00:04:48,550 --> 00:04:50,180
0,290 290,580 660,1145 1145,1355 1355,1630
in these algorithms you have

159
00:04:50,440 --> 00:04:52,400
0,400 810,1210 1290,1550 1550,1685 1685,1960
only access to the data.

160
00:04:52,420 --> 00:04:54,180
0,410 410,635 635,940 1020,1340 1340,1760
There's no notion of labels,

161
00:04:54,180 --> 00:04:54,900
0,210 210,360 360,480 480,600 600,720
right? This is what we

162
00:04:54,900 --> 00:04:57,435
0,180 180,500 520,920 1960,2265 2265,2535
learned about yesterday in these

163
00:04:57,435 --> 00:04:58,680
0,270 270,540 540,915 915,1110 1110,1245
types of algorithms. You're not

164
00:04:58,680 --> 00:04:59,990
0,270 270,540 540,810 810,1035 1035,1310
trying to predict a label,

165
00:05:00,160 --> 00:05:01,890
0,290 290,485 485,680 680,965 965,1730
but you're trying to uncover

166
00:05:01,890 --> 00:05:03,380
0,255 255,405 405,540 540,800 1090,1490
some of the underlying structure.

167
00:05:03,460 --> 00:05:04,580
0,290 290,440 440,575 575,785 785,1120
What we were calling basically

168
00:05:04,630 --> 00:05:06,300
0,320 320,725 725,1175 1175,1400 1400,1670
these latent variables, these hidden

169
00:05:06,300 --> 00:05:08,025
0,350 640,930 930,1110 1110,1400 1450,1725
features in your data. So

170
00:05:08,025 --> 00:05:08,990
0,165 165,375 375,555 555,690 690,965
for example, in this Apple

171
00:05:09,370 --> 00:05:11,780
0,380 380,760 960,1310 1310,2075 2075,2410
example, right using unsupervised learning.

172
00:05:12,010 --> 00:05:14,300
0,365 365,1090 1170,1570 1710,2000 2000,2290
The analogous example would basically

173
00:05:14,470 --> 00:05:15,860
0,400 540,815 815,980 980,1130 1130,1390
be to build a model

174
00:05:16,000 --> 00:05:18,320
0,320 320,640 840,1240 1560,1925 1925,2320
that could understand and cluster

175
00:05:18,490 --> 00:05:20,700
0,400 810,1190 1190,1550 1550,1910 1910,2210
certain certain parts of these

176
00:05:20,700 --> 00:05:22,110
0,320 520,855 855,1050 1050,1230 1230,1410
images together. And maybe it

177
00:05:22,110 --> 00:05:23,570
0,375 375,630 630,885 885,1155 1155,1460
doesn't have to understand that

178
00:05:23,590 --> 00:05:24,630
0,335 335,545 545,695 695,830 830,1040
necessarily this is an image

179
00:05:24,630 --> 00:05:26,070
0,210 210,345 345,620 1060,1320 1320,1440
of an Apple. But it

180
00:05:26,070 --> 00:05:27,165
0,165 165,450 450,750 750,960 960,1095
needs to understand that, you

181
00:05:27,165 --> 00:05:28,530
0,120 120,395 715,1035 1035,1230 1230,1365
know, this image of the

182
00:05:28,530 --> 00:05:29,835
0,135 135,410 490,825 825,1110 1110,1305
red Apple is similar. It

183
00:05:29,835 --> 00:05:31,170
0,150 150,330 330,605 625,1080 1080,1335
has the same latent features

184
00:05:31,170 --> 00:05:33,120
0,255 255,450 450,1005 1005,1310 1600,1950
and same semantic meaning as

185
00:05:33,120 --> 00:05:34,875
0,350 370,660 660,825 825,1100 1180,1755
this black and white outline

186
00:05:34,875 --> 00:05:36,340
0,345 345,570 570,705 705,965
sketch of the Apple.|
|

187
00:05:37,660 --> 00:05:39,660
0,380 380,650 650,1265 1265,1660 1710,2000
Now, in today's lecture we're
现在，在今天的课程中，我们将讨论另一种类型的学习算法，就是强化学习。我们将只得到所谓的状态作用对形式的数据。现在的状态是观察，对吧。这就是代理人，让我们这样称呼它，神经网络将会观察到。这就是它所看到的。这些操作是该代理在这些特定状态下采取的行为。因此，强化学习的目标是建立一个能够学习如何最大化所谓奖励的代理。这是第三个特定于强化学习的组件，您希望在未来的许多时间步骤中最大化所有这些回报。

188
00:05:39,660 --> 00:05:40,665
0,120 120,285 285,480 480,720 720,1005
going to talk about yet

189
00:05:40,665 --> 00:05:42,960
0,365 565,965 1045,1365 1365,1685 1825,2295
another type of learning algorithms,

190
00:05:42,960 --> 00:05:45,135
0,330 330,690 690,1485 1485,1760 1810,2175
right in reinforcement learning. We're

191
00:05:45,135 --> 00:05:46,280
0,195 195,360 360,480 480,750 750,1145
going to be only given

192
00:05:46,570 --> 00:05:47,775
0,400 450,710 710,860 860,1040 1040,1205
data in the form of

193
00:05:47,775 --> 00:05:49,130
0,135 135,285 285,575 625,990 990,1355
what are called state action

194
00:05:49,330 --> 00:05:52,100
0,460 930,1325 1325,1685 1685,2050 2370,2770
pairs. Right now states are

195
00:05:52,150 --> 00:05:53,985
0,520 960,1265 1265,1460 1460,1640 1640,1835
observations, right. This is what

196
00:05:53,985 --> 00:05:55,875
0,305 595,995 1225,1605 1605,1740 1740,1890
the agent, let's call it,

197
00:05:55,875 --> 00:05:57,060
0,165 165,420 420,690 690,975 975,1185
the neural network is going

198
00:05:57,060 --> 00:05:58,260
0,315 315,615 615,900 900,1020 1020,1200
to observe. It's what it

199
00:05:58,260 --> 00:06:00,405
0,320 1000,1245 1245,1490 1660,1935 1935,2145
sees. The actions are the

200
00:06:00,405 --> 00:06:02,210
0,525 525,765 765,960 960,1265 1405,1805
behaviors that this agent takes

201
00:06:02,740 --> 00:06:05,715
0,400 660,1060 1260,1660 1920,2320 2700,2975
in those particular states. So

202
00:06:05,715 --> 00:06:07,185
0,150 150,420 420,720 720,1305 1305,1470
the goal of reinforcement learning

203
00:06:07,185 --> 00:06:08,270
0,225 225,435 435,630 630,810 810,1085
is to build an agent

204
00:06:08,380 --> 00:06:09,660
0,305 305,530 530,830 830,1115 1115,1280
that can learn how to

205
00:06:09,660 --> 00:06:11,540
0,555 555,750 750,900 900,1190 1330,1880
maximize what are called rewards.

206
00:06:11,800 --> 00:06:13,065
0,260 260,410 410,575 575,850 900,1265
This is the third component

207
00:06:13,065 --> 00:06:15,000
0,210 210,360 360,665 1045,1305 1305,1935
that is specific to reinforcement

208
00:06:15,000 --> 00:06:16,365
0,290 550,825 825,960 960,1155 1155,1365
learning and you want to

209
00:06:16,365 --> 00:06:17,985
0,615 615,855 855,1020 1020,1200 1200,1620
maximize all of those rewards

210
00:06:17,985 --> 00:06:19,580
0,375 375,675 675,945 945,1245 1245,1595
over many, many time steps

211
00:06:19,600 --> 00:06:20,960
0,305 305,470 470,730
in the future.|
|

212
00:06:20,960 --> 00:06:22,240
0,330 330,615 615,825 825,990 990,1280
So again, in this Apple
所以，在这个苹果的例子中，我们现在可能会看到代理商不一定知道，好的，这是一个苹果，或者它看起来像其他苹果。现在它必须学会，比方说，吃苹果，采取行动，吃那个苹果，因为它已经了解到，吃苹果能让它活得更长，或者因为它不会挨饿而存活下来。

213
00:06:22,320 --> 00:06:24,110
0,400 840,1115 1115,1295 1295,1550 1550,1790
example, we might now see

214
00:06:24,110 --> 00:06:25,835
0,165 165,300 300,560 880,1395 1395,1725
that the agent doesn't necessarily

215
00:06:25,835 --> 00:06:26,750
0,270 270,495 495,675 675,795 795,915
learn that, okay, this is

216
00:06:26,750 --> 00:06:27,695
0,105 105,350 400,660 660,780 780,945
an Apple or it looks

217
00:06:27,695 --> 00:06:29,120
0,195 195,345 345,525 525,845 1135,1425
like these other apples. Now

218
00:06:29,120 --> 00:06:30,350
0,150 150,315 315,495 495,770 850,1230
it has to learn to,

219
00:06:30,350 --> 00:06:31,420
0,300 300,435 435,645 645,810 810,1070
let's say, eat the Apple,

220
00:06:31,560 --> 00:06:32,675
0,260 260,380 380,635 635,935 935,1115
take an action, eat that

221
00:06:32,675 --> 00:06:33,965
0,275 325,645 645,840 840,1035 1035,1290
Apple, because it has learned

222
00:06:33,965 --> 00:06:35,075
0,240 240,465 465,660 660,885 885,1110
that eating that Apple makes

223
00:06:35,075 --> 00:06:36,470
0,150 150,330 330,635 685,1065 1065,1395
it live longer or survive

224
00:06:36,470 --> 00:06:39,060
0,225 225,500 670,1095 1095,1580
because it doesn't starve.|
|

225
00:06:39,620 --> 00:06:41,140
0,335 335,560 560,850 960,1310 1310,1520
So in today, right, like
所以今天，对了，就像我说的，我们将专门关注第三种类型的学习范式，即强化学习。在我们继续之前，我只想先为大家建立一些非常关键的术语和背景知识，这样当我们开始讨论今天课程中一些更复杂的部分时，我们就都达成了一致。

226
00:06:41,140 --> 00:06:41,920
0,150 150,360 360,570 570,675 675,780
I said, we're going to

227
00:06:41,920 --> 00:06:43,915
0,105 105,380 700,1440 1440,1725 1725,1995
be focusing exclusively on this

228
00:06:43,915 --> 00:06:45,840
0,285 285,600 600,870 870,1155 1155,1925
third type of learning paradigm,

229
00:06:45,920 --> 00:06:48,130
0,305 305,590 590,1325 1325,1600 1950,2210
which is reinforcement learning. And

230
00:06:48,130 --> 00:06:49,110
0,195 195,375 375,480 480,660 660,980
before we go any further,

231
00:06:49,190 --> 00:06:50,125
0,275 275,410 410,530 530,695 695,935
I just want to start

232
00:06:50,125 --> 00:06:51,385
0,240 240,495 495,750 750,960 960,1260
by building up some very

233
00:06:51,385 --> 00:06:54,390
0,395 475,1415 1615,2015 2365,2685 2685,3005
key terminology and like, basically

234
00:06:55,250 --> 00:06:56,635
0,400 600,860 860,1010 1010,1175 1175,1385
background for all of you

235
00:06:56,635 --> 00:06:57,400
0,180 180,300 300,465 465,585 585,765
so that we're all on

236
00:06:57,400 --> 00:06:58,480
0,150 150,345 345,675 675,930 930,1080
the same page when we

237
00:06:58,480 --> 00:06:59,515
0,255 255,585 585,810 810,930 930,1035
start discussing some of the

238
00:06:59,515 --> 00:07:01,615
0,245 265,665 1015,1415 1435,1725 1725,2100
more complex components of today's

239
00:07:01,615 --> 00:07:02,640
0,275
lecture.|
|

240
00:07:02,920 --> 00:07:04,695
0,380 380,755 755,965 965,1300 1410,1775
So let's start by building
所以，让我们从建立一些这样的术语开始。第一个主要术语是代理人的术语。代理人基本上是一种可以采取行动的存在。例如，您可以将代理视为一台机器，对吗？也就是说，让我们假设一架正在送货的自动无人机。例如，在游戏中，可能是超级马里奥在你的视频游戏中导航。

241
00:07:04,695 --> 00:07:06,030
0,365 505,750 750,960 960,1200 1200,1335
up, you know, some of

242
00:07:06,030 --> 00:07:07,860
0,255 255,960 960,1110 1110,1400 1450,1830
this terminology. The first main

243
00:07:07,860 --> 00:07:09,390
0,270 270,450 450,1125 1125,1335 1335,1530
piece of terminology is that

244
00:07:09,390 --> 00:07:11,210
0,210 210,405 405,680 1270,1545 1545,1820
of an agent. An agent

245
00:07:11,290 --> 00:07:15,165
0,400 1080,1480 2400,2800 2880,3280 3570,3875
is a being basically that

246
00:07:15,165 --> 00:07:16,910
0,180 180,360 360,665 1165,1455 1455,1745
can take actions. For example,

247
00:07:16,930 --> 00:07:18,000
0,260 260,395 395,545 545,800 800,1070
you can think of an

248
00:07:18,000 --> 00:07:21,075
0,290 880,1280 1480,1880 2080,2480 2680,3075
agent as a machine, right?

249
00:07:21,075 --> 00:07:22,410
0,345 345,660 660,960 960,1140 1140,1335
That is, let's say an

250
00:07:22,410 --> 00:07:24,495
0,585 585,950 1180,1575 1575,1875 1875,2085
autonomous drone that is making

251
00:07:24,495 --> 00:07:25,920
0,240 240,575 625,930 930,1140 1140,1425
a delivery. Or for example,

252
00:07:25,920 --> 00:07:26,760
0,225 225,330 330,510 510,705 705,840
in a game, it could

253
00:07:26,760 --> 00:07:29,115
0,150 150,360 360,855 855,1310 1660,2355
be super Mario that's navigating

254
00:07:29,115 --> 00:07:30,165
0,255 255,375 375,510 510,750 750,1050
inside of your video video

255
00:07:30,165 --> 00:07:31,500
0,335
game.|
|

256
00:07:31,500 --> 00:07:33,615
0,285 285,810 810,1190 1510,1905 1905,2115
The algorithm itself, it's important
算法本身，重要的是要记住算法就是代理，对吗？我们正试图构建一个可以完成这些任务的代理，算法就是这个代理。例如，在生活中，你们所有人都是生活中的代理人。

257
00:07:33,615 --> 00:07:34,785
0,150 150,315 315,510 510,765 765,1170
to remember that the algorithm

258
00:07:34,785 --> 00:07:36,480
0,270 270,510 510,785 1045,1395 1395,1695
is the agent, right? We're

259
00:07:36,480 --> 00:07:37,490
0,165 165,345 345,555 555,750 750,1010
trying to build an agent

260
00:07:37,540 --> 00:07:38,670
0,275 275,425 425,590 590,815 815,1130
that can do these tasks,

261
00:07:38,670 --> 00:07:39,735
0,255 255,465 465,765 765,900 900,1065
and the algorithm is that

262
00:07:39,735 --> 00:07:41,475
0,275 685,945 945,1095 1095,1385 1435,1740
agent. So in life, for

263
00:07:41,475 --> 00:07:42,825
0,300 300,585 585,735 735,990 990,1350
example, all of you are

264
00:07:42,825 --> 00:07:45,160
0,365 655,945 945,1235
agents in life.|
|

265
00:07:45,420 --> 00:07:46,780
0,320 320,640 720,1010 1010,1130 1130,1360
The environment is the other
环境对代理人来说是另一种相反的方法或相反的观点。环境就是代理人生活和运作的世界，就在它存在的地方，它在里面活动。

266
00:07:46,860 --> 00:07:48,875
0,260 260,520 660,1265 1265,1660 1740,2015
kind of contrary approach or

267
00:07:48,875 --> 00:07:50,840
0,150 150,665 1045,1445 1615,1860 1860,1965
the contrary perspective to the

268
00:07:50,840 --> 00:07:52,910
0,260 610,915 915,1220 1360,1725 1725,2070
agent. The environment is simply

269
00:07:52,910 --> 00:07:54,370
0,255 255,530 640,960 960,1170 1170,1460
the world where that agent

270
00:07:54,450 --> 00:07:55,900
0,350 350,650 650,860 860,980 980,1450
lives and where it operates,

271
00:07:56,070 --> 00:07:57,800
0,400 540,815 815,1085 1085,1475 1475,1730
right where it exists and

272
00:07:57,800 --> 00:07:59,760
0,165 165,450 450,765 765,1100
it moves around in.|
|

273
00:07:59,950 --> 00:08:01,730
0,245 245,485 485,860 860,1190 1190,1780
The agent can send commands
代理可以将命令以所谓的操作的形式发送到该环境。您可以在该环境中执行操作，让我们调用表示法。假设它可以采取的所有行动的可能集合是，假设现在就是一组资本a。应该注意的是，工程师可以在任何时间点进行选择，比方说可能的操作列表。当然，在某些情况下，您的动作空间不一定需要是有限空间。也许你可以在一个连续的空间里采取行动。例如，当你驾驶一辆汽车时，你是在一个连续的角度空间上采取行动，该空间是你想要驾驶汽车的角度。它不一定只是向右、向左或直行，你可以在任何连续的角度上驾驶。

274
00:08:01,780 --> 00:08:02,985
0,260 260,470 470,815 815,1070 1070,1205
to that environment in the

275
00:08:02,985 --> 00:08:03,705
0,165 165,300 300,405 405,540 540,720
form of what are called

276
00:08:03,705 --> 00:08:05,540
0,305 925,1170 1170,1320 1320,1530 1530,1835
actions. You can take actions

277
00:08:05,590 --> 00:08:07,785
0,275 275,485 485,820 1290,1690 1710,2195
in that environment and let's

278
00:08:07,785 --> 00:08:10,185
0,300 300,635 1015,1590 1590,1955 2035,2400
call for notation purposes. Let's

279
00:08:10,185 --> 00:08:12,450
0,210 210,575 1105,1505 1705,2040 2040,2265
say the possible set of

280
00:08:12,450 --> 00:08:13,680
0,240 240,590 670,930 930,1065 1065,1230
all actions that it could

281
00:08:13,680 --> 00:08:15,165
0,290 550,930 930,1230 1230,1320 1320,1485
take is let's say a

282
00:08:15,165 --> 00:08:16,850
0,195 195,420 420,735 735,1115 1285,1685
set of capital a right

283
00:08:17,680 --> 00:08:18,585
0,275 275,425 425,545 545,665 665,905
now. It should be noted

284
00:08:18,585 --> 00:08:20,820
0,365 475,875 1525,1800 1800,2010 2010,2235
that agents at any point

285
00:08:20,820 --> 00:08:22,640
0,165 165,440 610,960 960,1310 1420,1820
in time could choose amongst

286
00:08:22,930 --> 00:08:24,360
0,400 420,770 770,905 905,1175 1175,1430
this, let's say list of

287
00:08:24,360 --> 00:08:25,860
0,290 340,740 970,1215 1215,1335 1335,1500
possible actions. But of course

288
00:08:25,860 --> 00:08:27,590
0,180 180,360 360,650 1180,1455 1455,1730
in some situations your action

289
00:08:27,730 --> 00:08:29,265
0,365 365,620 620,910 930,1295 1295,1535
space does not necessarily need

290
00:08:29,265 --> 00:08:30,740
0,120 120,240 240,420 420,905 1075,1475
to be a finite space.

291
00:08:30,790 --> 00:08:31,875
0,305 305,485 485,635 635,800 800,1085
Maybe you could take actions

292
00:08:31,875 --> 00:08:33,255
0,240 240,435 435,765 765,1110 1110,1380
in a continuous space. For

293
00:08:33,255 --> 00:08:34,215
0,255 255,450 450,615 615,765 765,960
example, when you're driving a

294
00:08:34,215 --> 00:08:36,330
0,275 595,1155 1155,1470 1470,1805 1855,2115
car, you're taking actions on

295
00:08:36,330 --> 00:08:38,340
0,210 210,560 1000,1400 1480,1800 1800,2010
a continuous angle space of

296
00:08:38,340 --> 00:08:39,435
0,180 180,465 465,720 720,885 885,1095
what angle you want to

297
00:08:39,435 --> 00:08:40,580
0,240 240,435 435,660 660,885 885,1145
steer that car. It's not

298
00:08:40,600 --> 00:08:41,760
0,350 350,575 575,755 755,965 965,1160
necessarily just going right or

299
00:08:41,760 --> 00:08:43,340
0,210 210,465 465,800 1060,1320 1320,1580
left or straight, you may

300
00:08:43,480 --> 00:08:45,050
0,275 275,380 380,640 690,1090 1170,1570
steer at any continuous degree.|
|

301
00:08:47,080 --> 00:08:48,915
0,485 485,860 860,1235 1235,1580 1580,1835
Observations is essentially how the
观察本质上是环境对代理人的反应，对吗？环境可以告诉代理，根据它刚刚采取的操作，您知道它应该看到什么，并且它以所谓的状态的形式做出响应。状态只是主体在那个特定时刻发现自己所处的具体和直接的情况。

302
00:08:48,915 --> 00:08:50,685
0,305 505,945 945,1235 1405,1650 1650,1770
environment responds back to the

303
00:08:50,685 --> 00:08:52,970
0,275 715,1050 1050,1305 1305,1625 1885,2285
agent, right? The environment can

304
00:08:53,170 --> 00:08:54,555
0,305 305,455 455,700 1020,1265 1265,1385
tell the agent you know

305
00:08:54,555 --> 00:08:55,875
0,135 135,390 390,735 735,1020 1020,1320
what it should be seeing

306
00:08:55,875 --> 00:08:57,210
0,300 300,510 510,675 675,965 1075,1335
based on those actions that

307
00:08:57,210 --> 00:08:58,965
0,135 135,345 345,680 1090,1425 1425,1755
it just took, and it

308
00:08:58,965 --> 00:09:01,070
0,570 570,750 750,915 915,1205 1705,2105
responds in the form of

309
00:09:01,120 --> 00:09:02,265
0,260 260,410 410,605 605,830 830,1145
what is called a state.

310
00:09:02,265 --> 00:09:03,470
0,300 300,495 495,660 660,870 870,1205
A state is simply a

311
00:09:03,610 --> 00:09:06,150
0,335 335,670 690,1090 1440,1840 2250,2540
concrete and immediate situation that

312
00:09:06,150 --> 00:09:07,635
0,150 150,410 460,860 880,1215 1215,1485
the agent finds itself in

313
00:09:07,635 --> 00:09:09,940
0,210 210,485 535,915 915,1295
at that particular moment.|
|

314
00:09:10,570 --> 00:09:12,240
0,400 630,1025 1025,1235 1235,1415 1415,1670
Now it's important to remember
现在重要的是要记住，不同于我们在本课程中讨论的其他类型的学习，强化学习是有点独特的，因为它除了这些其他组件之外，还有一个组件，称为奖励。现在，奖励是我们衡量的反馈，或者我们可以尝试衡量某个特定代理人在其环境中的成功。例如，在电子游戏中，当马里奥拿起一枚硬币时，他就会赢得积分，对吗？因此，在给定的状态下，代理可以发出任何形式的操作来做出某些决定，这些操作可能会也可能不会导致报酬随着时间的推移而收集和积累。

315
00:09:12,240 --> 00:09:14,385
0,380 820,1140 1140,1460 1600,1920 1920,2145
that unlike other types of

316
00:09:14,385 --> 00:09:16,065
0,305 415,690 690,945 945,1205 1285,1680
learning that we've covered in

317
00:09:16,065 --> 00:09:18,150
0,360 360,725 1075,1770 1770,1935 1935,2085
this course, reinforcement learning is

318
00:09:18,150 --> 00:09:19,335
0,120 120,255 255,530 790,1050 1050,1185
a bit unique because it

319
00:09:19,335 --> 00:09:20,960
0,255 255,540 540,840 840,1230 1230,1625
has one more component here

320
00:09:21,160 --> 00:09:22,550
0,320 320,640 690,950 950,1100 1100,1390
in addition to these other

321
00:09:22,570 --> 00:09:24,840
0,400 1320,1595 1595,1775 1775,2015 2015,2270
components, which is called the

322
00:09:24,840 --> 00:09:26,820
0,320 430,720 720,930 930,1250 1600,1980
reward. Now the reward is

323
00:09:26,820 --> 00:09:28,785
0,345 345,710 1180,1470 1470,1710 1710,1965
a feedback by which we

324
00:09:28,785 --> 00:09:30,150
0,305 475,750 750,915 915,1125 1125,1365
measure, or we can try

325
00:09:30,150 --> 00:09:32,415
0,180 180,440 790,1080 1080,1370 1990,2265
to measure the success of

326
00:09:32,415 --> 00:09:34,035
0,225 225,570 570,965 1135,1395 1395,1620
a particular agent in its

327
00:09:34,035 --> 00:09:35,580
0,365 595,870 870,1065 1065,1335 1335,1545
environment. So, for example, in

328
00:09:35,580 --> 00:09:37,515
0,105 105,300 300,650 1000,1395 1395,1935
a video game, when Mario

329
00:09:37,515 --> 00:09:38,960
0,270 270,405 405,855 855,1140 1140,1445
grabs a coin, for example,

330
00:09:39,430 --> 00:09:40,755
0,365 365,680 680,950 950,1160 1160,1325
he wins points, right? So

331
00:09:40,755 --> 00:09:42,105
0,135 135,255 255,510 510,905 1075,1350
from a given state, an

332
00:09:42,105 --> 00:09:43,950
0,275 355,690 690,900 900,1175 1495,1845
agent can send out any

333
00:09:43,950 --> 00:09:46,095
0,270 270,465 465,740 1090,1490 1840,2145
form of actions to take

334
00:09:46,095 --> 00:09:47,990
0,305 595,975 975,1305 1305,1575 1575,1895
some decisions, and those actions

335
00:09:48,190 --> 00:09:49,940
0,335 335,560 560,755 755,1060 1350,1750
may or may not result

336
00:09:50,170 --> 00:09:51,915
0,290 290,680 680,1040 1040,1420 1500,1745
in rewards being collected and

337
00:09:51,915 --> 00:09:53,560
0,570 570,825 825,1175
accumulated over time.|
|

338
00:09:53,630 --> 00:09:55,135
0,400 480,905 905,1100 1100,1280 1280,1505
Now, it's also very important
现在，同样重要的是要记住，并不是所有的行动都会带来立竿见影的回报。你可能会采取一些行动，以一种延迟的方式得到回报，也许在未来的几步时间里，也许在生活中，也许几年后，你今天采取的行动可能会在很长一段时间后产生回报。而且，但基本上所有这些都试图有效地评估某种方式来衡量代理人采取的特定行动的成功程度。

339
00:09:55,135 --> 00:09:56,215
0,180 180,405 405,660 660,870 870,1080
to remember that not all

340
00:09:56,215 --> 00:09:58,495
0,305 805,1140 1140,1455 1455,1815 1815,2280
actions result in immediate rewards.

341
00:09:58,495 --> 00:09:59,550
0,210 210,360 360,585 585,780 780,1055
You may take some actions

342
00:09:59,660 --> 00:10:00,985
0,380 380,725 725,995 995,1145 1145,1325
that will result in a

343
00:10:00,985 --> 00:10:02,910
0,335 865,1110 1110,1290 1290,1575 1575,1925
reward in a delayed fashion,

344
00:10:02,990 --> 00:10:03,970
0,305 305,440 440,560 560,740 740,980
maybe in a few time

345
00:10:03,970 --> 00:10:05,170
0,285 285,480 480,600 600,855 855,1200
steps down the future, maybe

346
00:10:05,170 --> 00:10:06,985
0,240 240,480 480,795 795,1160 1570,1815
in life, maybe years, you

347
00:10:06,985 --> 00:10:07,915
0,135 135,285 285,390 390,600 600,930
may take an action today

348
00:10:07,915 --> 00:10:09,330
0,330 330,615 615,840 840,1080 1080,1415
that results in a reward

349
00:10:09,470 --> 00:10:12,090
0,400 1470,1790 1790,2075 2075,2330 2330,2620
many some time from now.

350
00:10:12,530 --> 00:10:13,990
0,320 320,575 575,910 990,1280 1280,1460
And, but essentially all of

351
00:10:13,990 --> 00:10:15,930
0,290 490,810 810,1050 1050,1370 1540,1940
these try to effectively evaluate

352
00:10:16,100 --> 00:10:19,075
0,335 335,670 1200,1600 2280,2825 2825,2975
some way of measuring the

353
00:10:19,075 --> 00:10:21,150
0,275 775,1125 1125,1425 1425,1725 1725,2075
success of a particular action

354
00:10:21,380 --> 00:10:23,160
0,290 290,440 440,700 750,1150
that an agent takes.|
|

355
00:10:23,170 --> 00:10:24,225
0,290 290,470 470,710 710,920 920,1055
So, for example, when we
因此，例如，当我们查看代理在其生命周期中积累的总奖励时，我们可以简单地将代理在特定时间后获得的所有奖励相加，对吗？所以这个t的资本r是从那个点到未来无限远的所有回报的总和。

356
00:10:24,225 --> 00:10:25,940
0,150 150,345 345,665 835,1200 1200,1715
look at the total rewards

357
00:10:26,050 --> 00:10:27,570
0,290 290,425 425,650 650,1310 1310,1520
that an agent accumulates over

358
00:10:27,570 --> 00:10:28,820
0,210 210,405 405,540 540,770 850,1250
the course of its lifetime,

359
00:10:28,960 --> 00:10:30,350
0,290 290,485 485,790 810,1100 1100,1390
we can simply sum up

360
00:10:30,460 --> 00:10:32,370
0,320 320,530 530,665 665,1030 1620,1910
all of the rewards that

361
00:10:32,370 --> 00:10:34,065
0,150 150,410 520,920 1030,1425 1425,1695
an agent gets after a

362
00:10:34,065 --> 00:10:35,730
0,240 240,585 585,965 1165,1485 1485,1665
certain time t right? So

363
00:10:35,730 --> 00:10:37,070
0,180 180,480 480,780 780,1020 1020,1340
this capital r of t

364
00:10:37,210 --> 00:10:38,385
0,365 365,620 620,830 830,1025 1025,1175
is the sum of all

365
00:10:38,385 --> 00:10:40,260
0,425 655,930 930,1140 1140,1475 1495,1875
rewards from that point on

366
00:10:40,260 --> 00:10:42,230
0,315 315,525 525,800 940,1260 1260,1970
into the future into infinity.|
|

367
00:10:43,580 --> 00:10:44,610
0,245 245,365 365,500 500,695 695,1030
And that can be expanded
它可以扩展成完全像这样的样子。奖励时间t加上单词t加1，加t加2，以此类推。通常，对我们所有人来说，不仅考虑所有这些奖励的总和，而且考虑所谓的折扣金额，这实际上是非常有用的。所以你可以看到，我在所有奖励的前面添加了这个伽马因子。贴现因子实质上是乘以代理人看到并被代理人发现的每一笔未来奖励。我们想要这么做的原因实际上是这个抑制因素的设计，目的是使未来的奖励基本上比我们现在可能看到的此时此刻的奖励更有价值。

368
00:10:44,720 --> 00:10:46,540
0,320 320,640 660,1060 1230,1550 1550,1820
to look exactly like this.

369
00:10:46,540 --> 00:10:48,130
0,330 330,720 720,945 945,1280 1300,1590
It's rewarded time t plus

370
00:10:48,130 --> 00:10:49,075
0,180 180,405 405,630 630,780 780,945
the word time t plus

371
00:10:49,075 --> 00:10:50,310
0,285 285,570 570,735 735,915 915,1235
one, plus t plus two,

372
00:10:50,540 --> 00:10:51,325
0,290 290,425 425,500 500,605 605,785
and so on and so

373
00:10:51,325 --> 00:10:53,815
0,305 1135,1515 1515,1980 1980,2250 2250,2490
forth. Often it's actually very

374
00:10:53,815 --> 00:10:54,925
0,335 475,720 720,840 840,960 960,1110
useful for all of us

375
00:10:54,925 --> 00:10:56,755
0,270 270,635 745,1065 1065,1385 1495,1830
to consider not only the

376
00:10:56,755 --> 00:10:57,895
0,335 415,675 675,810 810,960 960,1140
sum of all of these

377
00:10:57,895 --> 00:10:59,530
0,455 535,885 885,1140 1140,1440 1440,1635
rewards, but instead what's called

378
00:10:59,530 --> 00:11:01,045
0,225 225,960 960,1245 1245,1440 1440,1515
the discounted sum. So you

379
00:11:01,045 --> 00:11:02,515
0,150 150,345 345,635 895,1245 1245,1470
can see here I've added

380
00:11:02,515 --> 00:11:04,225
0,360 360,825 825,1115 1195,1500 1500,1710
this gamma factor in front

381
00:11:04,225 --> 00:11:05,250
0,180 180,330 330,495 495,630 630,1025
of all of the rewards.

382
00:11:05,660 --> 00:11:08,020
0,400 840,1160 1160,1820 1820,2075 2075,2360
And that discounting factor is

383
00:11:08,020 --> 00:11:10,260
0,320 340,1100 1120,1425 1425,1730 1840,2240
essentially multiplied by every future

384
00:11:10,430 --> 00:11:12,780
0,365 365,665 665,1000 1200,1600 1950,2350
reward that the agent sees

385
00:11:13,160 --> 00:11:14,245
0,230 230,470 470,740 740,965 965,1085
and it's discovered by the

386
00:11:14,245 --> 00:11:15,700
0,245 265,525 525,645 645,905 1195,1455
agent. And the reason that

387
00:11:15,700 --> 00:11:16,480
0,120 120,270 270,420 420,555 555,780
we want to do this

388
00:11:16,480 --> 00:11:18,745
0,330 330,710 1060,1425 1425,2025 2025,2265
is actually this dampening factor

389
00:11:18,745 --> 00:11:20,790
0,345 345,725 1195,1455 1455,1680 1680,2045
is designed to make future

390
00:11:20,870 --> 00:11:23,905
0,580 780,1180 1380,1780 1800,2200 2700,3035
rewards essentially worth less than

391
00:11:23,905 --> 00:11:25,180
0,435 435,645 645,795 795,1035 1035,1275
rewards that we might see

392
00:11:25,180 --> 00:11:26,830
0,180 180,470 700,1100 1180,1455 1455,1650
at this instant, at this

393
00:11:26,830 --> 00:11:28,000
0,285 285,555 555,860
moment right now.|
|

394
00:11:28,010 --> 00:11:28,735
0,230 230,320 320,455 455,590 590,725
Now, you can think of
现在，你可以认为这基本上是在执行某种短期措施。

395
00:11:28,735 --> 00:11:30,190
0,135 135,315 315,600 600,1200 1200,1455
this as basically enforcing some

396
00:11:30,190 --> 00:11:33,740
0,255 255,480 480,765 765,1130
kind of short term.|
|

397
00:11:34,190 --> 00:11:36,070
0,695 695,980 980,1220 1220,1640 1640,1880
Greiness in the algorithm, right.
算法中的灰色，对。例如，如果我今天给你们5美元的奖励，或者10年后给你们5美元的奖励，我想你们所有人都会更喜欢今天的5美元，因为我们有同样的贴现因素。在这个过程中。我们有一个因素，那五美元对我们来说不值那么多钱，如果十年后给我们的话。这也正是我们在这里捕捉到的。从数学上讲，这个贴现因子就像乘法，就像我说的，乘以每一个。

398
00:11:36,070 --> 00:11:37,105
0,255 255,495 495,720 720,900 900,1035
So for example, if I

399
00:11:37,105 --> 00:11:38,575
0,240 240,605 685,1035 1035,1290 1290,1470
offered you a reward of

400
00:11:38,575 --> 00:11:40,345
0,195 195,515 565,965 1135,1500 1500,1770
five dollars today or a

401
00:11:40,345 --> 00:11:41,610
0,195 195,345 345,510 510,815 865,1265
reward of five dollars and

402
00:11:41,690 --> 00:11:43,135
0,275 275,485 485,695 695,970 1200,1445
ten years from now, I

403
00:11:43,135 --> 00:11:43,960
0,135 135,300 300,450 450,630 630,825
think all of you would

404
00:11:43,960 --> 00:11:45,450
0,290 400,690 690,870 870,1125 1125,1490
prefer that five dollars today

405
00:11:45,890 --> 00:11:47,485
0,400 450,850 960,1220 1220,1370 1370,1595
simply because we have that

406
00:11:47,485 --> 00:11:49,690
0,285 285,855 855,1145 1255,1655 1945,2205
same discounting factor applied to

407
00:11:49,690 --> 00:11:51,475
0,260 400,645 645,795 795,1100 1480,1785
this. To this processing. We

408
00:11:51,475 --> 00:11:52,660
0,195 195,450 450,780 780,1020 1020,1185
have that factor that that

409
00:11:52,660 --> 00:11:53,815
0,195 195,465 465,705 705,915 915,1155
five dollars is not worth

410
00:11:53,815 --> 00:11:55,150
0,240 240,510 510,690 690,935 1075,1335
as much to us if

411
00:11:55,150 --> 00:11:56,050
0,210 210,390 390,540 540,690 690,900
it's given to us ten

412
00:11:56,050 --> 00:11:57,145
0,285 285,540 540,660 660,870 870,1095
years in the future. And

413
00:11:57,145 --> 00:11:58,405
0,255 255,545 625,900 900,1065 1065,1260
that's exactly how this is

414
00:11:58,405 --> 00:12:00,150
0,285 285,540 540,690 690,965 1105,1745
captured here as well. Mathematically,

415
00:12:00,530 --> 00:12:02,370
0,290 290,815 815,1040 1040,1390 1440,1840
this discounting factor is like

416
00:12:02,390 --> 00:12:03,925
0,590 590,740 740,875 875,1055 1055,1535
multiply, like I said, multiply

417
00:12:03,925 --> 00:12:05,400
0,180 180,435 435,815
that every single.|
|

418
00:12:05,400 --> 00:12:09,390
0,380 400,800 1000,2000 2860,3260 3580,3990
Future award exponentially. And it's
未来的奖项将呈指数级增长。理解这一点也很重要。通常情况下，折现系数介于0和1之间，在某些特殊情况下，你可能想要一些奇怪的行为，折现系数大于1，但总的来说，这不是我们今天要讨论的问题。

419
00:12:09,390 --> 00:12:10,845
0,255 255,585 585,885 885,1155 1155,1455
important to understand that also.

420
00:12:10,845 --> 00:12:12,920
0,335 475,795 795,1365 1365,1655 1675,2075
Typically this discounting factor is,

421
00:12:13,150 --> 00:12:14,385
0,230 230,460 510,830 830,1040 1040,1235
you know, between zero and

422
00:12:14,385 --> 00:12:15,705
0,285 285,510 510,660 660,885 885,1320
one, there are some exceptional

423
00:12:15,705 --> 00:12:16,725
0,255 255,495 495,660 660,825 825,1020
cases where maybe you want

424
00:12:16,725 --> 00:12:18,360
0,335 385,765 765,1260 1260,1470 1470,1635
some strange behaviors and have

425
00:12:18,360 --> 00:12:19,755
0,120 120,615 615,870 870,1185 1185,1395
a discounting factor greater than

426
00:12:19,755 --> 00:12:21,410
0,195 195,360 360,495 495,785 1135,1655
one, but in general that's

427
00:12:21,430 --> 00:12:22,425
0,290 290,530 530,770 770,875 875,995
not something we're going to

428
00:12:22,425 --> 00:12:24,380
0,105 105,315 315,555 555,845
be talking about today.|
|

429
00:12:24,380 --> 00:12:26,320
0,315 315,675 675,1160 1180,1560 1560,1940
Now finally, it's very important
最后，这在强化学习中非常重要，这个特殊的函数被称为Q函数，它与我刚才与你们分享的许多不同的组件联系在一起。现在让我们来看看这个Q函数是什么，对吗？所以我们已经讨论了这个r of t函数，对吗？T的R是从时间t一直到未来时间无限的奖励的贴现和。

430
00:12:26,370 --> 00:12:29,120
0,400 480,1175 1175,1450 1800,2200 2370,2750
in reinforcement learning, this special

431
00:12:29,120 --> 00:12:31,330
0,380 1180,1515 1515,1740 1740,1920 1920,2210
function called the q function,

432
00:12:31,470 --> 00:12:32,780
0,365 365,710 710,965 965,1145 1145,1310
which ties in a lot

433
00:12:32,780 --> 00:12:34,160
0,165 165,330 330,620 670,1070 1120,1380
of these different components that

434
00:12:34,160 --> 00:12:35,170
0,195 195,330 330,540 540,735 735,1010
I've just shared with you

435
00:12:35,430 --> 00:12:37,715
0,400 1140,1535 1535,1910 1910,2045 2045,2285
altogether. Now let's look at

436
00:12:37,715 --> 00:12:39,230
0,270 270,480 480,660 660,935 1165,1515
what this q function is,

437
00:12:39,230 --> 00:12:40,535
0,255 255,465 465,720 720,990 990,1305
right? So we already covered

438
00:12:40,535 --> 00:12:42,380
0,395 595,900 900,1125 1125,1445 1465,1845
this r of t function

439
00:12:42,380 --> 00:12:43,310
0,270 270,435 435,600 600,750 750,930
right? R of t is

440
00:12:43,310 --> 00:12:45,730
0,320 610,1380 1380,1695 1695,1980 1980,2420
the discounted sum of rewards

441
00:12:45,990 --> 00:12:47,330
0,365 365,680 680,965 965,1190 1190,1340
from time t all the

442
00:12:47,330 --> 00:12:49,430
0,225 225,590 790,1065 1065,1340 1810,2100
way into the future time

443
00:12:49,430 --> 00:12:50,400
0,710
infinity.|
|

444
00:12:50,470 --> 00:12:53,220
0,335 335,670 1050,1445 1445,1840 2280,2750
But remember that this rf
但请记住，这个rf t，对，它是折扣的1号和2号，我们将尝试构建A，Q函数，它捕捉我们可以采取的最大或最好的行动，以最大化这一回报。所以让我用一种不同的方式再说一次。Q函数接受两个不同的东西作为输入。第一个是您当前所处的状态，第二个是您可以在此特定状态下执行的可能操作。所以这里测试是，停留在时间T，T的A是你可能想要在时间t采取的行动，这两个部分的Q函数将表示或捕捉，如果该代理在特定状态下采取该行动，其预期总回报是多少。

445
00:12:53,220 --> 00:12:54,825
0,240 240,495 495,750 750,1350 1350,1605
t right, it's discounted number

446
00:12:54,825 --> 00:12:57,450
0,335 475,735 735,975 975,1355 2335,2625
one and number two, we're

447
00:12:57,450 --> 00:12:58,310
0,120 120,285 285,420 420,570 570,860
going to try and build

448
00:12:58,900 --> 00:13:02,360
0,400 780,1180 1500,1900 2400,2795 2795,3460
A Q function that captures

449
00:13:02,830 --> 00:13:04,935
0,400 750,1085 1085,1420 1620,1910 1910,2105
the the maximum or the

450
00:13:04,935 --> 00:13:06,285
0,285 285,665 745,1020 1020,1170 1170,1350
best action that we could

451
00:13:06,285 --> 00:13:08,625
0,305 505,810 810,1065 1065,1805 1945,2340
take that will maximize this

452
00:13:08,625 --> 00:13:09,615
0,345 345,570 570,690 690,825 825,990
reward. So let me say

453
00:13:09,615 --> 00:13:10,485
0,135 135,270 270,450 450,690 690,870
that one more time in

454
00:13:10,485 --> 00:13:12,000
0,105 105,300 300,635 1045,1335 1335,1515
a different way. The q

455
00:13:12,000 --> 00:13:14,160
0,290 340,630 630,920 1000,1400 1900,2160
function takes as input two

456
00:13:14,160 --> 00:13:15,510
0,210 210,560 700,960 960,1140 1140,1350
different things. The first is

457
00:13:15,510 --> 00:13:16,485
0,195 195,420 420,585 585,765 765,975
the state that you're currently

458
00:13:16,485 --> 00:13:17,895
0,365 475,735 735,870 870,1080 1080,1410
in and the second is

459
00:13:17,895 --> 00:13:19,590
0,300 300,605 655,1055 1315,1575 1575,1695
a possible action that you

460
00:13:19,590 --> 00:13:21,530
0,260 340,740 880,1215 1215,1545 1545,1940
could execute in this particular

461
00:13:21,700 --> 00:13:23,910
0,400 1290,1640 1640,1880 1880,2075 2075,2210
state. So here's of t

462
00:13:23,910 --> 00:13:24,885
0,120 120,360 360,600 600,765 765,975
is that stay at time

463
00:13:24,885 --> 00:13:26,280
0,335 445,735 735,945 945,1200 1200,1395
T A of t is

464
00:13:26,280 --> 00:13:27,255
0,135 135,390 390,630 630,780 780,975
that action that you may

465
00:13:27,255 --> 00:13:28,395
0,180 180,345 345,600 600,870 870,1140
want to take at time

466
00:13:28,395 --> 00:13:29,900
0,365 595,855 855,1020 1020,1215 1215,1505
t, and the q function

467
00:13:29,920 --> 00:13:31,800
0,305 305,560 560,910 1080,1480 1560,1880
of these two pieces is

468
00:13:31,800 --> 00:13:33,740
0,240 240,450 450,890 1090,1490 1540,1940
going to denote or capture

469
00:13:34,480 --> 00:13:36,500
0,290 290,580 660,1060 1110,1510 1620,2020
what the expected total return

470
00:13:36,850 --> 00:13:37,935
0,305 305,500 500,650 650,800 800,1085
would be of that agent

471
00:13:37,935 --> 00:13:39,020
0,255 255,405 405,585 585,780 780,1085
if it took that action

472
00:13:39,370 --> 00:13:42,020
0,305 305,610 660,1060 1320,1720
in that particular state.|
|

473
00:13:42,780 --> 00:13:44,810
0,400 1080,1385 1385,1655 1655,1880 1880,2030
Now, one thing that I
现在，我认为我们现在都应该问问自己，这似乎是一个非常强大的功能，对吗？如果你可以访问这种类型的函数，这个Q函数，我想你实际上可以立即执行很多任务，对吗？例如，如果你想了解如何在特定的状态下采取什么动作，让我们假设我给了你这个神奇的提示函数，有人知道你如何转换这个提示函数来直接推断应该采取什么动作吗？

474
00:13:44,810 --> 00:13:46,475
0,290 460,860 1030,1350 1350,1515 1515,1665
think maybe we should all

475
00:13:46,475 --> 00:13:48,130
0,210 210,515 595,930 930,1260 1260,1655
be asking ourselves now is

476
00:13:48,960 --> 00:13:50,090
0,290 290,470 470,620 620,830 830,1130
this seems like a really

477
00:13:50,090 --> 00:13:51,470
0,350 370,765 765,1050 1050,1230 1230,1380
powerful function, right? If you

478
00:13:51,470 --> 00:13:52,880
0,260 370,720 720,945 945,1170 1170,1410
had access to this type

479
00:13:52,880 --> 00:13:53,930
0,165 165,300 300,525 525,825 825,1050
of a function, this q

480
00:13:53,930 --> 00:13:55,235
0,290 520,795 795,945 945,1080 1080,1305
function, I think you could

481
00:13:55,235 --> 00:13:56,510
0,365 475,765 765,945 945,1110 1110,1275
actually perform a lot of

482
00:13:56,510 --> 00:13:57,680
0,290 400,675 675,840 840,975 975,1170
tasks right off the BAT,

483
00:13:57,680 --> 00:13:59,440
0,240 240,530 940,1260 1260,1470 1470,1760
right? So if you wanted

484
00:13:59,460 --> 00:14:01,085
0,305 305,500 500,790 960,1310 1310,1625
to, for example, understand how

485
00:14:01,085 --> 00:14:03,160
0,365 745,1035 1035,1325 1495,1785 1785,2075
to what actions to take

486
00:14:03,270 --> 00:14:05,285
0,275 275,515 515,880 1020,1420 1770,2015
in a particular state, and

487
00:14:05,285 --> 00:14:06,170
0,210 210,360 360,525 525,720 720,885
let's suppose I gave you

488
00:14:06,170 --> 00:14:08,075
0,165 165,660 660,870 870,1130 1570,1905
this magical cue function, does

489
00:14:08,075 --> 00:14:09,080
0,195 195,315 315,525 525,780 780,1005
anyone have any ideas of

490
00:14:09,080 --> 00:14:10,295
0,210 210,390 390,680 700,1005 1005,1215
how you could transform that

491
00:14:10,295 --> 00:14:12,940
0,195 195,455 685,1085 1225,1625 2245,2645
cue function to directly infer

492
00:14:13,020 --> 00:14:14,260
0,275 275,550 570,815 815,950 950,1240
what action should be taken?|
|

493
00:14:17,390 --> 00:14:18,745
0,320 320,560 560,880 930,1205 1205,1355
Given a state, you can
给定一个状态，您可以查看您可能的操作空间，并选择给您最高Q值的操作空间。完全正确，所以这是完全正确的。再重复一次，Q函数告诉我们任何可能的操作，对吗？采取这一行动的预期回报是什么？因此，如果我们想要在特定的州采取特定的行动，最终我们需要，你知道，找出哪种行动是最好的行动。我们通过A Q函数做到这一点的方法很简单，就是选择将使我们未来的回报最大化的行动。我们可以简单地尝试第一个，如果我们有一个离散的动作空间，我们可以简单地尝试所有可能的动作，根据我们目前所处的状态，为每个可能的动作计算它们的Q值，然后我们选择将产生最高Q值的动作。

494
00:14:18,745 --> 00:14:20,010
0,150 150,345 345,570 570,870 870,1265
look at your possible action

495
00:14:20,090 --> 00:14:21,160
0,335 335,560 560,755 755,920 920,1070
space and pick the one

496
00:14:21,160 --> 00:14:21,985
0,165 165,330 330,480 480,600 600,825
that gives you the highest

497
00:14:21,985 --> 00:14:24,580
0,240 240,515 925,1325 1705,2105 2215,2595
q value. Exactly, so that's

498
00:14:24,580 --> 00:14:26,035
0,240 240,525 525,860 940,1230 1230,1455
exactly right. So just to

499
00:14:26,035 --> 00:14:27,030
0,210 210,360 360,510 510,690 690,995
repeat that one more time,

500
00:14:27,530 --> 00:14:28,675
0,275 275,425 425,680 680,950 950,1145
the q function tells us

501
00:14:28,675 --> 00:14:30,570
0,150 150,345 345,675 675,1055 1495,1895
for any possible action, right?

502
00:14:31,040 --> 00:14:33,000
0,260 260,470 470,820 870,1270 1560,1960
What is the expected reward

503
00:14:33,080 --> 00:14:34,315
0,260 260,395 395,670 870,1115 1115,1235
for that action to be

504
00:14:34,315 --> 00:14:35,275
0,275 295,540 540,645 645,765 765,960
taken? So if we wanted

505
00:14:35,275 --> 00:14:37,290
0,335 505,905 925,1305 1305,1650 1650,2015
to take a specific action

506
00:14:37,400 --> 00:14:38,730
0,305 305,440 440,560 560,850 930,1330
given in a specific state,

507
00:14:39,530 --> 00:14:41,230
0,400 570,875 875,1100 1100,1420 1440,1700
ultimately we need to, you

508
00:14:41,230 --> 00:14:42,660
0,225 225,510 510,810 810,1110 1110,1430
know, figure out which action

509
00:14:42,920 --> 00:14:44,065
0,290 290,425 425,590 590,905 905,1145
is the best action. The

510
00:14:44,065 --> 00:14:44,905
0,120 120,255 255,390 390,615 615,840
way we do that from

511
00:14:44,905 --> 00:14:47,100
0,275 295,585 585,875 1315,1715 1795,2195
A Q function is simply

512
00:14:47,120 --> 00:14:48,715
0,365 365,650 650,815 815,1060 1320,1595
to pick the action that

513
00:14:48,715 --> 00:14:50,880
0,165 165,845 1045,1365 1365,1685 1765,2165
will maximize our future reward.

514
00:14:51,050 --> 00:14:53,410
0,260 260,395 395,670 750,1150 2070,2360
And we can simply try

515
00:14:53,410 --> 00:14:54,655
0,285 285,600 600,855 855,1095 1095,1245
out number one, if we

516
00:14:54,655 --> 00:14:56,020
0,90 90,195 195,630 630,935 1015,1365
have a discrete action space,

517
00:14:56,020 --> 00:14:57,085
0,210 210,375 375,675 675,930 930,1065
we can simply try out

518
00:14:57,085 --> 00:14:59,125
0,210 210,540 540,935 1345,1800 1800,2040
all possible actions, compute their

519
00:14:59,125 --> 00:15:00,895
0,240 240,545 925,1200 1200,1455 1455,1770
q value for every single

520
00:15:00,895 --> 00:15:02,155
0,300 300,665 715,1005 1005,1140 1140,1260
possible actions based on the

521
00:15:02,155 --> 00:15:03,190
0,165 165,315 315,465 465,705 705,1035
state that we currently find

522
00:15:03,190 --> 00:15:04,435
0,300 300,620 760,1005 1005,1095 1095,1245
ourselves in, and then we

523
00:15:04,435 --> 00:15:05,905
0,225 225,390 390,635 1045,1305 1305,1470
pick the action that is

524
00:15:05,905 --> 00:15:07,360
0,210 210,515 595,995 1045,1320 1320,1455
going to result in the

525
00:15:07,360 --> 00:15:08,680
0,260 280,540 540,800
highest q value.|
|

526
00:15:09,690 --> 00:15:10,610
0,275 275,395 395,470 470,635 635,920
If we have a continuous
如果我们有一个连续的动作空间，也许我们会做一些更智能的事情，也许沿着这条Q值曲线的梯度，并将其最大化作为优化过程的一部分。

527
00:15:10,610 --> 00:15:12,065
0,350 400,795 795,1110 1110,1305 1305,1455
action space, maybe we do

528
00:15:12,065 --> 00:15:13,150
0,195 195,375 375,510 510,735 735,1085
something a bit more intelligent,

529
00:15:13,260 --> 00:15:15,380
0,400 570,965 965,1265 1265,1750 1800,2120
maybe following the gradients along

530
00:15:15,380 --> 00:15:17,120
0,225 225,390 390,645 645,1040 1450,1740
this q value curve and

531
00:15:17,120 --> 00:15:18,275
0,525 525,720 720,870 870,1035 1035,1155
maximizing it as part of

532
00:15:18,275 --> 00:15:20,620
0,180 180,705 705,1085
an optimization procedure.|
|

533
00:15:20,620 --> 00:15:22,560
0,380 490,890 1090,1380 1380,1605 1605,1940
But generally in this lecture,
但总的来说，在这节课中，我想要关注的是，我们如何才能得到这个Q函数。首先，我，我跳过了上一张幻灯片中的很多步骤，我刚才说，假设我给你一个神奇的Q函数，你如何确定要采取什么行动？但在现实中，我们没有得到Q函数。我们必须使用深度学习来学习Q函数。这就是今天的课程将主要讨论的，首先，我们如何从数据中构造和学习Q函数。

534
00:15:22,790 --> 00:15:23,665
0,245 245,350 350,500 500,650 650,875
what I want to focus

535
00:15:23,665 --> 00:15:25,615
0,365 415,815 1075,1440 1440,1740 1740,1950
on is actually how we

536
00:15:25,615 --> 00:15:27,220
0,120 120,605 745,1095 1095,1335 1335,1605
can obtain this q function.

537
00:15:27,220 --> 00:15:28,960
0,270 270,465 465,770 820,1220 1390,1740
To start with, I I

538
00:15:28,960 --> 00:15:29,800
0,195 195,345 345,555 555,690 690,840
kind of skipped a lot

539
00:15:29,800 --> 00:15:30,865
0,180 180,435 435,660 660,810 810,1065
of steps in that last

540
00:15:30,865 --> 00:15:31,735
0,300 300,465 465,570 570,720 720,870
slide where I just said

541
00:15:31,735 --> 00:15:32,770
0,285 285,435 435,615 615,840 840,1035
let's suppose I give you

542
00:15:32,770 --> 00:15:34,180
0,195 195,690 690,855 855,1125 1125,1410
this magical q function, how

543
00:15:34,180 --> 00:15:35,230
0,135 135,330 330,615 615,810 810,1050
can you determine what action

544
00:15:35,230 --> 00:15:36,600
0,240 240,500 760,990 990,1095 1095,1370
to take? But in reality

545
00:15:36,680 --> 00:15:37,735
0,320 320,440 440,635 635,860 860,1055
we're not given that q

546
00:15:37,735 --> 00:15:38,740
0,270 270,510 510,645 645,810 810,1005
function. We have to learn

547
00:15:38,740 --> 00:15:40,105
0,225 225,405 405,675 675,1065 1065,1365
that q function using deep

548
00:15:40,105 --> 00:15:41,725
0,305 655,915 915,1140 1140,1290 1290,1620
learning. And that's what today's

549
00:15:41,725 --> 00:15:42,570
0,180 180,375 375,510 510,615 615,845
lecture is going to be

550
00:15:42,770 --> 00:15:44,305
0,380 380,680 680,1000 1080,1370 1370,1535
talking about primarily is first

551
00:15:44,305 --> 00:15:45,150
0,90 90,195 195,390 390,570 570,845
of all, how can we

552
00:15:45,200 --> 00:15:46,825
0,400 600,935 935,1190 1190,1430 1430,1625
construct and learn that q

553
00:15:46,825 --> 00:15:48,340
0,270 270,570 570,875
function from data.|
|

554
00:15:48,340 --> 00:15:49,150
0,135 135,270 270,420 420,630 630,810
And then of course, the
当然，最后一步是使用Q函数，你知道，在现实世界中采取一些行动。总的来说，有两类强化学习算法，作为今天课程的一部分，我们将简要介绍一下。第一堂课就是所谓的价值学习，这就是我们刚才谈到的这个过程。价值学习试图估计我们的Q函数，对吗？找出给定状态和行为的Q函数，然后使用Q函数来优化给定的特定状态下要采取的最佳行动。

555
00:15:49,150 --> 00:15:50,290
0,255 255,540 540,735 735,930 930,1140
final step is use that

556
00:15:50,290 --> 00:15:51,970
0,210 210,500 670,1070 1270,1500 1500,1680
q function to, you know,

557
00:15:51,970 --> 00:15:53,200
0,255 255,420 420,680 820,1095 1095,1230
take some actions in the

558
00:15:53,200 --> 00:15:54,730
0,150 150,440 490,795 795,1230 1230,1530
real world. And broadly speaking,

559
00:15:54,730 --> 00:15:56,515
0,225 225,405 405,660 660,980 1480,1785
there are two classes of

560
00:15:56,515 --> 00:15:58,120
0,600 600,875 895,1290 1290,1425 1425,1605
reinforcement learning algorithms that we're

561
00:15:58,120 --> 00:15:59,050
0,105 105,240 240,480 480,750 750,930
going to briefly touch on

562
00:15:59,050 --> 00:16:00,090
0,165 165,285 285,420 420,780 780,1040
as part of today's lecture.

563
00:16:00,500 --> 00:16:01,855
0,275 275,500 500,785 785,1070 1070,1355
The first class is what's

564
00:16:01,855 --> 00:16:02,680
0,120 120,225 225,330 330,525 525,825
going to be called value

565
00:16:02,680 --> 00:16:04,120
0,360 360,600 600,900 900,1185 1185,1440
learning, and that's exactly this

566
00:16:04,120 --> 00:16:05,275
0,285 285,540 540,735 735,900 900,1155
process that we've just talked

567
00:16:05,275 --> 00:16:06,870
0,285 285,615 615,975 975,1275 1275,1595
about. Value learning tries to

568
00:16:06,920 --> 00:16:08,785
0,400 600,935 935,1145 1145,1420 1560,1865
estimate our q function, right?

569
00:16:08,785 --> 00:16:10,030
0,210 210,405 405,695 715,1035 1035,1245
So to find that q

570
00:16:10,030 --> 00:16:11,755
0,290 400,800 970,1260 1260,1485 1485,1725
function q given our state

571
00:16:11,755 --> 00:16:13,060
0,150 150,270 270,545 895,1155 1155,1305
and our action, and then

572
00:16:13,060 --> 00:16:14,730
0,225 225,510 510,750 750,1040 1270,1670
use that q function to,

573
00:16:14,900 --> 00:16:16,600
0,245 245,490 750,1205 1205,1490 1490,1700
you know, optimize for the

574
00:16:16,600 --> 00:16:18,390
0,225 225,560 610,900 900,1190 1390,1790
best action to take given

575
00:16:18,860 --> 00:16:20,125
0,350 350,700 720,1010 1010,1145 1145,1265
a particular state that we

576
00:16:20,125 --> 00:16:21,700
0,240 240,510 510,815
find ourselves in.|
|

577
00:16:21,700 --> 00:16:23,245
0,225 225,530 580,900 900,1185 1185,1545
The second class of algorithms,
第二类算法，我们将在今天课程的最后谈到，是这种方法的一种不同的框架。但是，不是首先优化Q函数并找到A Q值，然后使用Q函数来优化我们的操作，如果我们只尝试直接优化我们的策略，这是根据我们发现自己所处的特定状态采取的操作，会怎么样？如果我们这样做，如果我们能得到这个函数，对吗？

578
00:16:23,245 --> 00:16:24,205
0,180 180,390 390,510 510,735 735,960
which we'll touch on right

579
00:16:24,205 --> 00:16:25,225
0,150 150,270 270,450 450,675 675,1020
at the end of today's

580
00:16:25,225 --> 00:16:26,800
0,245 625,930 930,1125 1125,1305 1305,1575
lecture, is kind of a

581
00:16:26,800 --> 00:16:28,480
0,360 360,915 915,1185 1185,1395 1395,1680
different framing of this same

582
00:16:28,480 --> 00:16:30,150
0,375 375,720 720,930 930,1190 1270,1670
approach. But instead of first

583
00:16:30,320 --> 00:16:32,380
0,520 930,1250 1250,1460 1460,1750 1770,2060
optimizing the q function and

584
00:16:32,380 --> 00:16:33,385
0,240 240,480 480,630 630,825 825,1005
finding A Q value and

585
00:16:33,385 --> 00:16:34,500
0,135 135,375 375,630 630,825 825,1115
then using that q function

586
00:16:34,580 --> 00:16:36,940
0,400 720,1115 1115,1325 1325,1630 2100,2360
to optimize our actions, what

587
00:16:36,940 --> 00:16:38,305
0,150 150,390 390,720 720,1035 1035,1365
if we just try to

588
00:16:38,305 --> 00:16:40,660
0,395 595,1085 1135,1470 1470,1805 2065,2355
directly optimize our policy, which

589
00:16:40,660 --> 00:16:42,210
0,290 460,735 735,1010 1030,1290 1290,1550
is what action to take

590
00:16:42,590 --> 00:16:43,840
0,320 320,500 500,665 665,965 965,1250
based on a particular state

591
00:16:43,840 --> 00:16:45,000
0,150 150,270 270,525 525,840 840,1160
that we find ourselves in?

592
00:16:45,590 --> 00:16:46,855
0,275 275,425 425,575 575,850 990,1265
If we do that, if

593
00:16:46,855 --> 00:16:48,000
0,150 150,270 270,585 585,825 825,1145
we can obtain this function,

594
00:16:48,290 --> 00:16:49,120
0,400
right?|
|

595
00:16:49,120 --> 00:16:50,310
0,135 135,270 270,450 450,770 790,1190
Then we can directly sample
然后，我们可以直接从该策略分布中进行抽样，以获得最优操作。我们将在后面的课程中详细讨论这一点，但首先让我们讨论第一类方法，即Q学习方法，我们将在策略学习的第二部分建立直觉和知识。所以让我们从更深入地研究Q函数开始，特别是开始理解，你知道，我们如何在一开始估计它。

596
00:16:50,330 --> 00:16:52,630
0,290 290,515 515,850 1230,1630 2070,2300
from that policy distribution to

597
00:16:52,630 --> 00:16:54,355
0,330 330,570 570,825 825,1070 1390,1725
obtain the optimal action. We'll

598
00:16:54,355 --> 00:16:55,750
0,195 195,510 510,855 855,1110 1110,1395
talk more details about that

599
00:16:55,750 --> 00:16:56,875
0,285 285,480 480,630 630,855 855,1125
later in the lecture, but

600
00:16:56,875 --> 00:16:58,075
0,210 210,480 480,675 675,930 930,1200
first let's cover this first

601
00:16:58,075 --> 00:16:59,980
0,300 300,635 925,1325 1435,1710 1710,1905
class of approaches, which is

602
00:16:59,980 --> 00:17:02,080
0,195 195,465 465,860 1540,1845 1845,2100
q learning approaches, and we'll

603
00:17:02,080 --> 00:17:03,715
0,180 180,500 550,900 900,1425 1425,1635
build up that intuition and

604
00:17:03,715 --> 00:17:05,320
0,165 165,455 625,1025 1105,1380 1380,1605
that knowledge onto the second

605
00:17:05,320 --> 00:17:07,420
0,225 225,390 390,660 660,1040 1750,2100
part of policy learning. So

606
00:17:07,420 --> 00:17:08,575
0,270 270,555 555,750 750,960 960,1155
maybe let's start by just

607
00:17:08,575 --> 00:17:10,080
0,300 300,435 435,645 645,965 1105,1505
digging a bit deeper into

608
00:17:10,430 --> 00:17:12,550
0,290 290,470 470,760 930,1330 1830,2120
the q function specifically just

609
00:17:12,550 --> 00:17:13,585
0,195 195,390 390,645 645,900 900,1035
to start to understand, you

610
00:17:13,585 --> 00:17:14,800
0,135 135,300 300,450 450,725 925,1215
know, how we could estimate

611
00:17:14,800 --> 00:17:16,160
0,270 270,525 525,690 690,980
this in the beginning.|
|

612
00:17:16,440 --> 00:17:17,540
0,260 260,440 440,605 605,815 815,1100
So first let me introduce
所以首先让我来介绍一下这个游戏。也许你们中的一些人认识到这一点。这是一场叫做雅达利突围的游戏。这里的游戏本质上是一个代理能够向左或向右移动的游戏，这个桨在左下角或右下角。我们的目标是以这样一种方式移动它，你知道，这个向屏幕底部落下的球可以从你的球拍上反弹，反射回来，基本上你想要突破，对，把那个球反射回屏幕顶部的彩虹部分，然后继续打破。每当你点击屏幕顶部的一个像素，你就会打断那个像素。这个游戏的目标是基本上消除所有的彩虹像素，对吗？所以你想要继续击打屏幕顶部的球，直到你移除所有的像素。

613
00:17:17,540 --> 00:17:18,650
0,225 225,510 510,810 810,990 990,1110
this game. Maybe some of

614
00:17:18,650 --> 00:17:19,760
0,135 135,410 580,840 840,975 975,1110
you recognize this. This is

615
00:17:19,760 --> 00:17:21,275
0,135 135,315 315,570 570,920 970,1515
the game of called atari

616
00:17:21,275 --> 00:17:24,005
0,545 1735,2100 2100,2310 2310,2505 2505,2730
breakout. The the game here

617
00:17:24,005 --> 00:17:26,000
0,210 210,545 805,1140 1140,1475 1735,1995
is essentially one where the

618
00:17:26,000 --> 00:17:27,455
0,260 520,795 795,1020 1020,1230 1230,1455
agent is able to move

619
00:17:27,455 --> 00:17:28,640
0,255 255,420 420,630 630,885 885,1185
left or right, this paddle

620
00:17:28,640 --> 00:17:29,780
0,120 120,240 240,500 550,900 900,1140
on the bottom left or

621
00:17:29,780 --> 00:17:31,535
0,290 460,810 810,1065 1065,1370 1390,1755
right. And the objective is

622
00:17:31,535 --> 00:17:32,405
0,270 270,465 465,630 630,750 750,870
to move it in a

623
00:17:32,405 --> 00:17:34,145
0,275 295,630 630,885 885,1205 1255,1740
way that this ball that's

624
00:17:34,145 --> 00:17:36,080
0,335 595,995 1225,1560 1560,1755 1755,1935
coming down towards the bottom

625
00:17:36,080 --> 00:17:37,460
0,195 195,375 375,680 880,1155 1155,1380
of the screen can be,

626
00:17:37,460 --> 00:17:38,570
0,195 195,330 330,645 645,945 945,1110
you know, bounced off of

627
00:17:38,570 --> 00:17:40,355
0,150 150,590 910,1290 1290,1575 1575,1785
your paddle, reflected back up,

628
00:17:40,355 --> 00:17:41,710
0,240 240,540 540,780 780,1005 1005,1355
and essentially you want to

629
00:17:41,940 --> 00:17:43,775
0,305 305,610 720,1120 1350,1655 1655,1835
break out, right, reflect that

630
00:17:43,775 --> 00:17:44,660
0,195 195,420 420,630 630,765 765,885
ball back up to the

631
00:17:44,660 --> 00:17:45,515
0,165 165,270 270,405 405,630 630,855
top of the screen towards

632
00:17:45,515 --> 00:17:46,880
0,165 165,555 555,825 825,1125 1125,1365
the rainbow portion and keep

633
00:17:46,880 --> 00:17:48,065
0,300 300,600 600,840 840,1035 1035,1185
breaking off. Every time you

634
00:17:48,065 --> 00:17:49,130
0,210 210,480 480,855 855,945 945,1065
hit a pixel on the

635
00:17:49,130 --> 00:17:49,850
0,135 135,255 255,390 390,555 555,720
top of the screen, you

636
00:17:49,850 --> 00:17:51,275
0,195 195,405 405,615 615,1130 1180,1425
break off that pixel. And

637
00:17:51,275 --> 00:17:52,145
0,135 135,375 375,600 600,720 720,870
the objective of the game

638
00:17:52,145 --> 00:17:54,830
0,180 180,300 300,545 1885,2445 2445,2685
is to basically eliminate all

639
00:17:54,830 --> 00:17:56,300
0,165 165,345 345,750 750,1215 1215,1470
of those rainbow pixels, right?

640
00:17:56,300 --> 00:17:56,900
0,120 120,195 195,300 300,435 435,600
So you want to keep

641
00:17:56,900 --> 00:17:57,860
0,210 210,405 405,615 615,825 825,960
hitting that ball against the

642
00:17:57,860 --> 00:17:58,670
0,120 120,210 210,330 330,570 570,810
top of the screen until

643
00:17:58,670 --> 00:18:00,160
0,290 460,735 735,900 900,1050 1050,1490
you remove all the pixels.|
|

644
00:18:01,680 --> 00:18:03,425
0,380 380,635 635,800 800,1090 1410,1745
Now the q function tells
现在Q函数告诉我们，你知道，预期的总回报，或者我们可以基于给定的状态和行动对预期的总回报，我们可能会在这场游戏中发现自己。现在，我想在这里提出的第一点是，有时候，即使是我们人类，理解Q值应该是什么，有时也是相当不直观的，对吗？这里有一个例子。假设我们发现这两个状态动作对，这里是a和b，两个不同的选项，我们可以在这个游戏中看到。

645
00:18:03,425 --> 00:18:05,290
0,335 745,990 990,1155 1155,1470 1470,1865
us, you know, the expected

646
00:18:05,370 --> 00:18:07,240
0,400 720,1120 1140,1415 1415,1580 1580,1870
total return or the total

647
00:18:07,680 --> 00:18:09,430
0,400 690,965 965,1115 1115,1370 1370,1750
reward that we can expect

648
00:18:09,480 --> 00:18:10,780
0,335 335,530 530,635 635,880 900,1300
based on a given state

649
00:18:10,920 --> 00:18:12,620
0,320 320,640 750,1150 1320,1580 1580,1700
and action pair that we

650
00:18:12,620 --> 00:18:13,850
0,150 150,420 420,780 780,1035 1035,1230
may find ourselves in this

651
00:18:13,850 --> 00:18:15,830
0,320 1000,1365 1365,1605 1605,1785 1785,1980
game. Now, the first point

652
00:18:15,830 --> 00:18:16,640
0,135 135,255 255,375 375,540 540,810
I want to make here

653
00:18:16,640 --> 00:18:19,055
0,240 240,465 465,800 1150,1550 2170,2415
is that sometimes even for

654
00:18:19,055 --> 00:18:20,980
0,245 265,585 585,905 1015,1415 1525,1925
us as humans to understand

655
00:18:21,090 --> 00:18:22,370
0,320 320,530 530,695 695,970 990,1280
what the q value should

656
00:18:22,370 --> 00:18:24,755
0,290 430,830 850,1250 1300,1605 1605,2385
be is sometimes quite unintuitive,

657
00:18:24,755 --> 00:18:26,050
0,315 315,510 510,765 765,960 960,1295
right? So here's one example.

658
00:18:26,580 --> 00:18:27,860
0,335 335,455 455,650 650,935 935,1280
Let's say we find these

659
00:18:27,860 --> 00:18:29,810
0,330 330,585 585,890 1120,1575 1575,1950
two state action pairs right

660
00:18:29,810 --> 00:18:31,025
0,240 240,405 405,660 660,915 915,1215
here is a and b

661
00:18:31,025 --> 00:18:32,150
0,255 255,435 435,750 750,1005 1005,1125
two different options that we

662
00:18:32,150 --> 00:18:33,095
0,105 105,225 225,500 520,795 795,945
can be presented with in

663
00:18:33,095 --> 00:18:34,060
0,180 180,485
this game.|
|

664
00:18:34,060 --> 00:18:35,610
0,380 700,960 960,1095 1095,1260 1260,1550
A the ball is coming
A球正径直朝我们飞过来。那是我们的州。我们的行动是什么都不做，只是把球垂直向上反射回来。第二种情况。基本上，球的状态是以一个角度微微地飞过来。我们还没有完全处于它的下面，我们需要朝着它前进，并以一种你知道会成功而不是错失的方式来击球，希望是吧？所以希望那个球不会传到我们下面，比赛就结束了。您能想象一下，您知道这两个选项中哪一个可能对网络具有更高的Q值吗？哪一个会导致一个。

665
00:18:35,810 --> 00:18:37,120
0,290 290,515 515,755 755,1010 1010,1310
straight down towards us. That's

666
00:18:37,120 --> 00:18:38,665
0,195 195,560 640,930 930,1220 1240,1545
our state. Our action is

667
00:18:38,665 --> 00:18:39,840
0,150 150,270 270,545 595,885 885,1175
to do nothing and simply

668
00:18:39,920 --> 00:18:41,340
0,305 305,500 500,790 810,1115 1115,1420
reflect that ball back up

669
00:18:42,050 --> 00:18:45,210
0,500 500,760 1140,1415 1415,1690 2760,3160
vertically up. The second situation.

670
00:18:45,980 --> 00:18:47,485
0,350 350,650 650,890 890,1180 1230,1505
The state is basically that

671
00:18:47,485 --> 00:18:48,445
0,135 135,285 285,465 465,690 690,960
the ball is coming slightly

672
00:18:48,445 --> 00:18:49,795
0,180 180,315 315,605 745,1125 1125,1350
at an angle. We're not

673
00:18:49,795 --> 00:18:51,175
0,270 270,575 715,1005 1005,1215 1215,1380
quite underneath it yet and

674
00:18:51,175 --> 00:18:52,225
0,105 105,270 270,465 465,735 735,1050
we need to move towards

675
00:18:52,225 --> 00:18:54,925
0,335 565,965 1165,1565 2215,2505 2505,2700
it and actually hit that

676
00:18:54,925 --> 00:18:56,400
0,305 535,795 795,945 945,1155 1155,1475
ball in a way that

677
00:18:56,570 --> 00:18:57,985
0,245 245,490 510,910 930,1220 1220,1415
you know will will make

678
00:18:57,985 --> 00:18:59,185
0,180 180,455 535,855 855,1050 1050,1200
it and not Miss it

679
00:18:59,185 --> 00:19:00,400
0,275 325,645 645,825 825,1020 1020,1215
hopefully right? So hopefully that

680
00:19:00,400 --> 00:19:01,510
0,150 150,450 450,675 675,915 915,1110
ball doesn't pass below us

681
00:19:01,510 --> 00:19:02,215
0,150 150,270 270,405 405,570 570,705
and the game would be

682
00:19:02,215 --> 00:19:04,555
0,245 1135,1395 1395,1575 1575,1895 2095,2340
over. Can you imagine, you

683
00:19:04,555 --> 00:19:05,850
0,180 180,405 405,585 585,875 895,1295
know which of these two

684
00:19:06,140 --> 00:19:08,455
0,400 990,1310 1310,1630 1680,2000 2000,2315
options might have a higher

685
00:19:08,455 --> 00:19:09,955
0,285 285,575 895,1155 1155,1275 1275,1500
q value for the network?

686
00:19:09,955 --> 00:19:11,050
0,240 240,405 405,645 645,915 915,1095
Which one would result in

687
00:19:11,050 --> 00:19:11,940
0,260
a.|
|

688
00:19:12,010 --> 00:19:13,200
0,260 260,455 455,790 810,1070 1070,1190
Rate of reward for the
对神经网络或代理的报酬率。

689
00:19:13,200 --> 00:19:14,430
0,210 210,470 730,1005 1005,1125 1125,1230
neural network or for the

690
00:19:14,430 --> 00:19:15,960
0,260
agent.|
|

691
00:19:17,260 --> 00:19:18,285
0,260 260,380 380,515 515,755 755,1025
So how many people believe
那么，有多少人相信A会带来更高的回报呢？

692
00:19:18,285 --> 00:19:19,500
0,305 325,675 675,930 930,1095 1095,1215
a would result in a

693
00:19:19,500 --> 00:19:23,740
0,260 1510,1910
higher return?|
|

694
00:19:23,930 --> 00:19:25,680
0,260 260,410 410,700
How about b?|
B怎么样？|

695
00:19:26,400 --> 00:19:27,665
0,400 450,710 710,860 860,1070 1070,1265
Okay, how about someone who
好吧，那选b的人呢？你能告诉我为什么吗？胡麻B.

696
00:19:27,665 --> 00:19:28,985
0,210 210,510 510,765 765,1050 1050,1320
picked b? Can you tell

697
00:19:28,985 --> 00:19:30,620
0,150 150,390 390,755
me why? B.|
|

698
00:19:32,160 --> 00:19:34,420
0,400 960,1370 1370,1595 1595,1880 1880,2260
Agency, you're actually doing something.|
中情局，你真的在做一些事情。|

699
00:19:35,730 --> 00:19:39,155
0,400 600,1000 1740,2015 2015,2290 3180,3425
Okay, yeah about more for
好的，是的，关于更多，对于你只有一个，你可以拿到的最大值就像一个，因为当你自动地反映你的背景后，实际上有一个非常有趣的事情。所以当我第一次看到这个的时候，实际上是啊，这对我来说是非常不直观的。Y A实际上比b差得多，但总的来说，b的这个非常保守的动作，就像你说的，两个答案都在暗示，a是一个非常保守的动作。你只是上上下下而已。它将获得很好的回报。它会解决这个游戏的，对吗？事实上，它就像这样解决了这个游戏。你可以看到，总的来说，这一行动将是相当保守的。它只是反弹起来，从顶端一次击中一分，然后非常缓慢地脱离。你可以在这里看到的黑板。但总的来说，你可以看到黑板被拆分的部分是朝向黑板的中心的，对吗？不是很多。

700
00:19:39,155 --> 00:19:40,580
0,245 475,780 780,1065 1065,1290 1290,1425
the for a you only

701
00:19:40,580 --> 00:19:41,870
0,165 165,440 580,840 840,1065 1065,1290
have like the maximum you

702
00:19:41,870 --> 00:19:42,635
0,135 135,300 300,465 465,585 585,765
can take off is like

703
00:19:42,635 --> 00:19:44,440
0,335 385,750 750,1095 1095,1440 1440,1805
one because after you reflect

704
00:19:44,910 --> 00:19:49,180
0,335 335,670 840,1240 3750,4010 4010,4270
your automatically background more than

705
00:19:50,580 --> 00:19:52,280
0,400 510,905 905,1220 1220,1505 1505,1700
exactly and actually there's a

706
00:19:52,280 --> 00:19:54,215
0,350 670,990 990,1260 1260,1610 1690,1935
very interesting thing. So when

707
00:19:54,215 --> 00:19:55,475
0,135 135,330 330,525 525,815 895,1260
I first saw this actually

708
00:19:55,475 --> 00:19:57,905
0,420 420,695 1825,2085 2085,2235 2235,2430
it's ah, it was very

709
00:19:57,905 --> 00:19:59,465
0,615 615,765 765,990 990,1305 1305,1560
unintuitive for me. Y A

710
00:19:59,465 --> 00:20:01,130
0,305 325,690 690,1035 1035,1350 1350,1665
is actually working much worse

711
00:20:01,130 --> 00:20:02,230
0,285 285,510 510,675 675,810 810,1100
than b but in general

712
00:20:02,340 --> 00:20:04,240
0,260 260,520 600,1000 1050,1450 1500,1900
with this very conservative action

713
00:20:04,710 --> 00:20:06,005
0,335 335,665 665,965 965,1130 1130,1295
of b, your kind of

714
00:20:06,005 --> 00:20:07,370
0,210 210,390 390,555 555,845 1075,1365
exactly like you said, the

715
00:20:07,370 --> 00:20:10,115
0,290 430,830 1060,1455 1455,2120 2470,2745
two answers were implying is

716
00:20:10,115 --> 00:20:10,850
0,150 150,300 300,420 420,510 510,735
that a is a very

717
00:20:10,850 --> 00:20:11,915
0,315 315,600 600,870 870,945 945,1065
conservative action. You're kind of

718
00:20:11,915 --> 00:20:12,910
0,195 195,390 390,540 540,705 705,995
only going up and down.

719
00:20:13,410 --> 00:20:14,450
0,260 260,485 485,725 725,860 860,1040
It will achieve a good

720
00:20:14,450 --> 00:20:15,410
0,225 225,375 375,525 525,765 765,960
reward. It will solve the

721
00:20:15,410 --> 00:20:16,970
0,260 460,860 940,1200 1200,1350 1350,1560
game, right? In fact it

722
00:20:16,970 --> 00:20:18,275
0,300 300,420 420,675 675,1035 1035,1305
solves the game exactly like

723
00:20:18,275 --> 00:20:19,160
0,240 240,465 465,615 615,735 735,885
this right here. You can

724
00:20:19,160 --> 00:20:20,890
0,290 550,840 840,1095 1095,1395 1395,1730
see in general this action

725
00:20:21,060 --> 00:20:22,130
0,320 320,530 530,665 665,815 815,1070
is going to be quite

726
00:20:22,130 --> 00:20:23,470
0,300 300,570 570,675 675,1080 1080,1340
conservative. It's just bouncing up,

727
00:20:23,670 --> 00:20:24,800
0,350 350,635 635,860 860,995 995,1130
hitting one point at a

728
00:20:24,800 --> 00:20:25,940
0,290 340,615 615,765 765,945 945,1140
time from the top and

729
00:20:25,940 --> 00:20:27,875
0,225 225,495 495,825 825,1220 1660,1935
breaking off very slowly. The

730
00:20:27,875 --> 00:20:28,790
0,210 210,420 420,570 570,735 735,915
board that you can see

731
00:20:28,790 --> 00:20:30,740
0,290 670,1070 1150,1440 1440,1695 1695,1950
here. But in general you

732
00:20:30,740 --> 00:20:31,550
0,180 180,360 360,525 525,675 675,810
see the part of the

733
00:20:31,550 --> 00:20:32,795
0,180 180,510 510,675 675,915 915,1245
board that's being broken off

734
00:20:32,795 --> 00:20:34,340
0,315 315,555 555,765 765,1055 1255,1545
is towards the center of

735
00:20:34,340 --> 00:20:35,735
0,150 150,410 550,915 915,1200 1200,1395
the board, right? Not much

736
00:20:35,735 --> 00:20:36,000
0,225
on.|
|

737
00:20:36,000 --> 00:20:37,545
0,315 315,480 480,630 630,890 1270,1545
Edges of the board. If
板子的边缘。如果你现在把b和b放在一起，你就有了某种作用力。就像其中一个答案所说的，你正朝着球走来，这意味着你有时会撞到你的球拍的角落，在你的球拍上有一个非常极端的角度，也击中了板子的两边。事实证明，算法，代理人实际上可以了解到，击中棋盘的两边可能会产生某种意想不到的后果，就像这样。所以在这里，你可以看到它试图制定这项政策。它的目标是棋盘的侧面，但一旦它到达棋盘一侧的突破口，它就在解决方案中发现了这个黑客攻击，现在它正在断开大量的点数。所以这是这个神经网络学到的一种技巧。这是一种在球落下的时候它甚至离开球的方式，这样我们就可以向它移动，只是在角球上击球，在板子的角部执行，几乎是免费的突破了很多球。

738
00:20:37,545 --> 00:20:38,960
0,135 135,270 270,495 495,845 1015,1415
you look at b now

739
00:20:39,340 --> 00:20:40,550
0,320 320,545 545,800 800,935 935,1210
with b, you're kind of

740
00:20:40,570 --> 00:20:42,015
0,335 335,670 960,1235 1235,1355 1355,1445
having agency. Like one of

741
00:20:42,015 --> 00:20:43,185
0,90 90,335 355,675 675,930 930,1170
the answers said you're coming

742
00:20:43,185 --> 00:20:44,565
0,345 345,555 555,815 955,1230 1230,1380
towards the ball and what

743
00:20:44,565 --> 00:20:46,335
0,135 135,605 955,1230 1230,1410 1410,1770
that implies is that you're

744
00:20:46,335 --> 00:20:47,870
0,335 505,780 780,990 990,1230 1230,1535
sometimes going to actually hit

745
00:20:48,040 --> 00:20:49,520
0,320 320,590 590,800 800,980 980,1480
the corner of your paddle

746
00:20:49,540 --> 00:20:50,775
0,260 260,380 380,515 515,790 900,1235
and have a very extreme

747
00:20:50,775 --> 00:20:52,425
0,335 625,900 900,1080 1080,1455 1455,1650
angle on your paddle and

748
00:20:52,425 --> 00:20:53,400
0,165 165,360 360,615 615,840 840,975
hit the sides of the

749
00:20:53,400 --> 00:20:54,600
0,180 180,390 390,680 790,1050 1050,1200
board as well. And it

750
00:20:54,600 --> 00:20:56,180
0,210 210,495 495,810 810,1110 1110,1580
turns out that the algorithm,

751
00:20:56,260 --> 00:20:57,740
0,260 260,520 570,920 920,1175 1175,1480
the agent can actually learn

752
00:20:57,940 --> 00:20:59,535
0,395 395,790 960,1250 1250,1430 1430,1595
that hitting the sides of

753
00:20:59,535 --> 00:21:00,620
0,135 135,375 375,615 615,780 780,1085
the board can have some

754
00:21:00,670 --> 00:21:03,090
0,260 260,395 395,670 990,1390 2130,2420
kind of unexpected consequences that

755
00:21:03,090 --> 00:21:04,305
0,165 165,345 345,650 760,1050 1050,1215
look like this. So here

756
00:21:04,305 --> 00:21:05,085
0,165 165,300 300,435 435,630 630,780
you see it trying to

757
00:21:05,085 --> 00:21:06,735
0,270 270,480 480,765 765,1185 1185,1650
enact that policy. It's targeting

758
00:21:06,735 --> 00:21:07,760
0,180 180,420 420,630 630,765 765,1025
the sides of the board,

759
00:21:08,170 --> 00:21:09,540
0,290 290,485 485,665 665,1115 1115,1370
but once it reaches a

760
00:21:09,540 --> 00:21:10,410
0,390 390,540 540,660 660,780 780,870
breakout on the side of

761
00:21:10,410 --> 00:21:11,535
0,90 90,350 430,720 720,915 915,1125
the board, it found this

762
00:21:11,535 --> 00:21:12,960
0,305 355,630 630,795 795,1085 1165,1425
hack in the solution where

763
00:21:12,960 --> 00:21:13,830
0,120 120,315 315,510 510,705 705,870
now it's breaking off a

764
00:21:13,830 --> 00:21:15,405
0,150 150,300 300,590 1150,1425 1425,1575
ton of points. So that

765
00:21:15,405 --> 00:21:16,700
0,225 225,450 450,675 675,960 960,1295
was kind of a trick

766
00:21:16,720 --> 00:21:18,260
0,335 335,590 590,860 860,1120 1140,1540
that this neural network learned.

767
00:21:18,790 --> 00:21:19,800
0,290 290,455 455,605 605,800 800,1010
It was a way that

768
00:21:19,800 --> 00:21:21,405
0,165 165,420 420,800 850,1250 1330,1605
it even moves away from

769
00:21:21,405 --> 00:21:22,560
0,150 150,425 505,780 780,990 990,1155
the ball as it's coming

770
00:21:22,560 --> 00:21:23,430
0,240 240,450 450,600 600,720 720,870
down just so we could

771
00:21:23,430 --> 00:21:24,510
0,210 210,480 480,735 735,915 915,1080
move back towards it, just

772
00:21:24,510 --> 00:21:25,320
0,135 135,285 285,480 480,645 645,810
to hit it on the

773
00:21:25,320 --> 00:21:26,990
0,290 520,920 1000,1260 1260,1395 1395,1670
corner and execute on those

774
00:21:27,130 --> 00:21:28,515
0,365 365,730 750,1055 1055,1250 1250,1385
those corner parts of the

775
00:21:28,515 --> 00:21:29,505
0,195 195,450 450,645 645,825 825,990
board and break out a

776
00:21:29,505 --> 00:21:31,280
0,150 150,425 565,965 1135,1455 1455,1775
lot of pieces for free

777
00:21:31,360 --> 00:21:32,760
0,400
almost.|
|

778
00:21:33,800 --> 00:21:34,750
0,260 260,410 410,620 620,800 800,950
So now that we can
现在我们可以看到，有时获得Q函数可能有点不直观，但这里的关键点是，如果我们有Q函数，我们可以直接使用它来确定，你知道，在我们发现自己处于任何给定的状态下，我们可以采取的最佳行动是什么？因此，现在的问题自然是，我们如何才能训练出一个真正能够学习这个Q函数的神经网络？

779
00:21:34,750 --> 00:21:36,325
0,165 165,440 460,855 855,1365 1365,1575
see that sometimes obtaining the

780
00:21:36,325 --> 00:21:38,110
0,165 165,455 715,1115 1285,1605 1605,1785
q function can be a

781
00:21:38,110 --> 00:21:40,135
0,150 150,300 300,1130 1270,1670 1720,2025
little bit unintuitive, but the

782
00:21:40,135 --> 00:21:41,125
0,210 210,450 450,645 645,795 795,990
key point here is that

783
00:21:41,125 --> 00:21:41,950
0,180 180,330 330,510 510,675 675,825
if we have the q

784
00:21:41,950 --> 00:21:43,195
0,270 270,510 510,705 705,990 990,1245
function, we can directly use

785
00:21:43,195 --> 00:21:44,905
0,305 475,875 925,1325 1345,1590 1590,1710
it to determine, you know,

786
00:21:44,905 --> 00:21:46,080
0,165 165,390 390,600 600,825 825,1175
what is the best action

787
00:21:46,340 --> 00:21:47,365
0,350 350,560 560,695 695,860 860,1025
that we can take in

788
00:21:47,365 --> 00:21:48,745
0,195 195,510 510,905 1015,1260 1260,1380
any given state that we

789
00:21:48,745 --> 00:21:49,870
0,255 255,540 540,765 765,975 975,1125
find ourselves in? So now

790
00:21:49,870 --> 00:21:51,700
0,135 135,410 640,990 990,1340 1540,1830
the question naturally is how

791
00:21:51,700 --> 00:21:52,585
0,150 150,315 315,510 510,660 660,885
can we train a neural

792
00:21:52,585 --> 00:21:54,760
0,275 445,765 765,1085 1345,1745 1795,2175
network that can indeed learn

793
00:21:54,760 --> 00:21:57,100
0,330 330,570 570,860
this q function?|
|

794
00:21:57,340 --> 00:21:59,565
0,400 1350,1700 1700,1940 1940,2105 2105,2225
So the type of the
所以神经网络的类型，很自然地，因为我们有一个函数，它接受两个东西作为输入，让我们想象一下，我们的神经网络也将这两个对象作为输入。一个目标将是董事会的状态。您可以简单地将其视为屏幕上描述该电路板的像素。所以这是黑板在特定时间的图像。也许你甚至想提供两三张图片，给它一些时间信息和一些过去的历史的感觉。但所有这些信息都可以组合在一起，并以状态的形式提供给网络。

795
00:21:59,565 --> 00:22:01,320
0,195 195,435 435,815 985,1385 1465,1755
neural network here, naturally, because

796
00:22:01,320 --> 00:22:02,220
0,135 135,225 225,345 345,615 615,900
we have a function that

797
00:22:02,220 --> 00:22:03,315
0,180 180,420 420,675 675,870 870,1095
takes as input two things,

798
00:22:03,315 --> 00:22:04,545
0,330 330,540 540,765 765,1005 1005,1230
let's imagine our neural network

799
00:22:04,545 --> 00:22:06,110
0,345 345,660 660,870 870,1145 1165,1565
will also take as input

800
00:22:06,130 --> 00:22:08,025
0,395 395,790 1110,1415 1415,1610 1610,1895
these two objects as well.

801
00:22:08,025 --> 00:22:09,225
0,395 415,720 720,915 915,1080 1080,1200
One object is going to

802
00:22:09,225 --> 00:22:09,990
0,120 120,300 300,480 480,630 630,765
be the state of the

803
00:22:09,990 --> 00:22:10,740
0,180 180,345 345,465 465,615 615,750
board. You can think of

804
00:22:10,740 --> 00:22:12,015
0,150 150,330 330,585 585,840 840,1275
this as simply the pixels

805
00:22:12,015 --> 00:22:12,950
0,195 195,300 300,420 420,615 615,935
that are on the screen

806
00:22:13,210 --> 00:22:14,460
0,500 500,695 695,950 950,1115 1115,1250
describing that board. So it's

807
00:22:14,460 --> 00:22:15,435
0,90 90,350 400,675 675,810 810,975
an image of the board

808
00:22:15,435 --> 00:22:16,635
0,165 165,345 345,665 685,1005 1005,1200
at a particular time. Maybe

809
00:22:16,635 --> 00:22:17,660
0,135 135,285 285,405 405,630 630,1025
you want to even provide

810
00:22:17,920 --> 00:22:19,220
0,290 290,470 470,620 620,880 900,1300
two or three images to

811
00:22:19,270 --> 00:22:20,340
0,260 260,410 410,620 620,845 845,1070
give it some sense of

812
00:22:20,340 --> 00:22:22,410
0,500 610,1010 1360,1635 1635,1815 1815,2070
temporal information and some past

813
00:22:22,410 --> 00:22:24,075
0,315 315,585 585,825 825,1160 1390,1665
history as well. But all

814
00:22:24,075 --> 00:22:25,230
0,135 135,360 360,690 690,930 930,1155
of that information can be

815
00:22:25,230 --> 00:22:27,030
0,330 330,710 970,1305 1305,1605 1605,1800
combined together and provided to

816
00:22:27,030 --> 00:22:27,795
0,90 90,300 300,495 495,615 615,765
the network in the form

817
00:22:27,795 --> 00:22:28,940
0,135 135,300 300,605
of a state.|
|

818
00:22:28,950 --> 00:22:30,080
0,305 305,530 530,800 800,995 995,1130
And in addition to that,
除此之外，您可能还想提供一些操作，对吗？因此，在这种情况下，神经网络或代理人在这个游戏中可以采取的行动是向右移动，向左移动，保持不动。这可能是三个不同的动作，可以提供给神经网络的输入并将其参数化。这里的目标是，你知道，估计单个数字的产出。

819
00:22:30,080 --> 00:22:31,120
0,150 150,360 360,570 570,735 735,1040
you may also want to

820
00:22:31,140 --> 00:22:32,930
0,380 380,650 650,940 1200,1520 1520,1790
provide some actions as well,

821
00:22:32,930 --> 00:22:33,815
0,255 255,390 390,480 480,645 645,885
right? So in this case,

822
00:22:33,815 --> 00:22:36,035
0,180 180,425 535,935 1225,1625 1855,2220
the actions that a neural

823
00:22:36,035 --> 00:22:37,340
0,245 265,510 510,615 615,875 1015,1305
network or an agent could

824
00:22:37,340 --> 00:22:38,825
0,270 270,525 525,720 720,1040 1150,1485
take in this game is

825
00:22:38,825 --> 00:22:39,905
0,195 195,455 505,750 750,855 855,1080
to move to the right,

826
00:22:39,905 --> 00:22:40,910
0,195 195,285 285,510 510,765 765,1005
to the left, to stay

827
00:22:40,910 --> 00:22:42,530
0,350 910,1170 1170,1335 1335,1470 1470,1620
still. And those could be

828
00:22:42,530 --> 00:22:43,685
0,195 195,390 390,710 760,1035 1035,1155
three different actions that could

829
00:22:43,685 --> 00:22:45,845
0,245 295,695 775,1080 1080,1745 1915,2160
be provided and parameterized to

830
00:22:45,845 --> 00:22:46,655
0,195 195,405 405,510 510,615 615,810
the input of a neural

831
00:22:46,655 --> 00:22:48,305
0,245 595,885 885,1095 1095,1365 1365,1650
network. The goal here is

832
00:22:48,305 --> 00:22:49,850
0,285 285,495 495,755 1045,1335 1335,1545
to, you know, estimate the

833
00:22:49,850 --> 00:22:51,740
0,315 315,710 1090,1490
single number output.|
|

834
00:22:51,810 --> 00:22:53,105
0,305 305,610 660,920 920,1070 1070,1295
That measures what is the
它测量这个神经网络在这个特定状态作用对的期望值或期望值是多少。通常你会看到，如果你想评估，假设有一个非常大的动作空间，用一个非常大的动作空间来尝试左边的方法，效率会非常低。因为这意味着你将不得不多次向前运行你的神经网络，对于你动作空间的每一个元素一次。所以，如果你只给它提供了你所在状态的一个输入，作为你给它的输出，假设所有n个不同的Q值，每一个可能的动作都有一个Q值，那又会怎样呢？这样一来，你只需要针对你所处的特定状态运行一次神经网络。

835
00:22:53,105 --> 00:22:55,450
0,335 655,1055 1525,1800 1800,2010 2010,2345
expected value or the expected

836
00:22:55,470 --> 00:22:57,095
0,290 290,580 870,1190 1190,1400 1400,1625
q value of this neural

837
00:22:57,095 --> 00:22:58,730
0,225 225,495 495,785 835,1235 1345,1635
network at this particular state

838
00:22:58,730 --> 00:23:01,640
0,290 490,890 1450,1850 1900,2685 2685,2910
action pair. Now oftentimes what

839
00:23:01,640 --> 00:23:02,900
0,240 240,525 525,810 810,1035 1035,1260
you'll see is that if

840
00:23:02,900 --> 00:23:04,670
0,150 150,375 375,735 735,1130 1390,1770
you wanted to evaluate, let's

841
00:23:04,670 --> 00:23:05,800
0,150 150,300 300,525 525,795 795,1130
suppose a very large action

842
00:23:05,880 --> 00:23:06,710
0,290 290,515 515,620 620,710 710,830
space, it's going to be

843
00:23:06,710 --> 00:23:08,495
0,210 210,950 1090,1380 1380,1575 1575,1785
very inefficient to try the

844
00:23:08,495 --> 00:23:10,120
0,195 195,360 360,510 510,785 1225,1625
approach on the left with

845
00:23:10,260 --> 00:23:11,350
0,275 275,425 425,605 605,800 800,1090
with a very large action

846
00:23:11,370 --> 00:23:12,440
0,400 420,725 725,890 890,980 980,1070
space. Because what it would

847
00:23:12,440 --> 00:23:13,100
0,135 135,270 270,375 375,540 540,660
mean is that you'd have

848
00:23:13,100 --> 00:23:14,590
0,180 180,470 670,960 960,1215 1215,1490
to run your neural network

849
00:23:14,940 --> 00:23:17,075
0,400 990,1310 1310,1565 1565,1850 1850,2135
forward many different times one

850
00:23:17,075 --> 00:23:18,490
0,270 270,450 450,675 675,1035 1035,1415
time for every single element

851
00:23:18,540 --> 00:23:19,670
0,260 260,380 380,640 660,965 965,1130
of your action space. So

852
00:23:19,670 --> 00:23:21,010
0,105 105,315 315,680 790,1065 1065,1340
what if instead you only

853
00:23:21,090 --> 00:23:22,475
0,335 335,620 620,965 965,1235 1235,1385
provided it an input of

854
00:23:22,475 --> 00:23:24,670
0,210 210,545 925,1290 1290,1655 1795,2195
your state and as output

855
00:23:24,750 --> 00:23:26,015
0,275 275,455 455,760 780,1115 1115,1265
you gave it, let's say

856
00:23:26,015 --> 00:23:27,970
0,300 300,690 690,1085 1375,1665 1665,1955
all n different q values,

857
00:23:28,410 --> 00:23:29,675
0,350 350,575 575,830 830,1055 1055,1265
one q value for every

858
00:23:29,675 --> 00:23:31,010
0,270 270,555 555,935 955,1215 1215,1335
single possible action. That way

859
00:23:31,010 --> 00:23:31,895
0,150 150,375 375,600 600,750 750,885
you only need to run

860
00:23:31,895 --> 00:23:33,530
0,150 150,390 390,630 630,1025 1375,1635
your neural network once for

861
00:23:33,530 --> 00:23:34,490
0,120 120,345 345,615 615,780 780,960
the given state that you're

862
00:23:34,490 --> 00:23:35,160
0,230
in.|
|

863
00:23:35,290 --> 00:23:36,360
0,260 260,410 410,605 605,860 860,1070
And then that neural network
然后神经网络会告诉你所有可能的动作，最大值是多少？然后，您只需查看该输出并选择具有K S Q值的动作。

864
00:23:36,360 --> 00:23:37,650
0,255 255,465 465,765 765,1050 1050,1290
will tell you for all

865
00:23:37,650 --> 00:23:39,710
0,315 315,680 1030,1440 1440,1695 1695,2060
possible actions, what's the maximum?

866
00:23:40,390 --> 00:23:41,520
0,290 290,530 530,785 785,965 965,1130
You simply then look at

867
00:23:41,520 --> 00:23:43,155
0,290 430,830 970,1260 1260,1470 1470,1635
that output and pick the

868
00:23:43,155 --> 00:23:44,580
0,245 655,930 930,1110 1110,1290 1290,1425
action that has the K

869
00:23:44,580 --> 00:23:46,140
0,165 165,345 345,620
S Q value.|
|

870
00:23:46,940 --> 00:23:49,420
0,400 690,965 965,1115 1115,1390 2160,2480
Now, what would happen? Right.
现在，会发生什么呢？正确的。所以实际上我想在这里提出的问题是，你知道，我们想要训练这两个网络中的一个。为了简单起见，让我们继续使用右侧的网络，因为它是左侧网络的一个更高效的版本。问题是，你知道，我们如何真正在右边训练这个网络？具体地说，我希望你们所有人都考虑最好的情况，首先考虑一个代理人在特定情况下会如何理想地表现，或者如果一个代理人在任何给定的状态下采取所有理想的行动会发生什么。这将意味着，从本质上讲，目标回报是正确的，无论是我们试图预测的值，还是预测的值。目标总是最大化的，对吗？这基本上可以作为特工的基本事实。

871
00:23:49,420 --> 00:23:51,010
0,315 315,710 760,1140 1140,1425 1425,1590
So actually the question I

872
00:23:51,010 --> 00:23:51,865
0,135 135,285 285,525 525,720 720,855
want to pose here is

873
00:23:51,865 --> 00:23:53,770
0,275 505,735 735,965 1465,1740 1740,1905
really, you know, we want

874
00:23:53,770 --> 00:23:54,865
0,180 180,470 550,810 810,930 930,1095
to train one of these

875
00:23:54,865 --> 00:23:55,945
0,165 165,420 420,795 795,945 945,1080
two networks. Let's stick with

876
00:23:55,945 --> 00:23:56,800
0,120 120,300 300,510 510,645 645,855
the network on the right

877
00:23:56,800 --> 00:23:58,090
0,225 225,825 825,975 975,1125 1125,1290
for simplicity, just since it's

878
00:23:58,090 --> 00:23:59,155
0,75 75,225 225,450 450,735 735,1065
a much more efficient version

879
00:23:59,155 --> 00:24:00,220
0,315 315,525 525,720 720,930 930,1065
of the network on the

880
00:24:00,220 --> 00:24:02,320
0,260 790,1190 1270,1545 1545,1785 1785,2100
left. And the question is,

881
00:24:02,320 --> 00:24:03,310
0,210 210,390 390,615 615,765 765,990
you know, how do we

882
00:24:03,310 --> 00:24:05,125
0,300 300,620 700,1050 1050,1400 1510,1815
actually train that network on

883
00:24:05,125 --> 00:24:06,910
0,165 165,425 715,1095 1095,1475 1525,1785
the right? And specifically I

884
00:24:06,910 --> 00:24:07,645
0,135 135,285 285,420 420,570 570,735
want all of you to

885
00:24:07,645 --> 00:24:09,085
0,210 210,525 525,855 855,1125 1125,1440
think about really the best

886
00:24:09,085 --> 00:24:10,705
0,345 345,695 925,1230 1230,1425 1425,1620
case scenario just to start

887
00:24:10,705 --> 00:24:11,890
0,255 255,480 480,615 615,840 840,1185
with how an agent would

888
00:24:11,890 --> 00:24:13,920
0,285 285,1070 1210,1470 1470,1680 1680,2030
perform ideally in a particular

889
00:24:14,510 --> 00:24:16,110
0,400 660,980 980,1175 1175,1325 1325,1600
situation or what would happen

890
00:24:16,520 --> 00:24:18,235
0,260 260,395 395,670 750,1150 1380,1715
if an agent took all

891
00:24:18,235 --> 00:24:20,230
0,225 225,495 495,875 1015,1415 1705,1995
of the ideal actions at

892
00:24:20,230 --> 00:24:21,760
0,210 210,510 510,890 1120,1380 1380,1530
any given state. This would

893
00:24:21,760 --> 00:24:23,280
0,180 180,420 420,770 850,1185 1185,1520
mean that essentially the target

894
00:24:23,450 --> 00:24:26,100
0,400 600,950 950,1300 1680,1970 1970,2650
return right, the, the predicted

895
00:24:26,180 --> 00:24:27,430
0,320 320,575 575,770 770,1010 1010,1250
or the the value that

896
00:24:27,430 --> 00:24:28,615
0,240 240,465 465,675 675,915 915,1185
we're trying to predict. The

897
00:24:28,615 --> 00:24:30,220
0,305 625,915 915,1095 1095,1335 1335,1605
target is going to always

898
00:24:30,220 --> 00:24:32,350
0,315 315,1070 1420,1755 1755,1965 1965,2130
be maximized, right? And this

899
00:24:32,350 --> 00:24:33,625
0,195 195,420 420,660 660,975 975,1275
can serve as essentially the

900
00:24:33,625 --> 00:24:35,130
0,270 270,635 835,1125 1125,1260 1260,1505
ground truth to the agent.|
|

901
00:24:36,200 --> 00:24:38,330
0,350 430,720 720,1010 1690,1965 1965,2130
Now, for example, to do
现在，例如，要做到这一点，我们想要建立一个损失函数，它基本上代表了我们的预期回报，如果我们能够采取所有最好的行动，对吧。所以，举个例子，如果我们选择一个最初的奖励，加上在我们的行动空间中选择一些行动，以最大化我们的预期回报，那么对于下一个未来状态，我们需要应用这个贴现因子，并递归地应用相同的等式，这就变成了我们的目标。

902
00:24:38,330 --> 00:24:39,830
0,290 340,600 600,750 750,945 945,1500
this, we want to formulate

903
00:24:39,830 --> 00:24:41,900
0,240 240,495 495,860 1450,1845 1845,2070
a loss function that's going

904
00:24:41,900 --> 00:24:44,560
0,345 345,740 910,1310 1840,2240 2260,2660
to essentially represent our expected

905
00:24:44,670 --> 00:24:45,785
0,320 320,515 515,710 710,890 890,1115
return if we're able to

906
00:24:45,785 --> 00:24:46,820
0,240 240,480 480,630 630,780 780,1035
take all of the best

907
00:24:46,820 --> 00:24:48,815
0,380 970,1320 1320,1575 1575,1770 1770,1995
actions, right. So, for example,

908
00:24:48,815 --> 00:24:50,080
0,210 210,435 435,675 675,915 915,1265
if we select an initial

909
00:24:50,190 --> 00:24:52,480
0,400 810,1210 1230,1745 1745,1985 1985,2290
reward plus selecting some action

910
00:24:52,740 --> 00:24:54,020
0,260 260,410 410,700 780,1085 1085,1280
in our action space that

911
00:24:54,020 --> 00:24:56,800
0,630 630,960 960,1310 1420,1820 2380,2780
maximizes our expected return, then

912
00:24:57,000 --> 00:24:58,480
0,260 260,425 425,680 680,1030 1080,1480
for the next future state

913
00:24:58,740 --> 00:24:59,945
0,320 320,515 515,695 695,905 905,1205
we need to apply that

914
00:24:59,945 --> 00:25:02,930
0,645 645,935 1165,1565 1975,2775 2775,2985
discounting factor and recursively apply

915
00:25:02,930 --> 00:25:04,235
0,165 165,405 405,770 820,1095 1095,1305
the same equation, and that

916
00:25:04,235 --> 00:25:05,950
0,335 505,810 810,1095 1095,1395 1395,1715
simply turns into our target.|
|

917
00:25:06,420 --> 00:25:07,490
0,300 300,525 525,675 675,810 810,1070
Right now we can ask
现在我们基本上可以问，我们的神经网络预测了什么，对吗？这就是我们的目标。我们从以前的课程中回忆起，如果我们有一个目标值，在这种情况下，我们的Q值是一个连续变量。我们还有一个预测变量，它将作为可能采取的每一项潜在行动的输出的一部分。

918
00:25:07,570 --> 00:25:10,410
0,400 600,1000 1050,1450 2250,2555 2555,2840
basically what does our neural

919
00:25:10,410 --> 00:25:12,120
0,260 370,770 1030,1335 1335,1500 1500,1710
network predict, right? So that's

920
00:25:12,120 --> 00:25:14,420
0,150 150,470 820,1220 1240,1640 1900,2300
our target. And we recall

921
00:25:14,440 --> 00:25:16,125
0,290 290,560 560,1180 1320,1580 1580,1685
from previous lectures if we

922
00:25:16,125 --> 00:25:17,085
0,105 105,240 240,465 465,765 765,960
have a target value in

923
00:25:17,085 --> 00:25:17,955
0,135 135,315 315,510 510,675 675,870
this case, our q value

924
00:25:17,955 --> 00:25:20,085
0,195 195,420 420,780 780,1175 1855,2130
is a continuous variable. We

925
00:25:20,085 --> 00:25:21,650
0,275 385,660 660,825 825,1290 1290,1565
have also a predicted variable

926
00:25:22,150 --> 00:25:23,280
0,290 290,500 500,725 725,905 905,1130
that is going to come

927
00:25:23,280 --> 00:25:24,330
0,285 285,480 480,615 615,840 840,1050
as part of the output

928
00:25:24,330 --> 00:25:25,320
0,135 135,375 375,675 675,870 870,990
of every single one of

929
00:25:25,320 --> 00:25:26,775
0,255 255,600 600,950 1000,1290 1290,1455
these potential actions that could

930
00:25:26,775 --> 00:25:27,900
0,165 165,455
be taken.|
|

931
00:25:28,900 --> 00:25:30,165
0,260 260,520 540,860 860,1130 1130,1265
We can define what's called
我们可以定义所谓的A Q损失，它本质上就是这两个连续变量之间非常简单的均方误差损失。我们在这个环境中购买我们的神经网络，在这个环境中观察动作，不仅观察动作，最重要的是在动作之后，我们将他们之间的距离最小化。

932
00:25:30,165 --> 00:25:31,395
0,165 165,330 330,605 745,1020 1020,1230
A Q loss, which is

933
00:25:31,395 --> 00:25:33,120
0,335 355,755 925,1215 1215,1440 1440,1725
essentially just a very simple

934
00:25:33,120 --> 00:25:34,590
0,270 270,510 510,750 750,1100 1180,1470
mean squared error loss between

935
00:25:34,590 --> 00:25:36,675
0,210 210,495 495,855 855,1490 1810,2085
these two continuous variables. We

936
00:25:36,675 --> 00:25:39,240
0,420 420,645 645,935 1495,1895 2185,2565
minimize their distance over two

937
00:25:39,240 --> 00:25:41,330
0,345 345,645 645,900 900,1220 1480,2090
over many, many different iterations

938
00:25:41,530 --> 00:25:43,190
0,400 630,950 950,1160 1160,1400 1400,1660
of buying our neural network

939
00:25:43,240 --> 00:25:45,020
0,290 290,530 530,880 1050,1415 1415,1780
in this environment, observing actions

940
00:25:45,550 --> 00:25:47,160
0,400 510,910 960,1235 1235,1430 1430,1610
and observing not only the

941
00:25:47,160 --> 00:25:48,920
0,260 340,615 615,870 870,1250 1360,1760
actions but most importantly after

942
00:25:48,970 --> 00:25:51,140
0,245 245,470 470,850
the action is.|
|

943
00:25:51,150 --> 00:25:53,360
0,400 420,800 800,1330 1710,1985 1985,2210
Committed or executed, we can
无论是承诺还是执行，我们都能看到实情，预期回报，对吧？因此，我们有基本事实标签来直接训练和监督这个模型，例如，作为随机选择的一部分执行的操作。

944
00:25:53,360 --> 00:25:55,660
0,350 400,800 1300,1590 1590,1880 1900,2300
see exactly the ground truth,

945
00:25:56,940 --> 00:25:58,205
0,400 480,800 800,1010 1010,1145 1145,1265
expected return, right? So we

946
00:25:58,205 --> 00:25:59,560
0,195 195,390 390,600 600,840 840,1355
have the ground truth labels

947
00:25:59,880 --> 00:26:01,430
0,320 320,590 590,940 960,1310 1310,1550
to train and supervise this

948
00:26:01,430 --> 00:26:03,190
0,320 670,1070 1090,1380 1380,1515 1515,1760
model directly from the actions

949
00:26:03,300 --> 00:26:05,150
0,275 275,550 810,1325 1325,1640 1640,1850
that were executed as part

950
00:26:05,150 --> 00:26:07,210
0,290 430,830 1090,1485 1485,1770 1770,2060
of random selection, for example.|
|

951
00:26:08,420 --> 00:26:09,680
0,400
Now.|
现在。|

952
00:26:09,680 --> 00:26:10,940
0,180 180,345 345,650 790,1080 1080,1260
Let me just stop right
让我到此为止，也许再总结一次整个过程，也许会有一些不同的术语，只是为了让每个人对这个相同的问题有不同的看法。所以我们试图训练的深度神经网络是这样的，对吗？它接受作为输入。一个州正在尝试输出n个不同的数字。这n个不同的数字对应于与n个不同动作相关联的Q值。每个动作一个Q值。

953
00:26:10,940 --> 00:26:12,185
0,180 180,375 375,660 660,1110 1110,1245
there and maybe summarize the

954
00:26:12,185 --> 00:26:13,540
0,135 135,425 445,750 750,1005 1005,1355
whole process one more time

955
00:26:13,680 --> 00:26:14,930
0,305 305,530 530,770 770,995 995,1250
and maybe a bit different

956
00:26:14,930 --> 00:26:16,505
0,735 735,885 885,1005 1005,1245 1245,1575
terminology just to give everyone

957
00:26:16,505 --> 00:26:17,590
0,180 180,270 270,360 360,605 685,1085
kind of a different perspective

958
00:26:18,030 --> 00:26:20,180
0,320 320,545 545,785 785,1120 1860,2150
on this same problem. So

959
00:26:20,180 --> 00:26:21,590
0,180 180,360 360,630 630,890 1150,1410
our deep neural network that

960
00:26:21,590 --> 00:26:22,880
0,210 210,420 420,630 630,920 1000,1290
we're trying to train looks

961
00:26:22,880 --> 00:26:24,110
0,195 195,500 550,870 870,1065 1065,1230
like this, right? It takes

962
00:26:24,110 --> 00:26:25,580
0,225 225,420 420,585 585,890 1180,1470
as input. A state is

963
00:26:25,580 --> 00:26:27,100
0,180 180,465 465,810 810,1140 1140,1520
trying to output n different

964
00:26:27,180 --> 00:26:29,140
0,400 840,1145 1145,1355 1355,1610 1610,1960
numbers. Those n different numbers

965
00:26:29,160 --> 00:26:30,430
0,400 420,665 665,815 815,995 995,1270
correspond to the q value

966
00:26:30,900 --> 00:26:32,590
0,400 570,830 830,1040 1040,1340 1340,1690
associated to n different actions.

967
00:26:32,880 --> 00:26:34,480
0,350 350,560 560,820 900,1250 1250,1600
One q value per action.|
|

968
00:26:36,200 --> 00:26:38,520
0,400 510,755 755,1000 1110,1510 1770,2320
Here the actions in atari
例如，Atari Breakout中的行动应该是三个行动。我们要么向左走，要么向右走，或者什么都不做。我们可以呆在原地不动。

969
00:26:38,780 --> 00:26:40,600
0,610 630,935 935,1240 1290,1580 1580,1820
breakout, for example, should be

970
00:26:40,600 --> 00:26:41,770
0,225 225,500 640,900 900,1005 1005,1170
three actions. We can either

971
00:26:41,770 --> 00:26:42,655
0,195 195,420 420,630 630,750 750,885
go left, we can go

972
00:26:42,655 --> 00:26:43,990
0,275 505,795 795,975 975,1170 1170,1335
right, or we can do

973
00:26:43,990 --> 00:26:44,965
0,225 225,450 450,615 615,810 810,975
nothing. We can stay where

974
00:26:44,965 --> 00:26:46,180
0,120 120,365
we are.|
|

975
00:26:48,720 --> 00:26:50,860
0,400 750,1150 1290,1565 1565,1790 1790,2140
Right. So the next step
正确的。下一步，我们看到，如果我们有这个色调值输出，我们可以用它做一个动作，或者我们甚至可以让我更正式地对待它。我们可以开发所谓的政策功能。政策函数是一个给定状态的函数，它决定什么是最好的行动。这和Q函数是不同的，对吗？Q函数告诉我们一个给定的状态，什么是最好的，或者是什么值，以及我们可以采取的每个行动的回报。政策函数告诉我们的还不止这一步。

976
00:26:50,910 --> 00:26:52,565
0,305 305,610 750,1070 1070,1385 1385,1655
from this we saw if

977
00:26:52,565 --> 00:26:54,970
0,210 210,545 865,1265 1855,2145 2145,2405
we have this hue value

978
00:26:55,200 --> 00:26:56,135
0,320 320,500 500,620 620,770 770,935
output, what we can do

979
00:26:56,135 --> 00:26:58,220
0,165 165,455 745,1145 1555,1830 1830,2085
with it is we can

980
00:26:58,220 --> 00:26:59,345
0,255 255,375 375,620 700,975 975,1125
make an action or we

981
00:26:59,345 --> 00:27:01,085
0,240 240,605 1075,1335 1335,1530 1530,1740
can even let me be

982
00:27:01,085 --> 00:27:02,045
0,165 165,420 420,645 645,795 795,960
more formal about it. We

983
00:27:02,045 --> 00:27:03,140
0,225 225,495 495,765 765,915 915,1095
can develop what's called a

984
00:27:03,140 --> 00:27:05,150
0,290 310,710 730,1125 1125,1520 1750,2010
policy function. Policy function is

985
00:27:05,150 --> 00:27:06,515
0,135 135,410 550,900 900,1155 1155,1365
a function that given a

986
00:27:06,515 --> 00:27:08,540
0,305 655,1020 1020,1545 1545,1815 1815,2025
state, it determines what is

987
00:27:08,540 --> 00:27:09,875
0,225 225,465 465,800 880,1125 1125,1335
the best action. So that's

988
00:27:09,875 --> 00:27:11,020
0,240 240,480 480,660 660,855 855,1145
different than the q function,

989
00:27:11,070 --> 00:27:12,200
0,290 290,455 455,605 605,860 860,1130
right? The q function tells

990
00:27:12,200 --> 00:27:13,700
0,290 460,750 750,960 960,1275 1275,1500
us given a state, what

991
00:27:13,700 --> 00:27:14,720
0,120 120,270 270,530 610,885 885,1020
is the best or what

992
00:27:14,720 --> 00:27:16,820
0,180 180,500 670,1070 1360,1740 1740,2100
is the value, the return

993
00:27:16,820 --> 00:27:17,840
0,255 255,450 450,705 705,900 900,1020
of every action that we

994
00:27:17,840 --> 00:27:19,570
0,135 135,410 700,1005 1005,1310 1330,1730
could take. The policy function

995
00:27:19,620 --> 00:27:20,795
0,290 290,485 485,725 725,965 965,1175
tells us one step more

996
00:27:20,795 --> 00:27:21,580
0,180 180,455
than that.|
|

997
00:27:21,580 --> 00:27:23,725
0,320 580,930 930,1215 1215,1550 1870,2145
Given, given a state, what
给定，给定一个状态，最好的行动是什么，对吗？所以这是一种非常端到端的思考方式，你知道，根据我现在看到的代理的决策过程，我应该采取什么行动？我们可以直接从Q函数本身确定策略函数，只需最大化和优化所有不同的Q值，就像我们在这里看到的所有不同的操作。例如，我们在这里可以看到，在给定这种状态的情况下，这三个不同值的结果的Q函数的AQ值为20。如果它向左移动，A Q值为3，如果它停留在同一位置，它的A Q值为0，如果它向右移动，它基本上会在迭代后消亡，因为你可以看到球正在向它的左侧移动。如果它向右移动，游戏就结束了，对吧。所以它需要向左移动才能继续比赛。Q值反映了这一点。这里的最优作用就是这三个Q值中的最大值。在这种情况下，它将是。

998
00:27:23,725 --> 00:27:25,195
0,195 195,405 405,630 630,965 1165,1470
is the best action, right?

999
00:27:25,195 --> 00:27:26,095
0,150 150,300 300,390 390,615 615,900
So it's a very end

1000
00:27:26,095 --> 00:27:27,510
0,180 180,425 655,945 945,1125 1125,1415
to end way of thinking

1001
00:27:27,560 --> 00:27:29,320
0,400 690,920 920,1070 1070,1250 1250,1760
about, you know, the agent's

1002
00:27:29,320 --> 00:27:30,925
0,255 255,570 570,950 1120,1425 1425,1605
decision making process based on

1003
00:27:30,925 --> 00:27:31,860
0,135 135,300 300,465 465,630 630,935
what I see right now,

1004
00:27:32,210 --> 00:27:33,220
0,260 260,410 410,545 545,785 785,1010
what is the action that

1005
00:27:33,220 --> 00:27:34,450
0,120 120,300 300,590 880,1125 1125,1230
I should take? And we

1006
00:27:34,450 --> 00:27:36,090
0,225 225,525 525,840 840,1220 1240,1640
can determine that policy function

1007
00:27:36,680 --> 00:27:38,370
0,400 720,1040 1040,1235 1235,1400 1400,1690
directly from the q function

1008
00:27:38,420 --> 00:27:40,975
0,350 350,650 650,1000 1170,1870 2190,2555
itself simply by maximizing and

1009
00:27:40,975 --> 00:27:42,355
0,485 655,930 930,1065 1065,1170 1170,1380
optimizing all of the different

1010
00:27:42,355 --> 00:27:43,480
0,225 225,485 655,885 885,990 990,1125
q values for all of

1011
00:27:43,480 --> 00:27:44,920
0,120 120,380 550,950 1000,1275 1275,1440
the different actions that we

1012
00:27:44,920 --> 00:27:46,690
0,165 165,440 1030,1305 1305,1485 1485,1770
see here. So for example,

1013
00:27:46,690 --> 00:27:47,965
0,380 430,705 705,870 870,1020 1020,1275
here we can see that

1014
00:27:47,965 --> 00:27:50,005
0,330 330,600 600,935 1525,1830 1830,2040
given this state, the q

1015
00:27:50,005 --> 00:27:51,775
0,305 805,1110 1110,1350 1350,1590 1590,1770
function as the result of

1016
00:27:51,775 --> 00:27:53,515
0,210 210,420 420,660 660,1025 1465,1740
these three different values has

1017
00:27:53,515 --> 00:27:54,750
0,180 180,375 375,630 630,915 915,1235
A Q value of twenty.

1018
00:27:54,890 --> 00:27:55,675
0,245 245,350 350,530 530,695 695,785
If it goes to the

1019
00:27:55,675 --> 00:27:56,830
0,245 415,690 690,825 825,960 960,1155
left, has A Q value

1020
00:27:56,830 --> 00:27:58,045
0,225 225,530 580,825 825,975 975,1215
of three, if it stays

1021
00:27:58,045 --> 00:27:58,855
0,180 180,285 285,450 450,660 660,810
in the same place and

1022
00:27:58,855 --> 00:27:59,560
0,105 105,240 240,375 375,510 510,705
it has A Q value

1023
00:27:59,560 --> 00:28:01,240
0,240 240,560 820,1170 1170,1380 1380,1680
of zero, it's going to

1024
00:28:01,240 --> 00:28:02,880
0,380 400,720 720,975 975,1185 1185,1640
basically die after this iteration

1025
00:28:03,050 --> 00:28:03,880
0,260 260,380 380,560 560,725 725,830
if it moves to the

1026
00:28:03,880 --> 00:28:04,675
0,225 225,450 450,555 555,675 675,795
right because you can see

1027
00:28:04,675 --> 00:28:05,395
0,105 105,225 225,360 360,510 510,720
that the ball is coming

1028
00:28:05,395 --> 00:28:06,070
0,165 165,240 240,390 390,540 540,675
to the left of it.

1029
00:28:06,070 --> 00:28:06,730
0,135 135,240 240,405 405,555 555,660
If it moves to the

1030
00:28:06,730 --> 00:28:07,980
0,260 460,720 720,855 855,990 990,1250
right, the game is over,

1031
00:28:08,090 --> 00:28:09,055
0,305 305,455 455,575 575,785 785,965
right. So it needs to

1032
00:28:09,055 --> 00:28:10,300
0,150 150,315 315,435 435,695 985,1245
move to the left in

1033
00:28:10,300 --> 00:28:11,530
0,210 210,465 465,645 645,920 970,1230
order to do that in

1034
00:28:11,530 --> 00:28:12,630
0,165 165,405 405,660 660,840 840,1100
order to continue the game.

1035
00:28:12,830 --> 00:28:14,110
0,305 305,485 485,620 620,880 960,1280
And the q value reflects

1036
00:28:14,110 --> 00:28:16,600
0,320 1150,1545 1545,1890 1890,2150 2170,2490
that. The optimal action here

1037
00:28:16,600 --> 00:28:17,485
0,210 210,435 435,675 675,810 810,885
is simply going to be

1038
00:28:17,485 --> 00:28:19,260
0,120 120,395 745,1080 1080,1395 1395,1775
the maximum of these three

1039
00:28:19,310 --> 00:28:20,790
0,290 290,580 720,980 980,1160 1160,1480
q values. In this case

1040
00:28:20,990 --> 00:28:21,860
0,380 380,530 530,665 665,830
it's going to be.|
|

1041
00:28:21,860 --> 00:28:23,255
0,260 520,825 825,1020 1020,1155 1155,1395
Twenty, and then the action
20，然后动作将是来自这20个动作的相应动作，也就是向左移动。

1042
00:28:23,255 --> 00:28:24,035
0,270 270,435 435,555 555,645 645,780
is going to be the

1043
00:28:24,035 --> 00:28:25,850
0,690 690,965 1165,1455 1455,1650 1650,1815
corresponding action that comes from

1044
00:28:25,850 --> 00:28:27,005
0,165 165,405 405,615 615,840 840,1155
that twenty, which is moving

1045
00:28:27,005 --> 00:28:28,540
0,365
left.|
|

1046
00:28:29,340 --> 00:28:30,900
0,400
Now.|
现在。|

1047
00:28:31,120 --> 00:28:33,230
0,275 275,550 960,1360 1500,1805 1805,2110
We can send this action
我们可以将这个动作以游戏的形式发送回环境，以执行下一步，对吧。当代理在这个环境中移动时，它不仅会得到来自游戏的新像素的响应，更重要的是，会得到一些奖励信号。现在非常重要的是要记住，在乒乓球中的奖励信号，或在雅达利，对不起，突破。

1048
00:28:33,730 --> 00:28:35,985
0,400 690,1090 1110,1445 1445,1780 2010,2255
back to the environment in

1049
00:28:35,985 --> 00:28:36,800
0,120 120,270 270,405 405,540 540,815
the form of the game

1050
00:28:37,060 --> 00:28:38,600
0,400 480,770 770,935 935,1175 1175,1540
to execute the next step,

1051
00:28:38,860 --> 00:28:40,070
0,290 290,470 470,740 740,965 965,1210
right. And as the agent

1052
00:28:40,150 --> 00:28:42,240
0,400 450,725 725,965 965,1330 1740,2090
moves through this environment, it's

1053
00:28:42,240 --> 00:28:43,490
0,150 150,285 285,510 510,870 870,1250
going to be responded with

1054
00:28:43,510 --> 00:28:45,120
0,275 275,545 545,860 860,1085 1085,1610
not only by new pixels

1055
00:28:45,120 --> 00:28:46,130
0,270 270,450 450,630 630,765 765,1010
that come from the game,

1056
00:28:46,450 --> 00:28:48,315
0,275 275,500 500,850 1170,1550 1550,1865
but more importantly, some reward

1057
00:28:48,315 --> 00:28:49,530
0,335 385,630 630,810 810,975 975,1215
signal. Now it's very important

1058
00:28:49,530 --> 00:28:50,745
0,225 225,480 480,720 720,945 945,1215
to remember that the reward

1059
00:28:50,745 --> 00:28:52,580
0,405 405,645 645,995 1285,1560 1560,1835
signals in pong, or sorry

1060
00:28:52,600 --> 00:28:54,440
0,365 365,830 830,1390
in atari, breakout.|
|

1061
00:28:54,440 --> 00:28:56,300
0,260 280,645 645,1215 1215,1590 1590,1860
Are very sparse, right? You
是非常稀疏的，对吗？你会得到奖励，不一定是基于你在这一时刻所采取的行动。球通常需要几个时间步骤才能回到屏幕顶部。因此，通常情况下，你的奖励会非常延迟，可能至少会延迟几个时间步长，如果你在屏幕的角落里弹跳，有时甚至会延迟更多。

1062
00:28:56,300 --> 00:28:58,265
0,195 195,500 610,1010 1090,1490 1600,1965
get a reward, not necessarily

1063
00:28:58,265 --> 00:29:00,485
0,315 315,665 1585,1845 1845,2025 2025,2220
based on the action that

1064
00:29:00,485 --> 00:29:01,370
0,135 135,270 270,405 405,600 600,885
you take at this exact

1065
00:29:01,370 --> 00:29:02,960
0,350 490,780 780,1070 1090,1395 1395,1590
moment. It usually takes a

1066
00:29:02,960 --> 00:29:04,145
0,225 225,525 525,840 840,1035 1035,1185
few time steps for that

1067
00:29:04,145 --> 00:29:05,345
0,240 240,450 450,675 675,960 960,1200
ball to travel back up

1068
00:29:05,345 --> 00:29:06,140
0,150 150,285 285,465 465,615 615,795
to the top of the

1069
00:29:06,140 --> 00:29:08,045
0,320 550,855 855,1140 1140,1485 1485,1905
screen. So usually your rewards

1070
00:29:08,045 --> 00:29:09,770
0,210 210,405 405,720 720,1115 1405,1725
will be quite delayed, maybe

1071
00:29:09,770 --> 00:29:11,255
0,210 210,450 450,795 795,1170 1170,1485
at least by several time

1072
00:29:11,255 --> 00:29:12,710
0,285 285,635 775,1095 1095,1305 1305,1455
steps, sometimes even more if

1073
00:29:12,710 --> 00:29:13,655
0,150 150,480 480,630 630,795 795,945
you're bouncing off of the

1074
00:29:13,655 --> 00:29:16,060
0,425 655,930 930,1110 1110,1415
corners of the screen.|
|

1075
00:29:16,430 --> 00:29:20,035
0,400 1110,1460 1460,1810 2520,2920 3210,3605
Now, one very popular or
几年前，DeepMind Google DeepMind提出了一种非常流行或非常著名的方法来展示这一点，他们展示了你可以训练A Q值网络，你可以看到左侧的输入只是来自屏幕的原始像素，一直到右侧控制器的动作。你可以训练这个网络来完成整个雅达利突破游戏生态系统中的各种不同的任务。

1076
00:29:20,035 --> 00:29:21,910
0,395 415,815 985,1365 1365,1650 1650,1875
very famous approach that showed

1077
00:29:21,910 --> 00:29:24,420
0,240 240,560 730,1130 1210,1610 1870,2510
this was presented by deepmind

1078
00:29:24,470 --> 00:29:26,335
0,365 365,905 905,1220 1220,1550 1550,1865
Google deepmind several years ago,

1079
00:29:26,335 --> 00:29:27,325
0,210 210,405 405,630 630,825 825,990
where they showed that you

1080
00:29:27,325 --> 00:29:28,675
0,195 195,515 565,885 885,1080 1080,1350
could train A Q value

1081
00:29:28,675 --> 00:29:29,770
0,395 415,675 675,780 780,930 930,1095
network and you can see

1082
00:29:29,770 --> 00:29:30,595
0,210 210,420 420,540 540,660 660,825
the input on the left

1083
00:29:30,595 --> 00:29:32,125
0,225 225,545 745,1035 1035,1275 1275,1530
hand side is simply the

1084
00:29:32,125 --> 00:29:33,505
0,240 240,750 750,1020 1020,1215 1215,1380
raw pixels coming from the

1085
00:29:33,505 --> 00:29:35,170
0,305 685,990 990,1155 1155,1410 1410,1665
screen all the way to

1086
00:29:35,170 --> 00:29:37,140
0,135 135,410 970,1230 1230,1455 1455,1970
the actions of a controller

1087
00:29:37,190 --> 00:29:38,370
0,275 275,410 410,590 590,845 845,1180
on the right hand side.

1088
00:29:38,960 --> 00:29:40,315
0,260 260,395 395,590 590,910 1020,1355
And you could train this

1089
00:29:40,315 --> 00:29:41,850
0,255 255,575 865,1110 1110,1245 1245,1535
one network for a variety

1090
00:29:42,170 --> 00:29:44,065
0,290 290,575 575,970 1170,1550 1550,1895
of different tasks all across

1091
00:29:44,065 --> 00:29:46,915
0,195 195,605 685,1295 2125,2525 2545,2850
the atari breakout ecosystem of

1092
00:29:46,915 --> 00:29:47,720
0,305
games.|
|

1093
00:29:47,910 --> 00:29:49,115
0,365 365,590 590,770 770,980 980,1205
And for each of these
对于这些任务中的每一个，他们展示的真正令人着迷的是这个非常简单的算法，它真的依赖于随机选择动作，然后，你知道，从做得不是很好的动作中学习，你阻止他们，尝试做那些执行得更好的动作，非常简单的算法，但他们发现的与这种类型的算法不相上下。

1094
00:29:49,115 --> 00:29:51,125
0,335 625,885 885,1145 1405,1770 1770,2010
tasks, the really fascinating thing

1095
00:29:51,125 --> 00:29:52,190
0,150 150,330 330,555 555,840 840,1065
that they showed was for

1096
00:29:52,190 --> 00:29:53,885
0,165 165,450 450,830 970,1440 1440,1695
this very simple algorithm which

1097
00:29:53,885 --> 00:29:55,955
0,225 225,645 645,995 1105,1505 1705,2070
really relies on random choice

1098
00:29:55,955 --> 00:29:57,395
0,345 345,660 660,855 855,1115 1195,1440
of selection of actions and

1099
00:29:57,395 --> 00:29:59,180
0,245 355,600 600,845 895,1295 1405,1785
then, you know, learning from,

1100
00:29:59,180 --> 00:30:00,260
0,210 210,300 300,540 540,825 825,1080
you know, actions that don't

1101
00:30:00,260 --> 00:30:01,175
0,180 180,405 405,600 600,765 765,915
do very well, that you

1102
00:30:01,175 --> 00:30:03,040
0,450 450,780 780,1175 1255,1560 1560,1865
discourage them and trying to

1103
00:30:03,090 --> 00:30:04,715
0,275 275,550 600,920 920,1240 1320,1625
do actions that did perform

1104
00:30:04,715 --> 00:30:06,760
0,255 255,525 525,845 1375,1710 1710,2045
well more frequently, very simple

1105
00:30:06,810 --> 00:30:07,685
0,350 350,455 455,545 545,680 680,875
algorithm, but what they found

1106
00:30:07,685 --> 00:30:08,915
0,210 210,480 480,735 735,975 975,1230
was even with that type

1107
00:30:08,915 --> 00:30:10,160
0,300 300,815
of algorithm.|
|

1108
00:30:10,160 --> 00:30:11,405
0,195 195,330 330,525 525,750 750,1245
They were able to surpass
他们在比赛的一半以上时间里都超过了人类的水平。你可以在这里看到一些游戏的表现仍然低于人类的水平，但正如我们将看到的，这真的是一个如此令人兴奋的进步，因为算法很简单，你是如何清理训练的公式的，你只需要将非常少量的先验知识强加给这个神经网络，它就能够学习如何玩这些游戏。你从来没有教过任何游戏规则。

1109
00:30:11,405 --> 00:30:13,475
0,345 345,695 805,1205 1585,1860 1860,2070
human level performance on over

1110
00:30:13,475 --> 00:30:14,600
0,225 225,390 390,525 525,785 865,1125
half of the game. There

1111
00:30:14,600 --> 00:30:15,920
0,150 150,360 360,680 850,1155 1155,1320
were some games that you

1112
00:30:15,920 --> 00:30:17,015
0,150 150,330 330,570 570,855 855,1095
can see here were still

1113
00:30:17,015 --> 00:30:18,845
0,270 270,555 555,875 895,1295 1585,1830
below human level performance, but

1114
00:30:18,845 --> 00:30:19,670
0,120 120,345 345,495 495,660 660,825
as we'll see, this was

1115
00:30:19,670 --> 00:30:21,940
0,290 550,950 1390,1680 1680,1920 1920,2270
really like such an exciting

1116
00:30:21,990 --> 00:30:23,705
0,400 480,770 770,935 935,1085 1085,1715
advance because of the simplicity

1117
00:30:23,705 --> 00:30:25,180
0,135 135,390 390,875 895,1185 1185,1475
of the algorithm and how

1118
00:30:25,410 --> 00:30:27,170
0,395 395,740 740,965 965,1490 1490,1760
you clean the formulation of

1119
00:30:27,170 --> 00:30:28,745
0,180 180,450 450,830 1090,1350 1350,1575
the training was you only

1120
00:30:28,745 --> 00:30:30,110
0,300 300,525 525,765 765,1065 1065,1365
needed a very little amount

1121
00:30:30,110 --> 00:30:32,390
0,345 345,705 705,1070 1570,1830 1830,2280
of prior knowledge to impose

1122
00:30:32,390 --> 00:30:33,890
0,345 345,615 615,900 900,1160 1270,1500
onto this neural network for

1123
00:30:33,890 --> 00:30:34,730
0,105 105,240 240,345 345,555 555,840
it to be able to

1124
00:30:34,730 --> 00:30:35,540
0,225 225,375 375,480 480,630 630,810
learn how to play these

1125
00:30:35,540 --> 00:30:36,350
0,195 195,345 345,480 480,630 630,810
games. You never had to

1126
00:30:36,350 --> 00:30:37,310
0,240 240,450 450,600 600,735 735,960
teach any of the rules

1127
00:30:37,310 --> 00:30:38,340
0,225 225,375 375,650
of the game.|
|

1128
00:30:38,340 --> 00:30:39,375
0,270 270,525 525,765 765,900 900,1035
You only had to let
你只需要让它探索它的环境，与它自己玩这个游戏很多很多次，然后直接从这些数据中学习。

1129
00:30:39,375 --> 00:30:40,785
0,275 325,600 600,810 810,1140 1140,1410
it explore its environment, play

1130
00:30:40,785 --> 00:30:41,820
0,120 120,255 255,465 465,720 720,1035
the game many, many times

1131
00:30:41,820 --> 00:30:44,115
0,375 375,770 910,1230 1230,1550 1930,2295
against itself, and learn directly

1132
00:30:44,115 --> 00:30:45,800
0,240 240,405 405,695
from that data.|
|

1133
00:30:47,270 --> 00:30:49,225
0,400 690,935 935,1070 1070,1360 1590,1955
Now there are several very
QUE学习有几个非常重要的缺点，希望这些能激励我们今天课程的第二部分，也就是我们要讨论的。但我想在这里告诉大家的第一点是，你们的学习自然适用于离散的动作空间，对吗？因为你可以把我们提供的这个输出空间想象成每一个可以执行的操作有一个数字。现在，如果我们有一个连续的行动空间，我们必须想出聪明的方法来解决这个问题。事实上，现在有了更多的解决方案来实现QUE学习和连续的动作空间。但在很大程度上，队列学习非常适合离散动作空间。稍后我们会讨论用其他方法克服这个问题的方法。

1134
00:30:49,225 --> 00:30:50,695
0,285 285,870 870,1110 1110,1305 1305,1470
important downsides of que learning

1135
00:30:50,695 --> 00:30:51,610
0,165 165,390 390,630 630,765 765,915
and hopefully these are going

1136
00:30:51,610 --> 00:30:53,230
0,165 165,710 880,1155 1155,1395 1395,1620
to motivate the second part

1137
00:30:53,230 --> 00:30:54,820
0,150 150,585 585,890 1030,1335 1335,1590
of today's lecture, which we'll

1138
00:30:54,820 --> 00:30:56,110
0,180 180,500 670,930 930,1080 1080,1290
talk about. But the first

1139
00:30:56,110 --> 00:30:57,340
0,320 340,675 675,885 885,1065 1065,1230
one that I want to

1140
00:30:57,340 --> 00:30:58,855
0,255 255,800 820,1170 1170,1380 1380,1515
really convey to everyone here

1141
00:30:58,855 --> 00:31:00,570
0,180 180,485 925,1215 1215,1410 1410,1715
is that you learning is

1142
00:31:00,740 --> 00:31:04,290
0,400 1800,2470 2550,2855 2855,3260 3260,3550
naturally applicable to discrete action

1143
00:31:04,340 --> 00:31:05,635
0,400 510,830 830,1025 1025,1145 1145,1295
spaces, right? Because you can

1144
00:31:05,635 --> 00:31:07,915
0,180 180,455 835,1235 1615,1980 1980,2280
think of this output space

1145
00:31:07,915 --> 00:31:08,950
0,195 195,420 420,645 645,870 870,1035
that we're providing as kind

1146
00:31:08,950 --> 00:31:10,480
0,105 105,270 270,510 510,830 1240,1530
of like one number per

1147
00:31:10,480 --> 00:31:11,910
0,290 490,780 780,945 945,1125 1125,1430
action that could be taken.

1148
00:31:12,050 --> 00:31:12,850
0,275 275,410 410,515 515,605 605,800
Now if we have a

1149
00:31:12,850 --> 00:31:14,635
0,300 300,650 760,1160 1420,1665 1665,1785
continuous action space, we have

1150
00:31:14,635 --> 00:31:15,745
0,165 165,375 375,600 600,840 840,1110
to think about clever ways

1151
00:31:15,745 --> 00:31:16,825
0,195 195,420 420,705 705,915 915,1080
to work around that. In

1152
00:31:16,825 --> 00:31:18,490
0,165 165,330 330,555 555,905 1375,1665
fact, there are now more

1153
00:31:18,490 --> 00:31:19,920
0,290 580,825 825,945 945,1125 1125,1430
recently there are some solutions

1154
00:31:20,240 --> 00:31:21,655
0,335 335,620 620,875 875,1100 1100,1415
to achieve que learning and

1155
00:31:21,655 --> 00:31:23,680
0,315 315,665 685,1085 1195,1595 1765,2025
continuous action spaces. But for

1156
00:31:23,680 --> 00:31:25,080
0,120 120,330 330,680 820,1140 1140,1400
the most part, queue learning

1157
00:31:25,190 --> 00:31:26,830
0,400 510,845 845,1115 1115,1460 1460,1640
is very well suited for

1158
00:31:26,830 --> 00:31:28,870
0,375 375,650 730,1130 1570,1830 1830,2040
discrete action spaces. And we'll

1159
00:31:28,870 --> 00:31:30,040
0,180 180,375 375,540 540,690 690,1170
talk about ways of overcoming

1160
00:31:30,040 --> 00:31:31,525
0,240 240,390 390,630 630,1010 1210,1485
that with other approaches a

1161
00:31:31,525 --> 00:31:32,620
0,195 195,515
bit later.|
|

1162
00:31:32,660 --> 00:31:34,860
0,400 630,935 935,1240 1410,1805 1805,2200
And the second component here
这里的第二个组成部分是我们正在学习的策略，对，Q函数产生了该策略，这是我们实际上用来确定要采取什么行动的东西。假设策略是通过确定性地优化Q函数来确定的，我们只需查看Q函数的结果，然后应用我们的，或者我们，我们查看Q函数的结果，然后选择Q值最好或最高的操作。

1163
00:31:35,090 --> 00:31:38,110
0,305 305,610 1740,2045 2045,2350 2730,3020
is that the policy that

1164
00:31:38,110 --> 00:31:39,460
0,225 225,470 610,945 945,1170 1170,1350
we're learning, right, the q

1165
00:31:39,460 --> 00:31:41,545
0,290 760,1155 1155,1515 1515,1860 1860,2085
function is giving rise to

1166
00:31:41,545 --> 00:31:42,610
0,165 165,485 535,795 795,930 930,1065
that policy, which is the

1167
00:31:42,610 --> 00:31:43,590
0,150 150,300 300,555 555,720 720,980
thing that we're actually using

1168
00:31:43,610 --> 00:31:45,025
0,335 335,605 605,815 815,1090 1140,1415
to determine what action to

1169
00:31:45,025 --> 00:31:47,230
0,275 385,765 765,1140 1140,1535 1855,2205
take. Given any state that

1170
00:31:47,230 --> 00:31:50,725
0,350 910,1310 1600,2000 2170,2570 3250,3495
policy is determined by, you

1171
00:31:50,725 --> 00:31:54,250
0,245 445,1535 2335,2855 2995,3315 3315,3525
know, deterministically optimizing that q

1172
00:31:54,250 --> 00:31:55,630
0,290 340,630 630,915 915,1200 1200,1380
function, we simply look at

1173
00:31:55,630 --> 00:31:56,710
0,255 255,570 570,780 780,915 915,1080
the results from the q

1174
00:31:56,710 --> 00:31:59,320
0,290 880,1275 1275,1670 1720,2120 2320,2610
function and apply our, or

1175
00:31:59,320 --> 00:32:00,115
0,210 210,390 390,510 510,630 630,795
we, we look at the

1176
00:32:00,115 --> 00:32:00,985
0,210 210,360 360,465 465,600 600,870
results of the q function

1177
00:32:00,985 --> 00:32:02,220
0,270 270,480 480,765 765,975 975,1235
and we pick the action

1178
00:32:02,450 --> 00:32:03,640
0,275 275,455 455,665 665,935 935,1190
that has the best or

1179
00:32:03,640 --> 00:32:05,240
0,165 165,440 520,810 810,1100
the highest q value.|
|

1180
00:32:05,340 --> 00:32:07,610
0,305 305,610 780,1180 1350,1750 1980,2270
That is very dangerous in
这在许多情况下是非常危险的，因为它总是为给定的州挑选最佳的值。在这条管道中没有随机性，所以你经常会遇到这样的情况：你一直在重复相同的动作，而你没有学会探索你可能正在考虑的潜在的不同选择。因此，为了应对这些非常重要的挑战，这有望推动今天课程的下一部分，重点是策略学习，这是一种不同于队列学习算法的强化学习算法。

1181
00:32:07,610 --> 00:32:09,020
0,225 225,560 820,1110 1110,1275 1275,1410
many cases because of the

1182
00:32:09,020 --> 00:32:10,055
0,180 180,360 360,630 630,870 870,1035
fact that it's always going

1183
00:32:10,055 --> 00:32:11,350
0,150 150,375 375,630 630,915 915,1295
to pick the best value

1184
00:32:11,820 --> 00:32:12,905
0,245 245,365 365,605 605,845 845,1085
for a given state. There's

1185
00:32:12,905 --> 00:32:14,890
0,165 165,1175 1225,1500 1500,1680 1680,1985
no stochasticity in that pipeline,

1186
00:32:15,000 --> 00:32:16,120
0,245 245,365 365,530 530,770 770,1120
so you can very frequently

1187
00:32:16,170 --> 00:32:18,770
0,335 335,670 960,1360 1560,1960 2340,2600
get caught in situations where

1188
00:32:18,770 --> 00:32:19,820
0,165 165,420 420,750 750,885 885,1050
you keep repeating the same

1189
00:32:19,820 --> 00:32:21,020
0,290 310,555 555,690 690,975 975,1200
actions and you don't learn

1190
00:32:21,020 --> 00:32:23,410
0,330 330,705 705,1100 1450,1850 1990,2390
to explore potentially different options

1191
00:32:23,580 --> 00:32:24,755
0,290 290,455 455,710 710,965 965,1175
that you may be thinking

1192
00:32:24,755 --> 00:32:26,830
0,335 625,1025 1195,1500 1500,1740 1740,2075
of. So to address these

1193
00:32:27,060 --> 00:32:29,465
0,380 380,755 755,1150 1830,2210 2210,2405
very important challenges, that's hopefully

1194
00:32:29,465 --> 00:32:30,740
0,225 225,390 390,840 840,1065 1065,1275
going to motivate now the

1195
00:32:30,740 --> 00:32:32,230
0,255 255,540 540,795 795,1215 1215,1490
next part of today's lecture,

1196
00:32:32,610 --> 00:32:33,365
0,260 260,410 410,560 560,650 650,755
which is going to be

1197
00:32:33,365 --> 00:32:35,345
0,275 325,705 705,1085 1105,1505 1675,1980
focused on policy learning, which

1198
00:32:35,345 --> 00:32:37,115
0,300 300,645 645,990 990,1385 1465,1770
is a different class of

1199
00:32:37,115 --> 00:32:38,795
0,600 600,875 895,1335 1335,1515 1515,1680
reinforcement learning algorithms that are

1200
00:32:38,795 --> 00:32:41,140
0,305 325,725 955,1260 1260,1535 1855,2345
different than queue learning algorithms.|
|

1201
00:32:41,600 --> 00:32:42,815
0,320 490,765 765,915 915,1065 1065,1215
And like I said, those
就像我说的，这些算法被称为政策梯度算法和政策梯度算法。主要的不同之处在于，我们不会试图从Q函数中推断出政策，我们只需要构建一个神经网络，直接从数据中学习政策函数。所以它有点跳过了一步，我们将看看如何训练这些网络。

1202
00:32:42,815 --> 00:32:44,330
0,150 150,345 345,660 660,1155 1155,1515
are called policy gradient algorithms

1203
00:32:44,330 --> 00:32:45,875
0,195 195,480 480,990 990,1365 1365,1545
and policy gradient algorithms. The

1204
00:32:45,875 --> 00:32:47,645
0,180 180,465 465,720 720,995 1495,1770
main difference is that instead

1205
00:32:47,645 --> 00:32:49,925
0,275 655,1020 1020,1385 1435,1835 1975,2280
of trying to infer the

1206
00:32:49,925 --> 00:32:51,250
0,305 475,750 750,900 900,1050 1050,1325
policy from the q function,

1207
00:32:51,480 --> 00:32:52,355
0,320 320,425 425,590 590,725 725,875
we're just going to build

1208
00:32:52,355 --> 00:32:53,435
0,165 165,375 375,615 615,855 855,1080
a neural network that will

1209
00:32:53,435 --> 00:32:55,570
0,360 360,755 1045,1395 1395,1740 1740,2135
directly learn that policy function

1210
00:32:56,040 --> 00:32:57,335
0,290 290,440 440,700 930,1175 1175,1295
from the data. So it

1211
00:32:57,335 --> 00:32:58,720
0,135 135,360 360,690 690,1005 1005,1385
kind of skips one step

1212
00:32:58,920 --> 00:32:59,810
0,260 260,455 455,605 605,755 755,890
and we'll see how we

1213
00:32:59,810 --> 00:33:02,100
0,165 165,375 375,615 615,950
can train those networks.|
|

1214
00:33:03,170 --> 00:33:04,330
0,365 365,650 650,830 830,980 980,1160
So before we get there,
所以在我们到达那里之前，让我再回顾一次。我们看到的Q函数图，正确的Q函数，我们试图构建一个神经网络输出这些Q值，每个操作一个值，我们通过查看Q值的这种状态来确定策略，选择具有最高值的值。

1215
00:33:04,330 --> 00:33:05,515
0,135 135,270 270,450 450,975 975,1185
let me just revisit one

1216
00:33:05,515 --> 00:33:06,840
0,210 210,495 495,780 780,1005 1005,1325
more time. The q function

1217
00:33:07,310 --> 00:33:08,575
0,680 680,920 920,1010 1010,1100 1100,1265
illustration that we are looking

1218
00:33:08,575 --> 00:33:10,375
0,210 210,480 480,735 735,1025 1555,1800
at, right q function, we

1219
00:33:10,375 --> 00:33:11,530
0,245 445,735 735,885 885,1020 1020,1155
are trying to build a

1220
00:33:11,530 --> 00:33:13,045
0,210 210,470 730,1095 1095,1305 1305,1515
neural network outputs these q

1221
00:33:13,045 --> 00:33:14,730
0,275 565,885 885,1170 1170,1410 1410,1685
values, one value per action,

1222
00:33:15,290 --> 00:33:16,650
0,245 245,440 440,770 770,1055 1055,1360
and we determine the policy

1223
00:33:17,360 --> 00:33:18,880
0,335 335,605 605,890 890,1205 1205,1520
by looking over this state

1224
00:33:18,880 --> 00:33:20,680
0,255 255,435 435,710 1090,1490 1510,1800
of q values, picking the

1225
00:33:20,680 --> 00:33:21,720
0,255 255,480 480,630 630,780 780,1040
value that has the highest.|
|

1226
00:33:22,500 --> 00:33:24,075
0,330 330,570 570,795 795,1050 1050,1575
And looking at its corresponding
看看它现在通过政策网络采取的相应行动，我们想要保留的想法是，与其预测关键价值本身，不如直接尝试优化这一政策功能。这里我们叫S的政策函数pi，对吗？所以皮是政策，S是我们的国家。所以它是一个只接受状态作为输入的函数，它将直接输出动作。所以这里的输出给了我们在任何给定的状态下我们应该采取的行动，这不仅代表了我们应该采取的最佳行动，而且让我们把它表示为基本上选择该行动将产生非常理想的结果的概率。

1227
00:33:24,075 --> 00:33:26,570
0,245 1045,1425 1425,1740 1740,2075 2095,2495
action now with policy networks,

1228
00:33:26,920 --> 00:33:28,305
0,400 420,815 815,1070 1070,1205 1205,1385
the idea that we want

1229
00:33:28,305 --> 00:33:29,265
0,165 165,330 330,510 510,675 675,960
to keep here is that

1230
00:33:29,265 --> 00:33:30,675
0,240 240,375 375,930 930,1200 1200,1410
instead of predicting the key

1231
00:33:30,675 --> 00:33:33,420
0,275 685,1085 1675,2130 2130,2460 2460,2745
values themselves, let's directly try

1232
00:33:33,420 --> 00:33:35,520
0,255 255,710 1060,1395 1395,1725 1725,2100
to optimize this policy function.

1233
00:33:35,520 --> 00:33:36,570
0,285 285,495 495,645 645,810 810,1050
Here we're calling the policy

1234
00:33:36,570 --> 00:33:38,370
0,380 700,1020 1020,1200 1200,1460 1510,1800
function pi of s, right?

1235
00:33:38,370 --> 00:33:39,350
0,195 195,390 390,555 555,705 705,980
So pi is the policy,

1236
00:33:39,880 --> 00:33:41,370
0,350 350,575 575,785 785,1120 1260,1490
s is our state. So

1237
00:33:41,370 --> 00:33:42,780
0,320 340,645 645,945 945,1230 1230,1410
it's a function that takes

1238
00:33:42,780 --> 00:33:44,000
0,240 240,450 450,660 660,915 915,1220
as input only the state

1239
00:33:44,140 --> 00:33:45,230
0,230 230,365 365,500 500,740 740,1090
and it's going to directly

1240
00:33:45,400 --> 00:33:47,685
0,400 480,725 725,970 1800,2060 2060,2285
output the action. So the

1241
00:33:47,685 --> 00:33:49,485
0,330 330,605 745,1020 1020,1295 1525,1800
outputs here give us the

1242
00:33:49,485 --> 00:33:50,865
0,405 405,785 805,1065 1065,1200 1200,1380
desired action that we should

1243
00:33:50,865 --> 00:33:52,590
0,305 475,765 765,1020 1020,1385 1405,1725
take in any given state

1244
00:33:52,590 --> 00:33:53,780
0,180 180,315 315,585 585,885 885,1190
that we find ourselves in

1245
00:33:54,190 --> 00:33:56,810
0,305 305,610 1380,1685 1685,1990 2220,2620
that represents not only the

1246
00:33:56,830 --> 00:33:58,290
0,380 380,760 900,1175 1175,1310 1310,1460
best action that we should

1247
00:33:58,290 --> 00:34:00,075
0,255 255,480 480,860 1060,1545 1545,1785
take, but let's denote this

1248
00:34:00,075 --> 00:34:02,360
0,210 210,515 565,870 870,1445 1885,2285
as basically the probability that

1249
00:34:02,410 --> 00:34:04,760
0,545 545,755 755,1060 1590,1970 1970,2350
selecting that action would result

1250
00:34:04,840 --> 00:34:06,860
0,245 245,395 395,635 635,1450 1620,2020
in a very desirable outcome.|
|

1251
00:34:07,280 --> 00:34:08,760
0,290 290,560 560,935 935,1205 1205,1480
For our network, so not
对于我们的网络，不一定是该操作的值，而是选择该操作将是最高值的概率，对吗？因此，您并不关心选择此操作所采取或产生的确切数值是多少，而是。

1252
00:34:08,840 --> 00:34:11,515
0,400 420,710 710,1000 1590,1990 2310,2675
necessarily the value of that

1253
00:34:11,515 --> 00:34:13,690
0,255 255,545 1015,1335 1335,1655 1885,2175
that action, but rather the

1254
00:34:13,690 --> 00:34:15,570
0,530 700,1080 1080,1470 1470,1605 1605,1880
probability that selecting that action

1255
00:34:15,590 --> 00:34:17,190
0,305 305,590 590,875 875,1180 1200,1600
would be the highest value,

1256
00:34:17,810 --> 00:34:19,060
0,400 480,725 725,830 830,1040 1040,1250
right? So you don't care

1257
00:34:19,060 --> 00:34:20,490
0,350 370,705 705,915 915,1110 1110,1430
exactly about what is the

1258
00:34:20,780 --> 00:34:22,510
0,590 590,880 900,1235 1235,1610 1610,1730
numerical value that selecting this

1259
00:34:22,510 --> 00:34:24,595
0,260 310,710 1300,1605 1605,1830 1830,2085
action takes or gives rise

1260
00:34:24,595 --> 00:34:26,940
0,195 195,455 925,1230 1230,1535
to rather, but rather.|
|

1261
00:34:27,410 --> 00:34:29,545
0,275 275,500 500,850 1080,1805 1805,2135
What is the likelihood that
选择此操作将为您提供预期的最佳性能价值的可能性有多大？确切的价值本身并不重要。您只关心选择此操作是否会给您带来最好的结果，这种可能性很高。

1262
00:34:29,545 --> 00:34:31,420
0,420 420,555 555,815 1375,1680 1680,1875
selecting this action will give

1263
00:34:31,420 --> 00:34:32,875
0,270 270,525 525,800 820,1155 1155,1455
you the best performing value

1264
00:34:32,875 --> 00:34:35,005
0,225 225,390 390,660 660,1025 1795,2130
that you could expect? Exact

1265
00:34:35,005 --> 00:34:37,330
0,335 925,1325 1375,1800 1800,2070 2070,2325
value itself doesn't matter. You

1266
00:34:37,330 --> 00:34:38,560
0,210 210,465 465,645 645,870 870,1230
only care about if selecting

1267
00:34:38,560 --> 00:34:40,255
0,120 120,380 790,1140 1140,1395 1395,1695
this action is going to

1268
00:34:40,255 --> 00:34:41,395
0,270 270,405 405,540 540,705 705,1140
give you, with high likelihood,

1269
00:34:41,395 --> 00:34:43,000
0,135 135,315 315,635
the best one.|
|

1270
00:34:43,570 --> 00:34:44,745
0,400 480,740 740,890 890,1040 1040,1175
So we can see that
所以我们可以看到，如果这些预测的概率在这里，就在这个Atari的例子中。

1271
00:34:44,745 --> 00:34:47,030
0,180 180,485 565,1200 1200,1775 1885,2285
if these predicted probabilities here,

1272
00:34:47,110 --> 00:34:48,540
0,335 335,545 545,785 785,1130 1130,1430
right in this example of

1273
00:34:48,540 --> 00:34:50,120
0,500
atari.|
|

1274
00:34:50,560 --> 00:34:52,880
0,365 365,730 840,1240 1470,1760 1760,2320
Going left has the probability
左转的概率是最高值动作，90%留在中间的概率为{10%。-}右转为0%。在这种情况下，理想情况下，我们的神经网络应该做的是，在这种情况下，90%的时间向左，10%的时间，它仍然可以保持在原来的位置，但永远不应该向右。现在注意，这是一个概率分布。这与A Q函数有很大的不同。QUE函数实际上没有eh结构，对吗？Q值本身可以取任何实数。

1275
00:34:53,380 --> 00:34:56,150
0,335 335,665 665,965 965,1270 2370,2770
of being the highest value

1276
00:34:56,590 --> 00:34:59,340
0,400 720,1010 1010,1600 2130,2495 2495,2750
action with 90% staying in

1277
00:34:59,340 --> 00:35:00,915
0,165 165,440 550,840 840,1080 1080,1575
the center has a probability

1278
00:35:00,915 --> 00:35:04,455
0,335 1615,2225 2695,3030 3030,3300 3300,3540
of {{10%.,-}} Going {right -}

1279
00:35:04,455 --> 00:35:06,420
0,225 225,545 925,1185 1185,1770 1770,1965
is 0%. So ideally what

1280
00:35:06,420 --> 00:35:07,290
0,135 135,360 360,555 555,750 750,870
our neural networks should do

1281
00:35:07,290 --> 00:35:08,595
0,120 120,285 285,510 510,735 735,1305
in this case is 90%

1282
00:35:08,595 --> 00:35:09,615
0,225 225,375 375,615 615,840 840,1020
of the time in this

1283
00:35:09,615 --> 00:35:11,420
0,305 1015,1320 1320,1470 1470,1560 1560,1805
situation go to the left

1284
00:35:11,560 --> 00:35:12,660
0,515 515,680 680,800 800,965 965,1100
10% of the time it

1285
00:35:12,660 --> 00:35:14,340
0,165 165,390 390,710 1180,1500 1500,1680
could still try staying at

1286
00:35:14,340 --> 00:35:15,380
0,135 135,255 255,450 450,720 720,1040
where it is, but never

1287
00:35:15,430 --> 00:35:16,350
0,290 290,455 455,650 650,815 815,920
it should go to the

1288
00:35:16,350 --> 00:35:18,690
0,260 1030,1430 1450,1770 1770,2025 2025,2340
right. Now note that this

1289
00:35:18,690 --> 00:35:21,015
0,380 520,920 1030,1320 1320,1850 1930,2325
now is a probability distribution.

1290
00:35:21,015 --> 00:35:22,110
0,240 240,375 375,600 600,885 885,1095
This is very different than

1291
00:35:22,110 --> 00:35:22,965
0,135 135,285 285,510 510,720 720,855
A Q function. A que

1292
00:35:22,965 --> 00:35:25,160
0,240 240,605 655,1005 1005,1355 1795,2195
function has actually no eh

1293
00:35:25,180 --> 00:35:26,600
0,395 395,710 710,920 920,1115 1115,1420
structure, right? The q values

1294
00:35:26,830 --> 00:35:28,035
0,335 335,545 545,710 710,950 950,1205
themselves can take any real

1295
00:35:28,035 --> 00:35:28,860
0,305
number.|
|

1296
00:35:28,860 --> 00:35:31,670
0,290 880,1230 1230,1580 2200,2505 2505,2810
Right. But here the policy
正确的。但在这里，政策网络的输出是非常严谨的。输出中的所有数字都必须求和为1，因为这是一个概率分布，对吗？这给了它一个非常严格的版本，我们如何训练这个模型，使它比色调函数更容易训练。

1297
00:35:31,690 --> 00:35:34,310
0,400 570,875 875,1085 1085,1390 1860,2620
network has a very formulated

1298
00:35:34,630 --> 00:35:36,380
0,400 780,1100 1100,1325 1325,1490 1490,1750
output. All of the numbers

1299
00:35:36,400 --> 00:35:38,205
0,400 780,1040 1040,1265 1265,1550 1550,1805
here in the output have

1300
00:35:38,205 --> 00:35:39,375
0,210 210,390 390,555 555,815 865,1170
to sum to one because

1301
00:35:39,375 --> 00:35:40,760
0,180 180,315 315,435 435,905 985,1385
this is a probability distribution,

1302
00:35:41,410 --> 00:35:42,750
0,400 570,815 815,965 965,1160 1160,1340
right? And that gives it

1303
00:35:42,750 --> 00:35:45,620
0,210 210,530 760,1400 1840,2240 2470,2870
a very rigorous version of

1304
00:35:46,270 --> 00:35:47,310
0,275 275,410 410,590 590,815 815,1040
how we can train this

1305
00:35:47,310 --> 00:35:48,435
0,320 460,735 735,885 885,990 990,1125
model that makes it a

1306
00:35:48,435 --> 00:35:49,820
0,180 180,455 475,765 765,1020 1020,1385
bit easier to train than

1307
00:35:50,080 --> 00:35:53,540
0,305 305,580 690,995 995,1300
hue functions as well.|
|

1308
00:35:53,760 --> 00:35:55,600
0,335 335,665 665,1040 1040,1420 1440,1840
One other very important advantage
输出是概率分布的另一个非常重要的优点是，实际上将与我们之前看到的Q函数和Q神经网络的另一个问题联系在一起，这是Q函数自然适合离散动作空间的事实。现在，当我们看这个政策网络时，我们考虑的是分配，对吗？记住，这些分布也可以采取连续的形式。事实上，我们在上两节课中已经看到了这一点。在生成性讲座中，我们了解了如何使用VES来预测其潜在空间上的高斯分布。在上一节课中，我们还学习了如何使用数据来预测不确定性，这些不确定性是连续的概率分布。就像这样，我们也可以使用同样的公式来超越离散的行动空间，就像你在这里看到的，这是一个可能的行动，一个与一个可能的行动相关的概率。

1309
00:35:56,040 --> 00:35:58,010
0,400 750,1085 1085,1415 1415,1760 1760,1970
of having an output that

1310
00:35:58,010 --> 00:36:00,280
0,120 120,240 240,710 850,1250 1870,2270
is a probability distribution is

1311
00:36:00,450 --> 00:36:01,820
0,275 275,455 455,740 740,1055 1055,1370
actually going to tie back

1312
00:36:01,820 --> 00:36:03,200
0,240 240,360 360,555 555,890 1030,1380
to this other issue of

1313
00:36:03,200 --> 00:36:04,790
0,240 240,530 820,1140 1140,1350 1350,1590
q functions and q neural

1314
00:36:04,790 --> 00:36:05,860
0,255 255,495 495,615 615,780 780,1070
networks that we saw before

1315
00:36:06,150 --> 00:36:07,310
0,275 275,440 440,725 725,980 980,1160
and that is the fact

1316
00:36:07,310 --> 00:36:09,890
0,320 370,675 675,980 1780,2180 2230,2580
that q functions are naturally

1317
00:36:09,890 --> 00:36:11,950
0,405 405,690 690,1215 1215,1520 1660,2060
suited towards discrete action spaces.

1318
00:36:12,210 --> 00:36:13,565
0,380 380,650 650,875 875,1055 1055,1355
Now when we're looking at

1319
00:36:13,565 --> 00:36:16,025
0,365 505,905 925,1325 1735,2160 2160,2460
this policy network we're puttingting

1320
00:36:16,025 --> 00:36:17,930
0,180 180,635 895,1245 1245,1575 1575,1905
a distributions, right? And remember

1321
00:36:17,930 --> 00:36:20,255
0,285 285,620 700,1220 1540,1940 2020,2325
those those distributions can also

1322
00:36:20,255 --> 00:36:22,070
0,305 595,990 990,1380 1380,1650 1650,1815
take continuous forms. In fact,

1323
00:36:22,070 --> 00:36:23,240
0,240 240,375 375,650 790,1050 1050,1170
we've seen this in the

1324
00:36:23,240 --> 00:36:25,480
0,240 240,495 495,1040 1720,1980 1980,2240
last two lectures. In the

1325
00:36:25,590 --> 00:36:26,930
0,515 515,755 755,995 995,1145 1145,1340
generative lecture we saw how

1326
00:36:26,930 --> 00:36:28,745
0,525 525,765 765,1040 1150,1545 1545,1815
ves could be used to

1327
00:36:28,745 --> 00:36:31,115
0,275 685,1335 1335,1775 1825,2160 2160,2370
predict gaussian distributions over their

1328
00:36:31,115 --> 00:36:32,390
0,330 330,635 685,945 945,1065 1065,1275
latent space. In the last

1329
00:36:32,390 --> 00:36:33,860
0,300 300,585 585,810 810,1100 1210,1470
lecture we also saw how

1330
00:36:33,860 --> 00:36:34,750
0,120 120,255 255,435 435,615 615,890
we could learn to predict

1331
00:36:34,800 --> 00:36:36,910
0,400 630,905 905,1160 1160,1520 1520,2110
uncertainties which are continuous probability

1332
00:36:36,990 --> 00:36:40,565
0,490 1140,1540 1710,2110 3090,3380 3380,3575
distributions using data. And just

1333
00:36:40,565 --> 00:36:42,250
0,210 210,515 715,990 990,1265 1285,1685
like that, we could also

1334
00:36:42,630 --> 00:36:45,320
0,400 900,1300 1410,1730 1730,2290 2430,2690
use this same formulation to

1335
00:36:45,320 --> 00:36:47,615
0,225 225,590 1120,1575 1575,1850 1900,2295
move beyond discrete action spaces

1336
00:36:47,615 --> 00:36:48,700
0,270 270,390 390,540 540,765 765,1085
like you can see here,

1337
00:36:48,780 --> 00:36:51,370
0,275 275,550 960,1360 1740,2140 2190,2590
which are one possible action,

1338
00:36:51,420 --> 00:36:53,090
0,290 290,815 815,1190 1190,1460 1460,1670
a probability associated to one

1339
00:36:53,090 --> 00:36:54,140
0,300 300,680
possible action.|
|

1340
00:36:54,140 --> 00:36:55,385
0,120 120,270 270,765 765,1035 1035,1245
In a discrete set of
在一组离散的可能动作中，现在我们可能有一个空间，不是我应该采取什么动作，向左，向右，或者说是居中，而是我应该移动的速度有多快，我应该向右移动的方向？这是一个连续变量，而不是离散变量。你可以说现在的答案应该是这样的，向右移动的速度非常快，而向左移动的速度非常慢，抱歉，向左移动的速度非常快，而向左移动的速度非常慢，这是我们可能想要建模的连续谱。

1341
00:36:55,385 --> 00:36:57,905
0,300 300,695 1495,1895 2065,2340 2340,2520
possible actions, now we may

1342
00:36:57,905 --> 00:36:59,285
0,255 255,540 540,875 895,1170 1170,1380
have a space which is

1343
00:36:59,285 --> 00:37:00,800
0,330 330,630 630,935 1105,1350 1350,1515
not what action should I

1344
00:37:00,800 --> 00:37:02,195
0,320 340,645 645,915 915,1185 1185,1395
take, go left, right or

1345
00:37:02,195 --> 00:37:04,175
0,240 240,575 805,1110 1110,1415 1645,1980
say center, but rather how

1346
00:37:04,175 --> 00:37:05,405
0,335 445,690 690,825 825,1050 1050,1230
quickly should I move and

1347
00:37:05,405 --> 00:37:06,470
0,120 120,345 345,675 675,915 915,1065
in what direction should I

1348
00:37:06,470 --> 00:37:07,600
0,225 225,480 480,690 690,855 855,1130
move right? That is a

1349
00:37:07,650 --> 00:37:09,140
0,400 420,800 800,1115 1115,1355 1355,1490
continuous variable as opposed to

1350
00:37:09,140 --> 00:37:11,105
0,90 90,495 495,800 1330,1710 1710,1965
a discrete variable. And you

1351
00:37:11,105 --> 00:37:12,305
0,180 180,450 450,780 780,1050 1050,1200
could say that now the

1352
00:37:12,305 --> 00:37:13,690
0,245 265,540 540,780 780,1065 1065,1385
answer should look like this,

1353
00:37:14,490 --> 00:37:16,070
0,400 660,1010 1010,1310 1310,1490 1490,1580
moving very fast to the

1354
00:37:16,070 --> 00:37:17,375
0,195 195,495 495,825 825,1125 1125,1305
right versus very slow to

1355
00:37:17,375 --> 00:37:18,770
0,245 445,735 735,930 930,1140 1140,1395
the, excuse me, very fast

1356
00:37:18,770 --> 00:37:19,730
0,180 180,255 255,405 405,660 660,960
to the left versus very

1357
00:37:19,730 --> 00:37:21,370
0,255 255,405 405,510 510,770 1240,1640
slow to the left has

1358
00:37:21,480 --> 00:37:23,345
0,400 540,940 990,1390 1470,1730 1730,1865
this continuous spectrum that we

1359
00:37:23,345 --> 00:37:24,580
0,165 165,375 375,555 555,815
may want to model.|
|

1360
00:37:25,160 --> 00:37:26,430
0,350 350,560 560,740 740,965 965,1270
Now, when we plot this
现在，当我们画出采取行动的整个分布，给出一个状态，你可以在这里看到一个非常简单的图解。这个，这个分布的大部分质量都超过了，对不起，它的所有质量都在整个实数线上。首先，它在我们想要采取的最优行动空间中拥有大部分的质量权利。因此，如果我们想要确定采取的最佳行动，我们将简单地采用这种分布模式，最高点。这将是我们应该前进的速度和我们应该前进的方向。

1361
00:37:26,450 --> 00:37:28,480
0,400 510,910 1170,1520 1520,1820 1820,2030
entire distribution of taking an

1362
00:37:28,480 --> 00:37:30,520
0,260 670,990 990,1245 1245,1580 1780,2040
action, giving a state, you

1363
00:37:30,520 --> 00:37:32,050
0,210 210,560 670,1050 1050,1305 1305,1530
can see basically a very

1364
00:37:32,050 --> 00:37:33,745
0,285 285,855 855,1065 1065,1340 1390,1695
simple illustration of that right

1365
00:37:33,745 --> 00:37:36,385
0,255 255,605 1015,1415 1675,2075 2245,2640
here. This, this distribution has

1366
00:37:36,385 --> 00:37:38,910
0,345 345,540 540,765 765,1145 2125,2525
most of its mass over,

1367
00:37:39,350 --> 00:37:40,150
0,275 275,395 395,530 530,695 695,800
sorry, it has all of

1368
00:37:40,150 --> 00:37:41,200
0,135 135,360 360,600 600,825 825,1050
its mass over the entire

1369
00:37:41,200 --> 00:37:42,295
0,210 210,435 435,720 720,960 960,1095
real number line. First of

1370
00:37:42,295 --> 00:37:43,225
0,150 150,300 300,495 495,765 765,930
all, it has most of

1371
00:37:43,225 --> 00:37:45,265
0,165 165,485 925,1320 1320,1680 1680,2040
its mass right in the

1372
00:37:45,265 --> 00:37:46,660
0,345 345,605 775,1095 1095,1275 1275,1395
optimal action space that we

1373
00:37:46,660 --> 00:37:47,290
0,135 135,255 255,390 390,525 525,630
want to take. So if

1374
00:37:47,290 --> 00:37:48,340
0,105 105,240 240,450 450,765 765,1050
we want to determine the

1375
00:37:48,340 --> 00:37:49,930
0,240 240,585 585,855 855,1130 1330,1590
best action to take, we

1376
00:37:49,930 --> 00:37:51,630
0,195 195,495 495,860 910,1305 1305,1700
would simply take the mode

1377
00:37:51,650 --> 00:37:53,260
0,290 290,545 545,910 1140,1385 1385,1610
of this distribution, the highest

1378
00:37:53,260 --> 00:37:54,870
0,380 580,855 855,1005 1005,1245 1245,1610
point. That would be the

1379
00:37:54,980 --> 00:37:56,125
0,350 350,575 575,740 740,935 935,1145
speed at which we should

1380
00:37:56,125 --> 00:37:57,130
0,195 195,375 375,540 540,780 780,1005
move and the direction that

1381
00:37:57,130 --> 00:37:58,200
0,120 120,255 255,405 405,680
we should move in.|
|

1382
00:37:58,440 --> 00:38:00,245
0,290 290,455 455,730 840,1240 1410,1805
If we wanted to also,
如果我们也想尝试不同的东西，探索我们的空间，我们可以从这个分布中取样，仍然获得一些随机性。

1383
00:38:00,245 --> 00:38:01,355
0,255 255,435 435,630 630,810 810,1110
you know, try out different

1384
00:38:01,355 --> 00:38:02,830
0,315 315,615 615,900 900,1140 1140,1475
things and explore our space,

1385
00:38:03,060 --> 00:38:04,520
0,260 260,470 470,820 900,1190 1190,1460
we could sample from this

1386
00:38:04,520 --> 00:38:06,275
0,380 580,945 945,1155 1155,1515 1515,1755
distribution and still obtain some

1387
00:38:06,275 --> 00:38:08,280
0,935
stochasticity.|
|

1388
00:38:09,480 --> 00:38:10,925
0,400 450,785 785,890 890,1115 1115,1445
Now let's look at an
现在让我们来看一个例子，我们如何对这些连续分布进行建模，实际上我们已经在前面两节课中看到了一些例子，就像我提到的，但让我们具体看看强化学习和策略梯度学习的背景。所以不是预测采取行动的概率，而是给出所有可能的状态，在这种情况下，现在有无限多的状态，因为我们处于连续的域中，我们不能简单地预测每个可能的行动的单一概率，因为它们的数量是无限的。因此，如果我们通过分布来参数化我们的动作空间，对吗？因此，让我们以高斯分布为例，将高斯分布参数化。我们只需要两个输出，对，我们需要一个均值和一个方差。给定均值和方差，我们实际上可以有一个概率质量，我们可以计算任何可能的行动的概率，我们可能想要从这两个数字。例如，在这里的图像中，我们可能想要输出一个Gou。

1389
00:38:10,925 --> 00:38:11,975
0,345 345,585 585,735 735,870 870,1050
example of how we can

1390
00:38:11,975 --> 00:38:14,470
0,315 315,695 835,1235 1465,1865 1975,2495
actually model these continuous distributions

1391
00:38:14,520 --> 00:38:15,815
0,335 335,545 545,800 800,1025 1025,1295
and actually we've already seen

1392
00:38:15,815 --> 00:38:16,955
0,270 270,570 570,810 810,975 975,1140
some examples of this in

1393
00:38:16,955 --> 00:38:18,005
0,135 135,360 360,585 585,900 900,1050
the previous two lectures like

1394
00:38:18,005 --> 00:38:19,565
0,210 210,545 895,1140 1140,1395 1395,1560
I mentioned, but let's take

1395
00:38:19,565 --> 00:38:20,765
0,135 135,390 390,785 835,1080 1080,1200
a look specifically in the

1396
00:38:20,765 --> 00:38:22,490
0,275 385,690 690,1275 1275,1485 1485,1725
context of reinforcement learning and

1397
00:38:22,490 --> 00:38:24,620
0,270 270,705 705,980 1570,1920 1920,2130
policy gradient learning. So instead

1398
00:38:24,620 --> 00:38:27,335
0,150 150,765 765,1160 1330,2030 2410,2715
of predicting this probability of

1399
00:38:27,335 --> 00:38:28,655
0,225 225,405 405,665 715,1050 1050,1320
taking an action, giving all

1400
00:38:28,655 --> 00:38:30,470
0,335 655,1050 1050,1380 1380,1605 1605,1815
possible states, which in this

1401
00:38:30,470 --> 00:38:31,505
0,320 340,600 600,735 735,885 885,1035
case there is now an

1402
00:38:31,505 --> 00:38:33,095
0,510 510,765 765,1115 1135,1410 1410,1590
infinite number of, because we're

1403
00:38:33,095 --> 00:38:34,670
0,45 45,240 240,585 585,965 1285,1575
in the continuous domain, we

1404
00:38:34,670 --> 00:38:36,305
0,410 430,795 795,1110 1110,1380 1380,1635
can't simply predict a single

1405
00:38:36,305 --> 00:38:38,020
0,480 480,735 735,990 990,1335 1335,1715
probability for every possible action

1406
00:38:38,760 --> 00:38:39,845
0,275 275,500 500,575 575,935 935,1085
because there's an infinite number

1407
00:38:39,845 --> 00:38:41,870
0,180 180,455 715,1065 1065,1415 1765,2025
of them. So instead, what

1408
00:38:41,870 --> 00:38:44,020
0,150 150,440 460,1250 1570,1860 1860,2150
if we parameterized our action

1409
00:38:44,130 --> 00:38:47,015
0,400 510,910 1860,2260 2430,2735 2735,2885
space by distribution, right? So

1410
00:38:47,015 --> 00:38:48,110
0,195 195,345 345,525 525,815 835,1095
let's take for example, the

1411
00:38:48,110 --> 00:38:51,275
0,465 465,830 1870,2250 2250,2925 2925,3165
gaussian distribution to parameterize a

1412
00:38:51,275 --> 00:38:53,120
0,435 435,785 1045,1320 1320,1545 1545,1845
gaussian distribution. We only need

1413
00:38:53,120 --> 00:38:54,680
0,345 345,825 825,1185 1185,1440 1440,1560
two outputs, right, we need

1414
00:38:54,680 --> 00:38:55,870
0,135 135,360 360,570 570,690 690,1190
a mean and a variance.

1415
00:38:55,920 --> 00:38:56,810
0,305 305,470 470,620 620,770 770,890
Given a mean and a

1416
00:38:56,810 --> 00:38:58,355
0,500 640,900 900,1125 1125,1380 1380,1545
variance we can actually have

1417
00:38:58,355 --> 00:38:59,825
0,150 150,600 600,965 1075,1335 1335,1470
a probability mass and we

1418
00:38:59,825 --> 00:39:01,400
0,150 150,420 420,540 540,995 1225,1575
can compute a probability over

1419
00:39:01,400 --> 00:39:03,320
0,350 640,1020 1020,1400 1540,1800 1800,1920
any possible action that we

1420
00:39:03,320 --> 00:39:04,280
0,135 135,300 300,480 480,720 720,960
may want to take just

1421
00:39:04,280 --> 00:39:06,020
0,165 165,375 375,585 585,860 1480,1740
from those two numbers. So

1422
00:39:06,020 --> 00:39:07,025
0,150 150,360 360,555 555,720 720,1005
for example, in this image

1423
00:39:07,025 --> 00:39:08,945
0,395 685,975 975,1260 1260,1590 1590,1920
here we may want to

1424
00:39:08,945 --> 00:39:09,860
0,315 315,570 570,885
output a gou.|
|

1425
00:39:10,110 --> 00:39:11,300
0,290 290,485 485,695 695,950 950,1190
That looks like this right?
看起来像这样，对吗？它的平均值以负0.8为中心，表示我们基本上应该以每秒0.8米的速度向左移动。我们再一次看到，由于政策网络的格式，这是一个概率分布，对，我们强制要求这是一个概率分布。这意味着，这个的积分，这个输出的积分，按照它是高斯的定义，也必须积分为1。

1426
00:39:11,300 --> 00:39:13,690
0,240 240,590 700,1005 1005,1520 1990,2390
Its mean is centered at,

1427
00:39:14,550 --> 00:39:17,090
0,350 350,515 515,850 870,1750 1950,2540
let's see, negative 0.8 indicating

1428
00:39:17,090 --> 00:39:18,430
0,225 225,375 375,525 525,800 940,1340
that we should move basically

1429
00:39:18,750 --> 00:39:20,710
0,400 750,1025 1025,1265 1265,1595 1595,1960
left with a speed of

1430
00:39:21,090 --> 00:39:23,120
0,815 815,1160 1160,1415 1415,1690 1710,2030
0.8 meters per second for

1431
00:39:23,120 --> 00:39:24,890
0,320 820,1125 1125,1395 1395,1620 1620,1770
example. And again we can

1432
00:39:24,890 --> 00:39:26,690
0,150 150,410 1030,1410 1410,1665 1665,1800
see that because this is

1433
00:39:26,690 --> 00:39:29,975
0,260 370,980 1090,1490 1630,2030 3010,3285
a probability distribution because of

1434
00:39:29,975 --> 00:39:32,440
0,180 180,485 805,1205 1345,1745 2065,2465
the format of policy networks,

1435
00:39:32,910 --> 00:39:34,250
0,335 335,575 575,1100 1100,1220 1220,1340
right, we're enforcing that this

1436
00:39:34,250 --> 00:39:36,155
0,120 120,240 240,710 790,1190 1600,1905
is a probability distribution. That

1437
00:39:36,155 --> 00:39:37,960
0,305 415,690 690,840 840,1425 1425,1805
means that the integral now

1438
00:39:38,190 --> 00:39:40,115
0,335 335,670 780,1070 1070,1360 1410,1925
of this, of this outputs

1439
00:39:40,115 --> 00:39:41,630
0,395 505,780 780,1055 1135,1395 1395,1515
right by definition of it

1440
00:39:41,630 --> 00:39:43,240
0,135 135,285 285,780 780,1130 1210,1610
being a gaussian, must also

1441
00:39:43,260 --> 00:39:44,720
0,635 635,890 890,1150
integrate to one.|
|

1442
00:39:47,890 --> 00:39:49,485
0,290 290,515 515,830 830,1130 1130,1595
Okay, great. So now let's
好的，太好了。现在让我们来看看如何训练政策梯度网络，你知道，逐步完成这个过程，我们来看看一个非常具体的例子。让我们从回顾强化学习循环开始，这节课就是从现在开始的。

1443
00:39:49,485 --> 00:39:51,075
0,390 390,675 675,855 855,1145 1255,1590
maybe take a look at

1444
00:39:51,075 --> 00:39:53,565
0,335 565,965 985,1485 1485,1805 2215,2490
how policy gradient networks can

1445
00:39:53,565 --> 00:39:55,290
0,195 195,515 895,1295 1345,1590 1590,1725
be trained and, you know,

1446
00:39:55,290 --> 00:39:56,550
0,210 210,390 390,585 585,920 940,1260
step through that process as

1447
00:39:56,550 --> 00:39:58,560
0,320 670,1020 1020,1370 1570,1845 1845,2010
well as we look at

1448
00:39:58,560 --> 00:40:00,060
0,150 150,410 490,825 825,1160 1210,1500
a very concrete example. And

1449
00:40:00,060 --> 00:40:01,410
0,210 210,645 645,945 945,1155 1155,1350
maybe let's start by just

1450
00:40:01,410 --> 00:40:04,130
0,705 705,975 975,1635 1635,1910 2320,2720
revisiting this reinforcement learning loop

1451
00:40:04,180 --> 00:40:05,520
0,275 275,470 470,740 740,1040 1040,1340
that we started this class

1452
00:40:05,520 --> 00:40:07,620
0,350 1000,1400
with now.|
|

1453
00:40:07,870 --> 00:40:09,740
0,395 395,670 960,1280 1280,1535 1535,1870
Let's specifically consider the example
让我们具体考虑训练自动驾驶车辆的例子，因为我认为这是一个特别非常直观的例子，我们可以通过代理。这是车，对吗？

1454
00:40:09,940 --> 00:40:11,840
0,400 570,950 950,1175 1175,1625 1625,1900
of training an autonomous vehicles,

1455
00:40:11,860 --> 00:40:12,660
0,260 260,395 395,545 545,680 680,800
since I think that this

1456
00:40:12,660 --> 00:40:14,805
0,120 120,360 360,740 970,1335 1335,2145
is a particularly very intuitive

1457
00:40:14,805 --> 00:40:15,840
0,315 315,525 525,660 660,810 810,1035
example that we can walk

1458
00:40:15,840 --> 00:40:18,060
0,350 610,855 855,1095 1095,1490 1840,2220
through the agent. Here is

1459
00:40:18,060 --> 00:40:19,740
0,225 225,470 1090,1490
the vehicle, right?|
|

1460
00:40:20,010 --> 00:40:21,880
0,320 320,640 990,1250 1250,1370 1370,1870
The state could be obtained
可以通过安装在车辆本身上的许多传感器来获取状态。因此，例如，自动驾驶车辆通常配备摄像头、激光雷达、雷达等传感器，所有这些都为车辆提供观测输入。

1461
00:40:22,020 --> 00:40:23,600
0,305 305,560 560,1130 1130,1415 1415,1580
through many sensors that could

1462
00:40:23,600 --> 00:40:25,000
0,210 210,750 750,975 975,1140 1140,1400
be mounted on the vehicle

1463
00:40:25,110 --> 00:40:27,065
0,350 350,590 590,785 785,1090 1350,1955
itself. So, for example, autonomous

1464
00:40:27,065 --> 00:40:28,595
0,240 240,480 480,755 925,1290 1290,1530
vehicles are typically equipped with

1465
00:40:28,595 --> 00:40:31,220
0,480 480,815 865,1265 1435,2040 2040,2625
sensors like cameras, lidars, radars

1466
00:40:31,220 --> 00:40:32,660
0,530 730,1005 1005,1140 1140,1275 1275,1440
etc, all of these are

1467
00:40:32,660 --> 00:40:35,110
0,290 340,830 1000,1490 1780,2115 2115,2450
giving observational inputs to the

1468
00:40:35,370 --> 00:40:36,720
0,260 260,365 365,610
to the vehicle.|
|

1469
00:40:36,850 --> 00:40:38,100
0,245 245,490 720,980 980,1100 1100,1250
The action that we could
我们可以采取的行动是方向盘角度。这不是一个离散变量。这是一个连续变量。这实际上是一个可以取任何实数的角度。最后，在这个非常简单的例子中，奖励是我们在撞车前行驶的距离。

1470
00:40:38,100 --> 00:40:39,420
0,290 340,660 660,900 900,1185 1185,1320
take is a steering wheel

1471
00:40:39,420 --> 00:40:40,620
0,260 490,750 750,900 900,1065 1065,1200
angle. This is not a

1472
00:40:40,620 --> 00:40:41,880
0,375 375,675 675,930 930,1050 1050,1260
discrete variable. This is a

1473
00:40:41,880 --> 00:40:43,365
0,330 330,710 730,1170 1170,1365 1365,1485
continuous variable. It's actually an

1474
00:40:43,365 --> 00:40:44,415
0,255 255,510 510,660 660,825 825,1050
angle that could take any

1475
00:40:44,415 --> 00:40:46,830
0,255 255,575 1405,1695 1695,1985 2065,2415
real number. And finally, the

1476
00:40:46,830 --> 00:40:48,915
0,350 670,945 945,1155 1155,1440 1440,2085
reward in this very simplistic

1477
00:40:48,915 --> 00:40:50,145
0,285 285,540 540,720 720,975 975,1230
example is the distance that

1478
00:40:50,145 --> 00:40:51,440
0,150 150,425 475,795 795,1005 1005,1295
we travel before we crash.|
|

1479
00:40:53,660 --> 00:40:54,805
0,320 320,500 500,635 635,935 935,1145
Okay, so now let's take
好的，现在让我们来看看如何训练一个政策梯度神经网络来解决这个自动驾驶汽车的任务作为一个具体的例子。因此，我们从正确初始化代理开始。记住，我们没有训练数据，对吗？所以我们不得不思考，强化学习实际上就像是一条数据获取和学习的管道结合在一起。因此，数据采集管道的第一部分首先初始化我们的代理，以便出去收集一些数据。

1480
00:40:54,805 --> 00:40:55,555
0,135 135,270 270,420 420,585 585,750
a look at how we

1481
00:40:55,555 --> 00:40:57,300
0,210 210,540 540,840 840,1145 1165,1745
could train a policy gradient

1482
00:40:57,650 --> 00:40:59,410
0,365 365,640 870,1190 1190,1475 1475,1760
neural network to solve this

1483
00:40:59,410 --> 00:41:00,960
0,320 370,660 660,870 870,1170 1170,1550
task of self driving cars

1484
00:41:00,980 --> 00:41:03,750
0,275 275,545 545,860 860,1180 2370,2770
as a concrete example. So

1485
00:41:03,920 --> 00:41:06,520
0,335 335,670 1230,1630 1710,2330 2330,2600
we start by initializing our

1486
00:41:06,520 --> 00:41:08,365
0,320 640,1040 1270,1590 1590,1755 1755,1845
agent right. Remember that we

1487
00:41:08,365 --> 00:41:09,475
0,120 120,330 330,615 615,885 885,1110
have no training data, right?

1488
00:41:09,475 --> 00:41:10,450
0,285 285,510 510,645 645,795 795,975
So we have to think

1489
00:41:10,450 --> 00:41:12,205
0,300 300,630 630,1305 1305,1500 1500,1755
about actually reinforcement learning is

1490
00:41:12,205 --> 00:41:13,320
0,225 225,375 375,495 495,735 735,1115
almost like a data acquisition

1491
00:41:13,610 --> 00:41:16,090
0,380 380,760 810,1210 1740,2135 2135,2480
plus learning pipeline combined together.

1492
00:41:16,090 --> 00:41:16,945
0,195 195,300 300,495 495,705 705,855
So the first part of

1493
00:41:16,945 --> 00:41:19,210
0,275 325,720 720,1115 1195,1595 1975,2265
that data acquisition pipeline is

1494
00:41:19,210 --> 00:41:20,520
0,210 210,405 405,855 855,1035 1035,1310
first to initialize our agent

1495
00:41:20,660 --> 00:41:21,610
0,260 260,395 395,545 545,725 725,950
to go out and collect

1496
00:41:21,610 --> 00:41:22,680
0,210 210,500
some data.|
|

1497
00:41:24,510 --> 00:41:26,640
0,245 245,380 380,620 620,970
So we start our.|
所以我们开始我们的。|

1498
00:41:26,640 --> 00:41:28,740
0,380 430,735 735,1040 1450,1845 1845,2100
Vehicle, our agent. And in
车辆，我们的特工。当然，在一开始，它对驾驶一无所知。它以前从来没有接触过任何这些环境规则或观察。所以它运行它的政策，现在完全没有训练，直到它终止，直到它超出了我们定义的一些界限。我们基本上是用它在终止前行驶的距离来衡量奖励的。

1499
00:41:28,740 --> 00:41:29,625
0,135 135,330 330,540 540,720 720,885
the beginning, of course, it

1500
00:41:29,625 --> 00:41:31,125
0,210 210,510 510,795 795,1115 1135,1500
knows nothing about driving. It's

1501
00:41:31,125 --> 00:41:32,730
0,275 715,1080 1080,1350 1350,1485 1485,1605
never been exposed to any

1502
00:41:32,730 --> 00:41:33,945
0,165 165,360 360,680 730,1020 1020,1215
of these rules of the

1503
00:41:33,945 --> 00:41:35,600
0,305 355,630 630,825 825,1145 1255,1655
environment or the observation before.

1504
00:41:35,650 --> 00:41:37,310
0,400 540,860 860,1085 1085,1325 1325,1660
So it runs its policy,

1505
00:41:37,480 --> 00:41:39,290
0,400 450,740 740,920 920,1160 1160,1810
which right now is untrained

1506
00:41:39,520 --> 00:41:41,820
0,400 1140,1400 1400,1565 1565,2000 2000,2300
entirely until it terminates, right

1507
00:41:41,820 --> 00:41:42,915
0,255 255,405 405,680 700,960 960,1095
until it goes outside of

1508
00:41:42,915 --> 00:41:44,000
0,135 135,405 405,600 600,765 765,1085
some bounds that we define.

1509
00:41:44,260 --> 00:41:46,605
0,320 320,640 1230,1630 1680,2030 2030,2345
We measure basically the reward

1510
00:41:46,605 --> 00:41:47,940
0,300 300,525 525,815 895,1155 1155,1335
as the distance that it

1511
00:41:47,940 --> 00:41:50,540
0,480 480,830 1030,1335 1335,1910
traveled before it terminated.|
|

1512
00:41:50,670 --> 00:41:52,220
0,275 275,530 530,910 1020,1340 1340,1550
And we record all of
我们记录了所有的状态，所有的行动和最终的奖励，直到终止，对吗？这将成为我们的迷你数据集，我们将在第一轮培训中使用它。让我们来看看这些数据集。现在我们来做一个步骤的训练。我们要做的训练的第一步是带我去参加我们的后半段训练。

1513
00:41:52,220 --> 00:41:53,600
0,225 225,560 790,1080 1080,1245 1245,1380
the states, all of the

1514
00:41:53,600 --> 00:41:55,810
0,260 790,1190 1210,1485 1485,1760 1810,2210
actions and the final reward

1515
00:41:55,860 --> 00:41:58,130
0,260 260,380 380,850 1620,1985 1985,2270
that it obtained until that

1516
00:41:58,130 --> 00:42:00,605
0,530 700,1100 1150,1550 1660,2060 2080,2475
termination, right? This becomes our

1517
00:42:00,605 --> 00:42:02,300
0,395 475,825 825,1175 1195,1485 1485,1695
mini data set that we'll

1518
00:42:02,300 --> 00:42:03,380
0,240 240,480 480,600 600,795 795,1080
use for the first round

1519
00:42:03,380 --> 00:42:05,450
0,255 255,560 1240,1635 1635,1830 1830,2070
of training. Let's take those

1520
00:42:05,450 --> 00:42:07,205
0,285 285,650 790,1095 1095,1400 1450,1755
data sets. And now we'll

1521
00:42:07,205 --> 00:42:08,650
0,150 150,435 435,810 810,1125 1125,1445
do one step of training.

1522
00:42:08,700 --> 00:42:09,965
0,290 290,545 545,785 785,980 980,1265
The first step of training

1523
00:42:09,965 --> 00:42:11,465
0,225 225,435 435,695 985,1290 1290,1500
that we'll do is to

1524
00:42:11,465 --> 00:42:13,600
0,305 955,1320 1320,1575 1575,1800 1800,2135
take me to take the

1525
00:42:14,040 --> 00:42:16,640
0,400 480,880 1410,1685 1685,1960
later half of our.|
|

1526
00:42:17,260 --> 00:42:18,795
0,260 260,425 425,1145 1145,1385 1385,1535
Of our trajectory that our
我们的代理运行的轨迹，并减少了导致低回报的行动的可能性。现在，因为我们所知道的飞行器终止了，我们可以假设在这个轨迹的后半部分发生的所有动作都可能不是非常好的动作，因为它们非常接近终止。因此，让我们降低所有这些事情在未来再次发生的可能性，我们将考虑在我们的培训课程的前半部分发生的所有事情。

1527
00:42:18,795 --> 00:42:21,270
0,275 325,725 1195,1595 1825,2280 2280,2475
agent ran and decreased the

1528
00:42:21,270 --> 00:42:23,570
0,450 450,660 660,920 1480,1880 1900,2300
probability of actions that resulted

1529
00:42:23,770 --> 00:42:25,680
0,320 320,545 545,965 965,1360 1560,1910
in low rewards. Now, because

1530
00:42:25,680 --> 00:42:26,745
0,210 210,450 450,720 720,915 915,1065
the vehicle we know the

1531
00:42:26,745 --> 00:42:29,300
0,245 265,875 1405,1680 1680,1955 2155,2555
vehicle terminated, we can assume

1532
00:42:29,320 --> 00:42:30,470
0,335 335,575 575,770 770,905 905,1150
that all of the actions

1533
00:42:31,270 --> 00:42:32,610
0,400 450,755 755,920 920,1070 1070,1340
that occurred in the later

1534
00:42:32,610 --> 00:42:34,050
0,300 300,495 495,675 675,1245 1245,1440
half of this trajectory were

1535
00:42:34,050 --> 00:42:35,720
0,290 610,915 915,1155 1155,1380 1380,1670
probably not very good actions

1536
00:42:35,860 --> 00:42:36,885
0,260 260,395 395,560 560,785 785,1025
because they came very close

1537
00:42:36,885 --> 00:42:38,955
0,180 180,665 1225,1470 1470,1800 1800,2070
to termination. So let's decrease

1538
00:42:38,955 --> 00:42:40,035
0,165 165,525 525,735 735,915 915,1080
the probability of all of

1539
00:42:40,035 --> 00:42:41,250
0,195 195,435 435,755 835,1095 1095,1215
those things happening again in

1540
00:42:41,250 --> 00:42:42,570
0,135 135,410 700,975 975,1155 1155,1320
the future, and we'll take

1541
00:42:42,570 --> 00:42:43,320
0,180 180,300 300,435 435,600 600,750
all of the things that

1542
00:42:43,320 --> 00:42:44,480
0,195 195,390 390,555 555,810 810,1160
happen in the beginning half

1543
00:42:44,710 --> 00:42:46,620
0,275 275,470 470,790 930,1330
of our training episode.|
|

1544
00:42:46,620 --> 00:42:48,150
0,380 580,840 840,1065 1065,1335 1335,1530
And we will increase their
我们会增加他们的概率。现在，再说一次，我们没有理由不在这个轨迹的前半部分采取好的行动，而在后半部分采取糟糕的行动。但这只是因为后半部分的行动更接近失败，更接近确定，例如，我们可以假设，这些行动可能是次优行动。但这些也很可能是嘈杂的奖励，因为它是如此稀少的信号，很可能你在最后采取了一些好的行动，你实际上是在试图找回你的车，但你已经太晚了。

1545
00:42:48,150 --> 00:42:50,330
0,530 820,1140 1140,1440 1440,1860 1860,2180
probabilities. Now, again, there's no

1546
00:42:50,500 --> 00:42:53,760
0,400 510,910 1800,2200 2250,2800 2880,3260
reason why there shouldn't necessarily

1547
00:42:53,760 --> 00:42:55,245
0,315 315,585 585,840 840,1160 1210,1485
be a good action that

1548
00:42:55,245 --> 00:42:56,280
0,165 165,435 435,690 690,840 840,1035
we took in the first

1549
00:42:56,280 --> 00:42:57,600
0,240 240,450 450,690 690,1230 1230,1320
half of this trajectory and

1550
00:42:57,600 --> 00:42:58,395
0,90 90,240 240,480 480,690 690,795
a bad action in the

1551
00:42:58,395 --> 00:42:59,750
0,165 165,485 685,930 930,1110 1110,1355
later half. But it's simply

1552
00:42:59,980 --> 00:43:01,980
0,400 870,1270 1320,1580 1580,1760 1760,2000
because actions that are in

1553
00:43:01,980 --> 00:43:03,590
0,240 240,525 525,890 910,1260 1260,1610
the later half were closer

1554
00:43:03,670 --> 00:43:05,180
0,275 275,425 425,700 840,1175 1175,1510
to a failure and closer

1555
00:43:05,200 --> 00:43:07,070
0,560 560,770 770,905 905,1180 1470,1870
determination that we can assume,

1556
00:43:07,150 --> 00:43:08,865
0,305 305,610 900,1250 1250,1505 1505,1715
for example, that these were

1557
00:43:08,865 --> 00:43:11,280
0,305 505,1275 1275,1565 2005,2250 2250,2415
probably suboptimal actions. But it's

1558
00:43:11,280 --> 00:43:12,480
0,180 180,500 640,900 900,1035 1035,1200
very possible that these are

1559
00:43:12,480 --> 00:43:14,580
0,590 700,1230 1230,1515 1515,1815 1815,2100
noisy rewards as well, because

1560
00:43:14,580 --> 00:43:15,680
0,225 225,345 345,510 510,825 825,1100
it's such a sparse signal,

1561
00:43:16,000 --> 00:43:17,295
0,380 380,605 605,920 920,1160 1160,1295
it's very possible that you

1562
00:43:17,295 --> 00:43:18,405
0,195 195,405 405,570 570,840 840,1110
had some good actions at

1563
00:43:18,405 --> 00:43:19,125
0,135 135,285 285,420 420,525 525,720
the end and you were

1564
00:43:19,125 --> 00:43:20,190
0,210 210,375 375,600 600,855 855,1065
actually trying to recover your

1565
00:43:20,190 --> 00:43:21,230
0,290 310,555 555,645 645,765 765,1040
car, but you were just

1566
00:43:21,340 --> 00:43:22,340
0,275 275,550
too late.|
|

1567
00:43:24,180 --> 00:43:25,960
0,395 395,650 650,800 800,1090 1380,1780
Now repeat this process again.
现在再次重复这个过程。再次重新初始化代理并运行它，直到完成。现在代理走得更远了一点，因为您已经降低了末端的概率，增加了未来的概率，并且您不断重复这一过程，直到您注意到代理每次都学习执行得越来越好，直到它最终收敛。在结束时，代理基本上能够沿着车道行驶，通常在这样做的过程中会稍微左右转向。

1568
00:43:26,010 --> 00:43:27,365
0,290 290,740 740,905 905,1115 1115,1355
{Re-initialize -} the agent one

1569
00:43:27,365 --> 00:43:28,940
0,180 180,485 865,1170 1170,1365 1365,1575
more time and run it

1570
00:43:28,940 --> 00:43:30,260
0,285 285,645 645,960 960,1125 1125,1320
until completion. Now the agent

1571
00:43:30,260 --> 00:43:31,580
0,225 225,360 360,525 525,1020 1020,1320
goes a bit farther right

1572
00:43:31,580 --> 00:43:32,960
0,225 225,555 555,840 840,975 975,1380
because you've decreased the probabilities

1573
00:43:32,960 --> 00:43:33,995
0,225 225,330 330,560 610,885 885,1035
at the ends, increased the

1574
00:43:33,995 --> 00:43:35,830
0,420 420,660 660,795 795,1055 1435,1835
probabilities at the future, and

1575
00:43:35,880 --> 00:43:37,325
0,320 320,590 590,920 920,1130 1130,1445
you keep repeating this over

1576
00:43:37,325 --> 00:43:39,035
0,210 210,455 475,875 1135,1485 1485,1710
and over again until you

1577
00:43:39,035 --> 00:43:40,670
0,225 225,495 495,690 690,965 1075,1635
notice that the agent learns

1578
00:43:40,670 --> 00:43:41,950
0,240 240,525 525,795 795,990 990,1280
to perform better and better

1579
00:43:42,270 --> 00:43:43,750
0,380 380,760 840,1085 1085,1205 1205,1480
every time until it finally

1580
00:43:43,890 --> 00:43:45,520
0,545 545,880 960,1250 1250,1385 1385,1630
converges. And at the end

1581
00:43:46,350 --> 00:43:48,530
0,400 750,1150 1380,1640 1640,1895 1895,2180
the agent is able to

1582
00:43:48,530 --> 00:43:51,125
0,290 310,660 660,1040 1540,1940 2140,2595
basically follow lanes, usually swerving

1583
00:43:51,125 --> 00:43:52,040
0,105 105,270 270,480 480,660 660,915
a bit side to side

1584
00:43:52,040 --> 00:43:53,520
0,240 240,375 375,585 585,920
while it does that.|
|

1585
00:43:53,710 --> 00:43:55,310
0,380 380,965 965,1205 1205,1340 1340,1600
Without crashing. And this is
而不会坠毁。这真的很有趣，因为这是一辆我们从来没有教过的自动驾驶汽车。车道标志是什么意思，或者道路规则是什么，对吗？这是一辆完全通过外出学习的赛车，经常撞车，你知道，试图弄清楚如何做才能在未来不再继续这样做。

1586
00:43:55,510 --> 00:43:58,275
0,350 350,700 990,1390 1650,2050 2490,2765
actually really fascinating because this

1587
00:43:58,275 --> 00:43:59,340
0,135 135,255 255,420 420,705 705,1065
is a self driving car

1588
00:43:59,340 --> 00:44:01,040
0,255 255,420 420,690 690,1070 1300,1700
that we never taught anything

1589
00:44:01,120 --> 00:44:02,505
0,365 365,590 590,725 725,920 920,1385
about. What a Lane marker

1590
00:44:02,505 --> 00:44:03,555
0,335 385,660 660,795 795,915 915,1050
means or what are the

1591
00:44:03,555 --> 00:44:05,190
0,195 195,390 390,525 525,785 1345,1635
rules of the road, anything

1592
00:44:05,190 --> 00:44:06,375
0,195 195,420 420,740 760,1035 1035,1185
about that, right? This was

1593
00:44:06,375 --> 00:44:07,880
0,165 165,390 390,630 630,935 1105,1505
a car that learned entirely

1594
00:44:08,260 --> 00:44:09,780
0,275 275,455 455,695 695,1025 1025,1520
just by going out, crashing

1595
00:44:09,780 --> 00:44:11,445
0,105 105,350 790,1185 1185,1440 1440,1665
a lot and, you know,

1596
00:44:11,445 --> 00:44:12,480
0,285 285,465 465,600 600,795 795,1035
trying to figure out what

1597
00:44:12,480 --> 00:44:13,905
0,210 210,495 495,780 780,1070 1120,1425
to do to not keep

1598
00:44:13,905 --> 00:44:15,320
0,240 240,575 745,1020 1020,1155 1155,1415
doing that in the future.|
|

1599
00:44:16,070 --> 00:44:17,400
0,275 275,455 455,695 695,980 980,1330
And the remaining question is
剩下的问题实际上是我们如何更新，你知道，作为我在左边和右边展示给你们的这个算法的一部分，对吗？例如，我们如何才能基本上制定出相同的算法？具体地说，就是更新方程式，第四步和第五步。这是我们如何使用这两个步骤来训练我们的政策并降低发生坏事件的可能性，同时促进所有这些好事件的可能性的两个真正重要的步骤。

1600
00:44:17,450 --> 00:44:18,720
0,290 290,440 440,575 575,850 870,1270
actually how we can update,

1601
00:44:19,160 --> 00:44:20,770
0,215 215,430 450,755 755,1060 1290,1610
you know, that policy as

1602
00:44:20,770 --> 00:44:21,910
0,195 195,330 330,590 610,1005 1005,1140
part of this algorithm that

1603
00:44:21,910 --> 00:44:22,735
0,210 210,390 390,585 585,720 720,825
I'm showing you on the

1604
00:44:22,735 --> 00:44:23,680
0,210 210,435 435,555 555,720 720,945
right and the left hand

1605
00:44:23,680 --> 00:44:25,570
0,285 285,650 880,1280 1390,1695 1695,1890
side, right? Like how can

1606
00:44:25,570 --> 00:44:27,660
0,240 240,590 880,1500 1500,1755 1755,2090
we basically formulate that same

1607
00:44:27,770 --> 00:44:30,145
0,550 750,1070 1070,1390 1650,2030 2030,2375
algorithm? And specifically the update

1608
00:44:30,145 --> 00:44:31,770
0,395 445,810 810,1080 1080,1305 1305,1625
equations, steps four and five

1609
00:44:31,910 --> 00:44:33,310
0,335 335,670 810,1100 1100,1250 1250,1400
right here. These are the

1610
00:44:33,310 --> 00:44:35,110
0,165 165,405 405,770 790,1190 1510,1800
two really important steps of

1611
00:44:35,110 --> 00:44:36,400
0,165 165,315 315,590 670,1005 1005,1290
how we can use those

1612
00:44:36,400 --> 00:44:37,525
0,240 240,465 465,705 705,915 915,1125
two steps to train our

1613
00:44:37,525 --> 00:44:39,580
0,305 745,1145 1195,1530 1530,1725 1725,2055
policy and decrease the probability

1614
00:44:39,580 --> 00:44:41,080
0,165 165,345 345,600 600,920 1000,1500
of bad events while promoting

1615
00:44:41,080 --> 00:44:42,490
0,225 225,915 915,1050 1050,1200 1200,1410
these likelihoods of all these

1616
00:44:42,490 --> 00:44:43,600
0,240 240,560
good events.|
|

1617
00:44:44,870 --> 00:44:47,365
0,400 450,845 845,1070 1070,1390 1470,2495
So let's assume the let's
那么让我们假设，让我们来看看损失函数。首先，策略梯度神经网络的损失函数是这样的。然后我们将从剖析它开始，以了解为什么它是这样工作的。所以在这里我们可以看到，损失由两项组成。第一个术语是绿色术语。

1618
00:44:47,365 --> 00:44:48,150
0,90 90,195 195,300 300,465 465,785
look at the loss function.

1619
00:44:48,290 --> 00:44:49,315
0,305 305,440 440,605 605,815 815,1025
First of all, the loss

1620
00:44:49,315 --> 00:44:50,910
0,335 355,600 600,720 720,995 1015,1595
function for a policy gradient

1621
00:44:51,110 --> 00:44:53,440
0,380 380,640 1470,1775 1775,2045 2045,2330
neural network looks like this.

1622
00:44:53,440 --> 00:44:54,475
0,165 165,285 285,555 555,795 795,1035
And then we'll start by

1623
00:44:54,475 --> 00:44:56,730
0,645 645,905 1135,1515 1515,1875 1875,2255
dissecting it to understand why

1624
00:44:57,110 --> 00:44:58,285
0,335 335,670 690,935 935,1040 1040,1175
this works the way it

1625
00:44:58,285 --> 00:44:59,920
0,275 895,1185 1185,1365 1365,1500 1500,1635
does. So here we can

1626
00:44:59,920 --> 00:45:00,955
0,150 150,270 270,390 390,650 730,1035
see that the loss consists

1627
00:45:00,955 --> 00:45:02,365
0,305 325,645 645,960 960,1215 1215,1410
of two terms. The first

1628
00:45:02,365 --> 00:45:03,700
0,335 385,705 705,930 930,1125 1125,1335
term is this term in

1629
00:45:03,700 --> 00:45:04,620
0,320
green.|
|

1630
00:45:04,620 --> 00:45:06,830
0,210 210,465 465,830 1090,1485 1485,2210
Just called the log likelihood
刚刚被称为选择特定行动的对数概率，第二项是你们都非常熟悉的东西。这只是一个特定时间的回报，所以这是在这个时间点之后你将获得的预期回报。

1631
00:45:06,850 --> 00:45:08,750
0,380 380,890 890,1190 1190,1520 1520,1900
of selecting a particular action

1632
00:45:09,340 --> 00:45:10,470
0,260 260,485 485,740 740,905 905,1130
the second term is something

1633
00:45:10,470 --> 00:45:11,205
0,225 225,360 360,480 480,600 600,735
that all of you are

1634
00:45:11,205 --> 00:45:12,480
0,195 195,480 480,795 795,1080 1080,1275
very familiar with already. This

1635
00:45:12,480 --> 00:45:14,775
0,180 180,465 465,830 1300,1700 1990,2295
is simply the return at

1636
00:45:14,775 --> 00:45:16,755
0,210 210,515 565,965 1405,1680 1680,1980
a specific time, so that's

1637
00:45:16,755 --> 00:45:18,330
0,300 300,695 775,1065 1065,1230 1230,1575
the expected return on rewards

1638
00:45:18,330 --> 00:45:19,380
0,225 225,375 375,540 540,765 765,1050
that you would get after

1639
00:45:19,380 --> 00:45:21,200
0,270 270,495 495,800
this time point.|
|

1640
00:45:21,200 --> 00:45:22,940
0,290 790,1200 1200,1440 1440,1620 1620,1740
Now, let's assume that we
现在，让我们假设我们从具有高对数概率或高概率的特定操作中获得了大量奖励。如果我们从概率很高的特定行动中获得了很多奖励，这意味着我们想要进一步增加这种可能性。所以我们做的可能性更大，甚至更有可能，我们在未来再次采样这种行动。

1641
00:45:22,940 --> 00:45:24,100
0,165 165,375 375,570 570,810 810,1160
got a lot of reward

1642
00:45:24,330 --> 00:45:26,450
0,290 290,575 575,970 1020,1420 1800,2120
for a particular action that

1643
00:45:26,450 --> 00:45:28,990
0,320 400,780 780,1160 1450,1850 1870,2540
had a high log probability

1644
00:45:29,130 --> 00:45:31,310
0,335 335,605 605,1180 1800,2060 2060,2180
or high probability. If we

1645
00:45:31,310 --> 00:45:32,290
0,135 135,285 285,435 435,645 645,980
got a lot of reward

1646
00:45:32,400 --> 00:45:33,935
0,230 230,410 410,755 755,1150 1260,1535
for a particular action that

1647
00:45:33,935 --> 00:45:35,780
0,180 180,390 390,965 1405,1680 1680,1845
had high probability, that means

1648
00:45:35,780 --> 00:45:36,850
0,150 150,255 255,405 405,690 690,1070
that we want to increase

1649
00:45:36,960 --> 00:45:38,675
0,335 335,830 830,1130 1130,1460 1460,1715
that probability even further. So

1650
00:45:38,675 --> 00:45:40,115
0,275 385,630 630,795 795,1080 1080,1440
we do it even more

1651
00:45:40,115 --> 00:45:41,825
0,330 330,615 615,885 885,1500 1500,1710
or even more likelihood, we

1652
00:45:41,825 --> 00:45:43,520
0,390 390,585 585,905 1105,1425 1425,1695
sampled that action again into

1653
00:45:43,520 --> 00:45:44,580
0,210 210,470
the future.|
|

1654
00:45:45,060 --> 00:45:46,535
0,275 275,380 380,515 515,820 1170,1475
On the other hand, if
另一方面，如果我们选择或比方说，如果我们获得的奖励对于一个可能性很高的行动来说是非常低的，我们想要相反的效果。我们不想在未来再次尝试这种行为，因为它导致了低回报，对吗？你们会注意到，这个损失函数，通过包括这个负值，我们将最小化在这个轨道上实现任何回报较低的行动的可能性，现在在我们的简化例子中。

1655
00:45:46,535 --> 00:45:48,530
0,305 535,935 1345,1650 1650,1875 1875,1995
we selected or let's say

1656
00:45:48,530 --> 00:45:50,560
0,165 165,360 360,950 1300,1665 1665,2030
if we obtained a reward

1657
00:45:50,940 --> 00:45:52,370
0,305 305,575 575,890 890,1205 1205,1430
that was very low for

1658
00:45:52,370 --> 00:45:53,435
0,135 135,405 405,675 675,855 855,1065
an action that had high

1659
00:45:53,435 --> 00:45:55,220
0,635 775,1035 1035,1185 1185,1365 1365,1785
likelihood, we want the inverse

1660
00:45:55,220 --> 00:45:56,975
0,350 820,1095 1095,1320 1320,1575 1575,1755
effect. We never want to

1661
00:45:56,975 --> 00:45:58,250
0,270 270,540 540,815 895,1155 1155,1275
sample that action again in

1662
00:45:58,250 --> 00:45:59,420
0,120 120,360 360,615 615,840 840,1170
the future because it resulted

1663
00:45:59,420 --> 00:46:01,310
0,225 225,330 330,555 555,920 1570,1890
in a low reward, right?

1664
00:46:01,310 --> 00:46:02,620
0,210 210,435 435,680 730,1020 1020,1310
And you'll notice that this

1665
00:46:02,850 --> 00:46:04,955
0,350 350,700 780,1115 1115,1450 1740,2105
loss function right here, by

1666
00:46:04,955 --> 00:46:06,725
0,315 315,585 585,905 1285,1620 1620,1770
including this negative, we're going

1667
00:46:06,725 --> 00:46:09,335
0,180 180,695 1195,1470 1470,2165 2215,2610
to minimize the likelihood of

1668
00:46:09,335 --> 00:46:11,290
0,525 525,870 870,1265 1405,1680 1680,1955
achieving any action that had

1669
00:46:11,460 --> 00:46:13,840
0,305 305,760 960,1265 1265,1570 1620,2380
low rewards in this trajectory

1670
00:46:14,310 --> 00:46:16,205
0,335 335,530 530,790 870,1535 1535,1895
now in our simplified example

1671
00:46:16,205 --> 00:46:17,320
0,395
on.|
|

1672
00:46:17,320 --> 00:46:19,120
0,240 240,495 495,860 1270,1605 1605,1800
The car example, all the
以汽车为例，所有奖励较低的东西都是那些最接近车辆终止部分的行为。所有有高回报的东西都是一开始就来的。这只是我们在定义报酬结构时所做的假设。

1673
00:46:19,120 --> 00:46:20,365
0,165 165,345 345,555 555,795 795,1245
things that had low rewards

1674
00:46:20,365 --> 00:46:22,285
0,315 315,635 715,1095 1095,1475 1615,1920
were exactly those actions that

1675
00:46:22,285 --> 00:46:24,480
0,270 270,905 1135,1410 1410,1635 1635,2195
came closest to the termination

1676
00:46:24,620 --> 00:46:26,190
0,335 335,620 620,965 965,1265 1265,1570
part of the of the

1677
00:46:26,240 --> 00:46:27,640
0,400 750,1010 1010,1115 1115,1250 1250,1400
vehicle. All the things that

1678
00:46:27,640 --> 00:46:28,555
0,135 135,315 315,645 645,810 810,915
had high rewards were the

1679
00:46:28,555 --> 00:46:29,275
0,135 135,300 300,435 435,555 555,720
things that came in the

1680
00:46:29,275 --> 00:46:31,000
0,305 655,1035 1035,1185 1185,1365 1365,1725
beginning. That's just the assumption

1681
00:46:31,000 --> 00:46:32,200
0,120 120,240 240,420 420,735 735,1200
that we make when defining

1682
00:46:32,200 --> 00:46:33,740
0,210 210,510 510,860
our reward structure.|
|

1683
00:46:34,130 --> 00:46:35,400
0,365 365,575 575,725 725,950 950,1270
Now we can plug this
现在我们可以将其插入到损失梯度下降算法中来训练我们的神经网络。当我们看到，你知道，这个政策梯度算法，你可以在这里突出显示。这种梯度恰恰是神经网络的策略部分。这是选择一个动作的概率，甚至是一个特定的状态。如果你还记得我们之前定义的，你知道，作为一个政策职能意味着什么？这就是它的意思，对吗？给定您发现自己所处的特定状态，选择具有最高可能性的特定操作的概率是多少。

1684
00:46:35,480 --> 00:46:38,490
0,400 720,1120 1650,1940 1940,2230 2610,3010
into the the loss of

1685
00:46:38,510 --> 00:46:40,795
0,455 455,910 1500,1895 1895,2105 2105,2285
gradient descent algorithm to train

1686
00:46:40,795 --> 00:46:41,905
0,165 165,390 390,645 645,915 915,1110
our neural network. When we

1687
00:46:41,905 --> 00:46:43,350
0,305 505,735 735,855 855,1095 1095,1445
see, you know, this policy

1688
00:46:43,430 --> 00:46:45,175
0,610 690,1145 1145,1415 1415,1610 1610,1745
gradient algorithm which you can

1689
00:46:45,175 --> 00:46:46,980
0,165 165,630 630,840 840,1170 1170,1805
see highlighted here. This gradient

1690
00:46:47,180 --> 00:46:49,680
0,400 720,1120 1260,1660 1920,2210 2210,2500
is exactly of the policy

1691
00:46:50,000 --> 00:46:51,160
0,380 380,635 635,755 755,950 950,1160
part of the neural network.

1692
00:46:51,160 --> 00:46:52,855
0,315 315,450 450,960 960,1275 1275,1695
That's the probability of selecting

1693
00:46:52,855 --> 00:46:54,600
0,105 105,365 775,1155 1155,1440 1440,1745
an action, even a specific

1694
00:46:54,800 --> 00:46:55,825
0,350 350,530 530,635 635,785 785,1025
state. And if you remember

1695
00:46:55,825 --> 00:46:57,145
0,330 330,555 555,795 795,1125 1125,1320
before when we defined, you

1696
00:46:57,145 --> 00:46:58,080
0,120 120,315 315,495 495,645 645,935
know, what does it mean

1697
00:46:58,400 --> 00:46:59,725
0,275 275,470 470,665 665,935 935,1325
to be a policy function?

1698
00:46:59,725 --> 00:47:01,105
0,420 420,705 705,945 945,1110 1110,1380
That's exactly what it means,

1699
00:47:01,105 --> 00:47:03,055
0,315 315,665 865,1200 1200,1535 1615,1950
right? Given a particular state

1700
00:47:03,055 --> 00:47:04,260
0,195 195,345 345,615 615,900 900,1205
that you find yourself in,

1701
00:47:04,580 --> 00:47:06,625
0,290 290,545 545,910 1140,1745 1745,2045
what is the probability of

1702
00:47:06,625 --> 00:47:08,155
0,405 405,615 615,930 930,1260 1260,1530
selecting a particular action with

1703
00:47:08,155 --> 00:47:09,460
0,165 165,375 375,1025
the highest likelihood.|
|

1704
00:47:09,650 --> 00:47:10,825
0,260 260,560 560,725 725,890 890,1175
And that's, you know, exactly
你知道，这就是这个方法得名的原因，你可以在这里看到这个政策梯度部分。

1705
00:47:10,825 --> 00:47:12,025
0,270 270,480 480,750 750,1005 1005,1200
where this method gets its

1706
00:47:12,025 --> 00:47:13,680
0,285 285,555 555,750 750,1055 1075,1655
name from this policy gradient

1707
00:47:14,510 --> 00:47:15,790
0,400 570,875 875,1025 1025,1130 1130,1280
piece here that you can

1708
00:47:15,790 --> 00:47:17,320
0,165 165,440
see here.|
|

1709
00:47:18,040 --> 00:47:19,490
0,400 690,950 950,1070 1070,1190 1190,1450
Now, I want to take
现在，我想在这节课快结束的时候花很短的时间来谈谈，你们知道的，和今天的第一堂课保持一致的一些挑战，在现实世界中部署这些类型的算法的一些挑战，对吗？

1710
00:47:19,600 --> 00:47:20,910
0,380 380,635 635,815 815,1055 1055,1310
maybe just a very brief

1711
00:47:20,910 --> 00:47:21,960
0,320 340,660 660,810 810,915 915,1050
second towards the end of

1712
00:47:21,960 --> 00:47:23,055
0,135 135,345 345,645 645,915 915,1095
the class here just to

1713
00:47:23,055 --> 00:47:24,480
0,210 210,545 895,1110 1110,1245 1245,1425
talk about, you know, some

1714
00:47:24,480 --> 00:47:26,595
0,150 150,440 640,1040 1540,1875 1875,2115
of the challenges and keeping

1715
00:47:26,595 --> 00:47:27,495
0,195 195,405 405,600 600,735 735,900
in line with the first

1716
00:47:27,495 --> 00:47:28,635
0,305 325,660 660,855 855,975 975,1140
lecture today, some of the

1717
00:47:28,635 --> 00:47:30,585
0,305 535,935 1105,1545 1545,1740 1740,1950
challenges of deploying these types

1718
00:47:30,585 --> 00:47:32,330
0,270 270,785 1015,1290 1290,1455 1455,1745
of algorithms in the context

1719
00:47:32,440 --> 00:47:34,520
0,395 395,650 650,830 830,1150 1680,2080
of the real world, right?|
|

1720
00:47:35,060 --> 00:47:36,125
0,255 255,360 360,450 450,710 790,1065
What do you think when
当你看到这个训练算法时，你会怎么想？你认为这种训练算法的缺点是什么？我想，如果我们想要将这种方法部署到现实中，具体是哪一步？

1721
00:47:36,125 --> 00:47:37,210
0,180 180,375 375,555 555,765 765,1085
you look at this training

1722
00:47:37,290 --> 00:47:38,255
0,350 350,485 485,620 620,770 770,965
algorithm that you can see

1723
00:47:38,255 --> 00:47:39,695
0,305 775,1065 1065,1185 1185,1290 1290,1440
here? What do you think

1724
00:47:39,695 --> 00:47:41,225
0,135 135,300 300,990 990,1290 1290,1530
are the shortcomings of this

1725
00:47:41,225 --> 00:47:43,550
0,335 445,965 1315,1650 1650,1980 1980,2325
training algorithm and which step,

1726
00:47:43,550 --> 00:47:44,855
0,210 210,405 405,740 910,1170 1170,1305
I guess, specifically if we

1727
00:47:44,855 --> 00:47:47,080
0,210 210,495 495,795 795,1145 1825,2225
wanted to deploy this approach

1728
00:47:47,190 --> 00:47:49,140
0,365 365,730
into reality?|
|

1729
00:47:52,910 --> 00:47:54,730
0,400 540,920 920,1220 1220,1550 1550,1820
Yeah, exactly. So it's step
是啊，就是这样。所以这是第二步，对吧？如果你想在现实中做到这一点，对，那基本上意味着你想出去，取走你的车，把它撞上一堆次，就是为了学习如何不撞它，对吗？而这是，你知道，这是根本不可行的。对吗？第一，这也是非常危险的。第二，所以有办法绕过这一点，对吗？绕过这一问题的第一种方法是，人们试图在模拟中训练这些类型的模型，对吗？模拟是非常安全的，因为你知道我们实际上不会破坏任何真实的东西。它的效率仍然很低，因为我们不得不多次运行这些算法，并多次使它们崩溃。只要学会不要撞车，但至少现在从安全的角度来看是这样的。

1730
00:47:54,730 --> 00:47:56,485
0,375 375,770 1120,1410 1410,1560 1560,1755
two, right? If you wanted

1731
00:47:56,485 --> 00:47:57,810
0,195 195,345 345,635 745,1035 1035,1325
to do this in reality,

1732
00:47:58,490 --> 00:47:59,980
0,350 350,650 650,965 965,1250 1250,1490
right, that essentially means that

1733
00:47:59,980 --> 00:48:01,290
0,195 195,435 435,780 780,1035 1035,1310
you want to go out,

1734
00:48:01,310 --> 00:48:03,010
0,290 290,470 470,760 1020,1550 1550,1700
collect your car, crashing it

1735
00:48:03,010 --> 00:48:04,255
0,165 165,330 330,525 525,830 880,1245
a bunch of times just

1736
00:48:04,255 --> 00:48:06,025
0,285 285,605 1015,1350 1350,1560 1560,1770
to learn how to not

1737
00:48:06,025 --> 00:48:08,040
0,285 285,635 1015,1350 1350,1575 1575,2015
crash it, right? And that's,

1738
00:48:08,180 --> 00:48:09,580
0,260 260,520 540,920 920,1130 1130,1400
you know, that's simply not

1739
00:48:09,580 --> 00:48:11,485
0,620 670,990 990,1260 1260,1545 1545,1905
feasible. Right? Number one, it's

1740
00:48:11,485 --> 00:48:13,110
0,300 300,525 525,785 1045,1335 1335,1625
also, you know, very dangerous.

1741
00:48:13,220 --> 00:48:16,720
0,365 365,730 2220,2620 3000,3305 3305,3500
Number two, so there are

1742
00:48:16,720 --> 00:48:18,400
0,255 255,570 570,920 1000,1395 1395,1680
ways around this, right? The

1743
00:48:18,400 --> 00:48:19,915
0,270 270,600 600,930 930,1245 1245,1515
number one way around this

1744
00:48:19,915 --> 00:48:21,415
0,255 255,465 465,755 805,1155 1155,1500
is that people try to

1745
00:48:21,415 --> 00:48:22,650
0,345 345,600 600,810 810,975 975,1235
train these types of models

1746
00:48:23,060 --> 00:48:25,720
0,395 395,1000 1260,1660 1740,2330 2330,2660
in simulation, right? Simulation is

1747
00:48:25,720 --> 00:48:27,505
0,285 285,620 790,1190 1420,1650 1650,1785
very safe because you know

1748
00:48:27,505 --> 00:48:28,405
0,225 225,345 345,510 510,705 705,900
we're not going to actually

1749
00:48:28,405 --> 00:48:30,780
0,135 135,695 805,1140 1140,1475 1855,2375
be damaging anything real. It's

1750
00:48:30,860 --> 00:48:32,260
0,290 290,470 470,1100 1100,1295 1295,1400
still very inefficient because we

1751
00:48:32,260 --> 00:48:33,190
0,75 75,180 180,330 330,570 570,930
have to run these algorithms

1752
00:48:33,190 --> 00:48:34,530
0,105 105,240 240,390 390,650 940,1340
a bunch of times and

1753
00:48:34,550 --> 00:48:35,440
0,320 320,500 500,620 620,755 755,890
crash them a bunch of

1754
00:48:35,440 --> 00:48:36,595
0,165 165,390 390,705 705,990 990,1155
times. Just learn not to

1755
00:48:36,595 --> 00:48:37,930
0,275 505,750 750,870 870,1065 1065,1335
crash, but at least now

1756
00:48:37,930 --> 00:48:39,055
0,225 225,390 390,540 540,765 765,1125
at least from a safety

1757
00:48:39,055 --> 00:48:40,100
0,285 285,435 435,695
point of view.|
|

1758
00:48:40,220 --> 00:48:41,880
0,105 105,285 285,800
It's much safer.|
这样就安全多了。|

1759
00:48:41,950 --> 00:48:43,010
0,320 320,500 500,635 635,785 785,1060
But, you know, the problem
但是，你知道，问题是用于强化学习的现代模拟引擎，一般来说，特别是视觉的现代模拟器，根本不能非常准确地捕捉到现实。事实上，有一个非常著名的概念叫做道德鸿沟，这是当你在模拟中训练算法时存在的鸿沟，它们不会延伸到我们在现实中看到的许多现象和模式。

1760
00:48:43,420 --> 00:48:45,380
0,290 290,545 545,910 990,1580 1580,1960
is that modern simulation engines

1761
00:48:45,910 --> 00:48:47,750
0,400 450,1130 1130,1310 1310,1535 1535,1840
for reinforcement learning and generally,

1762
00:48:48,070 --> 00:48:50,660
0,335 335,830 830,1150 1620,2000 2000,2590
very broadly speaking, modern simators

1763
00:48:51,190 --> 00:48:53,610
0,400 450,850 1230,1630 1890,2195 2195,2420
for vision specifically, do not

1764
00:48:53,610 --> 00:48:56,030
0,180 180,390 390,740 1300,1700 2020,2420
at all capture reality very

1765
00:48:56,110 --> 00:48:59,010
0,575 575,830 830,1090 2430,2765 2765,2900
accurately. In fact, there's a

1766
00:48:59,010 --> 00:49:01,830
0,270 270,650 1840,2240 2350,2640 2640,2820
very famous notion called the

1767
00:49:01,830 --> 00:49:03,570
0,585 585,920 1180,1440 1440,1575 1575,1740
simoral gap, which is a

1768
00:49:03,570 --> 00:49:05,040
0,270 270,615 615,980 1000,1275 1275,1470
gap that exists when you

1769
00:49:05,040 --> 00:49:07,455
0,320 400,920 940,1260 1260,1790 2140,2415
train algorithms in simulation, and

1770
00:49:07,455 --> 00:49:09,300
0,195 195,660 660,1055 1375,1680 1680,1845
they don't extend to a

1771
00:49:09,300 --> 00:49:10,305
0,120 120,255 255,390 390,840 840,1005
lot of the phenomena that

1772
00:49:10,305 --> 00:49:11,310
0,180 180,420 420,615 615,750 750,1005
we see and the patterns

1773
00:49:11,310 --> 00:49:13,010
0,240 240,405 405,710 970,1335 1335,1700
that we see in reality.|
|

1774
00:49:13,720 --> 00:49:15,145
0,345 345,615 615,870 870,1170 1170,1425
And one really cool result
我想在这里强调的一个非常酷的结果是，当我们训练强化学习算法时，我们最终希望它们不是在模拟中运行。我们希望它们是真实存在的。作为我们麻省理工学院实验室的一部分，我们一直在开发这种非常、非常酷的全新照片、逼真的模拟引擎，它超越了当今模拟器工作的模式，基本上就是定义一个环境模型，并试图，你知道，合成这个模型。从本质上讲，这些模拟器就像是美化了的游戏引擎，对吗？当你看着他们的时候，他们看起来都很有活力。但我们所做的一件事是采用数据驱动的方法，使用真实世界的真实数据。我们可以建立一个超逼真的合成环境，看起来就像这样吗？这是我们麻省理工学院在开发照片真实感模拟引擎时创造的一个很酷的结果。这实际上是一个自动代理，而不是一辆真正的汽车通过。

1775
00:49:15,145 --> 00:49:15,895
0,150 150,255 255,405 405,555 555,750
that I want to just

1776
00:49:15,895 --> 00:49:17,050
0,330 330,630 630,810 810,975 975,1155
highlight here is that when

1777
00:49:17,050 --> 00:49:18,970
0,240 240,495 495,1140 1140,1400 1450,1920
we're training reinforcement learning algorithms,

1778
00:49:18,970 --> 00:49:20,350
0,285 285,615 615,915 915,1185 1185,1380
we ultimately want them to

1779
00:49:20,350 --> 00:49:21,895
0,260 400,645 645,825 825,1160 1210,1545
be, you know, not operating

1780
00:49:21,895 --> 00:49:23,110
0,210 210,600 600,810 810,1005 1005,1215
in simulation. We want them

1781
00:49:23,110 --> 00:49:25,050
0,135 135,240 240,480 480,860 1540,1940
to be in reality. And

1782
00:49:25,820 --> 00:49:27,295
0,335 335,560 560,850 900,1220 1220,1475
as part of our lab

1783
00:49:27,295 --> 00:49:28,600
0,210 210,390 390,675 675,1050 1050,1305
here at MIT, we've been

1784
00:49:28,600 --> 00:49:30,760
0,345 345,740 790,1190 1390,1785 1785,2160
developing this very, very cool

1785
00:49:30,760 --> 00:49:32,545
0,300 300,510 510,720 720,1260 1260,1785
brand new photo, realistic simulation

1786
00:49:32,545 --> 00:49:34,680
0,395 535,840 840,1140 1140,1535 1735,2135
engine that goes beyond basically

1787
00:49:34,760 --> 00:49:36,870
0,305 305,935 935,1130 1130,1450 1500,2110
the paradigm of how simulators

1788
00:49:36,920 --> 00:49:38,760
0,400 420,820 840,1100 1100,1360 1440,1840
work today, which is basically

1789
00:49:38,870 --> 00:49:40,105
0,455 455,575 575,800 800,1025 1025,1235
defining a model of their

1790
00:49:40,105 --> 00:49:42,115
0,335 805,1185 1185,1500 1500,1800 1800,2010
environment and trying to, you

1791
00:49:42,115 --> 00:49:43,920
0,195 195,810 810,1175 1195,1500 1500,1805
know, synthesize that that model.

1792
00:49:44,270 --> 00:49:46,090
0,395 395,790 810,1355 1355,1565 1565,1820
Essentially these simulators are like

1793
00:49:46,090 --> 00:49:47,830
0,540 540,765 765,1070 1210,1530 1530,1740
glorified game engines, right? They

1794
00:49:47,830 --> 00:49:49,170
0,210 210,450 450,720 720,1005 1005,1340
all look very game like

1795
00:49:49,370 --> 00:49:50,280
0,260 260,380 380,515 515,650 650,910
when you look at them.

1796
00:49:50,660 --> 00:49:52,030
0,400 420,710 710,875 875,1025 1025,1370
But one thing that we've

1797
00:49:52,030 --> 00:49:54,025
0,350 430,830 1210,1545 1545,1770 1770,1995
done is taken a data

1798
00:49:54,025 --> 00:49:55,810
0,315 315,695 715,1115 1165,1500 1500,1785
driven approach using real data

1799
00:49:55,810 --> 00:49:57,340
0,240 240,420 420,630 630,950 1240,1530
of the real world. Can

1800
00:49:57,340 --> 00:49:59,370
0,180 180,375 375,680 700,1310 1330,2030
we build up synthetic environments

1801
00:49:59,600 --> 00:50:01,630
0,350 350,700 750,1130 1130,1430 1430,2030
that are super photo realistic

1802
00:50:01,630 --> 00:50:02,965
0,180 180,345 345,540 540,860 1030,1335
and look like this, right?

1803
00:50:02,965 --> 00:50:04,090
0,150 150,270 270,545 595,885 885,1125
So this is a cool

1804
00:50:04,090 --> 00:50:05,920
0,240 240,390 390,555 555,860 1540,1830
result that we created here

1805
00:50:05,920 --> 00:50:08,125
0,240 240,590 1210,1590 1590,1920 1920,2205
at MIT developing this photo

1806
00:50:08,125 --> 00:50:10,000
0,630 630,1125 1125,1395 1395,1620 1620,1875
realistic simulation engine. This is

1807
00:50:10,000 --> 00:50:11,890
0,225 225,315 315,840 840,1100 1600,1890
actually an autonomous agent, not

1808
00:50:11,890 --> 00:50:13,890
0,150 150,345 345,680 1000,1400 1600,2000
a real car driving through.|
|

1809
00:50:14,270 --> 00:50:15,685
0,320 320,640 660,1190 1190,1310 1310,1415
Our virtual simulator and a
我们的虚拟模拟器和一堆不同类型的不同场景。所以这个模拟器被称为Vista，它允许我们基本上使用我们在真实世界中收集的真实数据，然后重新模拟那些相同的真实道路。例如，假设你开着你的车，开着大质量的车，收集质量数据，现在你可以把一个虚拟代理人放到同一个模拟环境中，从不同类型的扰动或，或它可能暴露的角度，观察那个场景可能是什么样子的新视角。

1810
00:50:15,685 --> 00:50:16,885
0,195 195,375 375,600 600,915 915,1200
bunch of different types of

1811
00:50:16,885 --> 00:50:19,060
0,330 330,1025 1345,1605 1605,1770 1770,2175
different scenarios. So this simulator

1812
00:50:19,060 --> 00:50:20,275
0,165 165,345 345,765 765,1005 1005,1215
is called vista, allows us

1813
00:50:20,275 --> 00:50:21,990
0,135 135,395 535,935 1045,1380 1380,1715
to basically use real data

1814
00:50:22,010 --> 00:50:23,110
0,260 260,425 425,695 695,950 950,1100
that we do collect in

1815
00:50:23,110 --> 00:50:24,535
0,120 120,300 300,620 940,1215 1215,1425
the real world, but then

1816
00:50:24,535 --> 00:50:26,590
0,285 285,935 955,1335 1335,1710 1710,2055
{re-simulate -} those same real

1817
00:50:26,590 --> 00:50:28,030
0,350 430,750 750,960 960,1185 1185,1440
roads. So for example, let's

1818
00:50:28,030 --> 00:50:28,735
0,105 105,270 270,420 420,555 555,705
say you take your car,

1819
00:50:28,735 --> 00:50:29,620
0,150 150,330 330,510 510,675 675,885
you drive out on mass,

1820
00:50:29,620 --> 00:50:31,120
0,320 550,870 870,1095 1095,1305 1305,1500
have you collect data of

1821
00:50:31,120 --> 00:50:32,610
0,225 225,560 820,1095 1095,1230 1230,1490
mass have you can now

1822
00:50:32,690 --> 00:50:34,770
0,395 395,710 710,1025 1025,1420 1680,2080
drop a virtual agent into

1823
00:50:34,850 --> 00:50:37,620
0,350 350,700 720,1355 1355,1750 2370,2770
that same simulated environment, observing

1824
00:50:37,820 --> 00:50:39,880
0,350 350,1150 1260,1580 1580,1805 1805,2060
new viewpoints of what that

1825
00:50:39,880 --> 00:50:40,900
0,255 255,420 420,570 570,780 780,1020
scene might have looked like

1826
00:50:40,900 --> 00:50:43,230
0,240 240,540 540,855 855,1190 1720,2330
from different types of perturbations

1827
00:50:43,400 --> 00:50:45,880
0,400 480,830 830,1100 1100,1420 1860,2480
or, or types of angles

1828
00:50:45,880 --> 00:50:47,035
0,240 240,360 360,510 510,795 795,1155
that it might be exposed

1829
00:50:47,035 --> 00:50:47,760
0,365
to.|
|

1830
00:50:47,800 --> 00:50:49,155
0,365 365,650 650,890 890,1130 1130,1355
And that allows us to
这允许我们现在完全使用强化学习来训练这些代理，没有人类的标签，但重要的是允许它们转移到现实中，因为不再有SIM到真正的差距。所以事实上，我们就是这么做的。我们将代理放入我们的模拟器中，我们使用你们在今天的课程中学到的确切算法来训练他们，这些策略梯度算法和所有的训练都完全在模拟中完成。然后我们采取了这些政策，并将它们部署在我们的全尺寸自动驾驶汽车上。这现在是在真实世界中，不再是在模拟中，在左边你可以看到。

1831
00:50:49,155 --> 00:50:50,840
0,240 240,450 450,725 775,1175 1285,1685
train these agents now entirely

1832
00:50:51,070 --> 00:50:53,250
0,365 365,1100 1100,1360 1560,1895 1895,2180
using reinforcement learning, no human

1833
00:50:53,250 --> 00:50:56,235
0,560 1180,1580 1750,2150 2320,2655 2655,2985
labels, but importantly allowed them

1834
00:50:56,235 --> 00:50:57,830
0,240 240,420 420,960 960,1245 1245,1595
to be transferred into reality

1835
00:50:57,940 --> 00:50:59,340
0,290 290,650 650,965 965,1235 1235,1400
because there's no SIM to

1836
00:50:59,340 --> 00:51:01,110
0,180 180,500 880,1280 1450,1680 1680,1770
real gap anymore. So in

1837
00:51:01,110 --> 00:51:03,140
0,180 180,500 910,1290 1290,1650 1650,2030
fact, we did exactly this.

1838
00:51:03,280 --> 00:51:05,445
0,320 320,640 660,1060 1620,1940 1940,2165
We placed agents into our

1839
00:51:05,445 --> 00:51:06,860
0,450 450,675 675,915 915,1125 1125,1415
simulator, we trained them using

1840
00:51:07,060 --> 00:51:09,090
0,350 350,700 930,1450 1620,1880 1880,2030
the exact algorithms that you

1841
00:51:09,090 --> 00:51:10,350
0,225 225,450 450,615 615,990 990,1260
learned about in today's lecture,

1842
00:51:10,350 --> 00:51:12,555
0,285 285,585 585,1140 1140,1640 1900,2205
these policy gradient algorithms and

1843
00:51:12,555 --> 00:51:13,590
0,195 195,360 360,525 525,780 780,1035
all of the training was

1844
00:51:13,590 --> 00:51:16,260
0,290 310,710 940,1275 1275,1850 2320,2670
done entirely in simulation. Then

1845
00:51:16,260 --> 00:51:17,535
0,225 225,390 390,585 585,890 1000,1275
we took these policies and

1846
00:51:17,535 --> 00:51:18,915
0,225 225,555 555,765 765,1050 1050,1380
we deployed them on board

1847
00:51:18,915 --> 00:51:20,630
0,345 345,630 630,885 885,1440 1440,1715
our full scale autonomous vehicle.

1848
00:51:20,740 --> 00:51:21,630
0,260 260,410 410,590 590,755 755,890
This is now in the

1849
00:51:21,630 --> 00:51:22,830
0,165 165,470 490,765 765,975 975,1200
real world, no longer in

1850
00:51:22,830 --> 00:51:24,960
0,500 1030,1430 1540,1830 1830,1965 1965,2130
simulation, and on the left

1851
00:51:24,960 --> 00:51:25,970
0,210 210,390 390,540 540,705 705,1010
hand side you can see.|
|

1852
00:51:26,850 --> 00:51:29,050
0,395 395,710 710,1030 1380,1780 1800,2200
Basically this car driving through
基本上这辆车在现实世界中完全自主地通过这个环境，没有迁移学习在这里做。不存在从真实世界数据中增加数据的情况。这完全是使用模拟来训练的，这实际上是有史以来第一次使用强化学习来端到端地训练策略。我们是一辆可以在现实中部署的自动驾驶汽车。这是我们在麻省理工学院创造的很酷的东西.但现在我们已经讨论了强化学习和策略学习的所有基础，我想谈谈我们正在看到的其他一些可能非常令人兴奋的应用。围棋是一个非常受欢迎的应用，很多人都会告诉你和谈论它。

1853
00:51:29,100 --> 00:51:31,535
0,350 350,700 1080,1445 1445,2180 2180,2435
this environment completely autonomous in

1854
00:51:31,535 --> 00:51:32,885
0,135 135,315 315,635 685,1020 1020,1350
the real world, no transfer

1855
00:51:32,885 --> 00:51:34,420
0,345 345,675 675,975 975,1215 1215,1535
learning is is done here.

1856
00:51:34,500 --> 00:51:36,815
0,245 245,425 425,760 1290,2075 2075,2315
There is no augmentation of

1857
00:51:36,815 --> 00:51:38,225
0,275 385,690 690,900 900,1140 1140,1410
data from real world data.

1858
00:51:38,225 --> 00:51:40,145
0,210 210,465 465,845 865,1265 1525,1920
This is entirely trained using

1859
00:51:40,145 --> 00:51:42,095
0,570 570,810 810,975 975,1265 1675,1950
simulation and this represented actually

1860
00:51:42,095 --> 00:51:43,715
0,150 150,360 360,615 615,935 1315,1620
the first time ever that

1861
00:51:43,715 --> 00:51:45,710
0,705 705,995 1105,1425 1425,1710 1710,1995
reinforcement learning was used to

1862
00:51:45,710 --> 00:51:47,075
0,270 270,510 510,800 880,1200 1200,1365
train a policy end to

1863
00:51:47,075 --> 00:51:48,785
0,245 685,975 975,1035 1035,1470 1470,1710
end. We're an autonomous vehicle

1864
00:51:48,785 --> 00:51:50,330
0,225 225,345 345,555 555,1085 1225,1545
that could be deployed in

1865
00:51:50,330 --> 00:51:51,485
0,320 460,705 705,810 810,930 930,1155
reality. So that was something

1866
00:51:51,485 --> 00:51:53,435
0,315 315,570 570,875 1135,1535 1585,1950
really cool that uh we

1867
00:51:53,435 --> 00:51:54,700
0,270 270,540 540,765 765,945 945,1265
we created here at MIT.

1868
00:51:54,870 --> 00:51:56,050
0,305 305,500 500,650 650,845 845,1180
But now that we covered,

1869
00:51:56,340 --> 00:51:57,335
0,245 245,425 425,665 665,830 830,995
you know, all of these

1870
00:51:57,335 --> 00:52:00,065
0,515 835,1230 1230,1995 1995,2285 2425,2730
foundations of reinforcement learning and

1871
00:52:00,065 --> 00:52:02,090
0,305 325,725 1375,1650 1650,1800 1800,2025
policy learning, I want to

1872
00:52:02,090 --> 00:52:03,245
0,240 240,420 420,555 555,800 820,1155
touch on some other maybe

1873
00:52:03,245 --> 00:52:05,870
0,300 300,665 925,1325 1705,2105 2245,2625
very exciting applications that we're

1874
00:52:05,870 --> 00:52:08,170
0,290 640,1040 1090,1440 1440,1790 1900,2300
seeing. And one very popular

1875
00:52:08,490 --> 00:52:09,410
0,350 350,545 545,650 650,785 785,920
application that a lot of

1876
00:52:09,410 --> 00:52:10,535
0,260 280,600 600,810 810,975 975,1125
people will tell you about

1877
00:52:10,535 --> 00:52:11,615
0,150 150,360 360,615 615,870 870,1080
and talk about is the

1878
00:52:11,615 --> 00:52:12,900
0,180 180,405 405,725
game of go.|
|

1879
00:52:12,940 --> 00:52:14,930
0,305 305,610 780,1460 1460,1655 1655,1990
So here reinforcement learning agents
所以在这里，强化学习代理实际上可以用来对抗围棋大师级别的棋手的测试，你知道，当时取得了令人难以置信的令人印象深刻的结果。所以，对于那些不熟悉围棋游戏的人来说，围棋是在19*19的棋盘上玩的。围棋的粗略目标是基本上比你的对手拥有更多的棋子，对吗？通过这个表示抱歉的网格，通过这个网格，你可以在这里看到这个19乘19的网格。

1880
00:52:15,130 --> 00:52:17,060
0,275 275,485 485,770 770,1120 1530,1930
could be actually tried to

1881
00:52:17,410 --> 00:52:18,930
0,335 335,590 590,830 830,1150 1170,1520
put against the test against

1882
00:52:18,930 --> 00:52:20,540
0,195 195,375 375,660 660,1010 1210,1610
you know, grand master level

1883
00:52:20,890 --> 00:52:23,205
0,335 335,670 1290,1690 1890,2120 2120,2315
go players and you know

1884
00:52:23,205 --> 00:52:24,950
0,240 240,420 420,725 835,1235 1345,1745
at the time achieved incredibly

1885
00:52:25,240 --> 00:52:26,655
0,400 570,905 905,1130 1130,1280 1280,1415
impressive results. So for those

1886
00:52:26,655 --> 00:52:27,410
0,120 120,255 255,390 390,495 495,755
of you who are not

1887
00:52:27,640 --> 00:52:28,740
0,400 420,695 695,815 815,935 935,1100
familiar with the game of

1888
00:52:28,740 --> 00:52:30,140
0,255 255,620 640,930 930,1110 1110,1400
go go game of go

1889
00:52:30,190 --> 00:52:31,250
0,305 305,485 485,620 620,770 770,1060
is played on a nineteen

1890
00:52:31,510 --> 00:52:33,830
0,305 305,610 840,1240 1620,1970 1970,2320
by nineteen board. The rough

1891
00:52:33,850 --> 00:52:35,790
0,400 480,800 800,1120 1380,1715 1715,1940
objective of go is to

1892
00:52:35,790 --> 00:52:37,610
0,210 210,530 670,1070 1090,1455 1455,1820
claim basically more board pieces

1893
00:52:37,720 --> 00:52:39,465
0,290 290,580 600,1180 1200,1505 1505,1745
than your opponent, right? And

1894
00:52:39,465 --> 00:52:42,465
0,240 240,545 865,1295 1405,1805 2605,3000
through the grid of sorry

1895
00:52:42,465 --> 00:52:43,560
0,285 285,495 495,780 780,975 975,1095
through the grid that you

1896
00:52:43,560 --> 00:52:44,475
0,150 150,315 315,480 480,660 660,915
can see here this nineteen

1897
00:52:44,475 --> 00:52:46,140
0,255 255,545 685,1115
by nineteen grid.|
|

1898
00:52:46,140 --> 00:52:48,020
0,290 640,945 945,1140 1140,1430 1480,1880
And while the game itself,
虽然游戏本身，逻辑规则，实际上非常简单，但这块棋盘可以放置的可能行动空间和可能状态的数量比宇宙中的原子数量更多。因此，这个游戏，尽管规则在逻辑定义上非常简单，但对于一个人工算法来说，它是一个非常复杂的游戏。

1899
00:52:48,160 --> 00:52:49,935
0,400 420,680 680,1145 1145,1445 1445,1775
the the logical rules, are

1900
00:52:49,935 --> 00:52:51,555
0,225 225,450 450,785 1075,1365 1365,1620
actually quite simple, the number

1901
00:52:51,555 --> 00:52:54,555
0,270 270,575 1105,1505 1795,2195 2665,3000
of possible action spaces and

1902
00:52:54,555 --> 00:52:56,295
0,335 385,785 955,1260 1260,1485 1485,1740
possible states that this board

1903
00:52:56,295 --> 00:52:57,795
0,195 195,345 345,570 570,905 1195,1500
could be placed into is

1904
00:52:57,795 --> 00:52:58,800
0,300 300,570 570,690 690,840 840,1005
greater than the number of

1905
00:52:58,800 --> 00:53:00,540
0,435 435,675 675,930 930,1310 1480,1740
atoms in the universe. So

1906
00:53:00,540 --> 00:53:02,385
0,260 820,1200 1200,1515 1515,1710 1710,1845
this game, even though the

1907
00:53:02,385 --> 00:53:04,020
0,210 210,435 435,725 925,1325 1345,1635
rules are very simple in

1908
00:53:04,020 --> 00:53:06,510
0,195 195,645 645,1160 1810,2160 2160,2490
their logical definitions, is an

1909
00:53:06,510 --> 00:53:08,955
0,590 880,1245 1245,1610 1810,2130 2130,2445
extraordinarily complex game for an

1910
00:53:08,955 --> 00:53:10,550
0,395 445,900 900,1125 1125,1305 1305,1595
artificial algorithm to try and

1911
00:53:10,780 --> 00:53:11,860
0,400
master.|
|

1912
00:53:11,860 --> 00:53:12,970
0,180 180,345 345,615 615,885 885,1110
So the objective here was
所以这里的目标是建立一种强化学习算法来掌握围棋游戏，不仅击败，你知道，这些黄金标准软件，而且在当时看起来像是一个惊人的结果是击败了大师级的棋手。因此，围棋世界排名第一的棋手显然是人类冠军。

1913
00:53:12,970 --> 00:53:15,120
0,320 460,860 970,1245 1245,1875 1875,2150
to build a reinforcement learning

1914
00:53:15,200 --> 00:53:16,720
0,530 530,785 785,1060 1110,1370 1370,1520
algorithm to master the game

1915
00:53:16,720 --> 00:53:18,300
0,180 180,470 520,825 825,1130 1180,1580
of go, not only beating,

1916
00:53:18,620 --> 00:53:20,190
0,260 260,425 425,730 750,1150 1170,1570
you know, these gold standard

1917
00:53:20,660 --> 00:53:22,615
0,635 635,1000 1020,1420 1530,1805 1805,1955
softwareares, but also what was

1918
00:53:22,615 --> 00:53:23,500
0,150 150,300 300,510 510,705 705,885
at the time like an

1919
00:53:23,500 --> 00:53:25,285
0,320 460,860 1060,1365 1365,1560 1560,1785
amazing result was to beat

1920
00:53:25,285 --> 00:53:27,130
0,225 225,480 480,845 865,1265 1495,1845
the grand master level players.

1921
00:53:27,130 --> 00:53:28,060
0,195 195,315 315,510 510,735 735,930
So the number one player

1922
00:53:28,060 --> 00:53:29,280
0,150 150,285 285,560 610,915 915,1220
in the world of go

1923
00:53:29,630 --> 00:53:31,740
0,290 290,455 455,730 1440,1775 1775,2110
was a human human champion,

1924
00:53:31,940 --> 00:53:33,000
0,400
obviously.|
|

1925
00:53:33,010 --> 00:53:34,800
0,380 380,710 710,1250 1250,1580 1580,1790
So Google deepmind Rose to
所以Google DeepMind Rose应对了他们几年前创造的这个挑战，开发了这个解决方案，它非常基于你们在今天的课程中学到的完全相同的算法，将这个网络的价值部分和残留层结合在一起，我们将在明天的下一堂课中讨论。而使用强化学习管道，他们能够击败人类玩家的大冠军，而其核心的想法其实很简单，第一步是你训练一个神经网络，基本上观看人类水平的专家。所以这不是使用强化学习，使用监督学习，使用我们在第一课、第二课和第三课中讨论的技术。

1926
00:53:34,800 --> 00:53:37,340
0,180 180,500 1150,1550 1600,2000 2140,2540
this challenge they created a

1927
00:53:37,630 --> 00:53:39,500
0,320 320,575 575,910 1020,1420 1470,1870
couple years ago developing this

1928
00:53:39,520 --> 00:53:42,200
0,400 720,1120 1260,1660 1890,2285 2285,2680
solution, which is very much

1929
00:53:42,790 --> 00:53:44,415
0,400 540,830 830,1055 1055,1325 1325,1625
based in the exact same

1930
00:53:44,415 --> 00:53:45,465
0,360 360,480 480,600 600,810 810,1050
algorithms that you learned about

1931
00:53:45,465 --> 00:53:47,750
0,180 180,525 525,785 1225,1775 1885,2285
in today's lecture, combining both

1932
00:53:47,770 --> 00:53:49,695
0,275 275,550 1320,1610 1610,1760 1760,1925
the value part of this

1933
00:53:49,695 --> 00:53:52,230
0,300 300,600 600,1265 1285,1865 2215,2535
network with residual layers, which

1934
00:53:52,230 --> 00:53:53,610
0,225 225,470 490,890 940,1200 1200,1380
we'll cover in the next

1935
00:53:53,610 --> 00:53:56,865
0,320 400,800 1780,2175 2175,2565 2565,3255
lecture tomorrow. And using reinforcement

1936
00:53:56,865 --> 00:53:58,800
0,225 225,575 1225,1530 1530,1695 1695,1935
learning pipeline, they were able

1937
00:53:58,800 --> 00:54:00,560
0,255 255,680 850,1125 1125,1380 1380,1760
to defeat the grand champion

1938
00:54:00,730 --> 00:54:01,980
0,350 350,650 650,860 860,1040 1040,1250
human players and the idea

1939
00:54:01,980 --> 00:54:03,110
0,135 135,300 300,510 510,765 765,1130
that its core was actually

1940
00:54:03,250 --> 00:54:04,880
0,350 350,700 780,1055 1055,1280 1280,1630
very simple, the first step

1941
00:54:04,900 --> 00:54:06,375
0,275 275,425 425,620 620,940 1200,1475
is that you train a

1942
00:54:06,375 --> 00:54:07,970
0,225 225,480 480,765 765,1055 1195,1595
neural network to basically watch

1943
00:54:08,230 --> 00:54:10,770
0,400 540,940 1200,1600 2190,2435 2435,2540
human level experts. So this

1944
00:54:10,770 --> 00:54:12,350
0,210 210,450 450,690 690,1320 1320,1580
is not using reinforcement learning,

1945
00:54:12,490 --> 00:54:14,745
0,400 660,1040 1040,1390 1650,2015 2015,2255
using supervised learning, using the

1946
00:54:14,745 --> 00:54:15,720
0,255 255,495 495,615 615,795 795,975
techniques that we covered in

1947
00:54:15,720 --> 00:54:16,910
0,330 330,555 555,735 735,885 885,1190
lectures one, two and three.|
|

1948
00:54:17,460 --> 00:54:19,200
0,380 610,930 930,1155 1155,1410 1410,1740
And from this first step,
从第一步开始，我们的目标是建立一个类似于一个政策，它将模仿人类类型的玩家或人类特级大师根据给定的棋盘采取的一些粗略模式，说明他们可能执行的动作类型。但给出这个预先训练的模型，基本上你可以用它来引导和强化学习算法，这将与它自己的竞争。

1949
00:54:19,200 --> 00:54:20,180
0,240 240,390 390,570 570,720 720,980
the goal is to build

1950
00:54:20,260 --> 00:54:21,525
0,260 260,410 410,700 870,1130 1130,1265
like a policy that would

1951
00:54:21,525 --> 00:54:22,965
0,575 595,855 855,1005 1005,1185 1185,1440
imitate some of the rough

1952
00:54:22,965 --> 00:54:24,705
0,365 775,1020 1020,1170 1170,1455 1455,1740
patterns that a human type

1953
00:54:24,705 --> 00:54:25,740
0,210 210,495 495,720 720,825 825,1035
of player or a human

1954
00:54:25,740 --> 00:54:27,675
0,560 610,915 915,1220 1450,1770 1770,1935
grandmaster would take based on

1955
00:54:27,675 --> 00:54:28,800
0,105 105,300 300,600 600,915 915,1125
a given board, state the

1956
00:54:28,800 --> 00:54:29,775
0,135 135,255 255,500 550,825 825,975
type of actions that they

1957
00:54:29,775 --> 00:54:31,770
0,275 475,875 1345,1590 1590,1725 1725,1995
might execute. But then given

1958
00:54:31,770 --> 00:54:33,740
0,380 520,855 855,1155 1155,1520 1570,1970
this pre trained model, essentially

1959
00:54:34,090 --> 00:54:35,010
0,260 260,395 395,575 575,755 755,920
you could use it to

1960
00:54:35,010 --> 00:54:38,000
0,590 730,1130 1330,2070 2070,2330 2470,2990
bootstrap and reinforcement learning algorithm

1961
00:54:38,170 --> 00:54:40,400
0,290 290,580 720,1120 1230,1630 1830,2230
that would play against itself.|
|

1962
00:54:41,130 --> 00:54:42,635
0,305 305,610 780,1055 1055,1265 1265,1505
In order to learn how
为了学习如何超越人类的水平来提高，对。因此，它需要人类的理解，首先试图模仿人类，但通过这种模仿，他们会将这两个神经网络与他们自己联系起来，与他们自己玩游戏，获胜者将获得奖励。失败者会试图否定他们可能从人类同行那里获得的所有行动，并试图实际学习新类型的规则和新类型的行动，这些规则和行动基本上可能对实现超人表现非常有益。而使这个想法成为可能的一个非常重要的辅助技巧是使用第二个网络，这个辅助网络将电路板的状态作为输入并试图预测，你知道，从这个特定的状态可能出现的所有不同的电路板状态，它们的值是什么？他们的潜在回报和结果会是什么？所以这个网络是一个辅助。

1963
00:54:42,635 --> 00:54:44,375
0,225 225,480 480,815 895,1295 1465,1740
to improve even beyond the

1964
00:54:44,375 --> 00:54:46,250
0,210 210,545 1165,1500 1500,1725 1725,1875
human levels, right. So it

1965
00:54:46,250 --> 00:54:47,825
0,180 180,450 450,690 690,980 1090,1575
would take its human understandings,

1966
00:54:47,825 --> 00:54:49,025
0,225 225,360 360,780 780,945 945,1200
try to imitate the humans

1967
00:54:49,025 --> 00:54:49,925
0,300 300,450 450,600 600,750 750,900
first of all, but then

1968
00:54:49,925 --> 00:54:51,875
0,195 195,360 360,845 1465,1740 1740,1950
from that imitation they would

1969
00:54:51,875 --> 00:54:53,050
0,255 255,495 495,690 690,915 915,1175
pin these two neural networks

1970
00:54:53,100 --> 00:54:54,740
0,400 420,820 1080,1340 1340,1460 1460,1640
against themselves, play a game

1971
00:54:54,740 --> 00:54:56,080
0,315 315,630 630,810 810,945 945,1340
against themselves and the winners

1972
00:54:56,310 --> 00:54:57,950
0,290 290,580 870,1175 1175,1400 1400,1640
would be receiving a reward.

1973
00:54:57,950 --> 00:54:59,150
0,180 180,585 585,810 810,1020 1020,1200
The losers would try to

1974
00:54:59,150 --> 00:55:00,340
0,380 460,720 720,840 840,945 945,1190
negate all of the actions

1975
00:55:00,450 --> 00:55:02,285
0,400 510,830 830,1010 1010,1270 1440,1835
that they may have acquired

1976
00:55:02,285 --> 00:55:04,810
0,300 300,525 525,845 1045,1925 2125,2525
from their human counterparts and

1977
00:55:04,920 --> 00:55:06,395
0,275 275,515 515,785 785,1090 1140,1475
try to actually learn new

1978
00:55:06,395 --> 00:55:07,490
0,240 240,435 435,705 705,945 945,1095
types of rules and new

1979
00:55:07,490 --> 00:55:09,200
0,180 180,315 315,560 760,1160 1390,1710
types of actions basically that

1980
00:55:09,200 --> 00:55:11,240
0,210 210,375 375,540 540,1070 1720,2040
might be very beneficial to

1981
00:55:11,240 --> 00:55:13,340
0,510 510,1275 1275,1640 1660,1950 1950,2100
achieving superhuman performance. And one

1982
00:55:13,340 --> 00:55:15,350
0,150 150,440 490,870 870,1170 1170,2010
of the very important auxiliary

1983
00:55:15,350 --> 00:55:18,160
0,560 700,1080 1080,1395 1395,1730 2410,2810
tricks that brought this idea

1984
00:55:18,330 --> 00:55:19,940
0,245 245,410 410,730 1170,1460 1460,1610
to be possible was the

1985
00:55:19,940 --> 00:55:21,400
0,360 360,570 570,780 780,1080 1080,1460
usage of this second network,

1986
00:55:21,420 --> 00:55:23,750
0,290 290,920 920,1210 1560,1960 2010,2330
this auxiliary network which took

1987
00:55:23,750 --> 00:55:25,175
0,320 370,750 750,1050 1050,1260 1260,1425
as input the state of

1988
00:55:25,175 --> 00:55:26,525
0,135 135,395 655,975 975,1185 1185,1350
the board and tried to

1989
00:55:26,525 --> 00:55:29,165
0,275 655,900 900,1145 2035,2370 2370,2640
predict, you know, what are

1990
00:55:29,165 --> 00:55:30,310
0,240 240,420 420,540 540,765 765,1145
all of the different possible

1991
00:55:31,470 --> 00:55:32,960
0,350 350,590 590,770 770,1040 1040,1490
board states that might emerge

1992
00:55:32,960 --> 00:55:35,300
0,195 195,500 1360,1760 1810,2130 2130,2340
from this particular state and

1993
00:55:35,300 --> 00:55:36,875
0,180 180,360 360,555 555,860 1270,1575
what would their values be?

1994
00:55:36,875 --> 00:55:38,180
0,180 180,315 315,540 540,905 955,1305
What would their potential returns

1995
00:55:38,180 --> 00:55:39,815
0,210 210,465 465,860 1120,1440 1440,1635
and their outcomes be? So

1996
00:55:39,815 --> 00:55:41,270
0,195 195,515 625,900 900,1035 1035,1455
this network was an auxiliary.|
|

1997
00:55:41,360 --> 00:55:43,440
0,400 720,980 980,1145 1145,1355 1355,2080
Network that was almost hallucinating,
几乎是幻觉的电视网，对吧？不同的董事会声明，它可以从这个特定的状态中采取行动，并使用这些预测值来指导它的规划，你知道，它应该在未来采取什么行动？最后，就在最近，他们扩展了这个算法，表明他们甚至不能从一开始就使用人类大师来模仿。引导这些算法。如果他们只是完全从头开始，只有两个从未训练过的神经网络，然后他们开始相互依附，你实际上可以看到，你可以在没有任何人类监督的情况下。

1998
00:55:43,550 --> 00:55:45,775
0,400 780,1175 1175,1550 1550,1925 1925,2225
right? Different board states that

1999
00:55:45,775 --> 00:55:46,890
0,195 195,390 390,630 630,840 840,1115
it could take from this

2000
00:55:46,940 --> 00:55:49,450
0,400 540,940 1320,1720 1800,2195 2195,2510
particular state and using those

2001
00:55:49,450 --> 00:55:51,670
0,465 465,740 880,1170 1170,1460 1840,2220
predicted values to guide its

2002
00:55:51,670 --> 00:55:53,755
0,380 670,1070 1210,1455 1455,1700 1780,2085
planning of, you know, what

2003
00:55:53,755 --> 00:55:54,865
0,300 300,540 540,675 675,870 870,1110
action should it take into

2004
00:55:54,865 --> 00:55:57,000
0,195 195,455 925,1215 1215,1505 1735,2135
the future? And finally, very

2005
00:55:57,230 --> 00:55:59,080
0,305 305,500 500,790 1170,1535 1535,1850
much more recently, they extended

2006
00:55:59,080 --> 00:56:00,205
0,330 330,660 660,810 810,975 975,1125
this algorithm and showed that

2007
00:56:00,205 --> 00:56:01,380
0,150 150,315 315,525 525,810 810,1175
they could not even use

2008
00:56:01,730 --> 00:56:03,115
0,275 275,530 530,830 830,1250 1250,1385
the human grand masters in

2009
00:56:03,115 --> 00:56:04,660
0,135 135,345 345,510 510,1055 1165,1545
the beginning to imitate from

2010
00:56:04,660 --> 00:56:05,785
0,240 240,390 390,570 570,735 735,1125
in the beginning. And bootstrap

2011
00:56:05,785 --> 00:56:06,805
0,240 240,630 630,780 780,885 885,1020
these algorithms. What if they

2012
00:56:06,805 --> 00:56:08,590
0,180 180,485 805,1185 1185,1485 1485,1785
just started entirely from scratch

2013
00:56:08,590 --> 00:56:09,640
0,240 240,405 405,630 630,825 825,1050
and just had two neural

2014
00:56:09,640 --> 00:56:11,980
0,260 670,1020 1020,1275 1275,1580 1960,2340
networks never trained before they

2015
00:56:11,980 --> 00:56:13,450
0,315 315,690 690,1005 1005,1275 1275,1470
start pinning themselves against each

2016
00:56:13,450 --> 00:56:14,845
0,290 520,795 795,930 930,1140 1140,1395
other and you could actually

2017
00:56:14,845 --> 00:56:16,165
0,180 180,330 330,495 495,785 955,1320
see that you could without

2018
00:56:16,165 --> 00:56:17,700
0,300 300,635 775,1125 1125,1290 1290,1535
any human supervision at all.|
|

2019
00:56:18,460 --> 00:56:20,220
0,380 380,650 650,890 890,1150 1380,1760
Have a neural network learn
让神经网络学习不仅要超越超越人类的解决方案，而且还要超越人类创造的解决方案，后者也是由人类引导的。

2020
00:56:20,220 --> 00:56:22,620
0,380 520,810 810,1080 1080,1850 2110,2400
to not only outperform the

2021
00:56:22,620 --> 00:56:26,330
0,290 880,1280 2590,3270 3270,3435 3435,3710
solution that outperform the humans,

2022
00:56:26,800 --> 00:56:28,785
0,395 395,650 650,1300 1440,1715 1715,1985
but also outperform the solution

2023
00:56:28,785 --> 00:56:30,030
0,255 255,420 420,725 775,1050 1050,1245
that was created, which was

2024
00:56:30,030 --> 00:56:32,300
0,735 735,1035 1035,1340 1630,1950 1950,2270
bootstrapped by humans as well.|
|

2025
00:56:34,460 --> 00:56:36,730
0,400 540,815 815,1010 1010,1540 1800,2270
So with that, I'll summarize
到此为止，我将非常快速地总结我们今天所学的内容，并结束这一天。所以我们已经讨论了很多关于强化学习的基本算法。我们看到了两种不同类型的强化学习方法来优化这些解决方案。首先是Q，学习，我们试图实际估计的地方，给定一个状态，你知道，我们可能期望的任何可能的行动的价值是什么，第二种方式是采取更端到端的方法，并说如何，考虑到我们所处的状态，我应该采取任何给定行动来最大化我在这种特定状态下的潜力的可能性有多大。

2026
00:56:36,730 --> 00:56:37,885
0,210 210,510 510,750 750,960 960,1155
very quickly what we've learned

2027
00:56:37,885 --> 00:56:39,280
0,255 255,540 540,905 925,1275 1275,1395
today and and conclude for

2028
00:56:39,280 --> 00:56:40,990
0,120 120,380 880,1155 1155,1500 1500,1710
the day. So we've talked

2029
00:56:40,990 --> 00:56:42,070
0,165 165,375 375,645 645,900 900,1080
a lot about really the

2030
00:56:42,070 --> 00:56:45,160
0,470 820,1340 1480,1880 2140,2865 2865,3090
foundational algorithms underlying reinforcement learning.

2031
00:56:45,160 --> 00:56:46,570
0,240 240,495 495,735 735,1010 1030,1410
We saw two different types

2032
00:56:46,570 --> 00:56:48,610
0,240 240,885 885,1160 1360,1760 1780,2040
of reinforcement learning approaches of

2033
00:56:48,610 --> 00:56:49,735
0,135 135,255 255,480 480,870 870,1125
how we can optimize these

2034
00:56:49,735 --> 00:56:51,810
0,335 925,1260 1260,1545 1545,1785 1785,2075
solutions. First being q, learning,

2035
00:56:51,830 --> 00:56:53,100
0,275 275,455 455,635 635,920 920,1270
where we're trying to actually

2036
00:56:53,480 --> 00:56:54,835
0,365 365,635 635,845 845,1130 1130,1355
estimate, given a state, you

2037
00:56:54,835 --> 00:56:55,980
0,245 295,555 555,720 720,885 885,1145
know, what is the value

2038
00:56:56,030 --> 00:56:57,580
0,260 260,395 395,665 665,1060 1290,1550
that we might expect for

2039
00:56:57,580 --> 00:56:58,735
0,165 165,435 435,765 765,1005 1005,1155
any possible action in the

2040
00:56:58,735 --> 00:57:00,010
0,210 210,545 715,1005 1005,1155 1155,1275
second way was to take

2041
00:57:00,010 --> 00:57:00,700
0,105 105,240 240,390 390,555 555,690
a much more end to

2042
00:57:00,700 --> 00:57:02,580
0,195 195,560 880,1185 1185,1485 1485,1880
end approach and say how,

2043
00:57:02,690 --> 00:57:03,670
0,320 320,530 530,695 695,830 830,980
given the state that we

2044
00:57:03,670 --> 00:57:04,825
0,255 255,540 540,810 810,1020 1020,1155
see ourselves in, what is

2045
00:57:04,825 --> 00:57:05,770
0,135 135,570 570,690 690,810 810,945
the likelihood that I should

2046
00:57:05,770 --> 00:57:07,435
0,135 135,315 315,525 525,830 1390,1665
take any given action to

2047
00:57:07,435 --> 00:57:09,450
0,585 585,870 870,1205 1495,1755 1755,2015
maximize the potential that I

2048
00:57:09,620 --> 00:57:11,130
0,305 305,610 660,935 935,1160 1160,1510
I have in this particular

2049
00:57:11,210 --> 00:57:11,940
0,400
state.|
|

2050
00:57:12,010 --> 00:57:13,350
0,350 350,560 560,725 725,1030 1080,1340
And I hope that all
我希望今天所有的这些对你们来说都非常令人兴奋。我们有一个非常令人兴奋的实验室，比赛开始了，比赛的最后期限会很好。原定是星期四，也就是明天晚上11点谢谢。

2051
00:57:13,350 --> 00:57:14,415
0,135 135,285 285,450 450,705 705,1065
of this was very exciting

2052
00:57:14,415 --> 00:57:15,345
0,210 210,390 390,675 675,825 825,930
to you today. We have

2053
00:57:15,345 --> 00:57:17,300
0,275 295,690 690,1080 1080,1475 1555,1955
a very exciting lab and

2054
00:57:17,440 --> 00:57:18,800
0,275 275,500 500,710 710,965 965,1360
kick off for the competition

2055
00:57:19,510 --> 00:57:20,805
0,350 350,575 575,980 980,1145 1145,1295
and the deadline for these

2056
00:57:20,805 --> 00:57:23,220
0,575 655,945 945,1235 1975,2265 2265,2415
competitions will be well. It

2057
00:57:23,220 --> 00:57:24,660
0,180 180,500 610,990 990,1215 1215,1440
was originally set to be

2058
00:57:24,660 --> 00:57:27,240
0,380 850,1125 1125,1400 1750,2150 2260,2580
thursday, which is tomorrow at

2059
00:57:27,240 --> 00:57:29,740
0,270 270,710 850,1140 1140,1430
eleven PM. Thank you.|
|

