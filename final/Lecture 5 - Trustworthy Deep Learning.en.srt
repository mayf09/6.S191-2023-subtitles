1
00:00:09,660 --> 00:00:11,740
I'm really excited especially for this lecture,

2
00:00:11,760 --> 00:00:13,340
which is a very special lecture

3
00:00:13,340 --> 00:00:15,280
on robust and trustworthy deep learning

4
00:00:15,420 --> 00:00:19,150
by one of our sponsors of this amazing course, Themis AI.

5
00:00:19,800 --> 00:00:21,070
And as you'll see today,

6
00:00:21,240 --> 00:00:26,590
Themis AI is a startup actually locally based here in Cambridge,

7
00:00:26,880 --> 00:00:33,430
our mission is to design, advance and deploy the future of AI and trustworthy AI specifically,

8
00:00:34,050 --> 00:00:36,790
I'm especially excited about today's lecture,

9
00:00:37,140 --> 00:00:40,240
because I co-founded Themis right here at MIT,

10
00:00:40,680 --> 00:00:42,370
right here in this very building, in fact,

11
00:00:42,930 --> 00:00:48,980
this all stemmed from really the incredible scientific innovation and advances that we created right here,

12
00:00:48,980 --> 00:00:51,610
just a few floors higher than where you're sitting today,

13
00:00:52,380 --> 00:00:58,780
and because of our background in really cutting edge scientific innovation stemming from MIT,

14
00:00:58,830 --> 00:01:04,120
Themis is very rooted deeply in science and like I said, innovation,

15
00:01:05,520 --> 00:01:09,810
we really aim to advance the future of deep learning and AI

16
00:01:09,810 --> 00:01:13,370
and much of our technology has already grown from published research,

17
00:01:14,080 --> 00:01:21,740
that we've published at top tier peer review conferences in the AI [venues] around the world

18
00:01:21,820 --> 00:01:25,940
and our work has been covered by high profile international media outlets,

19
00:01:26,920 --> 00:01:29,810
this scientific innovation, with this scientific innovation,

20
00:01:30,040 --> 00:01:36,885
Themis, we are tackling some of the biggest challenges in safety critical AI that exists today,

21
00:01:36,885 --> 00:01:39,165
and really that stems from the fact,

22
00:01:39,165 --> 00:01:42,740
that we want to take all of these amazing advances that you're learning as part of this course

23
00:01:42,940 --> 00:01:46,490
and actually achieve them in reality as part of our daily lives

24
00:01:46,810 --> 00:01:51,450
and we're working together with leading global industry partners across many different disciplines

25
00:01:51,450 --> 00:01:54,555
ranging from robotics, autonomy health care and more

26
00:01:54,555 --> 00:01:56,030
to develop a line of products,

27
00:01:56,650 --> 00:01:59,060
that will guarantee safe and trustworthy AI

28
00:01:59,440 --> 00:02:05,450
and we drive this really deeply with our technical engineering and machine learning team,

29
00:02:05,900 --> 00:02:09,510
and our focus is very much on the engineering,

30
00:02:09,770 --> 00:02:12,570
very flexible and very modular platforms

31
00:02:13,010 --> 00:02:16,740
to scale algorithms towards robust and trustworthy AI,

32
00:02:17,210 --> 00:02:23,460
this really enables this deployment towards grand challenges that our society faces with AI today,

33
00:02:23,810 --> 00:02:28,020
specifically the ability for AI solutions today are not very trustworthy at all,

34
00:02:28,190 --> 00:02:32,610
even if they may be very high performance on some of the tasks that we study as part of this course.

35
00:02:33,110 --> 00:02:36,715
So it's an incredibly exciting time for Themis

36
00:02:36,715 --> 00:02:39,355
and specific right now, where VC backed,

37
00:02:39,355 --> 00:02:42,660
we're located our offices are right here in Cambridge, so we're local

38
00:02:43,130 --> 00:02:45,210
and we have just closed around the funding,

39
00:02:45,920 --> 00:02:50,190
so we're actively hiring the best and the brightest engineers, like all of you

40
00:02:50,840 --> 00:02:53,485
to realize the future of safe and trustworthy AI

41
00:02:53,485 --> 00:02:56,340
and we hope that really today's lecture inspires you

42
00:02:56,570 --> 00:02:59,920
to join us on this mission to build the future of AI.

43
00:02:59,920 --> 00:03:03,480
And with that it's my great pleasure to introduce Sadhana,

44
00:03:03,830 --> 00:03:06,400
Sadhana is a machine learning scientist at Themis,

45
00:03:07,080 --> 00:03:10,060
she's also the lead TA of this course,

46
00:03:10,170 --> 00:03:11,860
intro to deep learning at MIT,

47
00:03:12,210 --> 00:03:14,705
her research at Themis focuses specifically on,

48
00:03:14,705 --> 00:03:19,190
how we can build very modular and flexible methods for AI

49
00:03:19,190 --> 00:03:22,270
and building what we call a safe and trustworthy AI.

50
00:03:22,590 --> 00:03:23,900
And today she'll be teaching us

51
00:03:23,900 --> 00:03:29,260
more about specifically the bias and the uncertainty realms of AI algorithms,

52
00:03:29,790 --> 00:03:32,500
which are really two key or critical components

53
00:03:32,550 --> 00:03:38,890
towards achieving this mission or this vision of safe and trustworthy deployment of AI all around us.

54
00:03:39,420 --> 00:03:43,750
So thank you and please give a big, warm round of applause for Sadhana.

55
00:03:48,040 --> 00:03:49,610
Thank you so much, Alexander, for the introduction.

56
00:03:50,800 --> 00:03:52,380
Hi everyone. I'm Sadhana,

57
00:03:52,380 --> 00:03:55,190
I'm a machine learning scientist here at Themis AI

58
00:03:55,570 --> 00:03:57,710
and the lead TA of the course this year,

59
00:03:57,910 --> 00:04:04,010
and today I'm super excited to talk to you all about robust and trustworthy deep learning on behalf of Themis.

60
00:04:06,230 --> 00:04:07,830
So over the past decade,

61
00:04:07,850 --> 00:04:12,420
we've seen some tremendous growth in artificial intelligence, across safety critical domains,

62
00:04:13,010 --> 00:04:15,270
in the spheres of autonomy and robotics,

63
00:04:15,530 --> 00:04:20,580
we now have models that can make critical decisions about things like self driving at a second's notice,

64
00:04:21,020 --> 00:04:24,360
and these are paving the way for fully autonomous vehicles and robots,

65
00:04:25,010 --> 00:04:26,370
and that's not where this stops,

66
00:04:26,480 --> 00:04:28,380
in the spheres of medicine and healthcare,

67
00:04:28,850 --> 00:04:31,680
robots are now equipped to conduct life saving surgery,

68
00:04:32,180 --> 00:04:35,130
we have algorithms that generate predictions for critical drugs,

69
00:04:35,150 --> 00:04:38,250
that may cure diseases that we previously thought were incurable,

70
00:04:38,750 --> 00:04:41,880
and we have models that can automatically diagnose diseases,

71
00:04:41,930 --> 00:04:44,850
without intervention from any health care professionals at all.

72
00:04:45,770 --> 00:04:47,575
These advances are revolutionary,

73
00:04:47,575 --> 00:04:50,190
and they have the potential to change life as we know it today.

74
00:04:51,550 --> 00:04:53,540
But there's another question that we need to ask,

75
00:04:53,620 --> 00:04:56,210
which is where are these models in real life,

76
00:04:56,620 --> 00:04:59,870
a lot of these technologies were innovated five, ten years ago,

77
00:04:59,950 --> 00:05:02,300
but you and I don't see them in our daily lives,

78
00:05:02,950 --> 00:05:06,740
so what is, what's the gap here between innovation and deployment.

79
00:05:09,270 --> 00:05:15,940
The reason why you and I can't go by self driving cars or robots don't typically assist in operating rooms is this,

80
00:05:16,650 --> 00:05:20,590
these are some headlines about the failures of AI from the last few years alone,

81
00:05:21,450 --> 00:05:24,470
in addition to these incredible advances,

82
00:05:24,470 --> 00:05:26,800
we've also seen catastrophic failures

83
00:05:26,910 --> 00:05:30,160
in every single one of the safety critical domains I just mentioned,

84
00:05:31,140 --> 00:05:33,730
these problems range from crashing autonomous vehicles

85
00:05:34,080 --> 00:05:37,150
to health care algorithms that don't actually work for everyone,

86
00:05:37,230 --> 00:05:39,310
even though they're deployed out in the real world,

87
00:05:39,360 --> 00:05:40,600
so everyone can use them.

88
00:05:41,790 --> 00:05:43,180
Now, at a first glance,

89
00:05:43,380 --> 00:05:44,860
this seems really demoralizing,

90
00:05:45,360 --> 00:05:47,950
if these are all of the things wrong with artificial intelligence,

91
00:05:48,330 --> 00:05:50,330
how are we ever going to achieve that vision

92
00:05:50,330 --> 00:05:53,830
of having our AI integrated into the fabric of our daily lives

93
00:05:54,090 --> 00:05:56,050
in terms of safety critical deployment,

94
00:05:57,450 --> 00:06:00,640
but at Themis, this is exactly the type of problem that we solve,

95
00:06:01,200 --> 00:06:03,770
we want to bring these advances to the real world,

96
00:06:03,770 --> 00:06:08,500
and the way we do this is by innovating in the spheres of safe and trustworthy artificial intelligence

97
00:06:08,850 --> 00:06:14,200
in order to bring the things that were developed in research labs around the world to customers like you and me.

98
00:06:16,450 --> 00:06:19,560
And we do this by, our core ideology is that,

99
00:06:19,560 --> 00:06:24,530
we believe that all of the problems on this slide are underlaid by two key notions,

100
00:06:24,760 --> 00:06:26,270
the first is bias,

101
00:06:27,110 --> 00:06:31,650
bias is what happens when machine learning models do better on some demographics than others,

102
00:06:32,390 --> 00:06:38,310
this results in things like facial detection systems, not being able to detect certain faces with high accuracy,

103
00:06:38,690 --> 00:06:41,460
Siri not being able to recognize voices with accents,

104
00:06:41,660 --> 00:06:44,400
or algorithms that are trained on imbalanced data sets,

105
00:06:44,510 --> 00:06:47,010
so what the algorithm believes is a good solution

106
00:06:47,450 --> 00:06:49,860
doesn't actually work for everyone in the real world.

107
00:06:52,030 --> 00:06:57,860
And the second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty,

108
00:06:58,750 --> 00:07:02,450
this is when models don't know when they can or can't be trusted,

109
00:07:03,160 --> 00:07:04,785
and this results in scenarios,

110
00:07:04,785 --> 00:07:10,220
such as self driving cars continuing to operate in environments when they're not 100% confident

111
00:07:10,510 --> 00:07:11,990
instead of giving control to users,

112
00:07:12,820 --> 00:07:18,590
or robots being moving around in environments that they've never been in before and have high unfamiliarity with.

113
00:07:21,160 --> 00:07:27,860
And a lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty.

114
00:07:30,980 --> 00:07:32,550
So today in this lecture,

115
00:07:32,750 --> 00:07:36,480
we're going to focus on investigating the root causes of all of these problems,

116
00:07:36,740 --> 00:07:39,510
these two big challenges to robust deep learning.

117
00:07:40,220 --> 00:07:42,100
We'll also talk about solutions for them,

118
00:07:42,100 --> 00:07:46,320
that can improve the robustness and safety of all of these algorithms for everyone.

119
00:07:46,850 --> 00:07:48,900
And we'll start by talking about bias.

120
00:07:50,280 --> 00:07:53,590
Bias is a word that we've all heard, outside the context of deep learning,

121
00:07:53,850 --> 00:07:58,120
but in the context of machine learning, it can be quantified and mathematically defined.

122
00:07:59,130 --> 00:08:00,700
Today we'll talk about how to do this

123
00:08:00,720 --> 00:08:03,880
and methods for mitigation of this bias algorithmically

124
00:08:03,960 --> 00:08:06,130
and how Themis is innovating in these areas

125
00:08:06,210 --> 00:08:11,080
in order to bring new algorithms in this space to industries around the world.

126
00:08:12,240 --> 00:08:14,080
Afterwards, we'll talk about uncertainty,

127
00:08:14,460 --> 00:08:19,750
which is, can we teach a model when it does or doesn't know the answer to to its given task,

128
00:08:20,100 --> 00:08:23,710
and we'll talk about the ramifications for this for real world AI.

129
00:08:27,900 --> 00:08:30,220
So what exactly does bias mean,

130
00:08:30,480 --> 00:08:33,430
and where is it present in the artificial intelligence lifecycle,

131
00:08:34,860 --> 00:08:37,610
the most intuitive form of bias comes from data.

132
00:08:38,260 --> 00:08:40,790
We have two different two main types of bias here,

133
00:08:40,930 --> 00:08:42,800
the first is sampling bias,

134
00:08:43,030 --> 00:08:46,790
which is when we over sample from some regions of our input data distribution

135
00:08:47,170 --> 00:08:48,560
and under sample from others,

136
00:08:49,060 --> 00:08:51,710
a good example of this is a lot of clinical data sets,

137
00:08:51,940 --> 00:08:56,750
where they often contain fewer examples of diseased patients than healthy patients,

138
00:08:56,890 --> 00:09:01,280
because it's much easier to acquire data for healthy patients than their disease counterparts.

139
00:09:02,610 --> 00:09:07,060
In addition, we also have selection bias at the data portion of the AI life cycle,

140
00:09:07,860 --> 00:09:10,930
think about Apple's series voice recognition algorithm,

141
00:09:11,580 --> 00:09:15,280
this model is trained largely on flawless American English,

142
00:09:15,600 --> 00:09:17,450
but it's deployed across the real world

143
00:09:17,450 --> 00:09:20,770
to be able to recognize voices with accents from all over the world,

144
00:09:21,850 --> 00:09:28,250
the distribution of the model's training data doesn't match the distribution of this type of language in the real world,

145
00:09:28,570 --> 00:09:32,780
because American English is highly overrepresented, as opposed to other demographics.

146
00:09:35,070 --> 00:09:38,350
But that's not where, that's not where bias and data stops,

147
00:09:39,000 --> 00:09:43,270
these biases can be propagated towards models training cycles themselves,

148
00:09:43,440 --> 00:09:45,910
which is what we'll focus on in the second half of this lecture.

149
00:09:48,130 --> 00:09:50,760
And then once the model is actually deployed,

150
00:09:50,760 --> 00:09:52,935
which means it's actually put out into the real world

151
00:09:52,935 --> 00:09:56,240
and customers or users can actually get the predictions from it,

152
00:09:56,560 --> 00:10:00,980
we may see further biases perpetuated that we haven't seen before,

153
00:10:01,630 --> 00:10:03,860
the first of these is distribution shifts,

154
00:10:04,330 --> 00:10:05,570
let's say I have a model,

155
00:10:05,620 --> 00:10:08,180
that I trained on the past twenty years of data,

156
00:10:08,530 --> 00:10:10,340
and then I deploy it into the real world,

157
00:10:10,810 --> 00:10:13,700
in 2023, This model will probably do fine,

158
00:10:13,840 --> 00:10:17,900
because the data input distribution is quite similar to data in the training distribution,

159
00:10:18,900 --> 00:10:22,395
but what would happen to this model in 2033,

160
00:10:22,395 --> 00:10:24,680
it probably would not work as well,

161
00:10:25,060 --> 00:10:29,660
because the distribution that the data is coming from would shift significantly across this decade,

162
00:10:29,920 --> 00:10:33,680
and if we don't continue to update our models with this input stream of data,

163
00:10:33,880 --> 00:10:36,530
we're going to have obsolete and incorrect predictions.

164
00:10:38,510 --> 00:10:42,090
And finally, after deployment, there the evaluation aspect,

165
00:10:42,470 --> 00:10:46,380
so think back to the Apple Siri example that we've been talking about,

166
00:10:46,820 --> 00:10:51,990
if the evaluation metric or the evaluation data set that Siri was evaluated on

167
00:10:52,160 --> 00:10:54,930
was also mostly comprised of American English,

168
00:10:55,250 --> 00:10:59,620
then to anybody this model will look like it does extremely well, right,

169
00:10:59,620 --> 00:11:04,050
it can detect, it can recognize American English voices with extremely high accuracy

170
00:11:04,130 --> 00:11:06,120
and therefore is deployed into the real world,

171
00:11:06,590 --> 00:11:09,010
but what about its accuracy on subgroups,

172
00:11:09,010 --> 00:11:13,440
on accented voices, on people who for whom English is not their first language,

173
00:11:13,880 --> 00:11:17,130
if we don't also test on subgroups in our evaluation metrics,

174
00:11:17,390 --> 00:11:19,320
we're going to face evaluation bias.

175
00:11:20,640 --> 00:11:23,540
So now let's talk about another example in the real world

176
00:11:23,540 --> 00:11:27,640
of how bias can perpetuate throughout the course of this artificial intelligence life cycle,

177
00:11:30,310 --> 00:11:32,720
commercial facial detection systems are everywhere,

178
00:11:32,890 --> 00:11:36,090
you actually played around with some of them in lab two,

179
00:11:36,090 --> 00:11:39,440
when you trained your VAE on a facial detection data set.

180
00:11:40,520 --> 00:11:42,820
In addition to the lock screens on your cell phones,

181
00:11:44,040 --> 00:11:47,350
facial detection systems are also present in the automatic filters,

182
00:11:47,370 --> 00:11:48,650
that your phone cameras apply,

183
00:11:48,650 --> 00:11:49,810
whenever you try to take a picture,

184
00:11:50,430 --> 00:11:52,840
and they're also used in criminal investigations.

185
00:11:54,120 --> 00:11:57,700
These are three commercial facial detection systems that were deployed,

186
00:11:57,780 --> 00:12:03,670
and we'll analyze the biases that might have been present in all of them for in the next few minutes.

187
00:12:05,730 --> 00:12:07,595
So the first thing you may notice is that,

188
00:12:07,595 --> 00:12:13,420
there is a huge accuracy gap between two different demographics in this plot,

189
00:12:14,520 --> 00:12:17,450
this accuracy gap can get up to 34%,

190
00:12:17,980 --> 00:12:22,220
keep in mind, that this facial detection is a binary classification task,

191
00:12:22,720 --> 00:12:24,650
everything is either a face or it's not a face,

192
00:12:25,240 --> 00:12:30,530
this means that a randomly initialized model would be expected to have an accuracy of 50%,

193
00:12:31,270 --> 00:12:34,280
because it's going to randomly assign whether or not something is a face or not,

194
00:12:35,150 --> 00:12:39,240
some of these facial detection classifiers do only barely better than random

195
00:12:39,290 --> 00:12:44,970
on these underrepresented data, on these underrepresented samples in this population.

196
00:12:46,850 --> 00:12:48,240
So how did this happen,

197
00:12:48,620 --> 00:12:52,830
why is there such a blatant gap in accuracy between these different demographic groups,

198
00:12:53,210 --> 00:12:56,490
and how did these models ever get deployed in the first place,

199
00:12:56,960 --> 00:12:59,610
what types of biases were present in these models.

200
00:13:01,310 --> 00:13:05,760
So a lot of facial detection systems exhibit very clear selection bias,

201
00:13:06,050 --> 00:13:09,210
this model was likely trained mostly on lighter skin faces

202
00:13:09,560 --> 00:13:14,220
and therefore learned those much more effectively than it learned to classify darker skin faces.

203
00:13:15,020 --> 00:13:16,920
But that's not the only bias that was present,

204
00:13:17,600 --> 00:13:24,090
the second bias that's often very present in facial detection systems is evaluation bias,

205
00:13:24,530 --> 00:13:28,015
because originally this data set that you see on the screen

206
00:13:28,015 --> 00:13:30,510
is not the data set that these models were evaluated on,

207
00:13:30,800 --> 00:13:33,870
they were evaluated on one big bulk data set,

208
00:13:33,980 --> 00:13:36,810
without any classification into subgroups at all,

209
00:13:37,130 --> 00:13:38,550
and therefore you can imagine,

210
00:13:38,570 --> 00:13:41,520
if the dataset was also comprised mostly of lighter skinned faces,

211
00:13:41,900 --> 00:13:44,340
these accuracy metrics would be incredibly inflated

212
00:13:44,390 --> 00:13:46,620
and therefore would cause unnecessary confidence

213
00:13:46,970 --> 00:13:49,020
and we could deploy them into the real world.

214
00:13:50,660 --> 00:13:53,845
In fact, the biases in these models were only uncovered,

215
00:13:53,845 --> 00:13:57,090
once an independent study actually constructed a data set,

216
00:13:57,170 --> 00:14:01,060
that is specifically designed to uncover these sorts of biases,

217
00:14:01,060 --> 00:14:03,060
by balancing across race and gender.

218
00:14:04,810 --> 00:14:07,620
However, there are other ways that data sets can be biased,

219
00:14:07,620 --> 00:14:08,960
that we haven't yet talked about.

220
00:14:11,060 --> 00:14:15,400
So, so far we've assumed a pretty key assumption in our data set,

221
00:14:15,400 --> 00:14:20,310
which is that the number of faces in our data is the exact same as the number of non faces in our data,

222
00:14:20,780 --> 00:14:21,600
but you can imagine,

223
00:14:21,680 --> 00:14:23,890
especially if you're looking at things like security feeds,

224
00:14:23,890 --> 00:14:25,230
this might not always be the case,

225
00:14:25,550 --> 00:14:29,880
you might be faced with many more negative samples than positive samples in your data set.

226
00:14:31,470 --> 00:14:32,110
In the most,

227
00:14:32,160 --> 00:14:33,610
so what's the problem here,

228
00:14:34,260 --> 00:14:35,620
in the most extreme case,

229
00:14:35,760 --> 00:14:39,670
we may assign the label non-face to every item in the data set,

230
00:14:40,140 --> 00:14:42,640
because the model sees items that are labeled as faces,

231
00:14:42,780 --> 00:14:45,220
so infrequently that it isn't able to learn,

232
00:14:45,930 --> 00:14:49,420
an accurate class boundary between the two sample, between the two classes.

233
00:14:52,490 --> 00:14:54,280
So how can we mitigate this,

234
00:14:54,280 --> 00:14:55,570
this is a really big problem

235
00:14:55,570 --> 00:14:59,820
and it's very common across a lot of different types of machine learning tasks and data sets.

236
00:15:00,440 --> 00:15:05,430
And the first way that we can try to mitigate class imbalance is using sample re-weighting,

237
00:15:05,660 --> 00:15:09,810
which is when instead of uniformly sampling from our data set at a rate,

238
00:15:10,070 --> 00:15:15,300
we instead sample at a rate that is inversely proportional to the incidence of a class in our data set,

239
00:15:15,920 --> 00:15:17,340
so in the previous example,

240
00:15:17,630 --> 00:15:20,430
if the likelihood if faces were much,

241
00:15:21,020 --> 00:15:25,140
if the number of faces was much lower than the number of non faces in our data set,

242
00:15:25,250 --> 00:15:28,740
we would sample the faces with a higher probability than the negatives,

243
00:15:28,820 --> 00:15:30,840
so that the model sees both classes equally.

244
00:15:33,160 --> 00:15:37,460
The second example, the second way we can mitigate class imbalance is through loss re-rating,

245
00:15:38,020 --> 00:15:45,165
which is when instead of having every single mistake that the model makes contribute equally to our total loss function,

246
00:15:45,165 --> 00:15:46,455
we're weight the samples,

247
00:15:46,455 --> 00:15:50,690
such that samples from underrepresented classes contribute more to the loss function,

248
00:15:51,250 --> 00:15:56,720
so instead of the model assigning every single input face to a as a negative,

249
00:15:57,400 --> 00:15:59,570
it'll be highly penalized, if it does so,

250
00:15:59,830 --> 00:16:05,180
because the loss of the faces would contribute more to the total loss function than the loss of the negatives.

251
00:16:07,210 --> 00:16:12,140
And the final way that we can mitigate class imbalance is through batch selection,

252
00:16:12,460 --> 00:16:14,480
which is when we choose randomly from classes,

253
00:16:14,740 --> 00:16:18,590
so that every single batch has an equal number of data points per class.

254
00:16:22,300 --> 00:16:23,900
So is everything solved,

255
00:16:25,510 --> 00:16:29,175
clearly, there are other forms of bias that exist,

256
00:16:29,175 --> 00:16:32,120
even when the classes are completely balanced,

257
00:16:32,530 --> 00:16:36,020
because the thing that we haven't thought about yet is latent features.

258
00:16:37,390 --> 00:16:40,460
So if you remember from lab two and the last lecture,

259
00:16:40,900 --> 00:16:46,850
latent features are the actual represent, is the actual representation of this image according to the model.

260
00:16:47,620 --> 00:16:52,910
And so far we've mitigated the problem of when we know that we have underrepresented classes,

261
00:16:53,410 --> 00:16:58,370
but we haven't mitigated the problem of when we have a lot of variability within the same class.

262
00:16:59,110 --> 00:17:02,990
Let's say we have an equal number of faces and negative examples in our data set,

263
00:17:03,580 --> 00:17:07,005
what happens if the majority of the faces are from a certain demographic

264
00:17:07,005 --> 00:17:08,360
or they have a certain set of features,

265
00:17:09,190 --> 00:17:11,510
can we still apply the techniques that we just learned about,

266
00:17:12,300 --> 00:17:14,080
the answer is that we cannot do this,

267
00:17:14,400 --> 00:17:18,190
and the problem is that the bias present right now is in our latent features.

268
00:17:19,570 --> 00:17:22,400
All of these images are labeled with the exact same label,

269
00:17:22,600 --> 00:17:27,380
so according to as the model, all we know is that they're all faces,

270
00:17:28,210 --> 00:17:32,090
so we have no information about any of these features, only from the label,

271
00:17:33,130 --> 00:17:38,625
therefore we can't apply any of the previous approaches that we used to mitigate class imbalanced,

272
00:17:38,625 --> 00:17:40,125
because our classes are balanced,

273
00:17:40,125 --> 00:17:41,720
but we have feature imbalance now.

274
00:17:42,640 --> 00:17:46,910
However, we can adapt the previous methods to account for bias in latent features,

275
00:17:47,200 --> 00:17:49,340
which we'll do in just a few slides.

276
00:17:52,060 --> 00:17:54,140
So let's unpack this a little bit further,

277
00:17:54,490 --> 00:17:56,810
we have our potentially biased data set,

278
00:17:56,980 --> 00:17:59,310
and we're trying to build and deploy a model,

279
00:17:59,310 --> 00:18:02,300
that classifies the faces in a traditional training pipeline,

280
00:18:02,800 --> 00:18:04,500
this is what that pipeline would look like,

281
00:18:04,500 --> 00:18:05,775
We would train our classifier

282
00:18:05,775 --> 00:18:07,430
and we would deploy it into the real world,

283
00:18:07,720 --> 00:18:11,090
but this training pipeline doesn't debias our inputs in any way.

284
00:18:13,440 --> 00:18:18,160
So one thing we could do is label our biased features and then apply resampling.

285
00:18:18,510 --> 00:18:19,750
So, let's say in reality,

286
00:18:20,040 --> 00:18:22,270
that this data set was biased on hair color,

287
00:18:22,590 --> 00:18:25,240
most of the data set is made up of people with blonde hair,

288
00:18:25,410 --> 00:18:28,300
with faces with black hair and red hair underrepresented,

289
00:18:29,160 --> 00:18:30,490
if we knew this information,

290
00:18:30,840 --> 00:18:34,270
we could label the hair color of every single person in this data set,

291
00:18:34,410 --> 00:18:37,265
and we could apply either sample weighting or loss re-weighting,

292
00:18:37,265 --> 00:18:38,230
just as we did previously.

293
00:18:39,720 --> 00:18:42,010
But does anyone want to tell me what the problem is here?

294
00:18:45,920 --> 00:18:48,700
You go through each samples, it takes a lot of time.

295
00:18:48,970 --> 00:18:51,270
Yeah, so there are a couple problems here,

296
00:18:51,270 --> 00:18:52,580
and that's definitely one of them.

297
00:18:53,080 --> 00:18:53,870
The first is,

298
00:18:53,920 --> 00:18:57,260
how do we know that hair color is a biased feature in this data set,

299
00:18:57,610 --> 00:19:01,260
unless we visually inspect every single sample in this data set,

300
00:19:01,260 --> 00:19:03,470
we're not going to know what the biased features are.

301
00:19:04,180 --> 00:19:06,170
And the second thing is exactly what you said,

302
00:19:06,340 --> 00:19:08,540
which is once we have our biased features,

303
00:19:08,890 --> 00:19:13,880
going through and annotating every image with this feature is an extremely labor intensive task,

304
00:19:14,080 --> 00:19:15,740
that is infeasible in the real world.

305
00:19:17,370 --> 00:19:18,760
So now the question is,

306
00:19:19,050 --> 00:19:22,420
what if we had a way to automatically learn latent features

307
00:19:22,890 --> 00:19:26,290
and use this learn feature representation to debias a model.

308
00:19:29,460 --> 00:19:32,950
So what we want is a way to learn the features of this data set

309
00:19:33,060 --> 00:19:39,490
and then automatically determine the samples with the highest feature bias and the samples with the lowest feature bias,

310
00:19:40,140 --> 00:19:42,130
we've already learned a method of doing this,

311
00:19:42,450 --> 00:19:46,720
in the generative modeling lecture, you all learned about variational autoencoders,

312
00:19:46,860 --> 00:19:49,870
which are models that learn the latent features of a data set,

313
00:19:50,640 --> 00:19:56,650
as a recap, variational autoencoders work by probabilistically sampling from a learn latent space,

314
00:19:57,000 --> 00:20:01,630
and then they decode this new latent vector into back into the original input space,

315
00:20:01,860 --> 00:20:04,805
measure the reconstruction loss between the inputs and the outputs,

316
00:20:04,805 --> 00:20:07,690
and continue to update their representation of the latent space.

317
00:20:08,590 --> 00:20:11,270
And the reason why we care so much about this latent space

318
00:20:11,440 --> 00:20:14,990
is that we want samples that are similar to each other in the input

319
00:20:15,160 --> 00:20:19,250
to decode to latent vectors that are very close to each other in this latent space,

320
00:20:19,690 --> 00:20:24,170
and samples that are far from each other or samples that are dissimilar to each other in the input

321
00:20:24,400 --> 00:20:29,150
should decode encode to latent vectors that are far from each other in the latent space.

322
00:20:32,090 --> 00:20:35,455
So now we'll walk through step by step a debiasing algorithm,

323
00:20:35,455 --> 00:20:40,560
that automatically uses the latent features learned by a variational autoencoder,

324
00:20:40,640 --> 00:20:44,460
to under sample and over sample from regions in our data set.

325
00:20:45,140 --> 00:20:46,225
Before I start,

326
00:20:46,225 --> 00:20:50,790
I want to point out that this debiasing model is actually the foundation of Themis's work,

327
00:20:51,200 --> 00:20:54,150
this work comes out of a paper that we published a few years ago,

328
00:20:54,230 --> 00:20:57,690
that has been demonstrated to debias commercial facial detection algorithms,

329
00:20:57,830 --> 00:20:59,995
and it was so impactful,

330
00:20:59,995 --> 00:21:04,135
that we decided to make it available and work with companies and industries,

331
00:21:04,135 --> 00:21:05,490
and that's how Themis was started.

332
00:21:06,700 --> 00:21:10,250
So let's first start by training a VAE on this data set,

333
00:21:10,630 --> 00:21:14,420
the z shown here in this diagram ends up being our latent space,

334
00:21:14,800 --> 00:21:19,550
and the latent space automatically captures features that were important for classification.

335
00:21:20,870 --> 00:21:24,090
So here's an example, latent feature that this model captured,

336
00:21:24,530 --> 00:21:26,970
this is the facial position of an input face,

337
00:21:27,440 --> 00:21:30,220
and something that's really crucial here is that,

338
00:21:30,220 --> 00:21:31,620
we never told the model

339
00:21:31,940 --> 00:21:38,460
to calculate, to encode the feature vector of the facial position of a given face,

340
00:21:38,720 --> 00:21:40,350
it learned this automatically,

341
00:21:40,970 --> 00:21:43,105
because this feature is important for the model

342
00:21:43,105 --> 00:21:46,740
to develop a good representation of what a face actually is.

343
00:21:48,660 --> 00:21:50,590
So now that we have our latent structure,

344
00:21:50,910 --> 00:21:55,540
we can use it to calculate a distribution of the inputs across every latent variable,

345
00:21:56,070 --> 00:21:58,270
and we can estimate a probability distribution

346
00:21:58,830 --> 00:22:03,130
depending on that's based on the features of every item in this data set,

347
00:22:03,870 --> 00:22:05,525
essentially, what this means is that,

348
00:22:05,525 --> 00:22:10,240
we can calculate the probability that a certain combination of features appears in our data set

349
00:22:10,380 --> 00:22:13,030
based on the latent space that we just learned,

350
00:22:13,640 --> 00:22:17,375
and then we can over sample denser, sparser areas of this data set

351
00:22:17,375 --> 00:22:19,930
and under sample from denser areas of this data set.

352
00:22:20,770 --> 00:22:23,355
So let's say our distribution looks something like this,

353
00:22:23,355 --> 00:22:24,500
this is an oversimplification,

354
00:22:24,610 --> 00:22:25,940
but for visualization purposes

355
00:22:26,590 --> 00:22:29,240
and the denser portions of this data set,

356
00:22:29,380 --> 00:22:35,000
we would expect to have a homogeneous skin color and pose in hair color and very good lighting,

357
00:22:35,770 --> 00:22:38,000
and then in the sparser portions of this data set,

358
00:22:38,080 --> 00:22:41,840
we would expect to see diverse skin color, pose and illumination,

359
00:22:46,280 --> 00:22:48,030
so now that we have this distribution

360
00:22:48,080 --> 00:22:52,380
and we know what areas of our distribution are dense and which areas are sparse,

361
00:22:52,550 --> 00:22:58,740
we want to under sample areas from the, under sample samples that fall in the denser areas of this distribution

362
00:22:59,120 --> 00:23:03,720
and over sample data points that fall in the sparser areas of this distribution.

363
00:23:04,760 --> 00:23:05,640
So for example,

364
00:23:05,780 --> 00:23:11,785
we would probably under sample points with the very common skin colors, hair colors and good lighting,

365
00:23:11,785 --> 00:23:13,830
that is extremely present in this data set

366
00:23:13,910 --> 00:23:17,160
and over sample the diverse images that we saw on the last slide,

367
00:23:17,840 --> 00:23:21,090
and this allows us to train in a fair and unbiased manner.

368
00:23:24,170 --> 00:23:28,170
To dig in a little bit more into the math behind how this resampling works,

369
00:23:28,850 --> 00:23:35,010
this approach basically approximates the latent space via a joint histogram over the individual latent variables,

370
00:23:35,450 --> 00:23:40,650
so we have a histogram for every latent variable zi,

371
00:23:41,060 --> 00:23:43,030
and what the histogram essentially does is

372
00:23:43,030 --> 00:23:45,270
it discretizes the continuous distribution,

373
00:23:45,590 --> 00:23:47,790
so that we can calculate probabilities more easily.

374
00:23:49,370 --> 00:23:53,820
Then we multiply the probabilities together across all of the latent distributions,

375
00:23:54,980 --> 00:24:00,840
and then after that we can develop an understanding of the joint distribution of all of the samples in our latent space.

376
00:24:03,170 --> 00:24:03,955
Based on this,

377
00:24:03,955 --> 00:24:08,670
we can define the adjusted probability for sampling for a particular data point as follows,

378
00:24:09,260 --> 00:24:14,100
the probability of selecting a sample data point x will be based on the latent space of x,

379
00:24:14,480 --> 00:24:17,610
such that it is the inverse of the joint approximated distribution,

380
00:24:18,710 --> 00:24:20,385
we have a parameter α here,

381
00:24:20,385 --> 00:24:22,080
which is a biasing parameter,

382
00:24:22,220 --> 00:24:23,520
and as α increases,

383
00:24:23,840 --> 00:24:26,490
this probability will tend to the uniform distribution,

384
00:24:26,870 --> 00:24:29,940
and if α increases, we tend to de bias more strongly.

385
00:24:32,570 --> 00:24:36,580
And this gives us the final weight of the sample in our data set,

386
00:24:36,580 --> 00:24:38,250
that we can calculate on the fly

387
00:24:38,480 --> 00:24:41,460
and use it to adaptively resample while training.

388
00:24:43,770 --> 00:24:46,565
And so once we apply this biasing,

389
00:24:46,565 --> 00:24:48,400
we have pretty remarkable results,

390
00:24:48,750 --> 00:24:50,675
this is the original graph,

391
00:24:50,675 --> 00:24:56,350
that shows the accuracy gap between the darker Males and the lighter Males in this dataset.

392
00:24:57,770 --> 00:24:59,890
Once we apply the debias algorithm,

393
00:24:59,890 --> 00:25:01,225
where as α gets smaller,

394
00:25:01,225 --> 00:25:02,610
we're debiasing more and more,

395
00:25:02,690 --> 00:25:03,840
as we just talked about,

396
00:25:04,610 --> 00:25:06,810
this accuracy gap decreases significantly

397
00:25:07,820 --> 00:25:11,520
and that's because we tend to over sample samples with darker skin color

398
00:25:11,720 --> 00:25:13,765
and therefore the model learns them better

399
00:25:13,765 --> 00:25:15,390
and tends to do better on them.

400
00:25:16,380 --> 00:25:17,680
Keep this algorithm in mind,

401
00:25:17,730 --> 00:25:19,930
because you're going to need it for the lab 3 competition,

402
00:25:20,130 --> 00:25:22,390
which I'll talk more about towards the end of this lecture.

403
00:25:25,420 --> 00:25:30,230
So, so far we've been focusing mainly on facial recognition systems and a couple of other systems

404
00:25:30,340 --> 00:25:32,360
as canonical examples of bias.

405
00:25:33,100 --> 00:25:36,710
However, bias is actually far more widespread in machine learning,

406
00:25:37,300 --> 00:25:39,260
consider the example of autonomous driving,

407
00:25:39,760 --> 00:25:44,390
many data sets are comprised mainly of cars driving down straight and sunny roads

408
00:25:44,440 --> 00:25:47,390
in really good weather conditions with very high visibility,

409
00:25:47,710 --> 00:25:50,865
and this is because the data for these cars for these algorithms

410
00:25:50,865 --> 00:25:53,990
is actually just collected by cars driving down roads,

411
00:25:55,220 --> 00:25:57,450
however, in some specific cases,

412
00:25:57,560 --> 00:26:02,820
you're going to face adverse weather, bad, bad visibility, near collision scenarios,

413
00:26:02,900 --> 00:26:06,510
and these are actually the samples that are the most important for the model to learn,

414
00:26:06,890 --> 00:26:08,275
because they're the hardest samples

415
00:26:08,275 --> 00:26:10,830
and they're the samples where the model is most likely to fail.

416
00:26:11,900 --> 00:26:14,350
But in a traditional autonomous driving pipeline,

417
00:26:14,550 --> 00:26:18,700
these samples are often extremely low, have extremely low representation,

418
00:26:19,260 --> 00:26:24,280
so this is an example where using the unsupervised latent biasing that we just talked about,

419
00:26:24,360 --> 00:26:28,460
we would be able to up sample these important data points

420
00:26:28,460 --> 00:26:32,560
and under sample the data points of driving down straight and sunny roads.

421
00:26:35,590 --> 00:26:38,630
Similarly, consider the example of large language models,

422
00:26:39,370 --> 00:26:43,080
an extremely famous paper a couple years ago showed that,

423
00:26:43,080 --> 00:26:48,800
if you put terms that imply female or woman into a large language model powered job search engine,

424
00:26:49,090 --> 00:26:52,580
you're going to get roles such as artist or things in the humanities,

425
00:26:53,080 --> 00:26:56,415
but if you input similar things, but of the male counterpart,

426
00:26:56,415 --> 00:26:58,790
you put things like male into the the search engine,

427
00:26:59,230 --> 00:27:02,090
you'll end up with roles for scientists and engineers,

428
00:27:02,800 --> 00:27:06,050
so this type of bias also occurs,

429
00:27:06,250 --> 00:27:09,140
regardless of the task at hand for a specific model.

430
00:27:11,190 --> 00:27:14,350
And finally, let's talk about health care recommendation algorithms,

431
00:27:14,610 --> 00:27:17,740
these recommendation algorithms tend to amplify racial biases,

432
00:27:17,910 --> 00:27:20,165
a paper from a couple years ago showed that,

433
00:27:20,165 --> 00:27:23,990
black patients need to be significantly sicker than their white counterparts

434
00:27:23,990 --> 00:27:25,480
to get the same level of care,

435
00:27:25,830 --> 00:27:29,260
and that's because of inherent bias in the data set of this model.

436
00:27:29,730 --> 00:27:31,330
And so in all of these examples,

437
00:27:31,380 --> 00:27:34,420
we can use the above algorithmic bias mitigation method

438
00:27:34,590 --> 00:27:36,760
to try and solve these problems and more.

439
00:27:40,360 --> 00:27:44,810
So we just went through how to mitigate some forms of bias in artificial intelligence

440
00:27:45,040 --> 00:27:46,880
and where these solutions may be applied,

441
00:27:47,470 --> 00:27:50,720
and we talked about a foundational algorithm that Themis uses,

442
00:27:50,830 --> 00:27:52,670
that you all will also be developing today.

443
00:27:53,410 --> 00:27:54,980
And for the next part of the lecture,

444
00:27:55,360 --> 00:27:56,870
we'll focus on uncertainty

445
00:27:57,190 --> 00:27:59,420
or when a model does not know the answer,

446
00:28:00,560 --> 00:28:02,560
we'll talk about why uncertainty is important

447
00:28:02,850 --> 00:28:04,565
and how we can estimate it,

448
00:28:04,565 --> 00:28:07,270
and also the applications of uncertainty estimation.

449
00:28:08,600 --> 00:28:10,890
So, to start with, what is uncertainty

450
00:28:11,060 --> 00:28:12,810
and why is it necessary to compute.

451
00:28:13,720 --> 00:28:15,090
Let's look at the following example,

452
00:28:15,560 --> 00:28:19,650
this is a binary classifier that is trained on images of cats and dogs,

453
00:28:20,240 --> 00:28:21,580
for every single input,

454
00:28:21,580 --> 00:28:25,020
it will output a probability distribution over these two classes.

455
00:28:27,610 --> 00:28:30,020
Now, let's say I give this model an image of a horse,

456
00:28:30,490 --> 00:28:31,790
it's never seen a horse before,

457
00:28:32,080 --> 00:28:34,490
the horse is clearly neither a cat nor a dog,

458
00:28:35,110 --> 00:28:38,960
however the model has no choice, but to output a probability distribution,

459
00:28:39,190 --> 00:28:40,910
because that's how this model is structured.

460
00:28:42,810 --> 00:28:45,010
However, what if in addition to this prediction,

461
00:28:45,300 --> 00:28:47,980
we also achieved a confidence estimate,

462
00:28:48,510 --> 00:28:50,950
in this case, the model which should be able to say,

463
00:28:51,630 --> 00:28:53,330
I've never seen anything like this before

464
00:28:53,330 --> 00:28:55,600
and I have very low confidence in this prediction,

465
00:28:55,890 --> 00:28:58,840
so you as the user should not trust my prediction on this model,

466
00:28:59,400 --> 00:29:02,050
and that's the core idea behind uncertainty estimation.

467
00:29:03,700 --> 00:29:04,880
So in the real world,

468
00:29:05,050 --> 00:29:07,790
uncertainty estimation is useful for scenarios like this,

469
00:29:08,230 --> 00:29:12,590
this is an example of a Tesla car driving behind a horse drawn buggy,

470
00:29:13,030 --> 00:29:15,320
which are very common in some parts of the United States,

471
00:29:15,760 --> 00:29:18,195
it has no idea what this horse drawn buggy is,

472
00:29:18,195 --> 00:29:21,770
it first thinks it's a truck and then a car and then a person,

473
00:29:22,120 --> 00:29:25,650
and it continues to output predictions,

474
00:29:25,650 --> 00:29:29,840
even though it is very clear that the model does not know what this image is.

475
00:29:32,160 --> 00:29:33,220
And now you might be asking,

476
00:29:33,450 --> 00:29:35,230
okay, so what's the big deal,

477
00:29:35,400 --> 00:29:37,510
it didn't recognize the horse drawn buggy,

478
00:29:37,620 --> 00:29:40,300
but it seemed to drive successfully anyway,

479
00:29:41,010 --> 00:29:44,320
however, the exact same problem that resulted in that video

480
00:29:44,670 --> 00:29:48,160
has also resulted in numerous autonomous car crashes.

481
00:29:51,210 --> 00:29:54,010
So let's go through why something like this might have happened,

482
00:29:54,720 --> 00:29:57,730
there are multiple different types of uncertainty in neural networks,

483
00:29:58,170 --> 00:30:00,790
which may cause incidents like the ones that we just saw,

484
00:30:01,320 --> 00:30:05,200
we'll go through a simple example that illustrates the two main types of uncertainty,

485
00:30:05,280 --> 00:30:06,850
that we'll focus on in this lecture.

486
00:30:09,980 --> 00:30:13,795
So let's say I'm trying to estimate the curve y equals x cubed

487
00:30:13,795 --> 00:30:15,210
as part of a regression task,

488
00:30:15,620 --> 00:30:18,780
the input here, x is some real number,

489
00:30:18,920 --> 00:30:21,550
and we want it to output f of x,

490
00:30:21,550 --> 00:30:24,090
which should be ideally x cubed.

491
00:30:24,990 --> 00:30:29,380
So right away you might notice that, there are some issues in this data set,

492
00:30:29,400 --> 00:30:32,830
assume the red points in this image are your training samples,

493
00:30:37,040 --> 00:30:41,950
so the box area of this image shows data points in our data set,

494
00:30:41,950 --> 00:30:43,560
where we have really high noise,

495
00:30:44,060 --> 00:30:46,945
these points do not follow the curve y equals x cubed,

496
00:30:46,945 --> 00:30:49,620
in fact, they don't really seem to follow any distribution at all,

497
00:30:50,120 --> 00:30:59,040
and the model won't be able to compute outputs for points in this region accurately,

498
00:30:59,330 --> 00:31:03,180
because very similar inputs have extremely different outputs,

499
00:31:03,230 --> 00:31:05,340
which is the definition of data uncertainty.

500
00:31:09,720 --> 00:31:12,820
We also have regions in this data set, where we have no data,

501
00:31:13,320 --> 00:31:18,190
so if we queried the model for a prediction in this part of in this region of the data set,

502
00:31:18,330 --> 00:31:21,130
we should not really expect to see an accurate result,

503
00:31:21,180 --> 00:31:23,230
because the model's never seen anything like this before,

504
00:31:24,000 --> 00:31:25,990
and this is what is called model uncertainty,

505
00:31:26,430 --> 00:31:28,325
when the model hasn't seen enough data points

506
00:31:28,325 --> 00:31:31,030
or cannot estimate that area of the input distribution

507
00:31:31,830 --> 00:31:34,810
accurately enough to output a correct prediction.

508
00:31:37,320 --> 00:31:38,560
So what would happen,

509
00:31:38,580 --> 00:31:41,110
if I added the following blue training points

510
00:31:41,340 --> 00:31:45,550
to the areas of the data set with high model uncertainty,

511
00:31:46,200 --> 00:31:48,790
do you think the model uncertainty would decrease,

512
00:31:49,230 --> 00:31:49,870
raise your hand.

513
00:31:52,020 --> 00:31:53,440
Does anyone think it would not change?

514
00:31:55,520 --> 00:31:57,330
Okay, so yeah, most of you were correct,

515
00:31:57,740 --> 00:32:00,010
model uncertainty can typically be reduced

516
00:32:00,010 --> 00:32:02,335
by adding in data into any region,

517
00:32:02,335 --> 00:32:04,650
but specifically regions with high model uncertainty.

518
00:32:06,020 --> 00:32:12,330
And now, what happens if we add these blue data points into this data set,

519
00:32:12,860 --> 00:32:15,900
would anyone expect the data uncertainty to decrease,

520
00:32:15,980 --> 00:32:16,710
raise your hand.

521
00:32:18,920 --> 00:32:22,350
That's correct, so data uncertainty is irreducible,

522
00:32:22,550 --> 00:32:23,490
in the real world,

523
00:32:23,690 --> 00:32:29,790
the blue points and the noisy red points on this image correspond to things like robot sensors,

524
00:32:30,020 --> 00:32:31,705
let's say I I have a robot,

525
00:32:31,705 --> 00:32:36,750
that's trained to has a sensor that is making measurements of depth,

526
00:32:37,130 --> 00:32:39,600
if the sensor has noise in it,

527
00:32:39,650 --> 00:32:44,460
there's no way that I can add any more data into the system to reduce that noise,

528
00:32:44,540 --> 00:32:46,290
unless I replace my sensor entirely.

529
00:32:49,770 --> 00:32:54,250
So now let's assign some names to the types of uncertainty that we just talked about,

530
00:32:54,570 --> 00:32:59,680
the blue area or the area of high data uncertainty is known as aleatoric uncertainty,

531
00:33:00,480 --> 00:33:02,530
it is irreducible, as we just mentioned,

532
00:33:03,090 --> 00:33:05,350
and it can be directly learned from data,

533
00:33:05,370 --> 00:33:06,880
which we'll talk about in a little bit.

534
00:33:08,650 --> 00:33:12,200
The green areas of, the green boxes that we talked about,

535
00:33:12,280 --> 00:33:15,860
which were model uncertainty, are known as epistemic uncertainty,

536
00:33:16,450 --> 00:33:18,830
and this cannot be learned directly from the data,

537
00:33:19,330 --> 00:33:23,570
however, it can be reduced by adding more data into our systems into these regions.

538
00:33:30,410 --> 00:33:33,210
Okay, so first let's go through aleatoric uncertainty,

539
00:33:34,340 --> 00:33:38,095
so the goal of estimating aleatoric uncertainty is

540
00:33:38,095 --> 00:33:41,550
to learn a set of variances that correspond to the input,

541
00:33:42,490 --> 00:33:45,170
keep in mind that we are not looking at a data distribution

542
00:33:45,310 --> 00:33:47,925
and we are as humans are not estimating the variance,

543
00:33:47,925 --> 00:33:49,880
we're training the model to do this task,

544
00:33:50,470 --> 00:33:52,430
and so what that means is typically,

545
00:33:52,870 --> 00:33:53,900
when we train a model,

546
00:33:53,950 --> 00:33:55,130
we give it an input x

547
00:33:55,330 --> 00:33:57,380
and we expect an output ŷ,

548
00:33:57,430 --> 00:33:58,940
which is the prediction of the model,

549
00:34:00,100 --> 00:34:03,200
now we also predict an additional σ squared,

550
00:34:03,280 --> 00:34:05,120
so we add another layer to our model,

551
00:34:05,530 --> 00:34:06,860
we have the same output size

552
00:34:06,970 --> 00:34:09,260
that predicts a variance for every output.

553
00:34:11,230 --> 00:34:13,140
So the reason why we do this is that,

554
00:34:13,140 --> 00:34:18,620
we expect that areas in our data set with high data uncertainty are going to have higher variance,

555
00:34:20,650 --> 00:34:22,980
and the crucial thing to remember here is that,

556
00:34:22,980 --> 00:34:24,710
this variance is not constant,

557
00:34:25,270 --> 00:34:27,200
it depends on the value of x,

558
00:34:27,490 --> 00:34:30,240
we typically tend to think of variance as a single number,

559
00:34:30,240 --> 00:34:32,240
that parameterizes an entire distribution,

560
00:34:32,950 --> 00:34:34,220
however, in this case,

561
00:34:34,420 --> 00:34:37,880
we may have areas of our input distribution with really high variance,

562
00:34:37,930 --> 00:34:40,010
and we may have areas with very low variance,

563
00:34:40,270 --> 00:34:43,095
so our variance cannot be independent of the input,

564
00:34:43,095 --> 00:34:44,930
and it depends on our input x.

565
00:34:47,250 --> 00:34:48,830
So now that we have this model,

566
00:34:48,830 --> 00:34:50,705
we have an extra layer attached to it,

567
00:34:50,705 --> 00:34:54,100
in addition to predicting ŷ, we also predict a σ squared,

568
00:34:54,810 --> 00:34:56,050
how do we train this model.

569
00:34:58,390 --> 00:35:02,300
Our current loss function does not take into account variance at any point,

570
00:35:02,380 --> 00:35:06,560
this is your typical mean squared error loss function that is used to train regression models,

571
00:35:06,970 --> 00:35:09,380
and there's no way training from this loss function,

572
00:35:09,670 --> 00:35:13,220
that we can learn whether or not the variance that we're estimating is accurate.

573
00:35:15,210 --> 00:35:19,930
So in addition to adding another layer to estimate aleatoric uncertainty correctly,

574
00:35:20,100 --> 00:35:22,030
we also have to change our loss function.

575
00:35:24,340 --> 00:35:30,045
So the mean squared error actually learns a multivariate Gaussian

576
00:35:30,045 --> 00:35:33,110
with a mean yi and constant variance,

577
00:35:33,220 --> 00:35:37,460
and we want to generalize this loss function to when we don't have constant variance,

578
00:35:40,010 --> 00:35:44,070
and the way we do this is by changing the loss function to the negative log likelihood,

579
00:35:44,330 --> 00:35:50,220
we can think about this for now as a generalization of the mean squared error loss to non constant variances,

580
00:35:50,720 --> 00:35:53,550
so now that we have a σ squared term in our loss function,

581
00:35:53,990 --> 00:35:57,990
we can determine how accurately the σ and the y that we're predicting,

582
00:35:58,220 --> 00:36:00,750
parameterize the distribution that is our input.

583
00:36:04,510 --> 00:36:07,370
So now that we know how to estimate aleatoric uncertainty,

584
00:36:07,750 --> 00:36:09,470
let's look at a real world example,

585
00:36:10,420 --> 00:36:13,430
for this task, we'll focus on semantic segmentation,

586
00:36:13,900 --> 00:36:17,960
which is when we label every pixel of an image with its corresponding class,

587
00:36:18,820 --> 00:36:20,565
we do this for scene understanding

588
00:36:20,565 --> 00:36:24,260
and because it is more fine grained than a typical object detection algorithm.

589
00:36:25,300 --> 00:36:30,170
So the inputs of this to this data set are known as it's from a data set called citycapes,

590
00:36:30,400 --> 00:36:33,200
and the inputs are RGB images of scenes,

591
00:36:34,030 --> 00:36:37,700
the labels are pixel wise annotations of this entire image,

592
00:36:37,780 --> 00:36:40,250
of which label every pixel belongs to,

593
00:36:40,930 --> 00:36:42,855
and the outputs try to mimic the labels,

594
00:36:42,855 --> 00:36:45,110
there are also predicted pixel wise masks,

595
00:36:45,850 --> 00:36:51,080
so why would we expect that this data set has high natural aleatoric uncertainty,

596
00:36:51,460 --> 00:36:54,890
and which parts of this data set do you think would have aleatoric uncertainty.

597
00:36:58,880 --> 00:37:03,270
Because labeling every single pixel of an image is such a labor intensive task,

598
00:37:03,500 --> 00:37:05,880
and it's also very hard to do accurately,

599
00:37:06,080 --> 00:37:12,480
we would expect that the boundaries between, between objects in this image have high aleatoric uncertainty,

600
00:37:13,040 --> 00:37:14,490
and that's exactly what we see,

601
00:37:14,570 --> 00:37:18,090
if you train a model to predict aleatoric uncertainty on this data set,

602
00:37:18,380 --> 00:37:21,270
corners and boundaries have the highest aleatoric uncertainty,

603
00:37:21,710 --> 00:37:25,230
because even if your pixels are like one row off or one column off,

604
00:37:25,430 --> 00:37:27,120
that introduces noise into the model,

605
00:37:27,620 --> 00:37:29,980
the model can still learn in the face of this noise,

606
00:37:30,090 --> 00:37:32,320
but it does exist and it can't be reduced.

607
00:37:36,240 --> 00:37:40,270
So now that we know about data uncertainty or aleatoric uncertainty,

608
00:37:40,950 --> 00:37:43,630
let's move on to learning about epistemic uncertainty.

609
00:37:44,280 --> 00:37:49,720
As a recap, epistemic uncertainty can best be described as uncertainty in the model itself,

610
00:37:50,130 --> 00:37:52,690
and it is reducible by adding data to the model.

611
00:37:56,360 --> 00:37:57,930
So with epistemic uncertainty,

612
00:37:58,040 --> 00:37:59,820
essentially what we're trying to ask is,

613
00:37:59,990 --> 00:38:02,610
is the model unconfident about a prediction.

614
00:38:03,200 --> 00:38:07,020
So a really simple and very smart way to do this is,

615
00:38:07,340 --> 00:38:11,460
let's say I train the same network multiple times with random initializations

616
00:38:11,930 --> 00:38:15,900
and I ask it to predict the exact, I call it on the same input.

617
00:38:16,340 --> 00:38:19,405
So let's say I give model 1 the exact same input,

618
00:38:19,405 --> 00:38:21,810
and the blue X is the output of this model,

619
00:38:23,270 --> 00:38:25,830
and then I do the same thing again with model 2,

620
00:38:26,990 --> 00:38:28,530
and then again with model 3,

621
00:38:29,630 --> 00:38:30,900
and again with model 4,

622
00:38:31,100 --> 00:38:35,100
these models all have the exact same hyper parameters, the exact same architecture,

623
00:38:35,450 --> 00:38:37,110
and their train in the same way,

624
00:38:37,310 --> 00:38:39,220
the only difference between them is that,

625
00:38:39,220 --> 00:38:41,370
their weights are all randomly initialized,

626
00:38:41,540 --> 00:38:43,530
so where they start from is different.

627
00:38:45,330 --> 00:38:49,640
And the reason why we can use this to determine epistemic uncertainty is,

628
00:38:49,640 --> 00:38:53,500
because we would expect, that with familiar inputs in our network,

629
00:38:53,880 --> 00:38:56,860
our networks should all converge to around the same answer,

630
00:38:57,150 --> 00:39:02,140
and we should see very little variance in the the log or the outputs that we're predicting.

631
00:39:03,100 --> 00:39:05,940
However, if a model has never seen a specific input before,

632
00:39:06,110 --> 00:39:08,070
or that input is very hard to learn,

633
00:39:08,390 --> 00:39:11,220
all of these models should predict slightly different answers,

634
00:39:11,570 --> 00:39:15,690
and the variance of them should be higher than if they were predicting a similar input.

635
00:39:18,520 --> 00:39:21,950
So creating an ensemble of networks is quite simple, quite simple,

636
00:39:22,210 --> 00:39:25,490
you start out with defining the num_ensembles you want,

637
00:39:25,510 --> 00:39:27,560
you create them all the exact same way,

638
00:39:27,730 --> 00:39:31,580
and then you fit them all on the same training data and training data.

639
00:39:33,090 --> 00:39:35,650
And then afterwards, when at inference time,

640
00:39:35,940 --> 00:39:41,410
we call all of the models, every model in the ensemble on our specific input,

641
00:39:42,180 --> 00:39:47,200
and then we can treat our new prediction as the average of all of the ensembles,

642
00:39:47,280 --> 00:39:50,110
this results in a usually more robust and accurate prediction,

643
00:39:50,850 --> 00:39:55,000
and we can treat the uncertainty as the variance of all of these predictions,

644
00:39:56,070 --> 00:40:00,520
again, remember that if we saw familiar inputs or inputs with low epistemic uncertainty,

645
00:40:00,870 --> 00:40:03,040
we should expect to have very little variance,

646
00:40:03,210 --> 00:40:07,180
and if we had a very unfamiliar input or something that was out of distribution

647
00:40:07,290 --> 00:40:08,980
or something the model hasn't seen before,

648
00:40:09,300 --> 00:40:12,250
we should have very high epistemic uncertainty or variance.

649
00:40:15,260 --> 00:40:17,035
So what's the problem with this,

650
00:40:17,035 --> 00:40:18,970
can anyone raise their hand and tell me,

651
00:40:18,970 --> 00:40:21,210
what a problem with training an ensemble of networks is.

652
00:40:25,440 --> 00:40:28,330
So training an ensemble of networks is really compute expensive,

653
00:40:29,250 --> 00:40:31,180
even if your model is not very large,

654
00:40:31,410 --> 00:40:37,420
training five copies of it or ten copies of it tends to, it takes up compute and time,

655
00:40:37,710 --> 00:40:42,460
and that's just not really feasible when we're training on specific tasks.

656
00:40:43,730 --> 00:40:46,645
However, the key insight for ensembles is that,

657
00:40:46,645 --> 00:40:51,090
by introducing some method of randomness or stochasticity into our networks,

658
00:40:51,530 --> 00:40:53,790
we're able to estimate epistemic uncertainty,

659
00:40:54,740 --> 00:41:01,530
so another way that we've seen about introducing stochasticity into networks is by using dropout layers,

660
00:41:02,030 --> 00:41:05,190
we've seen dropout layers as a method of reducing overfitting,

661
00:41:05,420 --> 00:41:09,120
because we randomly drop out different nodes in our, in our layer,

662
00:41:09,380 --> 00:41:11,965
and then we continue to propagate information through them

663
00:41:11,965 --> 00:41:14,100
and it prevents models from memorizing data.

664
00:41:15,190 --> 00:41:19,190
However, in the case of epistemic uncertainty,

665
00:41:19,660 --> 00:41:23,270
we can add dropout layers after every single layer in our model,

666
00:41:23,620 --> 00:41:27,350
and in addition, we can keep these dropout layers enabled at test time,

667
00:41:27,520 --> 00:41:30,375
usually we don't keep dropout layers enabled at test time,

668
00:41:30,375 --> 00:41:37,490
because we don't want to lose any information about the the network's process or any weights when we're at inference time,

669
00:41:38,050 --> 00:41:40,520
however, when we're estimating epistemic uncertainty,

670
00:41:40,840 --> 00:41:43,340
we do want to keep dropout enabled at test time,

671
00:41:43,480 --> 00:41:47,270
because that's how we can introduce randomness, inference time as well.

672
00:41:48,270 --> 00:41:50,360
So what we do here is we have one model,

673
00:41:50,360 --> 00:41:52,120
it's the same model the entire way through,

674
00:41:52,530 --> 00:41:54,760
we add dropout layers with a specific probability,

675
00:41:54,960 --> 00:41:57,160
and then we run multiple forward passes

676
00:41:57,390 --> 00:41:58,660
and at every forward passes,

677
00:41:58,920 --> 00:42:02,050
different layers get different nodes in a layer get dropped out,

678
00:42:02,280 --> 00:42:05,950
so we have that measure of randomness and stochasticity.

679
00:42:07,930 --> 00:42:09,945
So again, in order to implement this,

680
00:42:09,945 --> 00:42:13,370
what we have is a model with the exact one model,

681
00:42:13,480 --> 00:42:15,590
and then when we're running our forward passes,

682
00:42:15,910 --> 00:42:17,895
we can simply run T forward passes,

683
00:42:17,895 --> 00:42:19,520
where T is usually a number like twenty,

684
00:42:19,840 --> 00:42:22,250
we keep dropout enabled at test time,

685
00:42:22,300 --> 00:42:27,620
and then we use the mean of these samples as the new prediction

686
00:42:27,640 --> 00:42:31,490
and the variance of these samples as a measure of epistemic uncertainty.

687
00:42:34,300 --> 00:42:38,570
So both of the methods we talked about just now involves sampling,

688
00:42:38,680 --> 00:42:40,080
and sampling is expensive,

689
00:42:40,160 --> 00:42:41,840
unsampling is very expensive,

690
00:42:42,130 --> 00:42:43,880
but even if you have a pretty large model,

691
00:42:45,400 --> 00:42:51,240
having or introducing dropout layers and calling twenty forward passes might also be something that's pretty infeasible,

692
00:42:51,740 --> 00:42:57,780
and at Themis, we're dedicated to developing innovative methods of estimating epistemic uncertainty,

693
00:42:58,160 --> 00:43:00,280
that don't rely on things like sampling,

694
00:43:00,280 --> 00:43:01,920
so that they're more generalizable

695
00:43:01,970 --> 00:43:04,680
and they're usable by more industries and people.

696
00:43:05,680 --> 00:43:08,370
So, a method that we've developed to estimate,

697
00:43:08,570 --> 00:43:11,700
a method that we've studied to estimate epistemic uncertainty

698
00:43:12,080 --> 00:43:13,890
is by using generative modeling.

699
00:43:14,450 --> 00:43:16,800
So we've talked about VAE couple times now,

700
00:43:17,090 --> 00:43:22,230
but let's say I trained VAE on the exact same data set, we were talking about earlier,

701
00:43:22,370 --> 00:43:23,910
which is only dogs and cats,

702
00:43:24,710 --> 00:43:29,610
the latent space of this model would be comprised of features that relate to dogs and cats,

703
00:43:29,870 --> 00:43:31,800
and if I give it a prototypical dog,

704
00:43:32,090 --> 00:43:35,460
it should be able to generate a pretty good representation of this dog,

705
00:43:35,750 --> 00:43:38,130
and it should have pretty low reconstruction loss.

706
00:43:40,580 --> 00:43:44,160
Now, if I gave the same example of the horse to this VAE,

707
00:43:45,110 --> 00:43:51,240
the latent vector that this horse would be decoded to would be incomprehensible to the decoder of this network,

708
00:43:51,620 --> 00:43:56,850
the decoder wouldn't be able to know how to project the latent vector back into the original input space,

709
00:43:57,200 --> 00:44:00,870
and therefore we should expect to see a much worse reconstruction here,

710
00:44:01,100 --> 00:44:04,200
and we should see that the reconstruction loss is much higher

711
00:44:04,220 --> 00:44:07,950
than if we gave the model a familiar input or something that it was used to seeing.

712
00:44:12,970 --> 00:44:14,655
So now let's move on to

713
00:44:14,655 --> 00:44:18,950
what I think is the most exciting method of estimating epistemic uncertainty,

714
00:44:18,970 --> 00:44:20,120
that we'll talk about today.

715
00:44:20,950 --> 00:44:22,790
So in both of the examples before,

716
00:44:23,080 --> 00:44:24,920
sampling is compute intensive,

717
00:44:25,180 --> 00:44:27,560
but generative modeling can also be compute intensive,

718
00:44:27,670 --> 00:44:31,040
let's say you don't actually need a variational autoencoder for your task,

719
00:44:31,420 --> 00:44:33,920
then you're training an entire decoder for no reason

720
00:44:34,180 --> 00:44:36,590
and other than to estimate the epistemic uncertainty,

721
00:44:37,540 --> 00:44:39,060
so what if we had a method,

722
00:44:39,060 --> 00:44:43,970
that did not rely on generative modeling or sampling in order to estimate the epistemic uncertainty,

723
00:44:44,730 --> 00:44:48,400
that's exactly what a method that we've developed here at Themis does.

724
00:44:49,140 --> 00:44:52,570
So we view learning as an evidence based process,

725
00:44:52,980 --> 00:44:56,540
so if you remember from earlier when we were training the ensemble

726
00:44:56,540 --> 00:44:59,350
and calling multiple ensembles on the same input,

727
00:44:59,760 --> 00:45:01,445
we received multiple predictions

728
00:45:01,445 --> 00:45:03,130
and we calculated that variance.

729
00:45:04,260 --> 00:45:07,190
Now, the way we frame evidential learning is

730
00:45:07,190 --> 00:45:10,180
what if we assume that those data points, those predictions,

731
00:45:10,290 --> 00:45:12,550
were actually drawn from a distribution themselves,

732
00:45:13,230 --> 00:45:17,170
if we could estimate the parameters of this higher order evidential distribution,

733
00:45:17,700 --> 00:45:22,150
we would be able to learn this variance or this measure of epistemic uncertainty automatically

734
00:45:22,560 --> 00:45:25,090
without doing any sampling or generative modeling,

735
00:45:25,230 --> 00:45:27,550
and that's exactly what evidential uncertainty does.

736
00:45:30,920 --> 00:45:36,570
So now that we have many methods in our toolbox for estimating epistemic uncertainty,

737
00:45:36,950 --> 00:45:39,000
let's go back to our real world example.

738
00:45:40,120 --> 00:45:42,870
Let's say again, the input is the same as before,

739
00:45:42,950 --> 00:45:45,780
it's an RGB image of some scene in a city,

740
00:45:46,310 --> 00:45:49,090
and the output again is a pixel level mask

741
00:45:49,090 --> 00:45:52,890
of what every pixel in this image belongs to, which class it belongs to,

742
00:45:53,700 --> 00:45:57,790
which parts of the data set would you expect to have high epistemic uncertainty,

743
00:45:58,500 --> 00:46:01,990
in this example, take a look at the output of the model itself,

744
00:46:02,340 --> 00:46:05,050
the model does mostly well on semantic segmentation,

745
00:46:05,550 --> 00:46:07,450
however it gets the sidewalk wrong,

746
00:46:08,070 --> 00:46:10,300
it assigns some of the sidewalk to the road,

747
00:46:10,500 --> 00:46:13,420
and other parts of the sidewalk are labeled incorrectly.

748
00:46:15,610 --> 00:46:18,140
And we can, using epistemic uncertainty,

749
00:46:18,460 --> 00:46:19,790
we can see why this is,

750
00:46:20,170 --> 00:46:24,680
the areas of the sidewalk that are discolored have high levels of epistemic uncertainty,

751
00:46:25,270 --> 00:46:30,110
maybe this is because the model has never seen an example of a sidewalk with multiple different colors in it before,

752
00:46:30,490 --> 00:46:33,680
or maybe it hasn't been trained on examples with sidewalks generally,

753
00:46:34,390 --> 00:46:35,090
either way,

754
00:46:35,920 --> 00:46:41,300
epistemic uncertainty has isolated this specific area of the image as an area of high uncertainty.

755
00:46:45,040 --> 00:46:49,850
So today, we've gone through two major challenges for robust deep learning,

756
00:46:50,170 --> 00:46:51,495
we've talked about bias,

757
00:46:51,495 --> 00:46:54,950
which is what happens when models are skewed by sensitive feature inputs

758
00:46:55,090 --> 00:46:59,690
and uncertainty, which is when we can measure a level of confidence of a certain model.

759
00:47:00,700 --> 00:47:03,525
Now we'll talk about how Themis uses these concepts

760
00:47:03,525 --> 00:47:05,360
to build products that transform models

761
00:47:05,440 --> 00:47:06,830
to make them more risk aware,

762
00:47:06,970 --> 00:47:11,090
and how we're changing the AI landscape in terms of safe and trustworthy AI.

763
00:47:14,230 --> 00:47:15,300
So at Themis,

764
00:47:15,300 --> 00:47:17,480
we believe that uncertainty and bias mitigation

765
00:47:17,500 --> 00:47:20,570
unlock a host of new solutions

766
00:47:20,650 --> 00:47:23,630
to solving these problems with safe and responsible AI,

767
00:47:24,490 --> 00:47:29,270
we can use bias and uncertainty to mitigate risk in every part of the AI life cycle.

768
00:47:29,770 --> 00:47:31,250
Let's start with labeling data,

769
00:47:31,570 --> 00:47:33,710
today, we talked about aleatoric uncertainty,

770
00:47:34,030 --> 00:47:36,440
which is a method to detect mislabeled samples,

771
00:47:36,550 --> 00:47:37,820
to highlight label noise,

772
00:47:37,900 --> 00:47:43,340
and to generally maybe tell labelers to relabel images or samples that they've gotten,

773
00:47:43,390 --> 00:47:44,330
that may be wrong.

774
00:47:45,340 --> 00:47:46,640
In the second part of this cycle,

775
00:47:46,780 --> 00:47:48,410
we have analyzing the data

776
00:47:48,700 --> 00:47:51,050
before a model is even trained on any data,

777
00:47:51,100 --> 00:47:54,110
we can analyze the bias that is present in this data set

778
00:47:54,250 --> 00:47:57,690
and tell the creators whether or not they should add more samples,

779
00:47:57,690 --> 00:48:01,970
which demographics, which areas of the data set are underrepresented in the current data set

780
00:48:02,110 --> 00:48:03,860
before we even train a model on them.

781
00:48:05,160 --> 00:48:07,120
And then let's go to training the model,

782
00:48:07,710 --> 00:48:09,050
once we're actually training a model,

783
00:48:09,050 --> 00:48:11,710
if it's already been trained on a biased data set,

784
00:48:11,820 --> 00:48:14,200
we can debias it adaptively during training

785
00:48:14,250 --> 00:48:16,030
using the methods that we talked about today.

786
00:48:17,300 --> 00:48:23,850
And afterwards, we can also verify or certify deployed machine learning models,

787
00:48:24,410 --> 00:48:29,490
making sure that models that are actually out there are as safe and unbiased as they claim they are,

788
00:48:29,660 --> 00:48:30,925
and the way we can do this is

789
00:48:30,925 --> 00:48:33,660
by leveraging epistemic uncertainty or bias

790
00:48:33,830 --> 00:48:37,770
in order to calculate the samples or data points that the model will do the worst on,

791
00:48:37,910 --> 00:48:42,040
the model has the the most trouble learning or data set samples

792
00:48:42,040 --> 00:48:44,790
that are the most underrepresented in a model data set,

793
00:48:45,210 --> 00:48:47,210
if we can test the model on these samples,

794
00:48:47,210 --> 00:48:49,180
specifically the hardest samples for the model,

795
00:48:49,470 --> 00:48:50,440
and it does well,

796
00:48:50,670 --> 00:48:54,700
then we know that the model has probably been trained in a fair and unbiased manner,

797
00:48:54,810 --> 00:48:55,990
that mitigates uncertainty.

798
00:48:57,310 --> 00:48:58,970
And lastly, we can think about,

799
00:48:59,050 --> 00:49:02,840
we, we are developing a product at Themis called AI guardian,

800
00:49:02,980 --> 00:49:07,310
and that's essentially a layer between the artificial intelligence algorithm and the user,

801
00:49:07,690 --> 00:49:09,080
and the way this works is,

802
00:49:09,130 --> 00:49:10,410
this is a type of algorithm,

803
00:49:10,410 --> 00:49:12,320
that if you're driving an autonomous vehicle,

804
00:49:12,670 --> 00:49:17,870
would say, hey, the model doesn't actually know what is happening in the world around it right now,

805
00:49:17,890 --> 00:49:20,630
as the user, you should take control of this autonomous vehicle

806
00:49:20,980 --> 00:49:24,170
and we can apply this to areas outside autonomy as well.

807
00:49:27,050 --> 00:49:30,300
So you'll notice that I skipped one part of the cycle,

808
00:49:30,410 --> 00:49:32,370
I skipped the part about building the model

809
00:49:32,480 --> 00:49:38,520
and that's because today we're going to focus a little bit on famous AI's product called CAPSA,

810
00:49:38,600 --> 00:49:41,670
which is a model agnostic framework for risk estimation,

811
00:49:42,380 --> 00:49:44,580
so CAPSA is an open source library,

812
00:49:44,750 --> 00:49:47,370
you all will actually use it in your lab today,

813
00:49:47,480 --> 00:49:48,805
that transforms models,

814
00:49:48,805 --> 00:49:50,100
so that they're risk aware.

815
00:49:50,750 --> 00:49:52,530
So this is a typical training pipeline,

816
00:49:52,550 --> 00:49:54,480
you've seen this many times in the course, by now,

817
00:49:54,740 --> 00:49:56,490
we have our data, we have the model,

818
00:49:56,600 --> 00:49:58,315
and it's fed into the training algorithm

819
00:49:58,315 --> 00:49:59,890
and we get a trained model at the end

820
00:49:59,890 --> 00:50:02,250
that outputs a prediction for every input.

821
00:50:04,320 --> 00:50:06,170
But with CAPSA, what we can do is

822
00:50:06,170 --> 00:50:09,130
by adding a single line into any training workflow,

823
00:50:09,480 --> 00:50:12,190
we can turn this model into a risk aware variant,

824
00:50:12,390 --> 00:50:16,390
that essentially calculates biases, uncertainty and label noise for you,

825
00:50:16,800 --> 00:50:19,940
because today as you've heard by now,

826
00:50:19,940 --> 00:50:23,200
there are so many methods of estimating uncertainty and bias,

827
00:50:23,370 --> 00:50:25,720
and sometimes certain methods are better than others,

828
00:50:25,980 --> 00:50:29,735
it's really hard to determine what kind of uncertainty you're trying to estimate

829
00:50:29,735 --> 00:50:30,820
and how to do so,

830
00:50:30,960 --> 00:50:32,920
so CAPSA takes care of this for you,

831
00:50:32,970 --> 00:50:35,410
by inserting one line into your training workflow,

832
00:50:35,460 --> 00:50:37,460
you can achieve a risk aware model,

833
00:50:37,460 --> 00:50:39,070
that you can then further analyze.

834
00:50:41,500 --> 00:50:44,210
And so this is the one line that I've been talking about,

835
00:50:44,320 --> 00:50:45,870
after you build your model,

836
00:50:45,870 --> 00:50:48,630
you can just create a wrapper or you can call a wrapper,

837
00:50:48,630 --> 00:50:51,230
that CAPSA has an extensive library of,

838
00:50:51,550 --> 00:50:55,880
and then, in addition to achieving prediction, receiving predictions from your model,

839
00:50:56,080 --> 00:51:00,080
you can also receive whatever bias or uncertainty metric that you're trying to estimate.

840
00:51:03,180 --> 00:51:04,955
And the way CAPSA works is,

841
00:51:04,955 --> 00:51:06,490
it does this by wrapping models,

842
00:51:07,110 --> 00:51:09,820
for every uncertainty metric that we want to estimate,

843
00:51:09,990 --> 00:51:13,720
we can apply and create the minimal model modifications as necessary,

844
00:51:13,950 --> 00:51:16,690
while preserving the initial architecture and predictive capabilities,

845
00:51:17,700 --> 00:51:19,330
in the case of aleatoric uncertainty,

846
00:51:19,470 --> 00:51:20,800
this could be adding a new layer,

847
00:51:21,090 --> 00:51:23,045
in the case of a variational autoencoder,

848
00:51:23,045 --> 00:51:25,265
this could be creating and training the decoder

849
00:51:25,265 --> 00:51:27,520
and calculating the reconstruction loss on the fly.

850
00:51:29,990 --> 00:51:32,220
And this is an example of CAPSA

851
00:51:32,540 --> 00:51:35,340
working on one of the data sets that we talked about today,

852
00:51:35,420 --> 00:51:38,010
which was the cubic data set with added noise in it,

853
00:51:38,120 --> 00:51:40,350
and also another simple classification task,

854
00:51:40,580 --> 00:51:42,910
and the reason why I wanted to show this image

855
00:51:42,910 --> 00:51:44,545
is to show that using CAPSA,

856
00:51:44,545 --> 00:51:47,040
we can achieve all of these uncertainty estimates

857
00:51:47,120 --> 00:51:49,320
with very little additional added work.

858
00:51:53,840 --> 00:51:57,870
So using all of the products that I just talked about today and using CAPSA,

859
00:51:58,040 --> 00:52:02,250
Themis is unlocking the key to deploy deep learning models safely across fields,

860
00:52:02,930 --> 00:52:06,330
we can now answer a lot of the questions that the headlines were raising earlier,

861
00:52:06,590 --> 00:52:09,990
which is when should a human take control of an autonomous vehicle,

862
00:52:10,550 --> 00:52:14,340
what types of data are underrepresented in commercial autonomous driving pipelines,

863
00:52:14,720 --> 00:52:17,250
we now have educated answers to these questions

864
00:52:17,570 --> 00:52:19,350
due to products that Themis is developing.

865
00:52:21,440 --> 00:52:23,820
And in spheres such as medicine and health care,

866
00:52:23,930 --> 00:52:25,680
we can now answer questions such as,

867
00:52:25,820 --> 00:52:28,740
when is a model uncertain about a life threatening diagnosis,

868
00:52:29,060 --> 00:52:31,650
When should this diagnosis be passed to a medical professional

869
00:52:31,970 --> 00:52:34,050
before this information is conveyed to a patient,

870
00:52:34,790 --> 00:52:38,850
or what types of patients might drug discovery algorithms be biased against.

871
00:52:39,980 --> 00:52:44,590
And today, the, the application that you guys will focus on is on facial detection,

872
00:52:44,910 --> 00:52:46,690
you'll use CAPSA in today's lab

873
00:52:46,950 --> 00:52:51,520
to thoroughly analyze a common facial detection data set,

874
00:52:51,570 --> 00:52:53,480
that we've perturbed in some ways for you,

875
00:52:53,480 --> 00:52:55,030
so that you can discover them on your own,

876
00:52:55,470 --> 00:52:58,690
and we we highly encourage you to compete in the competition,

877
00:52:59,160 --> 00:53:01,210
which the details are described in the lab,

878
00:53:01,350 --> 00:53:03,905
but basically it's about analyzing this data set,

879
00:53:03,905 --> 00:53:05,500
creating risk aware models,

880
00:53:05,850 --> 00:53:09,310
that mitigate bias and uncertainty in the specific training pipeline.

881
00:53:11,410 --> 00:53:13,170
And so at Themis our goal is,

882
00:53:13,170 --> 00:53:17,600
to design, advance, and deploy trustworthy AI across industries and around the world,

883
00:53:18,100 --> 00:53:20,840
we're passionate about scientific innovation,

884
00:53:21,160 --> 00:53:23,840
we release open source tools like the ones you'll use today,

885
00:53:24,250 --> 00:53:29,000
and our products transform AI workflows and make artificial intelligence safer for everyone,

886
00:53:29,590 --> 00:53:31,640
we partner with industries around the globe

887
00:53:31,930 --> 00:53:35,090
and we're hiring for the upcoming summer and for full time roles,

888
00:53:35,380 --> 00:53:36,390
so if you're interested,

889
00:53:36,390 --> 00:53:39,260
please send an email to careers@themisai.io

890
00:53:39,490 --> 00:53:43,050
or apply by submitting your resume to the deep learning resume drop

891
00:53:43,050 --> 00:53:45,050
and we'll see those resumes and get back to you.

892
00:53:45,070 --> 00:53:45,680
Thank you.

